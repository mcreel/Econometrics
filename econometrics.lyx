#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extreport
\begin_preamble
\usepackage{lmodern}
\usepackage{dcolumn}
\usepackage{listings}
\usepackage{color}
\usepackage{longtable}
%
\definecolor{hellgelb}{rgb}{1,1,0.8}
\definecolor{colKeys}{rgb}{0,0,1}
\definecolor{colIdentifier}{rgb}{0,0,0}
\definecolor{colComments}{rgb}{1,0,0}
\definecolor{colString}{rgb}{0,0.5,0}
\let\endtitlepage\relax

\usepackage{newunicodechar}
% for OLS output of Nerlove example (nerlove.out)
\newunicodechar{σ}{\ensuremath{\sigma}}
\end_preamble
\options authordate
\use_default_options false
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\maintain_unincluded_children no
\language english
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize 20
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle true
\pdf_quoted_options "colorlinks=true,allcolors=blue"
\papersize a4
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation landscape
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\boxbgcolor #fefff3
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 2cm
\rightmargin 3cm
\bottommargin 2cm
\secnumdepth 1
\tocdepth 1
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle empty
\tablestyle default
\listings_params "morekeywords={AND,ASC,avg,CHECK,COMMIT,count,DECODE,DESC,DISTINCT,GROUP,IN,LIKE,NUMBER,ROLLBACK,SUBSTR,sum,VARCHAR2},float=hbp,basicstyle={\ttfamily\small},identifierstyle={\color{colIdentifier}},keywordstyle={\color{colKeys}},stringstyle={\color{colString}},commentstyle={\color{colComments}},columns=flexible,tabsize=2,frame=single,extendedchars=true,showspaces=false,showstringspaces=false,numbers=left,numberstyle={\tiny},breaklines=true,backgroundcolor={\color{hellgelb}},breakautoindent=true,captionpos=b"
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Title

\size larger
Econometrics
\end_layout

\begin_layout Author
Michael Creel
\end_layout

\begin_layout Date

\size small
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename logoUAB.jpg
	width 8cm

\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Chapter
About this document
\end_layout

\begin_layout Section
Prerequisites
\end_layout

\begin_layout Standard
These notes have been prepared under the assumption that the reader understands basic statistics,
 linear algebra,
 and mathematical optimization.
 There are many sources for this material,
 for example,
 the appendices to 
\emph on
Introductory Econometrics:
 A Modern Approach
\emph default
 by Jeffrey Wooldridge.
 It is the student's responsibility to get up to speed on this material,
 it will not be covered in class.
\end_layout

\begin_layout Standard
This document integrates lecture notes for a one year graduate level course with computer programs that illustrate and apply the methods that are studied.
 The immediate availability of executable (and modifiable) example programs when using the PDF version of the document is a distinguishing feature of these notes.
 If printed,
 the document is a somewhat terse approximation to a textbook.
 These notes are not intended to be a perfect substitute for a printed textbook.
 If you are a student of mine,
 please note that last sentence carefully.
 There are many good textbooks available.
 Students taking my courses should read the appropriate sections from at least one of the following books (or other textbooks with similar level and content)
\end_layout

\begin_layout Itemize
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 
\emph on
Microeconometrics - Methods and Applications.
 
\emph default
This is the book I recommend to use,
 if you don't have some reason to choose a different one.
\end_layout

\begin_layout Itemize
Davidson,
 R.
 and J.G.
 MacKinnon,
 
\emph on
Econometric Theory and Methods
\end_layout

\begin_layout Itemize
Gallant,
 A.R.,
 
\emph on
An Introduction to Econometric Theory
\end_layout

\begin_layout Itemize
Hamilton,
 J.D.,
 
\emph on
Time Series Analysis
\end_layout

\begin_layout Standard
Some more advanced books:
\end_layout

\begin_layout Itemize
Davidson,
 R.
 and J.G.
 MacKinnon (1993) 
\emph on
Estimation and Inference in Econometrics
\emph default
,
 Oxford Univ.
 Press.
 
\end_layout

\begin_layout Itemize
Gallant,
 
\emph on
Nonlinear Statistical Models
\emph default
.
 
\end_layout

\begin_layout Standard
Undergraduate level texts,
 if you need to catch up with some concepts 
\end_layout

\begin_layout Itemize
Wooldridge (2003),
 
\emph on
Introductory Econometrics:
 A Modern Approach
\emph default
 (undergraduate level,
 for supplementary use only.
 Be sure to see the appendices,
 which give good coverage of foundations).
\end_layout

\begin_layout Itemize
Stock and Watson,
 
\emph on

\begin_inset CommandInset href
LatexCommand href
name "Introduction to Econometrics"
target "https://ebookcentral.proquest.com/lib/uab/detail.action?docID=5174962#"
literal "false"

\end_inset

.
 
\emph default
This is the book used at the UAB for undergraduate courses in econometrics.
\end_layout

\begin_layout Section
Contents
\end_layout

\begin_layout Standard
With respect to contents,
 the emphasis is on estimation and inference within the world of stationary data.
 The notes have been used to teach first year masters and pre-doctoral students,
 in two 30-40 hour courses.
 The first part covers linear regression,
 and the second part goes on to cover ML and GMM estimation of potentially nonlinear models.
 There are some topical chapters after this core material that give introductions to more specialized methods.
 Student with interest in quantitative methods go on to study this material more deeply in elective courses,
 which is why the presentation here of the later chapters is more broad than deep.
\end_layout

\begin_layout Standard
The integrated examples and the support files (available online at the 
\begin_inset CommandInset href
LatexCommand href
name "github repository"
target "https://github.com/mcreel/Econometrics"
literal "false"

\end_inset

) are an important part of these notes.
 Julia 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://julialang.org}{(julialang.org)}
\end_layout

\end_inset

 has been used for most of the example programs,
 which are scattered though the document.
 The examples and code use the current stable version of Julia,
 version 1.x.
 This choice is motivated by several factors.
 Julia runs on all of the popular operating systems,
 it is free,
 and it is fast,
 thanks to just-in-time compilation.
 It is a relatively new language,
 but is stable,
 with performance improving with each point release.
 The fundamental tools (manipulation of matrices,
 statistical functions,
 minimization,
 
\emph on
etc.
\emph default
) exist and are implemented in a way that make extending them fairly easy,
 plus new packages for more advanced applications are appearing constantly.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Julia"
nolink "false"

\end_inset

 shows Julia running one of the examples from this document.
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Julia"

\end_inset

Julia
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename example.png
	lyxscale 25
	width 10in

\end_inset


\end_layout

\end_inset

 There are also some examples which use 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{http://gretl.sourceforge.net}{Gretl}
\end_layout

\end_inset

,
 the Gnu Regression,
 Econometrics,
 and Time-Series Library.
 This is an easy to use program,
 available in a number of languages,
 and it comes with a lot of data ready to use.
 It runs on the major operating systems.
 Sometimes,
 simple is better.
 
\end_layout

\begin_layout Standard
The main document was prepared using \SpecialChar LyX
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{http://www.lyx.org}{(www.lyx.org)}
\end_layout

\end_inset

.
 \SpecialChar LyX
 is a free
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Quotes sld
\end_inset

Free
\begin_inset Quotes srd
\end_inset

 is used in the sense of 
\begin_inset Quotes sld
\end_inset

freedom
\begin_inset Quotes srd
\end_inset

,
 but \SpecialChar LyX
 is also free of charge (free as in 
\begin_inset Quotes sld
\end_inset

free beer
\begin_inset Quotes srd
\end_inset

).
\end_layout

\end_inset

 
\begin_inset Quotes eld
\end_inset

what you see is what you mean
\begin_inset Quotes erd
\end_inset

 word processor,
 basically working as a graphical frontend to \SpecialChar LaTeX
.
 It (with help from other applications) can export your work in \SpecialChar LaTeX
,
 HTML,
 PDF and several other forms.
 It will run on Linux,
 Windows,
 and MacOS systems.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Picture of LyX"
nolink "false"

\end_inset

 shows \SpecialChar LyX
 editing this document.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Picture of LyX"

\end_inset

\SpecialChar LyX

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/lyx.png
	width 6in

\end_inset


\end_layout

\end_inset

.
 The \SpecialChar LyX
 source for the document is available on the web page.
\end_layout

\begin_layout Section
License
\end_layout

\begin_layout Standard
All materials are copyrighted by Michael Creel with the date that appears above,
 under the MIT license.
 See the file License.md
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Obtaining-the-materials"

\end_inset

Obtaining and using the materials
\end_layout

\begin_layout Standard

\color black
A video which explains the steps of this section
\color inherit
 
\begin_inset CommandInset href
LatexCommand href
name "is here"
target "https://www.youtube.com/watch?v=N_aWT7OiX4k"
literal "false"

\end_inset

.
 I 
\color red
recommend
\color inherit
 that you look at the video one time before installing.
\end_layout

\begin_layout Standard
The materials are available from a 
\begin_inset CommandInset href
LatexCommand href
name "github repository"
target "https://github.com/mcreel/Econometrics"
literal "false"

\end_inset

.
 To run the examples embedded in the document,
 you need to 
\end_layout

\begin_layout Itemize
install the 
\begin_inset CommandInset href
LatexCommand href
name "Julia language"
target "https://julialang.org/"
literal "false"

\end_inset

.
 See the download link on that page.
\end_layout

\begin_layout Itemize
and add files of the 
\begin_inset CommandInset href
LatexCommand href
name "github repository"
target "https://github.com/mcreel/Econometrics"
literal "false"

\end_inset

 as a Julia package.
 
\end_layout

\begin_layout Enumerate
download the code:
\end_layout

\begin_deeper
\begin_layout Enumerate
download a zip of the repo,
 and uncompress it in a convenient directory,
 or
\end_layout

\begin_layout Enumerate
git clone the repository to the desired location
\end_layout

\end_deeper
\begin_layout Enumerate
Go to that directory and start Julia 
\family typewriter
using 
\color magenta
julia --proj
\family default
\color inherit
 
\end_layout

\begin_layout Enumerate
In Julia,
 the first time you use the files,
 do 
\family typewriter
\color magenta
using Pkg;
 Pkg.instantiate()
\family default
\color inherit
 This will take some time,
 as Econometrics relies on a number of other packages.
\end_layout

\begin_layout Enumerate
then do 
\family typewriter
\color magenta
using Econometrics
\family default
\color inherit
 in Julia to use the package.
 The first time you do this,
 it will take a 
\series bold
long 
\series default
time,
 maybe 15 minutes or so.
 
\emph on
\color magenta
Don't worry
\emph default
\color inherit
,
 this is normal.
 All of the packages that were downloaded are being compiled for the first time.
 We will be able to make this go
\emph on
 
\color black
much,
 much faster
\emph default
\color inherit
 when we want to use the code,
 by following the steps explained in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Using-a-system"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

,
 below.
\end_layout

\begin_layout Enumerate
To run examples,
 cd into the relevant subdirectory of Econometrics/Examples,
 and then just include the script you would like to run.
\end_layout

\begin_layout Enumerate
Once this is done,
 you can use the code at any time by repeating steps 2 and 4.
\end_layout

\begin_layout Enumerate
I recommend setting your operating system to open .jl files with your favorite editor.
 
\end_layout

\begin_layout Standard
Please see the web page for links to videos that explain the installation and usage process,
 and for how to speed things up.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Introduction to Julia
\end_layout

\begin_layout Standard
This document uses the 
\begin_inset CommandInset href
LatexCommand href
name " Julia programming language"
target "https://julialang.org/"
literal "false"

\end_inset

 for most of the examples.
 This chapter gives a very bare bones introduction to Julia.
 There are much better introductory materials from other sources,
 some of which are noted below.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Why Julia?
\end_layout

\begin_layout Itemize
free:
 free in terms of $$$,
 and also,
 source code is free,
 so you can know exactly what it does,
 and you can modify it and contribute to it
\end_layout

\begin_layout Itemize
multi-platform:
 runs on all the popular operating systems.
 For teaching econometrics,
 this is nice,
 because all students have equal access to the materials.
\end_layout

\begin_layout Itemize
fast:
 speed of well-written code is close to C or Fortan.
 Code is relatively easy to write and to read,
 similar to Python,
 Matlab or other matrix scripting languages
\end_layout

\begin_layout Itemize
reproducible:
 it is not difficult to control software versions exactly,
 so that your results can be reproduced exactly.
\end_layout

\begin_layout Itemize
the above 4 considerations are essentially necessary for a language for modern science,
 which requires accessibility,
 verifiability,
 and performance
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Why not Julia?
\end_layout

\begin_layout Itemize
Julia code is compiled before it's run.
 This means that first calls to functions take a bit of time,
 as while are compiled.
 The second call will be much faster.
 So,
 interactive use may frustrate a bit,
 at least until you learn to work around this particularity.
\end_layout

\begin_deeper
\begin_layout Itemize
one can use a system image to solve this problem.
 See 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://www.youtube.com/watch?v=a-_ZNTBeLCw
\end_layout

\end_inset

 for a video that shows how to do this for the code that accompanies these notes.
\end_layout

\begin_layout Itemize
this is getting better all the time,
 as support for pre-compilation improves
\end_layout

\begin_layout Itemize
can be dealt with quite easily by warming up functions with toy usages,
 which you might include in your startup file.
\end_layout

\begin_layout Itemize
Also,
 keep your Julia session running,
 and the things you use often will already be compiled from previous uses.
 With Linux,
 you can have Julia running in a 
\family typewriter
byobu
\family default
 or 
\family typewriter
screen
\family default
 session,
 which you can re-connect to whenever you need it,
 which is amazingly convenient.
 
\end_layout

\end_deeper
\begin_layout Itemize
the speed is only needed if your work is computationally demanding.
 Dividing epsilon by 2 is not very important when epsilon is small.
 For getting fast results for linear models,
 you may prefer a more complete and specialized package,
 e.g.,
 Stata,
 etc.
\end_layout

\begin_layout Itemize
there is a lot of existing code for languages like Matlab,
 Python,
 Fortran,
 etc.
 You may prefer to use that code,
 depending on your research.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Resources
\end_layout

\begin_layout Itemize
Julia language:
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://julialang.org/
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
tutorials and resources:
\end_layout

\begin_deeper
\begin_layout Itemize
Recommended!
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/PaulSoderlind/JuliaTutorial
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Recommended!
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://julia.quantecon.org/intro.html
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Installation of Julia and packages
\end_layout

\begin_layout Itemize
install Julia stable version from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://julialang.org/downloads/
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Getting started:
 
\begin_inset CommandInset href
LatexCommand href
target "https://docs.julialang.org/en/v1/manual/getting-started/"

\end_inset


\end_layout

\begin_layout Itemize
Package manager documentation:
 
\begin_inset CommandInset href
LatexCommand href
target "https://docs.julialang.org/en/v1/stdlib/Pkg/"

\end_inset


\end_layout

\begin_layout Itemize
The first thing you will need to do to make full use of this document is to install the examples and the support code.
 The main commands for packages:
\end_layout

\begin_deeper
\begin_layout Itemize
from the REPL,
 press ] to enter package mode.
\end_layout

\begin_layout Itemize

\family typewriter
]?

\family default
 :
 help for package mode.
\end_layout

\begin_layout Itemize

\family typewriter
] add
\family default
 :
 Add a package.
 e.g.,
 to add a popular plotting package,
 do
\family typewriter
 
\color blue
] add Plots
\end_layout

\begin_layout Itemize
Recommended packages (amongst many others):
\end_layout

\begin_deeper
\begin_layout Itemize
Plots:
 well-established plotting package.
 Makie.jl is a newer package,
 also widely used.
\end_layout

\begin_layout Itemize
CSV:
 this is a more powerful package for working with CSV data.
 The DelimitedFiles module of basic Julia can also work in many cases.
\end_layout

\begin_layout Itemize
DataFrames:
 organization of data with names and manipulations
\end_layout

\begin_layout Itemize
Distributions:
 essential for statistics beyond the basics.
 
\end_layout

\begin_layout Itemize
Optim:
 a well-established optimization package.
 NLopt.jl is a little less easy to use,
 but is well-tested.
 Optimization.jl is a newer package that provides a common interface to many optimization packages,
 including Optim.jl and NLopt.jl.
\end_layout

\begin_layout Itemize
Revise:
 make development of code easier
\end_layout

\begin_layout Itemize
OhMyREPL:
 colors and other features for the Julia prompt.
\end_layout

\end_deeper
\begin_layout Itemize
Recommended:
 put 
\family typewriter
using Revise;
 using OhMyREPL
\family default
 in your 
\family typewriter
~/.julia/config/startup.jl
\family default
 file so that they are automatically used when you start Julia.
 If you installed the Econometric package (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Obtaining-the-materials"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

),
 you can add 
\family typewriter
using Econometrics
\family default
,
 too.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Using-a-system"

\end_inset

Using a system image
\end_layout

\begin_layout Standard
A system image is a file that contains precompiled images of functions that one might want to access as quickly as possible.
 The 
\begin_inset Quotes sld
\end_inset

time to first plot
\begin_inset Quotes srd
\end_inset

 (or first X) issue is the most common complaint about Julia of new users.
 Using a precompiled system image solves this problem.
 In the Econometrics code,
 the file MakeSysImage.jl
\begin_inset CommandInset href
LatexCommand href
name "MakeSysImage.jl"
target "https://github.com/mcreel/Econometrics/blob/main/MakeSysimage.jl"
literal "false"

\end_inset

 will create a system image that contains the Econometrics functions,
 and the packages upon which they depend.
 Please see 
\begin_inset CommandInset href
LatexCommand href
name "this video"
target "https://www.youtube.com/watch?v=a-_ZNTBeLCw"
literal "false"

\end_inset

 for instructions on how to make and use an image.
 Remember that you will need to create a new system image if you update the packages.
 See 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://www.youtube.com/watch?v=a-_ZNTBeLCw
\end_layout

\end_inset

 for a video that shows how to do this for the code that accompanies these notes.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Running Julia and the work flow
\end_layout

\begin_layout Standard
There are several ways to use Julia,
 here is a basic description of some of them:
\end_layout

\begin_layout Subsection
REPL and text editor
\end_layout

\begin_layout Standard
The REPL (
\begin_inset Quotes sld
\end_inset

read-eval-print loop
\begin_inset Quotes srd
\end_inset

 ),
 or in more plain parlance,
 the Julia command prompt,
 is my main way of working for research.
 Simple and easy to replicate.
 On Linux,
 just open a terminal and type 
\begin_inset Quotes sld
\end_inset

julia
\begin_inset Quotes srd
\end_inset

.
 You can run your code in one window,
 and edit it in another,
 using your favorite text editor.
 There are syntax highlighting schemes for many of the popular editors.
 This is what I use for research.
\end_layout

\begin_layout Subsection
Julia for VSCode
\end_layout

\begin_layout Standard
If you want something more modern and integrated looking than the REPL and a text editor,
 check out 
\begin_inset CommandInset href
LatexCommand href
name "Julia for VSCode "
target "https://www.julia-vscode.org/"
literal "false"

\end_inset

.
 This combines the editing,
 command,
 and plot windows all on one interface,
 and may be more what you're used to if coming from Matlab or RStudio,
 for example.
 This is what I use when teaching.
 A video showing how to use VSCode with the Econometrics package
\begin_inset CommandInset href
LatexCommand href
name " is here"
target "https://www.youtube.com/watch?v=Nbhmq4VWVJU"
literal "false"

\end_inset

.
 I recommend that my students have a look at the video,
 to better follow what I'm doing in class.
\end_layout

\begin_layout Subsection
Notebook interfaces
\end_layout

\begin_layout Standard
This is a nice way to interactively explore relatively simple code.
 
\end_layout

\begin_layout Itemize
IJulia and Jupyter notebooks:
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/JuliaLang/IJulia.jl
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Pluto notebooks:
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/fonsp/Pluto.jl
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Neptune notebooks.
 A fork of Pluto that does not automatically update all cells when any changes.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Loading/saving data
\end_layout

\begin_layout Standard
The examples that follow in later chapters provide some examples.
 Relevant commands are:
\end_layout

\begin_layout Itemize
using DelimitedFiles;
 ?readdlm;
 ?writedlm
\end_layout

\begin_layout Itemize
using CSV;
\end_layout

\begin_deeper
\begin_layout Itemize
?CSV.read for reading CSV files into with variable names in the first row into a dataframe
\end_layout

\begin_layout Itemize
?CSV.write
\end_layout

\end_deeper
\begin_layout Itemize
For more information,
 see 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/PaulSoderlind/JuliaTutorial/blob/master/Tutorial_09_LoadSaveData.ipynb"
target "https://github.com/PaulSoderlind/JuliaTutorial/blob/master/Tutorial_09_LoadSaveData.ipynb"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Example data sets to practice on:
\end_layout

\begin_layout Standard
CSV with names:
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/mcreel/Econometrics/blob/master/Examples/Data/card.csv
\end_layout

\end_inset

,
 and
\end_layout

\begin_layout Standard
plain text,
 space delimited:
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/mcreel/Econometrics/blob/master/Examples/Data/nerlove.data
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Exploratory-analysis-and"

\end_inset

Exploratory analysis and plotting
\end_layout

\begin_layout Standard
Here are a couple of examples of data preparation and basic analysis.
\end_layout

\begin_layout Itemize
Using DataFrames and StatPlots for exploratory analysis using the Card returns to education data set:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Julia/BasicDataAnalysis.jl}{BasicDataAnalysis.jl} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Data preparation and exploration using Oxford-Man realized library data on financial time series:
 
\begin_inset CommandInset href
LatexCommand href
name "Oxford-Man realized library data"
target "https://realized.oxford-man.ox.ac.uk/images/oxfordmanrealizedvolatilityindices.zip"
literal "false"

\end_inset

:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Julia/SP500.jl}{SP500.jl} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Introduction:
 Economic and econometric models
\end_layout

\begin_layout Standard
Here's some 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Intro/data.txt}{data}
\end_layout

\end_inset

:
 observations on 3 economic variables.
 
\end_layout

\begin_layout Standard

\series bold
\emph on
Draw a data block.
\end_layout

\begin_layout Standard
Let's do some exploratory analysis using Gretl:
\end_layout

\begin_layout Itemize
histograms
\end_layout

\begin_layout Itemize
correlations
\end_layout

\begin_layout Itemize
x-y scatterplots
\end_layout

\begin_layout Standard
So,
 what can we say?
 Correlations?
 Yes.
 Causality?
 Who knows?
\end_layout

\begin_layout Itemize

\emph on
What are these variables
\emph default
?
 So far,
 we don't know,
 so we have no mental model to sort out which variables might be causing others.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
We are missing a theoretical model!
 
\end_layout

\begin_layout Itemize
A theoretical model is a key ingredient to assign causal relationships (which we might subsequently try to test).
 Without a model (or the ability to do experiments) we can't distinguish correlation from causality.
\end_layout

\begin_layout Itemize
It turns out that the variables we're looking at are QUANTITY (q),
 PRICE (p),
 and INCOME (m),
 and the data were generated using 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Intro/SupplyDemand.jl}{this script}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Economic theory tells us that the quantity of a good that consumers will purchase (the demand function) is something like:
 
\begin_inset Formula 
\[
q=d(p,m,z)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $q$
\end_inset

 is the quantity demanded
\end_layout

\begin_layout Itemize
\begin_inset Formula $p$
\end_inset

 is the price of the good
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

 is income
\end_layout

\begin_layout Itemize
\begin_inset Formula $z$
\end_inset

 is a vector of other variables that may affect demand
\end_layout

\begin_layout Standard
The supply of the good to the market is the aggregation of the firms' supply functions.
 The market supply function is something like
\begin_inset Formula 
\[
q=s(p,z)
\]

\end_inset

Suppose we have a sample consisting of a number of observations on 
\begin_inset Formula $q$
\end_inset

 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $m$
\end_inset

 at different time periods 
\begin_inset Formula $t=1,2,...,n$
\end_inset

.
 Supply and demand in each period is
\begin_inset Formula 
\begin{align*}
q_{t} & =d(p_{t},m_{t},z_{t})\\
q_{t} & =s(p_{t},z_{t})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Draw a theory block.

\emph default
 (draw some graphs showing roles of 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
This is the basic economic model of supply and demand:
 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 are determined in the market equilibrium,
 given by the intersection of the two curves.
 
\end_layout

\begin_layout Itemize
These two variables are determined jointly by the model,
 and are the 
\emph on
endogenous variables
\emph default
.
 Income (
\begin_inset Formula $m$
\end_inset

) is not determined by this model,
 its value is determined independently of 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 by some other process.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

 is an 
\emph on
exogenous variable
\emph default
.
 So,
 
\begin_inset Formula $m$
\end_inset

 causes 
\begin_inset Formula $q$
\end_inset

,
 though the demand function.
 Because 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 are jointly determined,
 
\begin_inset Formula $m$
\end_inset

 also causes 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 do not cause 
\begin_inset Formula $m$
\end_inset

,
 according to this theoretical model.
 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 have a joint causal relationship.
\end_layout

\begin_layout Itemize
Economic theory can help us to determine the causality relationships between correlated variables.
 According to theory,
 income does not affect the supply equation,
 so when income changes,
 supply stays the same.
 You can see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Price-and-Quantity,"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 that when income increases,
 the upward movement of demand is tracing out the slope of the supply equation.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Price-and-Quantity,"

\end_inset

Price and Quantity,
 colored by income (blue is low,
 violet is high)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Intro/PriceQuantity.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The model is essentially a theoretical construct up to now:
\end_layout

\begin_layout Itemize
We don't know the forms of the functions 
\begin_inset Formula $s$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 (unless we studied the script that generated the data)
\end_layout

\begin_layout Itemize
Some components of 
\begin_inset Formula $z_{t}$
\end_inset

 may not be observable.
 For example,
 people don't eat the same lunch every day,
 and you can't tell what they will order just by looking at them.
 There are unobservable components to supply and demand,
 and we can model them as random variables.
 Suppose we can break 
\begin_inset Formula $z_{t}$
\end_inset

 into two unobservable components 
\begin_inset Formula $\varepsilon_{t1}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Theory can make some predictions,
 too.
 For example,
 theory tells us that demand functions are homogeneous of degree zero in prices and income.
 Also,
 the compensated demand functions have a negative slope with respect to price.
 But theory gives us 
\emph on
qualitative information,
 
\emph default
signs of effects and so forth,
 but not the actual values in a given economy,
 not the magnitudes.
 So,
 theory by itself has some limitations,
 just as data by itself has limitations.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An econometric model attempts to 
\series bold
quantify
\series default
 the relationship more precisely.
 A step toward an estimable econometric model is to make a series of assumptions.
 Suppose that the model may be written as
\begin_inset Formula 
\begin{align*}
q_{t} & =\alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}m_{t}+\varepsilon_{t1}\\
q_{t} & =\beta_{1}+\beta_{2}p_{t}+\varepsilon_{t2}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
The functions 
\begin_inset Formula $s$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 have been specified to be linear functions
\end_layout

\begin_layout Enumerate
The parameters (
\begin_inset Formula $\alpha_{1},$
\end_inset

 
\begin_inset Formula $\beta_{2},$
\end_inset

 etc.) are constant over time.
\end_layout

\begin_layout Enumerate
There is a single unobservable component in each equation,
 and it is additive.
\begin_inset VSpace defskip
\end_inset


\begin_inset Newline newline
\end_inset


\series bold
Examples of additional assumptions
\series default
 (perhaps not reasonable):
\end_layout

\begin_layout Enumerate
\begin_inset Formula $E(\epsilon_{tj})=0,\,j=1,2$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $E(\epsilon_{tj}|m_{t})=0,\,j=1,2$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $E(\epsilon_{t2}|p_{t})=0$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Draw an assumptions block.

\series default
\emph default
 
\end_layout

\begin_layout Itemize
Assumptions 1-3 by themselves really don't impose any restrictions.
 We can always write the two equations that precede them,
 as the errors simply make up the difference between the true demand and supply functions and the assumed forms.
 
\end_layout

\begin_layout Itemize
In order for the 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 coefficients in the two equations to exist in a sense that has 
\emph on
economic meaning
\emph default
,
 and in order to be able to use sample data to make reliable inferences about their values,
 we need to make assumptions like those in 4-6.

\series bold
 
\series default
These are assertions that the errors have mean zero and are uncorrelated with income,
 and that the second error is uncorrelated with price.
\end_layout

\begin_layout Itemize
These assumptions,
 in combination with assns.
 1-3,
 
\bar under
do
\bar default
 impose restrictions on the data generating process that go beyond what pure economic theory states.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
We can now use econometric methods to learn about the parameters.
 
\series bold
Draw an econometric model block.
 
\end_layout

\begin_layout Itemize
with assumptions 1-4 and 6,
 we could estimate the supply equation by OLS (do it with GRETL:
 and compare estimate with true parameter value)
\end_layout

\begin_layout Itemize
with assumptions 1-5,
 we could estimate the supply equation by instrumental variables (do it).
\end_layout

\begin_layout Itemize
All of the above assumptions 1-6 above have 
\series bold
no theoretical basis
\series default
,
 in that the theory of supply and demand doesn't imply these conditions.
 
\end_layout

\begin_layout Itemize
The validity of any econometric results we obtain using an econometric model will be contingent on these additional restrictions being at least approximately correct.
 If you study the script that generated the data,
 you will see that assumptions 1-5 are in fact valid for the data set,
 but assumption 6 is not.
 This data is an example of a 
\emph on
simultaneous equations 
\emph default
system
\emph on
,
 
\emph default
and we will see more about that in Chapter 11.
\end_layout

\begin_layout Itemize
Because validity of results depends on validity of assumptions,
 
\emph on
specification testing
\emph default
 will be needed,
 to check that the model seems to be reasonable.
 
\end_layout

\begin_layout Itemize
Only when we are convinced that the model is at least approximately correct should we use it for economic analysis,
 or for possibly rejecting theories
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Given that we know the variable names of the above data,
 estimate the supply equation by two stage least squares,
 if you know how to.
 Compare the coefficient estimates with the values that generated the data.
 Note that the estimates are not bad,
 and get very close to the true values if you increase the sample size.
 This is because the model is correctly specified when we use assumptions 1-5,
 and the 2SLS estimator is consistent in this case.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Ordinary Least Squares
\end_layout

\begin_layout Section
The Linear Model
\end_layout

\begin_layout Standard
Consider approximating a variable 
\begin_inset Formula $y$
\end_inset

 using the variables 
\begin_inset Formula $x_{1},x_{2},...,x_{k}$
\end_inset

.
 We can consider a model that is a linear approximation:
\end_layout

\begin_layout Standard

\series bold
Linearity
\series default
:
 the model is a linear function of the parameter vector 
\begin_inset Formula $\beta_{0}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
y & = & \beta_{1}^{0}x_{1}+\beta_{2}^{0}x_{2}+...+\beta_{k}^{0}x_{k}+\epsilon
\end{eqnarray*}

\end_inset

or,
 using vector notation:
 
\begin_inset Formula 
\[
y=\mathbf{x}^{\prime}\beta_{0}+\epsilon
\]

\end_inset

 The dependent variable 
\begin_inset Formula $y$
\end_inset

 is a scalar random variable,
 
\begin_inset Formula $\mathbf{x}=(\begin{array}{cccc}
x_{1} & x_{2} & \cdots & x_{k})^{'}\end{array}$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-vector of explanatory variables,
 and 
\begin_inset Formula $\beta_{0}=(\begin{array}{cccc}
\beta_{1}^{0} & \beta_{2}^{0} & \cdots & \beta_{k}^{0})^{'}\end{array}.$
\end_inset

 The superscript 
\begin_inset Quotes eld
\end_inset

0
\begin_inset Quotes erd
\end_inset

 in 
\begin_inset Formula $\beta_{0}$
\end_inset

 means this is the 
\begin_inset Quotes sld
\end_inset

true value
\begin_inset Quotes srd
\end_inset

 of the unknown parameter.
 It will be defined more precisely later,
 and usually suppressed when it's not necessary for clarity.
 
\end_layout

\begin_layout Standard
Suppose that we want to use data to try to determine the best linear approximation to 
\begin_inset Formula $y$
\end_inset

 using the variables 
\begin_inset Formula $\mathbf{x}.$
\end_inset

 The data 
\begin_inset Formula $\left\{ (y_{t},\mathbf{x}_{t})\right\} ,t=1,2,...,n$
\end_inset

 are obtained by some form of sampling
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
For example,
 cross-sectional data may be obtained by random sampling.
 Time series data accumulate historically.
\end_layout

\end_inset

.
 An individual observation is
\begin_inset Formula 
\[
y_{t}=\mathbf{x}_{t}^{\prime}\beta+\varepsilon_{t}
\]

\end_inset

 The 
\begin_inset Formula $n$
\end_inset

 observations can be written in matrix form as 
\begin_inset Formula 
\begin{equation}
\mathbf{y}=\mathbf{X}\beta+\mathbf{\varepsilon},
\end{equation}

\end_inset

 where 
\begin_inset Formula $\mathbf{y}=\left(\begin{array}{cccc}
y_{1} & y_{2} & \cdots & y_{n}\end{array}\right)^{\prime}$
\end_inset

 is 
\begin_inset Formula $n\times1$
\end_inset

 and 
\begin_inset Formula $\mathbf{X}=\left(\begin{array}{cccc}
\mathbf{x}_{1} & \mathbf{x}_{2} & \cdots & \mathbf{x}_{n}\end{array}\right)^{\prime}$
\end_inset

.
\end_layout

\begin_layout Standard
Linear models are more general than they might first appear,
 since one can employ nonlinear transformations of the variables:
 
\begin_inset Formula 
\[
\varphi_{0}(z)=\left[\begin{array}{cccc}
\varphi_{1}(w) & \varphi_{2}(w) & \cdots & \varphi_{p}(w)\end{array}\right]\beta+\varepsilon
\]

\end_inset

 where the 
\begin_inset Formula $\phi_{i}()$
\end_inset

 are known functions.
 Defining 
\begin_inset Formula $y=\varphi_{0}(z),$
\end_inset

 
\begin_inset Formula $x_{1}=\varphi_{1}(w),$
\end_inset

 
\emph on
etc
\emph default
.
 leads to a model in the form of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: linearity"
nolink "false"

\end_inset

.
 For example,
 the Cobb-Douglas model
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
Cobb-Douglas model
\end_layout

\end_inset

 
\begin_inset Formula 
\[
z=Aw_{2}^{\beta_{2}}w_{3}^{\beta_{3}}\exp(\varepsilon)
\]

\end_inset

 can be transformed logarithmically to obtain 
\begin_inset Formula 
\[
\ln z=\ln A+\beta_{2}\ln w_{2}+\beta_{3}\ln w_{3}+\varepsilon.
\]

\end_inset

If we define 
\begin_inset Formula $y=\ln z,$
\end_inset

 
\begin_inset Formula $\beta_{1}=\ln A,$
\end_inset

 
\emph on
etc.,

\emph default
 we can put the model in the form needed.
 The approximation is linear in the parameters,
 but not necessarily linear in the variables.
\end_layout

\begin_layout Section
Estimation by least squares
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Typical-data,-Classical"
nolink "false"

\end_inset

,
 obtained by running 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/TypicalData.jl}{TypicalData.jl}
\end_layout

\end_inset

 shows some data.
 We might want to find the straight line that best fits the data points.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Typical-data,-Classical"

\end_inset

Typical data,
 Classical Model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/TypicalData.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
ordinary least squares
\emph default
 (OLS) estimator is defined as the value that minimizes the sum of the squared errors:
 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \arg\min s(\beta)
\end{eqnarray*}

\end_inset

 where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
s(\beta) & = & \sum_{t=1}^{n}\left(y_{t}-\mathbf{x}_{t}^{\prime}\beta\right)^{2}\label{eq:OLS criterion function}\\
 & = & \left(\mathbf{y}-\mathbf{X}\beta\right)^{\prime}\left(\mathbf{y}-\mathbf{X}\beta\right)\nonumber \\
 & = & \mathbf{y}^{\prime}\mathbf{y}-2\mathbf{y}^{\prime}\mathbf{X}\beta+\beta^{\prime}\mathbf{X}^{\prime}\mathbf{X}\beta\nonumber \\
 & = & \parallel\mathbf{y}-\mathbf{X}\beta\parallel^{2}\nonumber 
\end{eqnarray}

\end_inset

This last expression makes it clear how the OLS estimator
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
estimator,
 OLS
\end_layout

\end_inset

 is defined:
 it minimizes the Euclidean distance between 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $X\beta.$
\end_inset

 The fitted OLS coefficients are those that give the best linear approximation to 
\begin_inset Formula $y$
\end_inset

 using 
\begin_inset Formula $\mathbf{x}$
\end_inset

 as basis functions,
 where 
\begin_inset Quotes sld
\end_inset

best
\begin_inset Quotes srd
\end_inset

 means minimum Euclidean distance.
 One could think of other estimators based upon other metrics.
 For example,
 the 
\emph on
minimum absolute distance
\emph default
 (MAD) minimizes 
\begin_inset Formula $\sum_{t=1}^{n}\left|y_{t}-\mathbf{x}_{t}^{\prime}\beta\right|$
\end_inset

.
 Later,
 we will see that which estimator is best in terms of their statistical properties,
 rather than in terms of the metrics that define them,
 depends upon the properties of 
\begin_inset Formula $\epsilon$
\end_inset

,
 about which we have as yet made no assumptions.
\end_layout

\begin_layout Itemize
To minimize the criterion 
\begin_inset Formula $s(\beta),$
\end_inset

 find the derivative with respect to 
\begin_inset Formula $\beta$
\end_inset

:
 
\begin_inset Formula 
\begin{eqnarray*}
D_{\beta}s(\beta) & = & -2\mathbf{X}^{\prime}\mathbf{y}+2\mathbf{X}^{\prime}\mathbf{X}\beta
\end{eqnarray*}

\end_inset

Then setting it to zeros gives
\begin_inset Formula 
\[
D_{\beta}s(\hat{\beta})=-2\mathbf{X}^{\prime}\mathbf{y}+2\mathbf{X}^{\prime}\mathbf{X}\hat{\beta}\equiv0
\]

\end_inset

 so
\begin_inset Formula 
\[
\hat{\beta}=(\mathbf{X}^{\prime}\mathbf{X})^{-1}\mathbf{X}^{\prime}\mathbf{y}.
\]

\end_inset


\end_layout

\begin_layout Itemize
To verify that this is a minimum,
 check the second order sufficient condition:
 
\begin_inset Formula 
\[
D_{\beta}^{2}s(\hat{\beta})=2\mathbf{X}^{\prime}\mathbf{X}
\]

\end_inset

 Since 
\begin_inset Formula $\rho(\mathbf{X})=K,$
\end_inset

 this matrix is positive definite,
 since it's a quadratic form in a p.d.
 matrix (identity matrix of order 
\begin_inset Formula $n)$
\end_inset

,
 so 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is in fact a minimizer.
\end_layout

\begin_layout Itemize
The 
\emph on
fitted values
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
fitted values
\end_layout

\end_inset


\emph default
 are the vector 
\begin_inset Formula $\hat{\mathbf{y}}=\mathbf{X}\hat{\beta}.$
\end_inset


\end_layout

\begin_layout Itemize
The 
\emph on
residuals
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
residuals
\end_layout

\end_inset


\emph default
 are the vector 
\begin_inset Formula $\hat{\varepsilon}=\mathbf{y}-\mathbf{X}\hat{\beta}$
\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{y} & = & \mathbf{X}\beta+\varepsilon\\
 & = & \mathbf{X}\hat{\beta}+\hat{\varepsilon}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Also,
 the first order conditions can be written as 
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{X}^{\prime}\mathbf{y}-\mathbf{X}^{\prime}\mathbf{X}\hat{\beta} & = & 0\\
\mathbf{X}^{\prime}\left(\mathbf{y}-\mathbf{X}\hat{\beta}\right) & = & 0\\
\mathbf{X}^{\prime}\hat{\varepsilon} & = & 0
\end{eqnarray*}

\end_inset

which is to say,
 the OLS residuals are orthogonal to 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 Let's look at this more carefully.
\end_layout

\begin_layout Section
Geometric interpretation of least squares estimation
\end_layout

\begin_layout Subsection
In 
\begin_inset Formula $X,Y$
\end_inset

 Space
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fitted in X,Y space"
nolink "false"

\end_inset

 shows a typical fit to data..
 This figure was created by running the Julia program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/OlsFit.jl}{OlsFit.jl} 
\end_layout

\end_inset

.
 You can experiment with changing the parameter values to see how this affects the fit.
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fitted in X,Y space"

\end_inset

Example OLS Fit
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/OlsFit.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
In Observation Space
\end_layout

\begin_layout Standard
If we want to plot in observation space,
 we'll need to use only two or three observations,
 or we'll encounter some limitations of the blackboard.
 If we try to use 3,
 we'll encounter the limits of my artistic ability,
 so let's use two.
 With only two observations,
 we can't have 
\begin_inset Formula $K>1.$
\end_inset

 
\begin_inset Float figure
placement htbp
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The fit in observation space
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/regression_obs_space.pdf
	width 6in

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Itemize
We can decompose 
\begin_inset Formula $y$
\end_inset

 into two components:
 the orthogonal projection onto the 
\begin_inset Formula $K-$
\end_inset

dimensional space spanned by 
\begin_inset Formula $X$
\end_inset

,
 
\begin_inset Formula $X\hat{\beta},$
\end_inset

 and the component that is the orthogonal projection onto the 
\begin_inset Formula $n-K$
\end_inset

 subpace that is orthogonal to the span of 
\begin_inset Formula $X,$
\end_inset

 
\begin_inset Formula $\hat{\varepsilon}.$
\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is chosen to make 
\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 as short as possible,
 
\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 will be orthogonal to the space spanned by 
\begin_inset Formula $X.$
\end_inset

 Since 
\begin_inset Formula $X$
\end_inset

 is in this space,
 
\begin_inset Formula $X^{\prime}\hat{\varepsilon}=0.$
\end_inset

 Note that the f.o.c.
 that define the least squares estimator imply that this is so.
\end_layout

\begin_layout Subsection
Projection Matrices
\end_layout

\begin_layout Standard
\begin_inset Formula $X\hat{\beta}$
\end_inset

 is the projection of 
\begin_inset Formula $y$
\end_inset

 onto the span of 
\begin_inset Formula $X,$
\end_inset

 or 
\begin_inset Formula 
\[
X\hat{\beta}=X\left(X^{\prime}X\right)^{-1}X^{\prime}y
\]

\end_inset

 Therefore,
 the matrix
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
matrix,
 projection
\end_layout

\end_inset

 that projects 
\begin_inset Formula $y$
\end_inset

 onto the span of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula 
\[
P_{X}=X(X^{\prime}X)^{-1}X^{\prime}
\]

\end_inset

 since 
\begin_inset Formula 
\[
X\hat{\beta}=P_{X}y.
\]

\end_inset


\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 is the projection of 
\begin_inset Formula $y$
\end_inset

 onto the 
\begin_inset Formula $N-K$
\end_inset

 dimensional space that is orthogonal to the span of 
\begin_inset Formula $X$
\end_inset

.
 We have that 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\varepsilon} & = & y-X\hat{\beta}\\
 & = & y-X(X^{\prime}X)^{-1}X^{\prime}y\\
 & = & \left[I_{n}-X(X^{\prime}X)^{-1}X^{\prime}\right]y.
\end{eqnarray*}

\end_inset

 So the matrix that projects 
\begin_inset Formula $y$
\end_inset

 onto the space orthogonal to the span of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
M_{X} & = & I_{n}-X(X^{\prime}X)^{-1}X^{\prime}\\
 & = & I_{n}-P_{X}.
\end{eqnarray*}

\end_inset

 We have 
\begin_inset Formula 
\[
\hat{\varepsilon}=M_{X}y.
\]

\end_inset

Therefore 
\begin_inset Formula 
\begin{eqnarray*}
y & = & P_{X}y+M_{X}y\\
 & = & X\hat{\beta}+\hat{\varepsilon}.
\end{eqnarray*}

\end_inset

These two projection matrices decompose the 
\begin_inset Formula $n$
\end_inset

 dimensional vector 
\begin_inset Formula $y$
\end_inset

 into two orthogonal components - the portion that lies in the 
\begin_inset Formula $K$
\end_inset

 dimensional space defined by 
\begin_inset Formula $X,$
\end_inset

 and the portion that lies in the orthogonal 
\begin_inset Formula $n-K$
\end_inset

 dimensional space.
\end_layout

\begin_layout Itemize
Note that both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are 
\emph on
symmetric
\emph default
 and 
\emph on
idempotent
\emph default
.
\end_layout

\begin_deeper
\begin_layout Itemize
A symmetric matrix
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
matrix,
 symmetric
\end_layout

\end_inset

 
\begin_inset Formula $A$
\end_inset

 is one such that 
\begin_inset Formula $A=A^{\prime}.$
\end_inset


\end_layout

\begin_layout Itemize
An idempotent matrix
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
matrix,
 idempotent
\end_layout

\end_inset

 
\begin_inset Formula $A$
\end_inset

 is one such that 
\begin_inset Formula $A=AA.$
\end_inset


\end_layout

\begin_layout Itemize
The only nonsingular idempotent matrix is the identity matrix.
 
\end_layout

\end_deeper
\begin_layout Section
Influential observations
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
observations,
 influential
\end_layout

\end_inset

 and outliers
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
outliers
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The OLS estimator of the 
\begin_inset Formula $i^{th}$
\end_inset

 element of the vector 
\begin_inset Formula $\beta_{0}$
\end_inset

 is simply 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{i} & = & \left[(X^{\prime}X)^{-1}X^{\prime}\right]_{i\cdot}y\\
 & = & c_{i}^{\prime}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is how we define a linear estimator
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
estimator,
 linear
\end_layout

\end_inset

 - it's a linear function of the dependent variable.
 Since it's a linear combination of the observations on the dependent variable,
 where the weights are determined by the observations on the regressors,
 some observations may have more influence than others.
\end_layout

\begin_layout Standard
To investigate this,
 let 
\begin_inset Formula $e_{t}$
\end_inset

 be an 
\begin_inset Formula $n$
\end_inset

 vector of zeros with a 
\begin_inset Formula $1$
\end_inset

 in the t
\begin_inset Formula $^{th}$
\end_inset

 position,
 
\emph on
i.e.,

\emph default
 it's the 
\begin_inset Formula $t\textrm{th column of the matrix \ensuremath{I_{n}}}$
\end_inset

.
 Define 
\begin_inset Formula 
\begin{eqnarray*}
h_{t} & = & \left(P_{X}\right)_{tt}\\
 & = & e_{t}^{\prime}P_{X}e_{t}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula $h_{t}$
\end_inset

 is the t
\begin_inset Formula $^{th}$
\end_inset

 element on the main diagonal of 
\begin_inset Formula $P_{X}$
\end_inset

.
 Note that 
\begin_inset Formula 
\begin{eqnarray*}
h_{t} & = & \parallel P_{X}e_{t}\parallel^{2}
\end{eqnarray*}

\end_inset

so 
\begin_inset Formula 
\[
h_{t}\leq\parallel e_{t}\parallel^{2}=1
\]

\end_inset

So 
\begin_inset Formula $0<h_{t}<1$
\end_inset

.
 Also,
 
\begin_inset Formula 
\[
TrP_{X}=K\Rightarrow\overline{h}=K/n.
\]

\end_inset

So the average of the 
\begin_inset Formula $h_{t}$
\end_inset

 is 
\begin_inset Formula $K/n$
\end_inset

.
 The value 
\begin_inset Formula $h_{t}$
\end_inset

 is referred to as the 
\emph on
leverage
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
leverage
\end_layout

\end_inset


\emph default
 of the observation.
 If the leverage is much higher than average,
 the observation has the potential to affect the OLS fit importantly.
 However,
 an observation may also be influential due to the value of 
\begin_inset Formula $y_{t}$
\end_inset

,
 rather than the weight it is multiplied by,
 which only depends on the 
\begin_inset Formula $x_{t}$
\end_inset

's.
\end_layout

\begin_layout Standard
To account for this,
 consider estimation of 
\begin_inset Formula $\beta$
\end_inset

 without using the 
\begin_inset Formula $t^{th}$
\end_inset

 observation (designate this estimator as 
\begin_inset Formula $\hat{\beta}^{(t)}).$
\end_inset

 One can show (see Davidson and MacKinnon,
 pp.
 32-5 for proof) that 
\begin_inset Formula 
\[
\hat{\beta}^{(t)}=\hat{\beta}-\left(\frac{1}{1-h_{t}}\right)(X^{\prime}X)^{-1}X_{t}^{\prime}\hat{\varepsilon}_{t}
\]

\end_inset

 so the change in the 
\begin_inset Formula $t^{th}$
\end_inset

 observations fitted value is 
\begin_inset Formula 
\[
\mathbf{x}_{t}^{\prime}\hat{\beta}-\mathbf{x}_{t}^{\prime}\hat{\beta}^{(t)}=\left(\frac{h_{t}}{1-h_{t}}\right)\hat{\varepsilon}_{t}
\]

\end_inset

 While an observation may be influential if it doesn't affect its own fitted value,
 it certainly 
\emph on
is
\emph default
 influential if it does.
 A fast means of identifying influential observations is to plot 
\begin_inset Formula $\left(\frac{h_{t}}{1-h_{t}}\right)\hat{\varepsilon}_{t}$
\end_inset

 (which I will refer to as the 
\emph on
own influence
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
own influence
\end_layout

\end_inset


\emph default
 of the observation) as a function of 
\begin_inset Formula $t$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Detection-of-influential"
nolink "false"

\end_inset

 gives an example plot of data,
 fit,
 leverage and influence.
 The Julia program is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/InfluentialObservation.jl}{InfluentialObservation.jl}
\end_layout

\end_inset

.
 If you re-run the program you will see that the leverage of the last observation (an outlying value of x) is always high,
 and the influence is sometimes high.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Detection-of-influential"

\end_inset

Detection of influential observations
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/InfluentialObservation.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
After influential observations are detected,
 one needs to determine 
\emph on
why
\emph default
 they are influential.
 Possible causes include:
\end_layout

\begin_layout Itemize
data entry error,
 which can easily be corrected once detected.
 Data entry errors 
\emph on
are very common.
\end_layout

\begin_layout Itemize
special economic factors that affect some observations.
 These would need to be identified and incorporated in the model.
 This is the idea behind 
\emph on
structural change
\emph default
:
 the parameters may not be constant across all observations.
\end_layout

\begin_layout Itemize
pure randomness may have caused us to sample a low-probability observation.
\end_layout

\begin_layout Standard
There exist 
\emph on
robust
\emph default
 estimation methods that downweight outliers.
\end_layout

\begin_layout Section
Goodness of fit
\end_layout

\begin_layout Standard
The fitted model is 
\begin_inset Formula 
\[
y=X\hat{\beta}+\hat{\varepsilon}
\]

\end_inset

 Take the inner product:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y^{\prime}y=\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}+2\hat{\beta}^{\prime}X^{\prime}\hat{\varepsilon}+\hat{\varepsilon}^{\prime}\hat{\varepsilon}
\]

\end_inset

 But the middle term of the RHS is zero since 
\begin_inset Formula $X^{\prime}\hat{\varepsilon}=0$
\end_inset

,
 so 
\begin_inset Formula 
\begin{equation}
y^{\prime}y=\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}+\hat{\varepsilon}^{\prime}\hat{\varepsilon}\label{rsquare development}
\end{equation}

\end_inset

 The 
\emph on
uncentered
\emph default
 
\begin_inset Formula $R_{u}^{2}$
\end_inset

 is defined as 
\begin_inset Formula 
\begin{eqnarray*}
R_{u}^{2} & = & 1-\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{y^{\prime}y}\\
 & = & \frac{\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}}{y^{\prime}y}\\
 & = & \frac{\parallel P_{X}y\parallel^{2}}{\parallel y\parallel^{2}}\\
 & = & \cos^{2}(\phi),
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\phi$
\end_inset

 is the angle between 
\begin_inset Formula $y$
\end_inset

 and the span of 
\begin_inset Formula $X$
\end_inset

 .
\end_layout

\begin_layout Itemize
The uncentered 
\begin_inset Formula $R^{2}$
\end_inset


\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
R- squared,
 uncentered
\end_layout

\end_inset

 changes if we add a constant to 
\begin_inset Formula $y,$
\end_inset

 since this changes 
\begin_inset Formula $\phi$
\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Uncentered--R^{2}"
nolink "false"

\end_inset

,
 the yellow vector is a constant,
 since it's on the 
\begin_inset Formula $45$
\end_inset

 degree line in observation space).
 
\begin_inset Float figure
placement htbp
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Uncentered--R^{2}"

\end_inset

Uncentered 
\begin_inset Formula $R^{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Examples/Figures/UncenteredRSquare.png
	width 5in
	rotateOrigin center
	special height=4in

\end_inset


\end_layout

\end_inset

Another,
 more common definition measures the contribution of the variables,
 other than the constant term,
 to explaining the variation in 
\begin_inset Formula $y.$
\end_inset

 Thus it measures the ability of the model to explain the variation of 
\begin_inset Formula $y$
\end_inset

 about its unconditional sample mean.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\iota=(1,1,...,1)^{\prime},$
\end_inset

 a 
\begin_inset Formula $n$
\end_inset

 -vector.
 So 
\begin_inset Formula 
\begin{eqnarray*}
M_{\iota} & = & I_{n}-\iota(\iota^{\prime}\iota)^{-1}\iota^{\prime}\\
 & = & I_{n}-\iota\iota^{\prime}/n
\end{eqnarray*}

\end_inset

 
\begin_inset Formula $M_{\iota}y$
\end_inset

 just returns the vector of deviations from the mean.
 In terms of deviations from the mean,
 equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "rsquare development"
nolink "false"

\end_inset

 becomes
\begin_inset Formula 
\[
y^{\prime}M_{\iota}y=\hat{\beta}^{\prime}X^{\prime}M_{\iota}X\hat{\beta}+\hat{\varepsilon}^{\prime}M_{\iota}\hat{\varepsilon}
\]

\end_inset

 
\end_layout

\begin_layout Standard
The 
\emph on
centered
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
R-squared,
 centered
\end_layout

\end_inset


\emph default
 
\begin_inset Formula $R_{c}^{2}$
\end_inset

 is defined as 
\begin_inset Formula 
\[
R_{c}^{2}=1-\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{y^{\prime}M_{\iota}y}=1-\frac{ESS}{TSS}
\]

\end_inset

where 
\begin_inset Formula $ESS=\hat{\varepsilon}^{\prime}\hat{\varepsilon}$
\end_inset

 and 
\begin_inset Formula $TSS=y^{\prime}M_{\iota}y$
\end_inset

=
\begin_inset Formula $\sum_{t=1}^{n}(y_{t}-\bar{y})^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
Supposing that 
\begin_inset Formula $X$
\end_inset

 contains a column of ones (
\emph on
i.e.,

\emph default
 there is a constant term),
 
\begin_inset Formula 
\[
X^{\prime}\hat{\varepsilon}=0\Rightarrow\sum_{t}\hat{\varepsilon}_{t}=0
\]

\end_inset

 so 
\begin_inset Formula $M_{\iota}\hat{\varepsilon}$
\end_inset

 
\begin_inset Formula $=\hat{\varepsilon}.$
\end_inset

 In this case 
\begin_inset Formula 
\[
y^{\prime}M_{\iota}y=\hat{\beta}^{\prime}X^{\prime}M_{\iota}X\hat{\beta}+\hat{\varepsilon}^{\prime}\hat{\varepsilon}
\]

\end_inset

 So 
\begin_inset Formula 
\[
R_{c}^{2}=\frac{RSS}{TSS}
\]

\end_inset

where 
\begin_inset Formula $RSS=\hat{\beta}^{\prime}X^{\prime}M_{\iota}X\hat{\beta}$
\end_inset

 
\end_layout

\begin_layout Itemize
Supposing that a column of ones is in the space spanned by 
\begin_inset Formula $X$
\end_inset

 (
\begin_inset Formula $P_{X}\iota=\iota),$
\end_inset

 then one can show that 
\begin_inset Formula $0\leq R_{c}^{2}\leq1.$
\end_inset


\end_layout

\begin_layout Section
The classical linear regression model
\begin_inset CommandInset label
LatexCommand label
name "sec:The-classical-linear"

\end_inset


\end_layout

\begin_layout Standard
Up to this point the model is empty of content beyond the definition of a best linear approximation to 
\begin_inset Formula $y$
\end_inset

 and some geometrical properties.
 There is no economic content to the model,
 and the regression parameters have no economic interpretation.
 For example,
 what is the partial derivative of 
\begin_inset Formula $y$
\end_inset

 with respect to 
\begin_inset Formula $x_{j}$
\end_inset

?
 The linear approximation is
\begin_inset Formula 
\[
y=\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{k}x_{k}+\epsilon
\]

\end_inset

The partial derivative is 
\begin_inset Formula 
\[
\frac{\partial y}{\partial x_{j}}=\beta_{j}+\frac{\partial\epsilon}{\partial x_{j}}
\]

\end_inset

Up to now,
 there's no guarantee that 
\begin_inset Formula $\frac{\partial\epsilon}{\partial x_{j}}$
\end_inset

=0.
 For the 
\begin_inset Formula $\beta$
\end_inset

 to have an economic meaning,
 we need to make additional assumptions.
 The assumptions that are appropriate to make depend on the data under consideration.
 We'll start with the classical linear regression model,
 which incorporates some assumptions that are clearly not realistic for economic data.
 This is to be able to explain some concepts with a minimum of confusion and notational clutter.
 Later we'll adapt the results to what we can get with more realistic assumptions.
\end_layout

\begin_layout Standard

\series bold
Linearity
\series default
:
 the model is a linear function of the parameter vector 
\begin_inset Formula $\beta_{0}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray}
y & = & \beta_{1}^{0}x_{1}+\beta_{2}^{0}x_{2}+...+\beta_{k}^{0}x_{k}+\epsilon\label{assumption: linearity}
\end{eqnarray}

\end_inset

or,
 using vector notation:
 
\begin_inset Formula 
\[
y=\mathbf{x}^{\prime}\beta_{0}+\epsilon
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Nonstochastic linearly independent regressors
\series default
:
 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a fixed matrix of constants,
 it has rank 
\begin_inset Formula $K$
\end_inset

 equal to its number of columns,
 and 
\begin_inset Formula 
\begin{align}
\lim\frac{1}{n}\mathbf{X}^{\prime}\mathbf{X} & =Q_{X}\label{assumption: linearly independent regressors}
\end{align}

\end_inset

where 
\begin_inset Formula $Q_{X}$
\end_inset

 is a finite positive definite matrix.
 This is needed to be able to identify the individual effects of the explanatory variables.
\end_layout

\begin_layout Standard

\series bold
Independently and identically distributed errors
\series default
:
\begin_inset Formula 
\begin{equation}
\epsilon\sim IID(0,\sigma^{2}I_{n})\label{assumption: IID errors}
\end{equation}

\end_inset


\begin_inset Formula $\varepsilon$
\end_inset

 is jointly distributed IID.
 This implies the following two properties:
\end_layout

\begin_layout Standard

\series bold
Homoscedastic errors
\series default
:
\begin_inset Formula 
\begin{equation}
V(\varepsilon_{t})=\sigma_{0}^{2},\forall t\label{assumption: homoscedasticity}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\series bold
Nonautocorrelated errors:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathcal{E}(\varepsilon_{t}\epsilon_{s})=0,\forall t\neq s\label{assumption: nonautocorrelation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Optionally,
 we will sometimes assume that the errors are normally distributed.
\end_layout

\begin_layout Standard

\series bold
Normally distributed errors:
\series default

\begin_inset Formula 
\begin{equation}
\epsilon\sim N(0,\sigma^{2}I_{n})\label{assumption: normal errors}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Small sample statistical properties of the least squares estimator
\end_layout

\begin_layout Standard
Up to now,
 we have only examined numeric properties of the OLS estimator,
 that always hold.
 Now we will examine statistical properties.
 The statistical properties depend upon the assumptions we make.
\end_layout

\begin_layout Subsection
Unbiasedness
\end_layout

\begin_layout Standard
We have 
\begin_inset Formula $\hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y$
\end_inset

.
 By linearity,
 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & (X^{\prime}X)^{-1}X^{\prime}\left(X\beta+\varepsilon\right)\\
 & = & \beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: linearly independent regressors"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: IID errors"
nolink "false"

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
E(X^{\prime}X)^{-1}X^{\prime}\varepsilon & = & E(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & (X^{\prime}X)^{-1}X^{\prime}E\varepsilon\\
 & = & 0
\end{eqnarray*}

\end_inset

 so the OLS estimator is unbiased under the assumptions of the classical model.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-unbiasedness"
nolink "false"

\end_inset

 shows the results of a small Monte Carlo experiment where the OLS estimator was calculated for 10000 samples from the classical model with 
\begin_inset Formula $y=1+2x+\varepsilon$
\end_inset

,
 where 
\begin_inset Formula $n=20$
\end_inset

,
 
\begin_inset Formula $\sigma_{\varepsilon}^{2}=9$
\end_inset

,
 and 
\begin_inset Formula $x$
\end_inset

 is fixed across samples.
 We can see that the 
\begin_inset Formula $\beta_{2}$
\end_inset

 appears to be estimated without bias.
 The program that generates the plot is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/Unbiased.jl}{Unbiased.jl} 
\end_layout

\end_inset

,
 if you would like to experiment with this.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "figure-unbiasedness"

\end_inset

Unbiasedness of OLS under classical assumptions:
 replications of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 minus true 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/Unbiased.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
With time series data,
 the OLS estimator will often be biased.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-biasedness"
nolink "false"

\end_inset

 shows the results of a small Monte Carlo experiment where the OLS estimator was calculated for 1000 samples from the AR(1) model with 
\begin_inset Formula $y_{t}=0+0.9y_{t-1}+\varepsilon_{t}$
\end_inset

,
 where 
\begin_inset Formula $n=20$
\end_inset

 and 
\begin_inset Formula $\sigma_{\varepsilon}^{2}=1$
\end_inset

.
 In this case,
 assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: linearly independent regressors"
nolink "false"

\end_inset

 does not hold:
 the regressors are stochastic.
 We can see that the bias in the estimation of 
\begin_inset Formula $\beta_{2}$
\end_inset

 is about -0.2.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "figure-biasedness"

\end_inset

Biasedness of OLS when an assumption fails:
 :
 replications of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 minus true 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/Biased.png

\end_inset


\end_layout

\end_inset

The program that generates the plot is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/Biased.jl}{Biased.jl} 
\end_layout

\end_inset

 ,
 if you would like to experiment with this.
\end_layout

\begin_layout Subsection
Normality
\end_layout

\begin_layout Standard
With the linearity assumption,
 we have 
\begin_inset Formula $\hat{\beta}=\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon.$
\end_inset

 This is a linear function of 
\begin_inset Formula $\varepsilon$
\end_inset

.
 Adding the assumption of normality (
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: normal errors"
nolink "false"

\end_inset

,
 which implies strong exogeneity),
 then
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\beta}\sim N\left(\beta,(X^{\prime}X)^{-1}\sigma_{0}^{2}\right)
\]

\end_inset

since a linear function of a normal random vector is also normally distributed.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-unbiasedness"
nolink "false"

\end_inset

 you can see that the estimator appears to be normally distributed.
 It in fact is normally distributed,
 since the DGP (see the Octave program) has normal errors.
 Even when the data may be taken to be IID,
 the assumption of normality is often questionable or simply untenable.
 For example,
 if the dependent variable is the number of automobile trips per week,
 it is a count variable with a discrete distribution,
 and is thus not normally distributed.
 Many variables in economics can take on only nonnegative values,
 which,
 strictly speaking,
 rules out normality.
\begin_inset Foot
status open

\begin_layout Plain Layout
Normality may be a good model nonetheless,
 as long as the probability of a negative value occurring is negligible under the model.
 This depends upon the mean being large enough in relation to the variance.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The variance of the OLS estimator and the Gauss-Markov theorem
\end_layout

\begin_layout Standard
Now let's make all the classical assumptions except the assumption of normality.
 We have 
\begin_inset Formula $\hat{\beta}=\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon$
\end_inset

 and we know that 
\begin_inset Formula $E(\hat{\beta})=\beta$
\end_inset

.
 So
\begin_inset Formula 
\begin{eqnarray*}
Var(\hat{\beta}) & = & E\left\{ \left(\mathbf{\hat{\beta}-\beta}\right)\left(\mathbf{\hat{\beta}-\beta}\right)^{\prime}\right\} \\
 & = & E\left\{ (X^{\prime}X)^{-1}X^{\prime}\varepsilon\varepsilon^{\prime}X(X^{\prime}X)^{-1}\right\} \\
 & = & (X^{\prime}X)^{-1}\sigma_{0}^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The OLS estimator is a 
\emph on
linear estimator
\emph default

\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
estimator,
 linear
\end_layout

\end_inset

,
 which means that it is a linear function of the dependent variable,
 
\begin_inset Formula $y.$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \left[(X^{\prime}X)^{-1}X^{\prime}\right]y\\
 & = & Cy
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is a function of the explanatory variables only,
 not the dependent variable.
 It is also 
\emph on
unbiased
\emph default
 under the present assumptions,
 as we proved above.
 One could consider other weights 
\begin_inset Formula $W$
\end_inset

 that are a function of 
\begin_inset Formula $X$
\end_inset

 that define some other linear estimator.
 We'll still insist upon unbiasedness.
 Consider 
\begin_inset Formula $\tilde{\beta}=Wy,$
\end_inset

 where 
\begin_inset Formula $W=W(X)$
\end_inset

 is some 
\begin_inset Formula $k\times n$
\end_inset

 matrix function of 
\begin_inset Formula $X.$
\end_inset

 Note that since 
\begin_inset Formula $W$
\end_inset

 is a function of 
\begin_inset Formula $X,$
\end_inset

 it is nonstochastic,
 too.
 If the estimator is unbiased,
 then we must have 
\begin_inset Formula $WX=I_{K}$
\end_inset

:
 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(Wy) & = & \mathcal{E}(WX\beta_{0}+W\varepsilon)\\
 & = & WX\beta_{0}\\
 & = & \beta_{0}\\
 & \Rightarrow\\
WX & = & I_{K}
\end{eqnarray*}

\end_inset

 The variance of 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 is 
\begin_inset Formula 
\[
V(\tilde{\beta})=WW^{\prime}\sigma_{0}^{2}.
\]

\end_inset

 Define 
\begin_inset Formula 
\[
D=W-(X^{\prime}X)^{-1}X^{\prime}
\]

\end_inset

 so 
\begin_inset Formula 
\[
W=D+(X^{\prime}X)^{-1}X^{\prime}
\]

\end_inset

 Since 
\begin_inset Formula $WX=I_{K},$
\end_inset

 
\begin_inset Formula $DX=0,$
\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
V(\tilde{\beta}) & = & \left(D+(X^{\prime}X)^{-1}X^{\prime}\right)\left(D+(X^{\prime}X)^{-1}X^{\prime}\right)^{\prime}\sigma_{0}^{2}\\
 & = & \left(DD^{\prime}+\left(X^{\prime}X\right)^{-1}\right)\sigma_{0}^{2}
\end{eqnarray*}

\end_inset

 So 
\begin_inset Formula 
\[
V(\tilde{\beta})\geq V(\hat{\beta})
\]

\end_inset

 The inequality is a shorthand means of expressing,
 more formally,
 that 
\begin_inset Formula $V(\tilde{\beta})-V(\hat{\beta})$
\end_inset

 is a positive semi-definite matrix.
 This is a proof of the Gauss-Markov Theorem.
 The OLS estimator is the 
\begin_inset Quotes sld
\end_inset

best linear unbiased estimator
\begin_inset Quotes srd
\end_inset

 (BLUE).
\end_layout

\begin_layout Itemize
It is worth emphasizing again that we have not used the normality assumption in any way to prove the Gauss-Markov theorem,
 so it is valid if the errors are not normally distributed,
 as long as the other assumptions hold.
 
\end_layout

\begin_layout Standard
To illustrate the Gauss-Markov result,
 consider the estimator that results from splitting the sample into 
\begin_inset Formula $p$
\end_inset

 equally-sized parts,
 estimating using each part of the data separately by OLS,
 then averaging the 
\begin_inset Formula $p$
\end_inset

 resulting estimators.
 You should be able to show that this estimator is unbiased,
 but inefficient with respect to the OLS estimator.
 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/Efficiency.jl}{Efficiency.jl} 
\end_layout

\end_inset

 illustrates this using a small Monte Carlo experiment,
 which compares the OLS estimator and a 3-way split sample estimator.
 The data generating process follows the classical model,
 with 
\begin_inset Formula $n=21$
\end_inset

.
 The true parameter value is 
\begin_inset Formula $\beta=2.$
\end_inset

 In Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Gauss-Markov-Result: OLS"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Gauss-Markov-Result: split sample"
nolink "false"

\end_inset

 we can see that the OLS estimator is more efficient,
 since the tails of its histogram are more narrow.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Gauss-Markov-Result: OLS"

\end_inset

Gauss-Markov Result:
 The OLS estimator
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/efficiency1.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Gauss-Markov Resul
\begin_inset CommandInset label
LatexCommand label
name "cap:Gauss-Markov-Result: split sample"

\end_inset

:
 The split sample estimator
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/efficiency2.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We have that 
\begin_inset Formula $E(\hat{\beta})=\beta$
\end_inset

 and 
\begin_inset Formula $Var(\hat{\beta})=\left(X^{'}X\right)^{-1}\sigma_{0}^{2},$
\end_inset

 but we still need to estimate the variance of 
\begin_inset Formula $\epsilon$
\end_inset

,
 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

,
 in order to have an idea of the precision of the estimates of 
\begin_inset Formula $\beta$
\end_inset

.
 A commonly used estimator of 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\sigma_{0}^{2}}=\frac{1}{n-K}\hat{\varepsilon}^{\prime}\hat{\varepsilon}
\]

\end_inset

This estimator is unbiased:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\widehat{\sigma_{0}^{2}} & = & \frac{1}{n-K}\hat{\varepsilon}^{\prime}\hat{\varepsilon}\\
 & = & \frac{1}{n-K}\varepsilon^{\prime}M\varepsilon\\
\mathcal{E}(\widehat{\sigma_{0}^{2}}) & = & \frac{1}{n-K}E(Tr\varepsilon^{\prime}M\varepsilon)\\
 & = & \frac{1}{n-K}E(TrM\varepsilon\varepsilon^{\prime})\\
 & = & \frac{1}{n-K}TrE(M\varepsilon\varepsilon^{\prime})\\
 & = & \frac{1}{n-K}\sigma_{0}^{2}TrM\\
 & = & \frac{1}{n-K}\sigma_{0}^{2}\left(n-k\right)\\
 & = & \sigma_{0}^{2}
\end{eqnarray*}

\end_inset

where we use the fact that 
\begin_inset Formula $Tr(AB)=Tr(BA)$
\end_inset

 when both products are conformable.
 Thus,
 this estimator is also unbiased under these assumptions.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example:-The-Nerlove"

\end_inset

Example:
 The Nerlove model 
\end_layout

\begin_layout Subsection
Theoretical background
\end_layout

\begin_layout Standard
For a firm that takes input prices 
\begin_inset Formula $w$
\end_inset

 and the output level 
\begin_inset Formula $q$
\end_inset

 as given,
 the cost minimization problem is to choose the quantities of inputs 
\begin_inset Formula $x$
\end_inset

 to solve the problem
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{x}w'x
\]

\end_inset

 subject to the restriction
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x)=q.
\]

\end_inset

 The solution is the vector of factor demands 
\begin_inset Formula $x(w,q)$
\end_inset

.
 The 
\emph on
cost function
\emph default
 is obtained by substituting the factor demands into the criterion function:
 
\begin_inset Formula 
\[
Cw,q)=w'x(w,q).
\]

\end_inset

 
\end_layout

\begin_layout Itemize

\series bold
Monotonicity
\series default
 Increasing factor prices cannot decrease cost,
 so 
\begin_inset Formula 
\[
\frac{\partial C(w,q)}{\partial w}\geq0
\]

\end_inset

Remember that these derivatives give the conditional factor demands (Shephard's Lemma).
\end_layout

\begin_layout Itemize

\series bold
Homogeneity
\series default
 The cost function is homogeneous of degree 1 in input prices:
 
\begin_inset Formula $C(tw,q)=tC(w,q)$
\end_inset

 where 
\begin_inset Formula $t$
\end_inset

 is a scalar constant.
 This is because the factor demands are homogeneous of degree zero in factor prices - they only depend upon relative prices.
\end_layout

\begin_layout Itemize

\series bold
Returns to scale
\series default
 The 
\emph on
returns to scale
\emph default
 parameter 
\begin_inset Formula $\gamma$
\end_inset

 is defined as the inverse of the elasticity of cost with respect to output:
\begin_inset Formula 
\[
\gamma=\left(\frac{\partial C(w,q)}{\partial q}\frac{q}{C(w,q)}\right)^{-1}
\]

\end_inset


\emph on
Constant returns to scale
\emph default
 is the case where increasing production 
\begin_inset Formula $q$
\end_inset

 implies that cost increases in the proportion 1:1.
 If this is the case,
 then 
\begin_inset Formula $\gamma=1$
\end_inset

.
\end_layout

\begin_layout Subsection
Cobb-Douglas functional form
\end_layout

\begin_layout Standard
The Cobb-Douglas functional form is linear in the logarithms of the regressors and the dependent variable.
 For a cost function,
 if there are 
\begin_inset Formula $g$
\end_inset

 factors,
 the Cobb-Douglas cost function has the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C=Aw_{1}^{\beta_{1}}...w_{g}^{\beta_{g}}q^{\beta_{q}}e^{\varepsilon}
\]

\end_inset

What is the elasticity of 
\begin_inset Formula $C$
\end_inset

 with respect to 
\begin_inset Formula $w_{j}$
\end_inset

?
\begin_inset Formula 
\begin{eqnarray*}
e_{w_{j}}^{C} & = & \left(\frac{\partial C}{\partial_{W_{J}}}\right)\left(\frac{w_{j}}{C}\right)\\
 & = & \beta_{j}Aw_{1}^{\beta_{1}}.w_{j}^{\beta_{j}-1}..w_{g}^{\beta_{g}}q^{\beta_{q}}e^{\varepsilon}\frac{w_{j}}{Aw_{1}^{\beta_{1}}...w_{g}^{\beta_{g}}q^{\beta_{q}}e^{\varepsilon}}\\
 & = & \beta_{j}
\end{eqnarray*}

\end_inset

This is one of the reasons the Cobb-Douglas form is popular - the coefficients are easy to interpret,
 since they are the elasticities of the dependent variable with respect to the explanatory variable.
 Not that in this case,
\begin_inset Formula 
\begin{eqnarray*}
e_{w_{j}}^{C} & = & \left(\frac{\partial C}{\partial_{W_{J}}}\right)\left(\frac{w_{j}}{C}\right)\\
 & = & x_{j}(w,q)\frac{w_{j}}{C}\\
 & \equiv & s_{j}(w,q)
\end{eqnarray*}

\end_inset

the 
\emph on
cost share
\emph default
 of the 
\begin_inset Formula $j^{th}$
\end_inset

 input.
 So with a Cobb-Douglas cost function,
 
\begin_inset Formula $\beta_{j}=s_{j}(w,q)$
\end_inset

.
 The cost shares are constants.
\end_layout

\begin_layout Standard
Note that after a logarithmic transformation we obtain
\begin_inset Formula 
\[
\ln C=\alpha+\beta_{1}\ln w_{1}+...+\beta_{g}\ln w_{g}+\beta_{q}\ln q+\epsilon
\]

\end_inset

where 
\begin_inset Formula $\alpha=\ln A$
\end_inset

 .
 So we see that the transformed model is linear in the logs of the data.
\end_layout

\begin_layout Standard
One can verify that the property of HOD1 implies that 
\begin_inset Formula 
\[
\sum_{i=1}^{g}\beta_{i}=1
\]

\end_inset

In other words,
 the cost shares add up to 1.
 
\end_layout

\begin_layout Standard
The hypothesis that the technology exhibits CRTS implies that 
\begin_inset Formula 
\[
\gamma=\frac{1}{\beta_{q}}=1
\]

\end_inset

so 
\begin_inset Formula $\beta_{q}=1.$
\end_inset

 Likewise,
 monotonicity implies that the coefficients 
\begin_inset Formula $\beta_{i}\geq0,i=1,...,g$
\end_inset

.
\end_layout

\begin_layout Subsection
The Nerlove data and OLS
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-Nerlove-data"

\end_inset


\end_layout

\begin_layout Standard
The file 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/nerlove.data}{nerlove.data} 
\end_layout

\end_inset

 contains data on 145 electric utility companies' cost of production,
 output and input prices.
 The data are for the U.S.,
 and were collected by M.
 Nerlove.
 The observations are by row,
 and the columns are 
\series bold
COMPANY,
 COST
\series default
 
\begin_inset Formula $(C)$
\end_inset


\series bold
,
 OUTPUT
\series default
 
\begin_inset Formula $(Q),$
\end_inset

 
\series bold
PRICE OF LABOR
\series default
 
\begin_inset Formula $(P_{L})$
\end_inset

,
 
\series bold
PRICE OF FUEL
\series default
 
\begin_inset Formula $(P_{F})$
\end_inset

 and 
\series bold
PRICE OF CAPITAL
\series default
 
\begin_inset Formula $(P_{K}).$
\end_inset

 Note that the data are sorted by output level (the third column).
\end_layout

\begin_layout Standard
We will estimate the Cobb-Douglas model 
\begin_inset Formula 
\begin{equation}
\ln C=\beta_{1}+\beta_{Q}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon\label{simple nerlove model}
\end{equation}

\end_inset

 by OLS,
 using the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/Nerlove.jl}{Nerlove.jl} 
\end_layout

\end_inset

,
 which uses 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./src/LinearRegression/ols.jl}{ols.jl} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\paragraph_spacing single
The results are 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Examples/OLS/Nerlove.png
	width 15cm

\end_inset


\end_layout

\begin_layout Itemize
Do the theoretical restrictions hold?
\end_layout

\begin_layout Itemize
Does the model fit well?
\end_layout

\begin_layout Itemize
What do you think about RTS?
\end_layout

\begin_layout Standard
We will most often use Julia programs that more or less directly implement the theory we learn in examples in this document.
 This is because following such transparent programming statements is a useful way of learning how theory is put into practice.
 Nevertheless,
 you may be interested in a more 
\begin_inset Quotes sld
\end_inset

user-friendly
\begin_inset Quotes srd
\end_inset

 environment for doing econometrics,
 especially after you have mastered the theory.
 Julia itself offers packages such as DataFrames.jl and GLM.jl which will allow you to avoid some of the nuts and bolts of econometric modeling.
 For example,
 see 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/NerloveDF.jl}{NerloveDF.jl} 
\end_layout

\end_inset

 for estimating the Nerlove model using these packages.
 If you run that,
 you will see that the estimated standard errors differ from what Nerlove.jl reports,
 we will get to the reason for that later.
 For a 
\begin_inset Quotes sld
\end_inset

canned
\begin_inset Quotes srd
\end_inset

 package,
 apart from what Julia offers,
 I heartily recommend 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{http://gretl.sourceforge.net}{Gretl}
\end_layout

\end_inset

,
 the GNU Regression,
 Econometrics,
 and Time-Series Library.
 Gretl is free software.
 This is an easy to use program,
 available in English,
 French,
 and Spanish,
 and it comes with a lot of data ready to use.
 It even has an option to save output as \SpecialChar LaTeX
 fragments,
 so that I can just include the results into this document,
 no muss,
 no fuss.
 Here is the Nerlove data in the form of a GRETL data set:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Data/nerlove.gdt}{nerlove.gdt} 
\end_layout

\end_inset

.
 Here the results of the Nerlove model from GRETL:
 
\begin_inset Graphics
	filename Examples/OLS/NerloveGretl.png
	lyxscale 25
	width 12cm

\end_inset

Fortunately,
 Gretl and my OLS program agree upon the results.
 I recommend using GRETL to repeat the examples that are done using Julia.
 
\end_layout

\begin_layout Standard
The previous properties hold for finite sample sizes.
 Before considering the asymptotic properties of the OLS estimator it is useful to review the MLE estimator,
 since under the assumption of normal errors the two estimators coincide.
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Prove that the split sample estimator used to generate figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Gauss-Markov-Result: split sample"
nolink "false"

\end_inset

 is unbiased.
\end_layout

\begin_layout Enumerate
Calculate the OLS estimates of the Nerlove model using Julia and GRETL,
 and provide printouts of the results.
 Interpret the results.
 
\end_layout

\begin_layout Enumerate
Do an analysis of whether or not there are influential observations for OLS estimation of the Nerlove model.
 Discuss.
\end_layout

\begin_layout Enumerate
Using GRETL,
 examine the residuals after OLS estimation and tell me whether or not you believe that the assumption of independent identically distributed normal errors is warranted.
 No need to do formal tests,
 just look at the plots.
 Print out any that you think are relevant,
 and interpret them.
\end_layout

\begin_layout Enumerate
For a random vector 
\begin_inset Formula $X\sim N(\mu_{x},\Sigma),$
\end_inset

 what is the distribution of 
\begin_inset Formula $AX+b$
\end_inset

,
 where 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are conformable matrices of constants?
\end_layout

\begin_layout Enumerate
Using Julia,
 write a little program that verifies that 
\begin_inset Formula $Tr(AB)=Tr(BA)$
\end_inset

 for 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 4x4 matrices of random numbers.
 Note:
 there is a Julia function 
\family typewriter
trace().
\end_layout

\begin_layout Enumerate
For the model with a constant and a single regressor,
 
\begin_inset Formula $y_{t}=\beta_{1}+\beta_{2}x_{t}+\epsilon_{t}$
\end_inset

,
 which satisfies the classical assumptions,
 prove that the variance of the OLS estimator declines to zero as the sample size increases.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Asymptotic properties of the least squares estimator
\end_layout

\begin_layout Standard
The OLS estimator under the classical assumptions is BLUE
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
BLUE 
\begin_inset Formula $\equiv$
\end_inset

 best linear unbiased estimator if I haven't defined it before
\end_layout

\end_inset

,
 for all sample sizes.
 Now let's see what happens when the sample size tends to infinity.
 
\end_layout

\begin_layout Section
Consistency
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & (X^{\prime}X)^{-1}X^{\prime}y\\
 & = & (X^{\prime}X)^{-1}X^{\prime}\left(X\beta+\varepsilon\right)\\
 & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & \beta_{0}+\left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{n}
\end{eqnarray*}

\end_inset

 Consider the last two terms.
 By assumption 
\begin_inset Formula $\lim_{n\rightarrow\infty}\left(\frac{X^{\prime}X}{n}\right)=Q_{X}\Rightarrow\lim_{n\rightarrow\infty}\left(\frac{X^{\prime}X}{n}\right)^{-1}=Q_{X}^{-1},$
\end_inset

 since the inverse of a nonsingular matrix is a continuous function of the elements of the matrix.
 Considering 
\begin_inset Formula $\frac{X^{\prime}\varepsilon}{n},$
\end_inset


\begin_inset Formula 
\[
\frac{X^{\prime}\varepsilon}{n}=\frac{1}{n}\sum_{t=1}^{n}x_{t}\varepsilon_{t}
\]

\end_inset

 Each 
\begin_inset Formula $x_{t}\varepsilon_{t}$
\end_inset

 has expectation zero,
 so 
\begin_inset Formula 
\[
E\left(\frac{X^{\prime}\varepsilon}{n}\right)=0
\]

\end_inset

The variance of each term is
\begin_inset Formula 
\begin{eqnarray*}
V\left(x_{t}\epsilon_{t}\right) & = & x_{t}x_{t}^{\prime}\sigma^{2}.
\end{eqnarray*}

\end_inset

As long as these are finite,
 and given a technical condition
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
For application of LLN's and CLT's,
 of which there are very many to choose from,
 I'm going to avoid the technicalities.
 Basically,
 as long as terms that make up an average have finite variances and are not too strongly dependent,
 one will be able to find a LLN or CLT to apply.
 Which one it is doesn't matter,
 we only need the result.
 When working with particular models,
 it will be more relevant to consider which particular theorems will apply.
\end_layout

\end_inset

,
 the Kolmogorov SLLN applies,
 so
\begin_inset Formula 
\[
\frac{1}{n}\sum_{t=1}^{n}x_{t}\varepsilon_{t}\overset{a.s.}{\rightarrow}0.
\]

\end_inset

 This implies that 
\begin_inset Formula 
\[
\hat{\beta}\overset{a.s.}{\rightarrow}\beta_{0}.
\]

\end_inset

 This is the property of 
\emph on
strong consistency:

\emph default
 the estimator converges in almost surely to the true value.
\end_layout

\begin_layout Itemize
The consistency proof does not use the normality assumption.
 
\end_layout

\begin_layout Itemize
Remember that almost sure convergence implies convergence in probability.
\end_layout

\begin_layout Section
Asymptotic normality
\end_layout

\begin_layout Standard
We've seen that the OLS estimator is normally distributed 
\emph on
under the assumption of normal errors.

\emph default
 If the error distribution is unknown,
 we of course don't know the distribution of the estimator.
 However,
 we can get asymptotic results.
 
\emph on
Assuming the distribution of
\emph default
 
\begin_inset Formula $\varepsilon$
\end_inset

 is unknown,
 but the the other classical assumptions hold:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
\hat{\beta}-\beta_{0} & = & (X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right) & = & \left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{\sqrt{n}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Now as before,
 
\begin_inset Formula $\left(\frac{X^{\prime}X}{n}\right)^{-1}\rightarrow Q_{X}^{-1}.$
\end_inset


\end_layout

\begin_layout Itemize
Considering 
\begin_inset Formula $\frac{X^{\prime}\varepsilon}{\sqrt{n}},$
\end_inset

 the limit of the variance is 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}V\left(\frac{X^{\prime}\varepsilon}{\sqrt{n}}\right) & = & \lim_{n\rightarrow\infty}E\left(\frac{X^{\prime}\epsilon\epsilon^{\prime}X}{n}\right)\\
 & = & \sigma_{0}^{2}Q_{X}
\end{eqnarray*}

\end_inset

 The mean is of course zero.
 To get asymptotic normality,
 we need to apply a CLT.
 The best known CLTs are for averages of IID terms,
 but CLTs exist for averages of dependent,
 non-identically distributed terms,
 too.
 The basic requirement is that variances of the terms in the average must not explode,
 and the terms in the average can not be too highly dependent.
 Without getting into the technical details,
 which are appropriate to discuss when working with some particular type of data,
 we assume one holds,
 so
\begin_inset Formula 
\[
\frac{X^{\prime}\varepsilon}{\sqrt{n}}\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}Q_{X}\right)
\]

\end_inset

 Therefore,
 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}Q_{X}^{-1}\right)\label{eq:asymp normality OLS}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
In summary,
 the OLS estimator is normally distributed in small and large samples if 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed.
 If 
\begin_inset Formula $\varepsilon$
\end_inset

 is not normally distributed,
 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is asymptotically normally distributed when a CLT can be applied.
\end_layout

\begin_layout Section
Asymptotic efficiency
\begin_inset CommandInset label
LatexCommand label
name "sec:Asymptotic-efficiency"

\end_inset


\end_layout

\begin_layout Standard
The least squares objective function is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
s(\beta) & = & \sum_{t=1}^{n}\left(y_{t}-x_{t}^{\prime}\beta\right)^{2}
\end{eqnarray*}

\end_inset

 Supposing that 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed,
 the model is 
\begin_inset Formula 
\[
y=X\beta_{0}+\varepsilon,
\]

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\varepsilon & \sim & N(0,\sigma_{0}^{2}I_{n}),\textrm{ so}\\
f(\varepsilon) & = & \prod_{t=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{\varepsilon_{t}^{2}}{2\sigma^{2}}\right)
\end{eqnarray*}

\end_inset

 The joint density for 
\begin_inset Formula $y$
\end_inset

 can be constructed using a change of variables.
 We have 
\begin_inset Formula $\varepsilon=y-X\beta,$
\end_inset

 so 
\begin_inset Formula $\frac{\partial\varepsilon}{\partial y^{\prime}}=I_{n}$
\end_inset

 and 
\begin_inset Formula $|\frac{\partial\varepsilon}{\partial y^{\prime}}|=1,$
\end_inset

 so 
\begin_inset Formula 
\[
f(y)=\prod_{t=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(y_{t}-x_{t}^{\prime}\beta)^{2}}{2\sigma^{2}}\right).
\]

\end_inset

 Taking logs,
 
\begin_inset Formula 
\[
\ln L(\beta,\sigma)=-n\ln\sqrt{2\pi}-n\ln\sigma-\sum_{t=1}^{n}\frac{\left(y_{t}-x_{t}'\beta\right)^{2}}{2\sigma^{2}}.
\]

\end_inset

 Maximizing this function with respect to 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 gives what is known as the maximum likelihood (ML) estimator.
 It turns out that ML estimators are asymptotically efficient,
 a concept that will be explained in detail later.
 It's clear that the first order conditions for the MLE of 
\begin_inset Formula $\beta_{0}$
\end_inset

 are the same as the first order conditions that define the OLS estimator (up to multiplication by a constant),
 so the OLS estimator of 
\begin_inset Formula $\beta$
\end_inset

 is also the ML estimator.
 
\emph on
The estimators are the same,
 under the present assumptions.

\emph default
 Therefore,
 their properties are the same.
 
\emph on
In particular,
 under the classical assumptions with normality,
 the OLS estimator
\emph default
 
\begin_inset Formula $\hat{\beta}$
\end_inset

 
\emph on
is asymptotically efficient.
 
\emph default
Note that one needs to make an assumption about the distribution of the errors to compute the ML estimator.
 If the errors had a distribution other than the normal,
 then the OLS estimator and the ML estimator would not coincide.
\end_layout

\begin_layout Standard
As we'll see later,
 it will be possible to use (iterated) linear estimation methods and still achieve asymptotic efficiency even if the assumption that 
\begin_inset Formula $Var(\varepsilon)\neq\sigma^{2}I_{n},$
\end_inset

 as long as 
\begin_inset Formula $\varepsilon$
\end_inset

 is still normally distributed.
 This is 
\series bold
not
\series default
 the case if 
\begin_inset Formula $\varepsilon$
\end_inset

 is nonnormal.
 In general with nonnormal errors it will be necessary to use nonlinear estimation methods to achieve asymptotically efficient estimation.
 
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Write an Octave program that generates a histogram for 
\begin_inset Formula $R$
\end_inset

 Monte Carlo replications of 
\begin_inset Formula $\sqrt{n}\left(\hat{\beta}_{j}-\beta_{j}\right)$
\end_inset

,
 where 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is the OLS estimator and 
\begin_inset Formula $\beta_{j}$
\end_inset

 is one of the 
\begin_inset Formula $k$
\end_inset

 slope parameters.
 
\begin_inset Formula $R$
\end_inset

 should be a large number,
 at least 1000.
 The model used to generate data should follow the classical assumptions,
 except that the errors should not be normally distributed (try 
\begin_inset Formula $U(-a,a)$
\end_inset

,
 
\begin_inset Formula $t(p)$
\end_inset

,
 
\begin_inset Formula $\chi^{2}(p)-p,$
\end_inset

 etc).
 Generate histograms for 
\begin_inset Formula $n\in\left\{ 20,50,100,1000\right\} $
\end_inset

.
 Do you observe evidence of asymptotic normality?
 Comment.
\end_layout

\begin_layout Enumerate
Consider the following regression model:
\begin_inset Formula 
\[
y_{i}=\beta_{1}x_{1i}+\beta_{2}x_{2i}+u_{i}
\]

\end_inset

where 
\begin_inset Formula $E(x_{1i})=0,E(x_{2i})=0,Var(x_{1i})=\sigma_{1}^{2},Var(x_{2i})=\sigma_{2}^{2}$
\end_inset

 
\begin_inset space \space{}
\end_inset

and 
\begin_inset Formula $Cov(x_{1i},x_{2i})$
\end_inset

=
\begin_inset Formula $\sigma_{12}.$
\end_inset

 The model satisfies the basic OLS
\begin_inset space \space{}
\end_inset

assumptions.
 However,
 a loose econometrician estimates the following model:
 
\begin_inset Formula 
\[
y_{i}=\beta_{1}x_{1i}+v_{i}
\]

\end_inset

where 
\begin_inset Formula $\tilde{\beta}_{1}$
\end_inset

 is the estimator of the parameter 
\begin_inset Formula $\beta_{1}.$
\end_inset


\begin_inset Newline newline
\end_inset

 a) Compute 
\begin_inset Formula $plim(\tilde{\beta}_{1})$
\end_inset

.
\begin_inset Newline newline
\end_inset

 b) Compute the asymptotic bias of 
\begin_inset Formula $\tilde{\beta}_{1}$
\end_inset

 (i.e.
 
\begin_inset Formula $plim(\tilde{\beta}_{1})-\beta_{1}$
\end_inset

).
\begin_inset Newline newline
\end_inset

 c) Under which conditions is 
\begin_inset Formula $\tilde{\beta}_{1}$
\end_inset

 a consistent estimate of 
\begin_inset Formula $\beta_{1}$
\end_inset

?
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Restrictions and hypothesis tests
\end_layout

\begin_layout Section
Exact linear restrictions
\end_layout

\begin_layout Standard
In many cases,
 economic theory suggests restrictions on the parameters of a model.
 For example,
 a demand function is supposed to be homogeneous of degree zero in prices and income.
 If we have a Cobb-Douglas (log-linear) model,
 
\begin_inset Formula 
\[
\ln q=\beta_{0}+\beta_{1}\ln p_{1}+\beta_{2}\ln p_{2}+\beta_{3}\ln m+\varepsilon,
\]

\end_inset

 then we need that 
\begin_inset Formula 
\[
k^{0}\ln q=\beta_{0}+\beta_{1}\ln kp_{1}+\beta_{2}\ln kp_{2}+\beta_{3}\ln km+\varepsilon,
\]

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\beta_{1}\ln p_{1}+\beta_{2}\ln p_{2}+\beta_{3}\ln m & = & \beta_{1}\ln kp_{1}+\beta_{2}\ln kp_{2}+\beta_{3}\ln km\\
 & = & \left(\ln k\right)(\beta_{1}+\beta_{2}+\beta_{3})+\beta_{1}\ln p_{1}+\beta_{2}\ln p_{2}+\beta_{3}\ln m.
\end{eqnarray*}

\end_inset

 The only way to guarantee this for arbitrary 
\begin_inset Formula $k$
\end_inset

 is to set 
\begin_inset Formula 
\[
\beta_{1}+\beta_{2}+\beta_{3}=0,
\]

\end_inset

 which is a 
\emph on
parameter restriction.

\emph default
 In particular,
 this is a linear equality restriction,
 which is probably the most commonly encountered case.
\end_layout

\begin_layout Subsection
Imposition
\end_layout

\begin_layout Standard
The general formulation of linear equality restrictions is the model 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
R\beta & = & r
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $R$
\end_inset

 is a 
\begin_inset Formula $Q\times K$
\end_inset

 matrix,
 
\begin_inset Formula $Q<K$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 is a 
\begin_inset Formula $Q\times1$
\end_inset

 vector of constants.
\end_layout

\begin_layout Itemize
We assume 
\begin_inset Formula $R$
\end_inset

 is of rank 
\begin_inset Formula $Q,$
\end_inset

 so that there are no redundant restrictions.
\end_layout

\begin_layout Itemize
We also assume that 
\begin_inset Formula $\exists\beta$
\end_inset

 that satisfies the restrictions:
 they aren't infeasible.
 
\end_layout

\begin_layout Standard
Let's consider how to estimate 
\begin_inset Formula $\beta$
\end_inset

 subject to the restrictions 
\begin_inset Formula $R\beta=r.$
\end_inset

 The most obvious approach is to set up the Lagrangean 
\begin_inset Formula 
\[
\min_{\beta,\lambda}s(\beta,\lambda)=\frac{1}{n}\left(y-X\beta\right)^{\prime}\left(y-X\beta\right)+2\lambda^{\prime}(R\beta-r).
\]

\end_inset

 The Lagrange multipliers are scaled by 2,
 which makes things less messy.
 The fonc are 
\begin_inset Formula 
\begin{eqnarray*}
D_{\beta}s(\hat{\beta},\hat{\lambda}) & = & -2X^{\prime}y+2X^{\prime}X\hat{\beta}_{R}+2R^{\prime}\hat{\lambda}\equiv0\\
D_{\lambda}s(\hat{\beta},\hat{\lambda}) & = & R\hat{\beta}_{R}-r\equiv0,
\end{eqnarray*}

\end_inset

 which can be written as 
\begin_inset Formula 
\[
\left[\begin{array}{cc}
X^{\prime}X & R^{\prime}\\
R & 0
\end{array}\right]\left[\begin{array}{c}
\hat{\beta}_{R}\\
\hat{\lambda}
\end{array}\right]=\left[\begin{array}{c}
X^{\prime}y\\
r
\end{array}\right].
\]

\end_inset

 We get 
\begin_inset Formula 
\[
\left[\begin{array}{c}
\hat{\beta}_{R}\\
\hat{\lambda}
\end{array}\right]=\left[\begin{array}{cc}
X^{\prime}X & R^{\prime}\\
R & 0
\end{array}\right]^{-1}\left[\begin{array}{c}
X^{\prime}y\\
r
\end{array}\right].
\]

\end_inset

 Maybe you're curious about how to invert a partitioned matrix?
 I can help you with that:
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1} & 0\\
-R\left(X^{\prime}X\right)^{-1} & I_{Q}
\end{array}\right]\left[\begin{array}{cc}
X^{\prime}X & R^{\prime}\\
R & 0
\end{array}\right] & \equiv & AB\\
 & = & \left[\begin{array}{cc}
I_{K} & \left(X^{\prime}X\right)^{-1}R^{\prime}\\
0 & -R\left(X^{\prime}X\right)^{-1}R^{\prime}
\end{array}\right]\\
 & \equiv & \left[\begin{array}{cc}
I_{K} & \left(X^{\prime}X\right)^{-1}R^{\prime}\\
0 & -P
\end{array}\right]\\
 & \equiv & C,
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{cc}
I_{K} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
0 & -P^{-1}
\end{array}\right]\left[\begin{array}{cc}
I_{K} & \left(X^{\prime}X\right)^{-1}R^{\prime}\\
0 & -P
\end{array}\right] & \equiv & DC\\
 & = & I_{K+Q},
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
DAB & = & I_{K+Q}\\
DA & = & B^{-1}\\
B^{-1} & = & \left[\begin{array}{cc}
I_{K} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
0 & -P^{-1}
\end{array}\right]\left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1} & 0\\
-R\left(X^{\prime}X\right)^{-1} & I_{Q}
\end{array}\right]\\
 & = & \left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R\left(X^{\prime}X\right)^{-1} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
P^{-1}R\left(X^{\prime}X\right)^{-1} & -P^{-1}
\end{array}\right],
\end{eqnarray*}

\end_inset

If you weren't curious about that,
 please start paying attention again.
 Also,
 note that we have made the definition 
\begin_inset Formula $P=R\left(X^{\prime}X\right)^{-1}R^{\prime}$
\end_inset

)
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{c}
\hat{\beta}_{R}\\
\hat{\lambda}
\end{array}\right] & = & \left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R\left(X^{\prime}X\right)^{-1} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
P^{-1}R\left(X^{\prime}X\right)^{-1} & -P^{-1}
\end{array}\right]\left[\begin{array}{c}
X^{\prime}y\\
r
\end{array}\right]\\
 & = & \left[\begin{array}{c}
\hat{\beta}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}\left(R\hat{\beta}-r\right)\\
P^{-1}\left(R\hat{\beta}-r\right)
\end{array}\right]\\
 & = & \left[\begin{array}{c}
\left(I_{K}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R\right)\\
P^{-1}R
\end{array}\right]\hat{\beta}+\left[\begin{array}{c}
(X^{\prime}X)^{-1}R^{\prime}P^{-1}r\\
-P^{-1}r
\end{array}\right]
\end{eqnarray*}

\end_inset

 The fact that 
\begin_inset Formula $\hat{\beta}_{R}$
\end_inset

 and 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 are linear functions of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 makes it easy to determine their distributions,
 since the distribution of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is already known.
 Recall that for 
\begin_inset Formula $x$
\end_inset

 a random vector,
 and for 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 a matrix and vector of constants,
 respectively,
 
\begin_inset Formula $Var\left(Ax+b\right)=AVar(x)A^{\prime}.$
\end_inset


\end_layout

\begin_layout Standard
Though this is the obvious way to go about finding the restricted estimator,
 an easier way,
 if the number of restrictions is small,
 is to impose them by substitution.
 Write 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X_{1}\beta_{1}+X_{2}\beta_{2}+\varepsilon\\
\left[\begin{array}{cc}
R_{1} & R_{2}\end{array}\right]\left[\begin{array}{c}
\beta_{1}\\
\beta_{2}
\end{array}\right] & = & r
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $R_{1}$
\end_inset

 is 
\begin_inset Formula $Q\times Q$
\end_inset

 nonsingular.
 Supposing the 
\begin_inset Formula $Q$
\end_inset

 restrictions are linearly independent,
 one can always make 
\begin_inset Formula $R_{1}$
\end_inset

 nonsingular by reorganizing the columns of 
\begin_inset Formula $X.$
\end_inset

 Then 
\begin_inset Formula 
\[
\beta_{1}=R_{1}^{-1}r-R_{1}^{-1}R_{2}\beta_{2}.
\]

\end_inset

 Substitute this into the model 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X_{1}R_{1}^{-1}r-X_{1}R_{1}^{-1}R_{2}\beta_{2}+X_{2}\beta_{2}+\varepsilon\\
y-X_{1}R_{1}^{-1}r & = & \left[X_{2}-X_{1}R_{1}^{-1}R_{2}\right]\beta_{2}+\varepsilon
\end{eqnarray*}

\end_inset

 or with the appropriate definitions,
 
\begin_inset Formula 
\[
y_{R}=X_{R}\beta_{2}+\varepsilon.
\]

\end_inset

 This model satisfies the classical assumptions,
 
\emph on
supposing the restriction is true
\emph default
.
 One can estimate by OLS.
 The variance of 
\begin_inset Formula $\hat{\beta}_{2}$
\end_inset

 is as before 
\begin_inset Formula 
\[
V(\hat{\beta}_{2})=\left(X_{R}^{\prime}X_{R}\right)^{-1}\sigma_{0}^{2}
\]

\end_inset

 and the estimator is 
\begin_inset Formula 
\[
\hat{V}(\hat{\beta}_{2})=\left(X_{R}^{\prime}X_{R}\right)^{-1}\hat{\sigma}^{2}
\]

\end_inset

 where one estimates 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 in the normal way,
 using the restricted model,
 
\emph on
i.e.,
\emph default

\begin_inset Formula 
\[
\widehat{\sigma_{0}^{2}}=\frac{\left(y_{R}-X_{R}\widehat{\beta_{2}}\right)^{\prime}\left(y_{R}-X_{R}\widehat{\beta_{2}}\right)}{n-\left(K-Q\right)}
\]

\end_inset

 To recover 
\begin_inset Formula $\hat{\beta}_{1},$
\end_inset

 use the restriction.
 To find the variance of 
\begin_inset Formula $\hat{\beta}_{1},$
\end_inset

 use the fact that it is a linear function of 
\begin_inset Formula $\hat{\beta}_{2},$
\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
V(\hat{\beta}_{1}) & = & R_{1}^{-1}R_{2}V(\hat{\beta}_{2})R_{2}^{\prime}\left(R_{1}^{-1}\right)^{\prime}\\
 & = & R_{1}^{-1}R_{2}\left(X_{2}^{\prime}X_{2}\right)^{-1}R_{2}^{\prime}\left(R_{1}^{-1}\right)^{\prime}\sigma_{0}^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Properties of the restricted estimator
\end_layout

\begin_layout Standard
We have that 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{R} & = & \hat{\beta}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}\left(R\hat{\beta}-r\right)\\
 & = & \hat{\beta}+(X^{\prime}X)^{-1}R^{\prime}P^{-1}r-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}X^{\prime}y\\
 & = & \beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon+(X^{\prime}X)^{-1}R^{\prime}P^{-1}\left[r-R\beta\right]-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
\hat{\beta}_{R}-\beta & = & (X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & + & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\left[r-R\beta\right]\\
 & - & (X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Mean squared error is 
\begin_inset Formula 
\[
MSE(\hat{\beta}_{R})=\mathcal{E}(\hat{\beta}_{R}-\beta)(\hat{\beta}_{R}-\beta)^{\prime}
\]

\end_inset

 Noting that the crosses between the second term and the other terms expect to zero,
 and that the cross of the first and third has a cancellation with the square of the third,
 we obtain 
\begin_inset Formula 
\begin{eqnarray*}
MSE(\hat{\beta}_{R}) & = & (X^{\prime}X)^{-1}\sigma^{2}\\
 & + & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\left[r-R\beta\right]\left[r-R\beta\right]^{\prime}P^{-1}R(X^{\prime}X)^{-1}\\
 & - & (X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}\sigma^{2}
\end{eqnarray*}

\end_inset

 So,
 the first term is the OLS covariance.
 The second term is PSD,
 and the third term is NSD.
\end_layout

\begin_layout Itemize
If the restriction is true,
 the second term is 0,
 so we are better off.
 
\emph on
True restrictions improve efficiency of estimation.
\end_layout

\begin_layout Itemize
If the restriction is false,
 we may be better or worse off,
 in terms of MSE,
 depending on the magnitudes of 
\begin_inset Formula $r-R\beta$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}.$
\end_inset


\end_layout

\begin_layout Section
Testing
\end_layout

\begin_layout Standard
In many cases,
 one wishes to test economic theories.
 If theory suggests parameter restrictions,
 as in the above homogeneity example,
 one can test theory by testing parameter restrictions.
 A number of tests are available.
 The first two (t and F) have a known small sample distributions,
 when the errors are normally distributed.
 The third and fourth (Wald and score) do not require normality of the errors,
 but their distributions are known only approximately,
 so that they are not exactly valid with finite samples.
\end_layout

\begin_layout Subsection
t-test
\end_layout

\begin_layout Standard
Suppose one has the model
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

 and one wishes to test the 
\emph on
single restriction
\emph default
 
\begin_inset Formula $H_{0}:$
\end_inset


\begin_inset Formula $R\beta=r$
\end_inset

 vs.
 
\begin_inset Formula $H_{A}:$
\end_inset


\begin_inset Formula $R\beta\neq r$
\end_inset

 .
 Under 
\begin_inset Formula $H_{0},$
\end_inset

 with normality of the errors,
 
\begin_inset Formula 
\[
R\hat{\beta}-r\sim N\left(0,R(X^{\prime}X)^{-1}R^{\prime}\sigma_{0}^{2}\right)
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{R\hat{\beta}-r}{\sqrt{R(X^{\prime}X)^{-1}R^{\prime}\sigma_{0}^{2}}}=\frac{R\hat{\beta}-r}{\sigma_{0}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}\sim N\left(0,1\right).
\]

\end_inset

 The problem is that 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 is unknown.
 One could use the consistent estimator 
\begin_inset Formula $\widehat{\sigma_{0}^{2}}$
\end_inset

 in place of 
\begin_inset Formula $\sigma_{0}^{2},$
\end_inset

 but the test would only be valid asymptotically in this case.
\end_layout

\begin_layout Proposition
\begin_inset Formula $\frac{N(0,1)}{\sqrt{\frac{\chi^{2}(q)}{q}}}\sim t(q)$
\end_inset


\end_layout

\begin_layout Proposition
as long as the 
\begin_inset Formula $N(0,1)$
\end_inset

 and the 
\begin_inset Formula $\chi^{2}(q)$
\end_inset

 are independent.
 
\end_layout

\begin_layout Standard
We need a few results on the 
\begin_inset Formula $\chi{}^{2}$
\end_inset

 distribution.
\end_layout

\begin_layout Proposition
If 
\begin_inset Formula $x\sim N(\mu,I_{n})$
\end_inset

 is a vector of 
\begin_inset Formula $n$
\end_inset

 independent r.v.'s.,
 then 
\begin_inset Formula $x^{\prime}x\sim\chi^{2}(n,\lambda)$
\end_inset

 where 
\begin_inset Formula $\lambda=\sum_{i}\mu_{i}^{2}=\mu^{\prime}\mu$
\end_inset

 is the 
\emph on
noncentrality parameter
\emph default
.
\end_layout

\begin_layout Standard
When a 
\begin_inset Formula $\chi^{2}$
\end_inset

 r.v.
 has the noncentrality parameter equal to zero,
 it is referred to as a central 
\begin_inset Formula $\chi^{2}$
\end_inset

 r.v.,
 and it's distribution is written as 
\begin_inset Formula $\chi^{2}(n),$
\end_inset

 suppressing the noncentrality parameter.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "quadratic form in inverse variance"

\end_inset

If the 
\begin_inset Formula $n$
\end_inset

 dimensional random vector 
\begin_inset Formula $x\sim N(0,V),$
\end_inset

 then 
\begin_inset Formula $x^{\prime}V^{-1}x\sim\chi^{2}(n).$
\end_inset


\end_layout

\begin_layout Standard
We'll prove this one as an indication of how the following unproven propositions could be proved.
\end_layout

\begin_layout Standard
Proof:
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

Factor 
\end_layout

\end_inset


\begin_inset Formula $V^{-1}$
\end_inset

 as 
\begin_inset Formula $P'P$
\end_inset

 (this is the Cholesky factorization,
 where 
\begin_inset Formula $P$
\end_inset

 is defined to be upper triangular).
 Then consider 
\begin_inset Formula $y=Px.$
\end_inset

 We have 
\begin_inset Formula 
\[
y\sim N(0,PVP')
\]

\end_inset

 but 
\begin_inset Formula 
\begin{eqnarray*}
VP'P & = & I_{n}\\
PVP'P & = & P
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula $PVP^{\prime}=I_{n}$
\end_inset

 and thus 
\begin_inset Formula $y\sim N(0,I_{n})$
\end_inset

.
 Thus 
\begin_inset Formula $y^{\prime}y\sim\chi^{2}(n)$
\end_inset

 but
\begin_inset Formula 
\[
y^{\prime}y=x^{\prime}P'Px=xV^{-1}x
\]

\end_inset

 and we get the result we wanted.
\end_layout

\begin_layout Standard
A more general proposition which implies this result is
\end_layout

\begin_layout Proposition
If the 
\begin_inset Formula $n$
\end_inset

 dimensional random vector 
\begin_inset Formula $x\sim N(0,V),$
\end_inset

 then 
\begin_inset Formula $x^{\prime}Bx\sim\chi^{2}(\rho(B))$
\end_inset

 if and only if 
\begin_inset Formula $BV$
\end_inset

 is idempotent.
\end_layout

\begin_layout Standard
An immediate consequence is
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "quadratic form in idempotent matrix"

\end_inset

If the random vector (of dimension 
\begin_inset Formula $n$
\end_inset

) 
\begin_inset Formula $x\sim N(0,I),$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is idempotent with rank 
\begin_inset Formula $r,$
\end_inset

 then 
\begin_inset Formula $x^{\prime}Bx\sim\chi^{2}(r)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Consider the random variable 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{\sigma_{0}^{2}} & = & \frac{\varepsilon^{\prime}M_{X}\varepsilon}{\sigma_{0}^{2}}\\
 & = & \left(\frac{\varepsilon}{\sigma_{0}}\right)^{\prime}M_{X}\left(\frac{\varepsilon}{\sigma_{0}}\right)\\
 & \sim & \chi^{2}(n-K)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proposition
If the random vector (of dimension 
\begin_inset Formula $n$
\end_inset

) 
\begin_inset Formula $x\sim N(0,I),$
\end_inset

 then 
\begin_inset Formula $Ax$
\end_inset

 and 
\begin_inset Formula $x^{\prime}Bx$
\end_inset

 are independent if 
\begin_inset Formula $AB=0.$
\end_inset


\end_layout

\begin_layout Standard
Now consider (remember that we have only one restriction in this case)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\frac{R\hat{\beta}-r}{\sigma_{0}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}}{\sqrt{\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{(n-K)\sigma_{0}^{2}}}}=\frac{R\hat{\beta}-r}{\widehat{\sigma_{0}}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}
\]

\end_inset

 This will have the 
\begin_inset Formula $t(n-K)$
\end_inset

 distribution if 
\begin_inset Formula $\hat{\beta}$
\end_inset

 and 
\begin_inset Formula $\hat{\varepsilon}^{\prime}\hat{\varepsilon}$
\end_inset

 are independent.
 But 
\begin_inset Formula $\hat{\beta}=\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon$
\end_inset

 and 
\begin_inset Formula 
\[
(X^{\prime}X)^{-1}X^{\prime}M_{X}=0,
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{R\hat{\beta}-r}{\widehat{\sigma_{0}}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}=\frac{R\hat{\beta}-r}{\hat{\sigma}_{R\hat{\beta}}}\sim t(n-K)
\]

\end_inset

 In particular,
 for the commonly encountered 
\emph on
test of significance
\emph default
 of an individual coefficient,
 for which 
\begin_inset Formula $H_{0}:\beta_{i}=0$
\end_inset

 vs.
 
\begin_inset Formula $H_{0}:\beta_{i}\neq0$
\end_inset

 ,
 the test statistic is 
\begin_inset Formula 
\[
\frac{\hat{\beta}_{i}}{\hat{\sigma}_{\hat{\beta}i}}\sim t(n-K)
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Note
\series default
:
 the 
\begin_inset Formula $t-$
\end_inset

 test is strictly valid only if the errors are actually normally distributed.
 If one has nonnormal errors,
 one could use the above asymptotic result to justify taking critical values from the 
\begin_inset Formula $N(0,1)$
\end_inset

 distribution,
 since 
\begin_inset Formula $t(n-K)\overset{d}{\rightarrow}N(0,1)$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 In practice,
 a conservative procedure is to take critical values from the 
\begin_inset Formula $t$
\end_inset

 distribution if nonnormality is suspected.
 This will reject 
\begin_inset Formula $H_{0}$
\end_inset

 less often since the 
\begin_inset Formula $t$
\end_inset

 distribution is fatter-tailed than is the normal.
\end_layout

\begin_layout Subsection
\begin_inset Formula $F$
\end_inset

 test
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $F$
\end_inset

 test allows testing multiple restrictions jointly.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "F statistic"

\end_inset

If 
\begin_inset Formula $x\sim\chi^{2}(r)$
\end_inset

 and 
\begin_inset Formula $y\sim\chi^{2}(s),$
\end_inset

 then 
\begin_inset Formula $\frac{x/r}{y/s}\sim F(r,s)$
\end_inset

,
 provided that 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are independent.
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Proposition
If the random vector (of dimension 
\begin_inset Formula $n$
\end_inset

) 
\begin_inset Formula $x\sim N(0,I),$
\end_inset

 then 
\begin_inset Formula $x^{\prime}Ax$
\end_inset


\end_layout

\begin_layout Proposition
and 
\begin_inset Formula $x^{\prime}Bx$
\end_inset

 are independent if 
\begin_inset Formula $AB=0.$
\end_inset


\end_layout

\begin_layout Standard
Using these results,
 and previous results on the 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution,
 it is simple to show that the following statistic has the 
\begin_inset Formula $F$
\end_inset

 distribution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F=\frac{\left(R\hat{\beta}-r\right)^{\prime}\left(R\left(X^{\prime}X\right)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)}{q\hat{\sigma}^{2}}\sim F(q,n-K).
\]

\end_inset

 A numerically equivalent expression is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\left(ESS_{R}-ESS_{U}\right)/q}{ESS_{U}/(n-K)}\sim F(q,n-K).
\]

\end_inset

 
\end_layout

\begin_layout Itemize

\series bold
Note:

\series default
 The 
\begin_inset Formula $F$
\end_inset

 test is strictly valid only if the errors are truly normally distributed.
 The following tests will be appropriate when one cannot assume normally distributed errors.
\end_layout

\begin_layout Subsection
Wald-type tests
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $F$
\end_inset

 tests require normality of the errors.
 The Wald test does not,
 but it is an asymptotic test - it is only approximately valid in finite samples.
\end_layout

\begin_layout Standard
The Wald principle is based on the idea that if a restriction is true,
 the unrestricted model should 
\begin_inset Quotes eld
\end_inset

approximately
\begin_inset Quotes erd
\end_inset

 satisfy the restriction.
 Given that the least squares estimator is asymptotically normally distributed:
 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}Q_{X}^{-1}\right)
\]

\end_inset

 then under 
\begin_inset Formula $H_{0}:R\beta_{0}=r,$
\end_inset

 we have 
\begin_inset Formula 
\[
\sqrt{n}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)
\]

\end_inset

 so by Proposition [
\begin_inset CommandInset ref
LatexCommand ref
reference "quadratic form in inverse variance"
nolink "false"

\end_inset

] 
\begin_inset Formula 
\[
n\left(R\hat{\beta}-r\right)^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 Note that 
\begin_inset Formula $Q_{X}^{-1}$
\end_inset

 or 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 are not observable.
 The test statistic we use substitutes the consistent estimators.
 Use 
\begin_inset Formula $(X^{\prime}X/n)^{-1}$
\end_inset

 as the consistent estimator of 
\begin_inset Formula $Q_{X}^{-1}.$
\end_inset

 With this,
 there is a cancellation of 
\begin_inset Formula $n^{\prime}s,$
\end_inset

 and the statistic to use is 
\begin_inset Formula 
\[
\left(R\hat{\beta}-r\right)^{\prime}\left(\widehat{\sigma_{0}^{2}}R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset


\end_layout

\begin_layout Itemize
The Wald test is a simple way to test restrictions without having to estimate the restricted model.
\end_layout

\begin_layout Itemize
Note that this formula is similar to one of the formulae provided for the 
\begin_inset Formula $F$
\end_inset

 test.
 
\end_layout

\begin_layout Subsection
Score-type tests (Rao tests,
 Lagrange multiplier tests)
\end_layout

\begin_layout Standard
The score test is another asymptotically valid test that does not require normality of the errors.
\end_layout

\begin_layout Standard
In some cases,
 an unrestricted model may be nonlinear in the parameters,
 but the model is linear in the parameters under the null hypothesis.
 For example,
 the model 
\begin_inset Formula 
\[
y=\left(X\beta\right)^{\gamma}+\varepsilon
\]

\end_inset

 is nonlinear in 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\gamma,$
\end_inset

 but is linear in 
\begin_inset Formula $\beta$
\end_inset

 under 
\begin_inset Formula $H_{0}:\gamma=1.$
\end_inset

 Estimation of nonlinear models is a bit more complicated,
 so one might prefer to have a test based upon the restricted,
 linear model.
 The score test is useful in this situation.
\end_layout

\begin_layout Itemize
Score-type tests are based upon the general principle that the gradient vector of the unrestricted model,
 evaluated at the restricted estimate,
 should be asymptotically normally distributed with mean zero,
 if the restrictions are true.
 The original development was for ML estimation,
 but the principle is valid for a wide variety of estimation methods.
 
\end_layout

\begin_layout Standard
We have seen that 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\lambda} & = & \left(R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\\
 & = & P^{-1}\left(R\hat{\beta}-r\right)
\end{eqnarray*}

\end_inset

so
\begin_inset Formula 
\[
\sqrt{n}\hat{P\lambda}=\sqrt{n}\left(R\hat{\beta}-r\right)
\]

\end_inset

Given that 
\begin_inset Formula 
\[
\sqrt{n}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)
\]

\end_inset

 under the null hypothesis,
 we obtain 
\begin_inset Formula 
\[
\sqrt{n}\hat{P\lambda}\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)
\]

\end_inset

 So
\begin_inset Formula 
\[
\left(\sqrt{n}\hat{P\lambda}\right)^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}\left(\sqrt{n}\hat{P\lambda}\right)\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

Noting that 
\begin_inset Formula $\lim nP=RQ_{X}^{-1}R^{\prime},$
\end_inset

 we obtain,
 
\begin_inset Formula 
\[
\hat{\lambda}^{\prime}\left(\frac{R(X^{\prime}X)^{-1}R^{\prime}}{\sigma_{0}^{2}}\right)\hat{\lambda}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 since the powers of 
\begin_inset Formula $n$
\end_inset

 cancel.
 To get a usable test statistic substitute a consistent estimator of 
\begin_inset Formula $\sigma_{0}^{2}.$
\end_inset


\end_layout

\begin_layout Itemize
This makes it clear why the test is sometimes referred to as a Lagrange multiplier test.
 It may seem that one needs the actual Lagrange multipliers to calculate this.
 If we impose the restrictions by substitution,
 these are not available.
 Note that the test can be written as 
\begin_inset Formula 
\[
\frac{\left(R^{\prime}\hat{\lambda}\right)^{\prime}(X^{\prime}X)^{-1}R^{\prime}\hat{\lambda}}{\sigma_{0}^{2}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 However,
 we can use the fonc for the restricted estimator:
 
\begin_inset Formula 
\[
-X^{\prime}y+X^{\prime}X\hat{\beta}_{R}+R^{\prime}\hat{\lambda}
\]

\end_inset

 to get that 
\begin_inset Formula 
\begin{eqnarray*}
R^{\prime}\hat{\lambda} & = & X^{\prime}(y-X\hat{\beta}_{R})\\
 & = & X^{\prime}\hat{\varepsilon}_{R}
\end{eqnarray*}

\end_inset

 Substituting this into the above,
 we get 
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}_{R}^{\prime}X(X^{\prime}X)^{-1}X^{\prime}\hat{\varepsilon}_{R}}{\sigma_{0}^{2}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 but this is simply 
\begin_inset Formula 
\[
\hat{\varepsilon}_{R}^{\prime}\frac{P_{X}}{\sigma_{0}^{2}}\hat{\varepsilon}_{R}\overset{d}{\rightarrow}\chi^{2}(q).
\]

\end_inset


\end_layout

\begin_layout Standard
To see why the test is also known as a score test,
 note that the fonc for restricted least squares 
\begin_inset Formula 
\[
-X^{\prime}y+X^{\prime}X\hat{\beta}_{R}+R^{\prime}\hat{\lambda}
\]

\end_inset

 give us 
\begin_inset Formula 
\[
R^{\prime}\hat{\lambda}=X^{\prime}y-X^{\prime}X\hat{\beta}_{R}
\]

\end_inset

 and the rhs is simply the gradient (score) of the unrestricted model,
 evaluated at the restricted estimator.
 The scores evaluated at the unrestricted estimate are identically zero.
 The logic behind the score test is that the scores evaluated at the restricted estimate should be approximately zero,
 if the restriction is true.
 The test is also known as a Rao test,
 since P.
 Rao first proposed it in 1948.
\end_layout

\begin_layout Section
The asymptotic equivalence of the LR,
 Wald and score tests
\end_layout

\begin_layout Standard
Note:
 the discussion of the LR test has been moved forward in these notes.
 I no longer teach the material in this section,
 but I'm leaving it here for reference.
 
\end_layout

\begin_layout Standard
We have seen that the three tests all converge to 
\begin_inset Formula $\chi^{2}$
\end_inset

 random variables.
 In fact,
 they all converge to the 
\emph on
same
\emph default
 
\begin_inset Formula $\chi^{2}$
\end_inset

 rv,
 under the null hypothesis.
 We'll show that the Wald and LR tests are asymptotically equivalent.
 We have seen that the Wald test is asymptotically equivalent to 
\begin_inset Formula 
\begin{equation}
W\overset{a}{=}n\left(R\hat{\beta}-r\right)^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}\chi^{2}(q)\label{eq:Wald}
\end{equation}

\end_inset

 Using 
\begin_inset Formula 
\[
\hat{\beta}-\beta_{0}=(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\]

\end_inset

 and 
\begin_inset Formula 
\[
R\hat{\beta}-r=R(\hat{\beta}-\beta_{0})
\]

\end_inset

 we get 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}R(\hat{\beta}-\beta_{0}) & = & \sqrt{n}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & R\left(\frac{X^{\prime}X}{n}\right)^{-1}n^{-1/2}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Substitute this into [
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Wald"
nolink "false"

\end_inset

] to get 
\begin_inset Formula 
\begin{eqnarray*}
W & \overset{a}{=} & n^{-1}\varepsilon^{\prime}XQ_{X}^{-1}R^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}RQ_{X}^{-1}X^{\prime}\varepsilon\\
 & \overset{a}{=} & \varepsilon^{\prime}X(X^{\prime}X)^{-1}R^{\prime}\left(\sigma_{0}^{2}R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & \overset{a}{=} & \frac{\varepsilon^{\prime}A(A^{\prime}A)^{-1}A^{\prime}\varepsilon}{\sigma_{0}^{2}}\\
 & \overset{a}{=} & \frac{\varepsilon^{\prime}P_{R}\varepsilon}{\sigma_{0}^{2}}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $P_{R}$
\end_inset

 is the projection matrix formed by the matrix 
\begin_inset Formula $X(X^{\prime}X)^{-1}R^{\prime}$
\end_inset

.
\end_layout

\begin_layout Itemize
Note that this matrix is idempotent and has 
\begin_inset Formula $q$
\end_inset

 columns,
 so the projection matrix has rank 
\begin_inset Formula $q.$
\end_inset


\end_layout

\begin_layout Standard
Now consider the likelihood ratio statistic 
\begin_inset Formula 
\begin{equation}
LR\overset{a}{=}n^{1/2}g(\theta_{0})^{\prime}\mathcal{I}(\theta_{0})^{-1}R^{\prime}\left(R\mathcal{I}(\theta_{0})^{-1}R^{\prime}\right)^{-1}R\mathcal{I}(\theta_{0})^{-1}n^{1/2}g(\theta_{0})\label{eq:LR}
\end{equation}

\end_inset

 Under normality,
 we have seen that the likelihood function is 
\begin_inset Formula 
\[
\ln L(\beta,\sigma)=-n\ln\sqrt{2\pi}-n\ln\sigma-\frac{1}{2}\frac{\left(y-X\beta\right)^{\prime}\left(y-X\beta\right)}{\sigma^{2}}.
\]

\end_inset

 Using this,
 
\begin_inset Formula 
\begin{eqnarray*}
g(\beta_{0}) & \equiv & D_{\beta}\frac{1}{n}\ln L(\beta,\sigma)\\
 & = & \frac{X^{\prime}(y-X\beta_{0})}{n\sigma^{2}}\\
 & = & \frac{X^{\prime}\varepsilon}{n\sigma^{2}}
\end{eqnarray*}

\end_inset

 Also,
 by the information matrix equality:
 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}(\theta_{0}) & = & -H_{\infty}(\theta_{0})\\
 & = & \lim-D_{\beta^{\prime}}g(\beta_{0})\\
 & = & \lim-D_{\beta^{\prime}}\frac{X^{\prime}(y-X\beta_{0})}{n\sigma^{2}}\\
 & = & \lim\frac{X^{\prime}X}{n\sigma^{2}}\\
 & = & \frac{Q_{X}}{\sigma^{2}}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
\mathcal{I}(\theta_{0})^{-1}=\sigma^{2}Q_{X}^{-1}
\]

\end_inset

 Substituting these last expressions into [
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LR"
nolink "false"

\end_inset

],
 we get 
\begin_inset Formula 
\begin{eqnarray*}
LR & \overset{a}{=} & \varepsilon^{\prime}X^{\prime}(X^{\prime}X)^{-1}R^{\prime}\left(\sigma_{0}^{2}R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & \overset{a}{=} & \frac{\varepsilon^{\prime}P_{R}\varepsilon}{\sigma_{0}^{2}}\\
 & \overset{a}{=} & W
\end{eqnarray*}

\end_inset

 This completes the proof that the Wald and LR tests are asymptotically equivalent.
 Similarly,
 one can show that,
 
\emph on
under the null hypothesis
\emph default
,
 
\begin_inset Formula 
\[
qF\overset{a}{=}W\overset{a}{=}LM\overset{a}{=}LR
\]

\end_inset


\end_layout

\begin_layout Itemize
The proof for the statistics except for 
\begin_inset Formula $LR$
\end_inset

 does not depend upon normality of the errors,
 as can be verified by examining the expressions for the statistics.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $LR$
\end_inset

 statistic 
\emph on
is
\emph default
 based upon distributional assumptions,
 since one can't write the likelihood function without them.
\end_layout

\begin_layout Itemize
However,
 due to the close relationship between the statistics 
\begin_inset Formula $qF$
\end_inset

 and 
\begin_inset Formula $LR,$
\end_inset

 supposing normality,
 the 
\begin_inset Formula $qF$
\end_inset

 statistic can be thought of as a 
\emph on
pseudo-LR statistic,

\emph default
 in that it's like a LR statistic in that it uses the value of the objective functions of the restricted and unrestricted models,
 but it doesn't require distributional assumptions.
\end_layout

\begin_layout Itemize
The presentation of the score and Wald tests has been done in the context of the linear model.
 This is readily generalizable to nonlinear models and/or other estimation methods.
 
\end_layout

\begin_layout Standard
Though the four statistics 
\emph on
are
\emph default
 asymptotically equivalent,
 they are numerically different in small samples.
 The numeric values of the tests also depend upon how 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is estimated,
 and we've already seen than there are several ways to do this.
 For example all of the following are consistent for 
\begin_inset Formula $\sigma^{2}$
\end_inset

 under 
\begin_inset Formula $H_{0}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
 & \frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{n-k}\\
 & \frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{n}\\
 & \frac{\hat{\varepsilon}_{R}^{\prime}\hat{\varepsilon}_{R}}{n-k+q}\\
 & \frac{\hat{\varepsilon}_{R}^{\prime}\hat{\varepsilon}_{R}}{n}
\end{eqnarray*}

\end_inset

 and in general the denominator call be replaced with any quantity 
\begin_inset Formula $a$
\end_inset

 such that 
\begin_inset Formula $\lim a/n=1.$
\end_inset


\end_layout

\begin_layout Standard
It can be shown,
 for linear regression models subject to linear restrictions,
 and if 
\begin_inset Formula $\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{n}$
\end_inset

 is used to calculate the Wald test and 
\begin_inset Formula $\frac{\hat{\varepsilon}_{R}^{\prime}\hat{\varepsilon}_{R}}{n}$
\end_inset

 is used for the score test,
 that 
\begin_inset Formula 
\[
W>LR>LM.
\]

\end_inset

 For this reason,
 the Wald test will always reject if the LR test rejects,
 and in turn the LR test rejects if the LM test rejects.
 This is a bit problematic:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

there is the possibility that by careful choice of the statistic used,
 one can manipulate reported results to favor or disfavor a hypothesis.
 A conservative/honest approach would be to report all three test statistics when they are available.
 In the case of linear models with normal errors the 
\begin_inset Formula $F\;$
\end_inset

test is to be preferred,
 since asymptotic approximations are not an issue.
\end_layout

\begin_layout Standard
The small sample behavior of the tests can be quite different.
 The true size (probability of rejection of the null when the null is true) of the Wald test is often dramatically higher than the nominal size associated with the asymptotic distribution.
 Likewise,
 the true size of the score test is often smaller than the nominal size.
\end_layout

\begin_layout Section
Interpretation of test statistics
\end_layout

\begin_layout Standard
Now that we have a menu of test statistics,
 we need to know how to use them.
\end_layout

\begin_layout Section
Confidence intervals
\end_layout

\begin_layout Standard
Confidence intervals for single coefficients are generated in the normal manner.
 Given the 
\begin_inset Formula $t$
\end_inset

 statistic 
\begin_inset Formula 
\[
t(\beta)=\frac{\hat{\beta}-\beta}{\widehat{\sigma_{\hat{\beta}}}}
\]

\end_inset

 a 
\begin_inset Formula $100\left(1-\alpha\right)\%$
\end_inset

 confidence interval for 
\begin_inset Formula $\beta_{0}$
\end_inset

 is defined by the bounds of the set of 
\begin_inset Formula $\beta$
\end_inset

 such that 
\begin_inset Formula $t(\beta)$
\end_inset

 does not reject 
\begin_inset Formula $H_{0}:\beta_{0}=\beta,$
\end_inset

 using a 
\begin_inset Formula $\alpha$
\end_inset

 significance level:
 
\begin_inset Formula 
\[
C(\alpha)=\{\beta:-c_{\alpha/2}<\frac{\hat{\beta}-\beta}{\widehat{\sigma_{\hat{\beta}}}}<c_{\alpha/2}\}
\]

\end_inset

 The set of such 
\begin_inset Formula $\beta$
\end_inset

 is the interval 
\begin_inset Formula 
\[
\hat{\beta}\pm\widehat{\sigma_{\hat{\beta}}}c_{\alpha/2}
\]

\end_inset


\end_layout

\begin_layout Standard
A confidence ellipse for two coefficients jointly would be,
 analogously,
 the set of {
\begin_inset Formula $\beta_{1},\beta_{2}\}$
\end_inset

 such that the 
\begin_inset Formula $F$
\end_inset

 (or some other test statistic) doesn't reject at the specified critical value.
 This generates an ellipse,
 if the estimators are correlated.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Joint and Individual Confidence Regions
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/JointConfidenceRegion.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The region is an ellipse,
 since the CI for an individual coefficient defines a (infinitely long) rectangle with total prob.
 mass 
\begin_inset Formula $1-\alpha,$
\end_inset

 since the other coefficient is marginalized (e.g.,
 can take on any value).
 Since the ellipse is bounded in both dimensions but also contains mass 
\begin_inset Formula $1-\alpha,$
\end_inset

 it must extend beyond the bounds of the individual CI.
\end_layout

\begin_layout Itemize
From the picture we can see that:
\end_layout

\begin_deeper
\begin_layout Itemize
Rejection of hypotheses individually does not imply that the joint test will reject.
\end_layout

\begin_layout Itemize
Joint rejection does not imply individual tests will reject.
 
\end_layout

\end_deeper
\begin_layout Section
Bootstrapping
\end_layout

\begin_layout Standard
When we rely on asymptotic theory to use the normal distribution-based tests and confidence intervals,
 we're often at serious risk of making important errors.
 If the sample size is small and errors are highly nonnormal,
 the small sample distribution of 
\begin_inset Formula $\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)$
\end_inset

 may be very different from its large sample distribution.
 Also,
 the distributions of test statistics may not resemble their limiting distributions at all.
 A means of trying to gain information on the small sample distribution of test statistics and estimators is the 
\emph on
bootstrap.

\emph default
 We'll consider a simple example,
 just to get the main idea.
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta_{0}+\varepsilon\\
\varepsilon & \sim & IID(0,\sigma_{0}^{2})\\
 & X\textrm{ is nonstochastic}
\end{eqnarray*}

\end_inset

 Given that the distribution of 
\begin_inset Formula $\varepsilon$
\end_inset

 is unknown,
 the distribution of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 will be unknown in small samples.
 However,
 since we have random sampling,
 we could generate 
\emph on
artificial data.

\emph default
 The steps are:
\end_layout

\begin_layout Enumerate
Draw 
\begin_inset Formula $n$
\end_inset

 observations from 
\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 
\series bold
with replacement
\series default
.
 Call this vector 
\begin_inset Formula $\tilde{\varepsilon}^{j}$
\end_inset

 (it's a 
\begin_inset Formula $n\times1).$
\end_inset


\end_layout

\begin_layout Enumerate
Then generate the data by 
\begin_inset Formula $\tilde{y}^{j}=X\hat{\beta}+\tilde{\varepsilon}^{j}$
\end_inset


\end_layout

\begin_layout Enumerate
Now take this and estimate 
\begin_inset Formula 
\[
\tilde{\beta}^{j}=(X^{\prime}X)^{-1}X^{\prime}\tilde{y}^{j}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
Save 
\begin_inset Formula $\tilde{\beta}^{j}$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat steps 1-4,
 until we have a large number,
 
\begin_inset Formula $J,$
\end_inset

 of 
\begin_inset Formula $\tilde{\beta}^{j}.$
\end_inset


\end_layout

\begin_layout Standard
With this,
 we can use the replications to calculate the 
\emph on
empirical distribution of
\emph default
 
\begin_inset Formula $\tilde{\beta}_{j}.$
\end_inset

 One way to form a 100(1-
\begin_inset Formula $\alpha)\%$
\end_inset

 confidence interval for 
\begin_inset Formula $\beta_{0}$
\end_inset

 would be to order the 
\begin_inset Formula $\tilde{\beta}^{j}$
\end_inset

 from smallest to largest,
 and drop the first and last 
\begin_inset Formula $J\alpha/2$
\end_inset

 of the replications,
 and use the remaining endpoints as the limits of the CI.
 Note that this will not give the shortest CI if the empirical distribution is skewed.
\end_layout

\begin_layout Itemize
Suppose one was interested in the distribution of some function of 
\begin_inset Formula $\hat{\beta},$
\end_inset

 for example a test statistic.
 Simple:
 just calculate the transformation for each 
\begin_inset Formula $j,$
\end_inset

 and work with the empirical distribution of the transformation.
\end_layout

\begin_layout Itemize
If the assumption of iid errors is too strong (for example if there is heteroscedasticity or autocorrelation,
 see below) one can work with a bootstrap defined by sampling from 
\begin_inset Formula $(y,x)$
\end_inset

 with replacement.
\end_layout

\begin_layout Itemize
How to choose 
\begin_inset Formula $J$
\end_inset

:
 
\begin_inset Formula $J$
\end_inset

 should be large enough that the results don't change with repetition of the entire bootstrap.
 This is easy to check.
 If you find the results change a lot,
 increase 
\begin_inset Formula $J$
\end_inset

 and try again.
\end_layout

\begin_layout Itemize
The bootstrap is based fundamentally on the idea that the empirical distribution of the sample data converges to the actual sampling distribution as 
\begin_inset Formula $n$
\end_inset

 becomes large,
 so statistics based on sampling from the empirical distribution should converge in distribution to statistics based on sampling from the actual sampling distribution.
\end_layout

\begin_layout Itemize
In finite samples,
 this doesn't hold.
 At a minimum,
 the bootstrap is a good way to check if asymptotic theory results offer a decent approximation to the small sample distribution.
\end_layout

\begin_layout Itemize
Bootstrapping can be used to test hypotheses.
 Basically,
 use the bootstrap to get an approximation to the empirical distribution of the test statistic under the alternative hypothesis,
 and use this to get critical values.
 Compare the test statistic calculated using the real data,
 under the null,
 to the bootstrap critical values.
 There are many variations on this theme,
 which we won't go into here.
 
\end_layout

\begin_layout Section
Wald test for nonlinear restrictions:
 the delta method
\end_layout

\begin_layout Standard
Testing nonlinear restrictions of a linear model is not much more difficult,
 at least when the model is linear.
 Since estimation subject to nonlinear restrictions requires nonlinear estimation methods,
 which are beyond the score of this course,
 we'll just consider the Wald test for nonlinear restrictions on a linear model.
\end_layout

\begin_layout Standard
Consider the 
\begin_inset Formula $q$
\end_inset

 nonlinear restrictions 
\begin_inset Formula 
\[
r(\beta_{0})=0.
\]

\end_inset

 where 
\begin_inset Formula $r(\cdot)\;$
\end_inset

 is a 
\begin_inset Formula $q$
\end_inset

-vector valued function.
 Write the derivative of the restriction evaluated at 
\begin_inset Formula $\beta$
\end_inset

 as 
\begin_inset Formula 
\[
\left.D_{\beta^{\prime}}r(\beta)\right|_{\beta}=R(\beta)
\]

\end_inset

 We suppose that the restrictions are not redundant in a neighborhood of 
\begin_inset Formula $\beta_{0}$
\end_inset

,
 so that 
\begin_inset Formula 
\[
\rho(R(\beta))=q
\]

\end_inset

 in a neighborhood of 
\begin_inset Formula $\beta_{0}.$
\end_inset

 Take a first order Taylor's series expansion of 
\begin_inset Formula $r(\hat{\beta})$
\end_inset

 about 
\begin_inset Formula $\beta_{0}$
\end_inset

:
 
\begin_inset Formula 
\[
r(\hat{\beta})=r(\beta_{0})+R(\beta^{*})(\hat{\beta}-\beta_{0})
\]

\end_inset

 where 
\begin_inset Formula $\beta^{*}$
\end_inset

 is a convex combination of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 and 
\begin_inset Formula $\beta_{0}.$
\end_inset

 Under the null hypothesis we have 
\begin_inset Formula 
\[
r(\hat{\beta})=R(\beta^{*})(\hat{\beta}-\beta_{0})
\]

\end_inset

 Due to consistency of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 we can replace 
\begin_inset Formula $\beta^{*}$
\end_inset

 by 
\begin_inset Formula $\beta_{0}$
\end_inset

,
 asymptotically,
 so 
\begin_inset Formula 
\[
\sqrt{n}r(\hat{\beta})\overset{a}{=}\sqrt{n}R(\beta_{0})(\hat{\beta}-\beta_{0})
\]

\end_inset

 We've already seen the distribution of 
\begin_inset Formula $\sqrt{n}(\hat{\beta}-\beta_{0}).$
\end_inset

 Using this we get 
\begin_inset Formula 
\[
\sqrt{n}r(\hat{\beta})\overset{d}{\rightarrow}N\left(0,R(\beta_{0})Q_{X}^{-1}R(\beta_{0})^{\prime}\sigma_{0}^{2}\right).
\]

\end_inset

 Considering the quadratic form 
\begin_inset Formula 
\[
\frac{nr(\hat{\beta})^{\prime}\left(R(\beta_{0})Q_{X}^{-1}R(\beta_{0})^{\prime}\right)^{-1}r(\hat{\beta})}{\sigma_{0}^{2}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 under the null hypothesis.
 Substituting consistent estimators for 
\begin_inset Formula $\beta_{0,}Q_{X}$
\end_inset

 and 
\begin_inset Formula $\sigma_{0}^{2},$
\end_inset

 the resulting statistic is 
\begin_inset Formula 
\[
\frac{r(\hat{\beta})^{\prime}\left(R(\hat{\beta})(X^{\prime}X)^{-1}R(\hat{\beta})^{\prime}\right)^{-1}r(\hat{\beta})}{\widehat{\sigma^{2}}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 under the null hypothesis.
\end_layout

\begin_layout Itemize
This is known in the literature as the 
\emph on
delta method
\emph default
,
 or as 
\emph on
Klein's approximation
\emph default
.
\end_layout

\begin_layout Itemize
Since this is a Wald test,
 it will tend to over-reject in finite samples.
 The score and LR tests are also possibilities,
 but they require estimation methods for nonlinear models,
 which aren't in the scope of this course.
 
\end_layout

\begin_layout Standard
Note that this also gives a convenient way to estimate nonlinear functions and associated asymptotic confidence intervals.
 If the nonlinear function 
\begin_inset Formula $r(\beta_{0})$
\end_inset

 is not hypothesized to be zero,
 we just have 
\begin_inset Formula 
\[
\sqrt{n}\left(r(\hat{\beta})-r(\beta_{0})\right)\overset{d}{\rightarrow}N\left(0,R(\beta_{0})Q_{X}^{-1}R(\beta_{0})^{\prime}\sigma_{0}^{2}\right)
\]

\end_inset

 so an approximation to the distribution of the function of the estimator is 
\begin_inset Formula 
\[
r(\hat{\beta})\approx N(r(\beta_{0}),R(\beta_{0})(X^{\prime}X)^{-1}R(\beta_{0})^{\prime}\sigma_{0}^{2})
\]

\end_inset

 For example,
 the vector of elasticities of a function 
\begin_inset Formula $f(x)$
\end_inset

 is 
\begin_inset Formula 
\[
\eta(x)=\frac{\partial f(x)}{\partial x}\odot\frac{x}{f(x)}
\]

\end_inset

where 
\begin_inset Formula $\odot$
\end_inset

 means element-by-element multiplication.
 Suppose we estimate a linear function 
\begin_inset Formula 
\[
y=x^{\prime}\beta+\varepsilon.
\]

\end_inset

 The elasticities of 
\begin_inset Formula $y$
\end_inset

 w.r.t.
 
\begin_inset Formula $x$
\end_inset

 are 
\begin_inset Formula 
\[
\eta(x)=\frac{\beta}{x^{\prime}\beta}\odot x
\]

\end_inset

(note that this is the entire vector of elasticities).
 The estimated elasticities are 
\begin_inset Formula 
\[
\widehat{\eta}(x)=\frac{\hat{\beta}}{x^{\prime}\hat{\beta}}\odot x
\]

\end_inset

 To calculate the estimated standard errors of all five elasticities,
 use 
\begin_inset Formula 
\begin{eqnarray*}
R(\beta) & = & \frac{\partial\eta(x)}{\partial\beta^{\prime}}\\
 & = & \frac{\left[\begin{array}{cccc}
x_{1} & 0 & \cdots & 0\\
0 & x_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & x_{k}
\end{array}\right]x'\beta-\left[\begin{array}{cccc}
\beta_{1}x_{1}^{2} & 0 & \cdots & 0\\
0 & \beta_{2}x_{2}^{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \beta_{k}x_{k}^{2}
\end{array}\right]}{(x'\beta)^{2}}.
\end{eqnarray*}

\end_inset

To get a consistent estimator just substitute in 
\begin_inset Formula $\hat{\beta}$
\end_inset

.
 Note that the elasticity and the standard error are functions of 
\begin_inset Formula $x.$
\end_inset

 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Restrictions/ExampleDeltaMethod.jl}{ExampleDeltaMethod.jl}
\end_layout

\end_inset

 shows how this can be done.
 
\end_layout

\begin_layout Standard
In many cases,
 nonlinear restrictions can also involve the data,
 not just the parameters.
 For example,
 consider a model of expenditure shares.
 Let 
\begin_inset Formula $x(p,m)$
\end_inset

 be a demand function,
 where 
\begin_inset Formula $p$
\end_inset

 is prices and 
\begin_inset Formula $m$
\end_inset

 is income.
 An expenditure share system for 
\begin_inset Formula $G$
\end_inset

 goods is 
\begin_inset Formula 
\[
s_{i}(p,m)=\frac{p_{i}x_{i}(p,m)}{m},i=1,2,...,G.
\]

\end_inset

 Now demand must be positive,
 and we assume that expenditures sum to income,
 so we have the restrictions 
\begin_inset Formula 
\begin{eqnarray*}
0 & \leq s_{i}(p,m)\leq1, & \forall i\\
\sum_{i=1}^{G}s_{i}(p,m) & = & 1
\end{eqnarray*}

\end_inset

 Suppose we postulate a linear model for the expenditure shares:
 
\begin_inset Formula 
\[
s_{i}(p,m)=\beta_{1}^{i}+p^{\prime}\beta_{p}^{i}+m\beta_{m}^{i}+\varepsilon^{i}
\]

\end_inset

 It is fairly easy to write restrictions such that the shares sum to one,
 but the restriction that the shares lie in the 
\begin_inset Formula $[0,1]$
\end_inset

 interval depends on both parameters and the values of 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $m.$
\end_inset

 It is impossible to impose the restriction that 
\begin_inset Formula $0\leq s_{i}(p,m)\leq1$
\end_inset

 for all possible 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $m.$
\end_inset

 In such cases,
 one might consider whether or not a linear model is a reasonable specification.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example: Nerlove restrictions"

\end_inset

Example:
 the Nerlove data
\end_layout

\begin_layout Standard
Remember that we in a previous example (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"
nolink "false"

\end_inset

) that the OLS results for the Nerlove model are 
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/OLS/nerlove.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $s_{K}=\beta_{K}<0$
\end_inset

,
 and that 
\begin_inset Formula $\beta_{L}+\beta_{F}+\beta_{K}\neq1$
\end_inset

.
\end_layout

\begin_layout Standard
Remember that if we have constant returns to scale,
 then 
\begin_inset Formula $\beta_{Q}=1,$
\end_inset

 and if there is homogeneity of degree 1 then 
\begin_inset Formula $\beta_{L}+\beta_{F}+\beta_{K}=1$
\end_inset

.
 We can test these hypotheses either separately or jointly.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Restrictions/NerloveRestrictions.jl}{NerloveRestrictions.jl} 
\end_layout

\end_inset

 imposes and tests CRTS and then HOD1.
 From it we obtain the results that follow:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Examples/Restrictions/CRTS.png
	width 15cm

\end_inset

 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Examples/Restrictions/HOD1.png

\end_inset


\end_layout

\begin_layout Standard
Notice that the input price coefficients in fact sum to 1 when HOD1 is imposed.
 HOD1 is not rejected at usual significance levels (
\emph on
e.g.,
 
\begin_inset Formula $\alpha=0.10$
\end_inset


\emph default
).
 Also,
 
\begin_inset Formula $R^{2}$
\end_inset

 does not drop much when the restriction is imposed,
 compared to the unrestricted results.
 For CRTS,
 you should note that 
\begin_inset Formula $\beta_{Q}=1$
\end_inset

,
 so the restriction is satisfied.
 Also note that the hypothesis that 
\begin_inset Formula $\beta_{Q}=1$
\end_inset

 is rejected by the test statistics at all reasonable significance levels.
 Note that 
\begin_inset Formula $R^{2}$
\end_inset

 drops quite a bit when imposing CRTS.
 If you look at the unrestricted estimation results,
 you can see that a t-test for 
\begin_inset Formula $\beta_{Q}=1$
\end_inset

 also rejects,
 and that a confidence interval for 
\begin_inset Formula $\beta_{Q}$
\end_inset

 does not overlap 1.
 
\end_layout

\begin_layout Standard
From the point of view of neoclassical economic theory,
 these results are not anomalous:
 HOD1 is an implication of the theory,
 but CRTS is not.
\end_layout

\begin_layout Exercise
Modify the NerloveRestrictions.jl program to impose and test the restrictions jointly.
\end_layout

\begin_layout Paragraph
\begin_inset CommandInset label
LatexCommand label
name "Chow test"

\end_inset

The Chow test
\end_layout

\begin_layout Standard
Since CRTS is rejected,
 let's examine the possibilities more carefully.
 Recall that the data is sorted by output (the third column).
 Define 5 subsamples of firms,
 with the first group being the 29 firms with the lowest output levels,
 then the next 29 firms,
 etc.
 The five subsamples can be indexed by 
\begin_inset Formula $j=1,2,...,5,$
\end_inset

 where 
\begin_inset Formula $j=1$
\end_inset

 for 
\begin_inset Formula $t=1,2,...29$
\end_inset

,
 
\begin_inset Formula $j=2$
\end_inset

 for 
\begin_inset Formula $t=30,31,...58$
\end_inset

,
 etc.
 Define 
\emph on
indicator variables
\emph default
 
\begin_inset Formula $D_{1},D_{2},...,D_{5}$
\end_inset

 where
\begin_inset Formula 
\begin{align*}
D_{1} & =\begin{cases}
1 & t\in\{1,2,...29\}\\
0 & t\notin\{1,2,...29\}
\end{cases}\\
D_{2} & =\begin{cases}
1 & t\in\{30,31,...58\}\\
0 & t\notin\{30,31,...58\}
\end{cases}\\
\vdots\\
D_{5} & =\begin{cases}
1 & t\in\{117,118,...,145\}\\
0 & t\notin\{117,118,...,145\}
\end{cases}
\end{align*}

\end_inset

 Define the model
\begin_inset Formula 
\begin{equation}
\ln C_{t}=\sum_{j=1}^{5}\alpha_{1}D_{j}+\sum_{j=1}^{5}\gamma_{j}D_{j}\ln Q_{t}+\sum_{j=1}^{5}\beta_{Lj}D_{j}\ln P_{Lt}+\sum_{j=1}^{5}\beta_{Fj}D_{j}\ln P_{Ft}+\sum_{j=1}^{5}\beta_{Kj}D_{j}\ln P_{Kt}+\epsilon_{t}
\end{equation}

\end_inset

Note that the first column of nerlove.data indicates this way of breaking up the sample,
 and provides and easy way of defining the indicator variables.
 The new model may be written as
\begin_inset Formula 
\begin{equation}
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
\\y_{5}
\end{array}\right]=\left[\begin{array}{lllll}
X_{1} & 0 & \cdots &  & 0\\
0 & X_{2}\\
\vdots &  & X_{3}\\
 &  &  & X_{4} & 0\\
0 &  &  &  & X_{5}
\end{array}\right]\left[\begin{array}{l}
\beta^{1}\\
\beta^{2}\\
\\\\\beta^{5}
\end{array}\right]+\left[\begin{array}{l}
\epsilon^{1}\\
\epsilon^{2}\\
\vdots\\
\\\epsilon^{5}
\end{array}\right]\label{nerlove - all coefficients vary}
\end{equation}

\end_inset

 where 
\begin_inset Formula $y_{1}$
\end_inset

 is 29
\begin_inset Formula $\times1,$
\end_inset

 
\begin_inset Formula $X_{1}$
\end_inset

 is 29
\begin_inset Formula $\times5,$
\end_inset

 
\begin_inset Formula $\beta^{j}$
\end_inset

 is the 
\begin_inset Formula $5\times1$
\end_inset

 vector of coefficients for the 
\begin_inset Formula $j^{th}$
\end_inset

 subsample (e.g.,
 
\begin_inset Formula $\beta^{1}=\mbox{\left(\alpha_{1},\gamma_{1},\beta_{L1},\beta_{F1},\beta_{K1}\right)\ensuremath{^{\prime}}}$
\end_inset

),
 and 
\begin_inset Formula $\epsilon^{j}$
\end_inset

 is the 
\begin_inset Formula $29\times1$
\end_inset

 vector of errors for the 
\begin_inset Formula $j^{th}$
\end_inset

 subsample.
\end_layout

\begin_layout Standard
The Julia program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Restrictions/ChowTest.jl}{Restrictions/ChowTest.jl}
\end_layout

\end_inset

 estimates the above model.
 It also tests the hypothesis that the five subsamples share the same parameter vector,
 or in other words,
 that there is coefficient stability across the five subsamples.
 The null to test is that the parameter vectors for the separate groups are all the same,
 that is,
\begin_inset Formula 
\[
\beta^{1}=\beta^{2}=\beta^{3}=\beta^{4}=\beta^{5}
\]

\end_inset

 This type of test,
 that parameters are constant across different sets of data,
 is sometimes referred to as a 
\emph on
Chow
\emph default
 
\emph on
test.
\end_layout

\begin_layout Itemize
There are 20 restrictions.
 If that's not clear to you,
 look at the Julia program.
\end_layout

\begin_layout Itemize
The restrictions are rejected at all conventional significance levels.
\end_layout

\begin_layout Standard
Since the restrictions are rejected,
 we should probably use the unrestricted model for analysis.
 What is the pattern of RTS as a function of the output group (small to large)?
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Nerlove RTS"
nolink "false"

\end_inset

 plots RTS.
 We can see that there is increasing RTS for small firms,
 but that RTS is approximately constant for large firms.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Nerlove RTS"

\end_inset

RTS as a function of firm size
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Restrictions/rts.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Using the Chow test on the Nerlove model,
 we reject that there is coefficient stability across the 5 groups.
 But perhaps we could restrict the input price coefficients to be the same but let the constant and output coefficients vary by group size.
 This new model is
\begin_inset Formula 
\begin{equation}
\ln C=\sum_{j=1}^{5}\alpha_{j}D_{j}+\sum_{j=1}^{5}\gamma_{j}D_{j}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon\label{Nerlove, favorite model}
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
estimate this model by OLS,
 giving 
\begin_inset Formula $R^{2}$
\end_inset

,
 estimated standard errors for coefficients,
 t-statistics for tests of significance,
 and the associated p-values.
 Interpret the results in detail.
\end_layout

\begin_layout Enumerate
Test the restrictions implied by this model (relative to the model that lets all coefficients vary across groups) using the F,
 qF,
 Wald,
 score and likelihood ratio tests.
 Comment on the results.
\end_layout

\begin_layout Enumerate
Estimate this model but imposing the HOD1 restriction,
 
\emph on
using an OLS
\emph default
 estimation program.
 Give estimated standard errors for all coefficients.
\end_layout

\begin_layout Enumerate
Plot the estimated RTS parameters as a function of firm size.
 Compare the plot to that given in the notes for the unrestricted model.
 Comment on the results.
\end_layout

\end_deeper
\begin_layout Enumerate
For the model of the above question,
 compute 95% confidence intervals for RTS for each of the 5 groups of firms,
 using the delta method to compute standard errors.
 Comment on the results.
\end_layout

\begin_layout Enumerate
Perform a Monte Carlo study that generates data from the model 
\begin_inset Formula 
\[
y=-2+1x_{2}+1x_{3}+\epsilon
\]

\end_inset

where the sample size is 30,
 
\begin_inset Formula $x_{2}$
\end_inset

 and 
\begin_inset Formula $x_{3}$
\end_inset

 are independently uniformly distributed on 
\begin_inset Formula $[0,1]$
\end_inset

 and 
\begin_inset Formula $\epsilon\sim IIN(0,1)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Compare the means and standard errors of the estimated coefficients using OLS and restricted OLS,
 imposing the restriction that 
\begin_inset Formula $\beta_{2}+\beta_{3}=2.$
\end_inset


\end_layout

\begin_layout Enumerate
Compare the means and standard errors of the estimated coefficients using OLS and restricted OLS,
 imposing the restriction that 
\begin_inset Formula $\beta_{2}+\beta_{3}=1.$
\end_inset


\end_layout

\begin_layout Enumerate
Discuss the results.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Stochastic-regressors"

\end_inset

Stochastic regressors
\end_layout

\begin_layout Standard
Up to now we have treated the regressors as fixed,
 which is clearly unrealistic.
 Now we will assume they are random.
 There are several ways to think of the problem.
 First,
 if we are interested in an analysis 
\emph on
conditional
\emph default
 on the explanatory variables,
 then it is irrelevant if they are stochastic or not,
 since conditional on the values of they regressors take on,
 they are nonstochastic,
 which is the case already considered.
\end_layout

\begin_layout Itemize
In cross-sectional analysis it is usually reasonable to make the analysis conditional on the regressors.
\end_layout

\begin_layout Itemize
In dynamic models,
 where 
\begin_inset Formula $y_{t}$
\end_inset

 may depend on 
\begin_inset Formula $y_{t-1},$
\end_inset

 a conditional analysis is not sufficiently general,
 since we may want to predict into the future many periods out,
 so we need to consider the behavior of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 and the relevant test statistics unconditional on 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Standard
The model we'll deal will involve a combination of the following assumptions
\end_layout

\begin_layout Assumption

\series bold
Linearity
\series default
:
 the model is a linear function of the parameter vector 
\begin_inset Formula $\beta_{0}:$
\end_inset


\begin_inset Formula 
\[
y_{t}=x_{t}^{\prime}\beta_{0}+\varepsilon_{t},
\]

\end_inset

 or in matrix form,
 
\begin_inset Formula 
\[
y=X\beta_{0}+\varepsilon,
\]

\end_inset

 where 
\begin_inset Formula $y$
\end_inset

 is 
\begin_inset Formula $n\times1,$
\end_inset

 
\begin_inset Formula $X=\left(\begin{array}{cccc}
x_{1} & x_{2} & \cdots & x_{n}\end{array}\right)^{\prime},$
\end_inset

 where 
\begin_inset Formula $x_{t}$
\end_inset

 is 
\begin_inset Formula $K\times1,$
\end_inset

 and 
\begin_inset Formula $\beta_{0}$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are conformable.
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Stochastic,
 linearly independent regressors
\end_layout

\begin_layout Assumption
\begin_inset Formula $X$
\end_inset

 has rank 
\begin_inset Formula $K$
\end_inset

 with probability 1
\end_layout

\begin_layout Assumption
\begin_inset Formula $X$
\end_inset

 is stochastic
\end_layout

\begin_layout Assumption
\begin_inset Formula $\lim_{n\rightarrow\infty}\Pr\left(\frac{1}{n}X^{\prime}X=Q_{X}\right)=1,$
\end_inset

 where 
\begin_inset Formula $Q_{X}$
\end_inset

 is a finite positive definite matrix.
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Central limit theorem
\end_layout

\begin_layout Assumption
\begin_inset Formula $n^{-1/2}X^{\prime}\varepsilon\overset{d}{\rightarrow}N(0,Q_{X}\sigma_{0}^{2})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Normality (Optional)
\series default
:
 
\begin_inset Formula $\varepsilon|X\sim N(0,\sigma^{2}I_{n})$
\end_inset

:
 
\begin_inset Formula $\epsilon$
\end_inset

 is normally distributed 
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Strongly exogenous
\series default
 
\series bold
regressors
\series default
.
 The regressors 
\begin_inset Formula $\mathbf{X}$
\end_inset

 are strongly exogenous if 
\begin_inset Formula 
\begin{eqnarray}
\mathcal{E}(\varepsilon_{t}|\mathbf{X}) & = & 0,\forall t\label{assumption: strong exogeneity}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
\begin_inset CommandInset label
LatexCommand label
name "ass:Weakly-exogenous-regressors:"

\end_inset

Weakly exogenous regressors
\series default
:
 The regressors are weakly exogenous if 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\varepsilon_{t}|\mathbf{x}_{t}) & = & 0,\forall t
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In both cases,
 
\begin_inset Formula $\mathbf{x}_{t}^{\prime}\beta$
\end_inset

 is the conditional mean of 
\begin_inset Formula $y_{t}$
\end_inset

 given 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

:
 
\begin_inset Formula $E(y_{t}|\mathbf{x}_{t})=\mathbf{x}_{t}^{\prime}\beta$
\end_inset


\end_layout

\begin_layout Section
Case 1
\end_layout

\begin_layout Standard

\emph on
Normality of
\emph default
 
\begin_inset Formula $\varepsilon,$
\end_inset

 strongly exogenous regressors
\end_layout

\begin_layout Standard
In this case,
 
\begin_inset Formula 
\[
\hat{\beta}=\beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\]

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\hat{\beta}|X) & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\mathcal{E}(\varepsilon|X)\\
 & = & \beta_{0}
\end{eqnarray*}

\end_inset

 and since this holds for all 
\begin_inset Formula $X,$
\end_inset

 
\begin_inset Formula $E(\hat{\beta})=\beta$
\end_inset

,
 unconditional on 
\begin_inset Formula $X.$
\end_inset

 Likewise,
\begin_inset Formula 
\[
\hat{\beta}|X\sim N\left(\beta,(X^{\prime}X)^{-1}\sigma_{0}^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
If the density of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula $d\mu(X),$
\end_inset

 the marginal density of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is obtained by multiplying the conditional density by 
\begin_inset Formula $d\mu(X)$
\end_inset

 and integrating over 
\begin_inset Formula $X.$
\end_inset

 Doing this leads to a nonnormal density for 
\begin_inset Formula $\hat{\beta},$
\end_inset

 in small samples.
\end_layout

\begin_layout Itemize
However,
 conditional on 
\begin_inset Formula $X,$
\end_inset

 the usual test statistics have the 
\begin_inset Formula $t,$
\end_inset

 
\begin_inset Formula $F$
\end_inset

 and 
\begin_inset Formula $\chi^{2}$
\end_inset

 distributions.
 
\emph on
Importantly,

\emph default
 these distributions don't depend on 
\begin_inset Formula $X,$
\end_inset

 so when marginalizing to obtain the unconditional distribution,
 nothing changes.
 The tests are valid in small samples.
\end_layout

\begin_layout Itemize
Summary:
 When 
\begin_inset Formula $X$
\end_inset

 is stochastic but strongly exogenous and 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\hat{\beta}$
\end_inset

 is unbiased
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\hat{\beta}$
\end_inset

 is nonnormally distributed
\end_layout

\begin_layout Enumerate
The usual test statistics have the same distribution as with nonstochastic 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Enumerate
The Gauss-Markov theorem still holds,
 since it holds conditionally on 
\begin_inset Formula $X,$
\end_inset

 and this is true for all 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Enumerate
Asymptotic properties are treated in the next section.
 
\end_layout

\end_deeper
\begin_layout Section
Case 2
\end_layout

\begin_layout Standard
\begin_inset Formula $\varepsilon$
\end_inset


\emph on
 nonnormally distributed,
 strongly exogenous regressors
\end_layout

\begin_layout Standard
The unbiasedness of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 carries through as before.
 However,
 the argument regarding test statistics doesn't hold,
 due to nonnormality of 
\begin_inset Formula $\varepsilon.$
\end_inset

 Still,
 we have 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & \beta_{0}+\left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{n}
\end{eqnarray*}

\end_inset

 Now 
\begin_inset Formula 
\[
\left(\frac{X^{\prime}X}{n}\right)^{-1}\overset{p}{\rightarrow}Q_{X}^{-1}
\]

\end_inset

 by assumption,
 and 
\begin_inset Formula 
\[
\frac{X^{\prime}\varepsilon}{n}=\frac{n^{-1/2}X^{\prime}\varepsilon}{\sqrt{n}}\overset{p}{\rightarrow}0
\]

\end_inset

 since the numerator converges to a 
\begin_inset Formula $N(0,Q_{X}\sigma^{2})$
\end_inset

 r.v.
 and the denominator still goes to infinity.
 We have unbiasedness and the variance disappearing,
 so,
 
\emph on
the estimator is consistent
\emph default
:
 
\begin_inset Formula 
\[
\hat{\beta}\overset{p}{\rightarrow}\beta_{0}.
\]

\end_inset


\end_layout

\begin_layout Standard
Considering the asymptotic distribution 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right) & = & \sqrt{n}\left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{n}\\
 & = & \left(\frac{X^{\prime}X}{n}\right)^{-1}n^{-1/2}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\overset{d}{\rightarrow}N(0,Q_{X}^{-1}\sigma_{0}^{2})
\]

\end_inset

 directly following the assumptions.
 
\emph on
Asymptotic normality of the estimator still holds.

\emph default
 Since the asymptotic results on all test statistics only require this,
 all the previous asymptotic results on test statistics are also valid in this case.
\end_layout

\begin_layout Itemize
Summary:
 Under strongly exogenous regressors,
 with 
\begin_inset Formula $\varepsilon$
\end_inset

 normal or nonnormal,
 
\begin_inset Formula $\hat{\beta}$
\end_inset

 has the properties:
\end_layout

\begin_deeper
\begin_layout Enumerate
Unbiasedness
\end_layout

\begin_layout Enumerate
Consistency
\end_layout

\begin_layout Enumerate
Gauss-Markov theorem holds,
 since it holds in the previous case and doesn't depend on normality.
\end_layout

\begin_layout Enumerate
Asymptotic normality
\end_layout

\begin_layout Enumerate
Tests are asymptotically valid
\end_layout

\begin_layout Enumerate
Tests are not valid in small samples if the error is normally distributed
\end_layout

\end_deeper
\begin_layout Section
Case 3
\end_layout

\begin_layout Standard

\emph on
Weakly exogenous regressors
\end_layout

\begin_layout Standard
An important class of models are 
\emph on
dynamic models
\emph default
,
 where lagged dependent variables have an impact on the current value.
 A simple version of these models that captures the important points is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & z_{t}^{\prime}\alpha+\sum_{s=1}^{p}\gamma_{s}y_{t-s}+\varepsilon_{t}\\
 & = & x_{t}^{\prime}\beta+\varepsilon_{t}
\end{eqnarray*}

\end_inset

 where now 
\begin_inset Formula $x_{t}$
\end_inset

 contains lagged dependent variables.
 Clearly,
 even with 
\begin_inset Formula $E(\epsilon_{t}|\mathbf{x}_{t})=0,$
\end_inset

 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are not uncorrelated,
 so one can't show unbiasedness.
 For example,
 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon_{t-1}x_{t})\neq0
\]

\end_inset

 since 
\begin_inset Formula $x_{t}$
\end_inset

 contains 
\begin_inset Formula $y_{t-1}$
\end_inset

 (which is a function of 
\begin_inset Formula $\varepsilon_{t-1})$
\end_inset

 as an element.
\end_layout

\begin_layout Itemize
This fact implies that all of the small sample properties such as unbiasedness,
 Gauss-Markov theorem,
 and small sample validity of test statistics 
\emph on
do not hold
\emph default
 in this case.
 Recall Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-biasedness"
nolink "false"

\end_inset

.
 This is a case of weakly exogenous regressors,
 and we see that the OLS estimator is biased in this case.
\end_layout

\begin_layout Itemize
Nevertheless,
 under the above assumptions,
 all asymptotic properties continue to hold,
 using the same arguments as before.
\end_layout

\begin_layout Section
When are the assumptions reasonable?
\end_layout

\begin_layout Standard
The two assumptions we've added are
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\lim_{n\rightarrow\infty}\Pr\left(\frac{1}{n}X^{\prime}X=Q_{X}\right)=1,$
\end_inset

 a 
\begin_inset Formula $Q_{X}$
\end_inset

 finite positive definite matrix.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $n^{-1/2}X^{\prime}\varepsilon\overset{d}{\rightarrow}N(0,Q_{X}\sigma_{0}^{2})$
\end_inset


\end_layout

\begin_layout Standard
The most complicated case is that of dynamic models,
 since the other cases can be treated as nested in this case.
 There exist a number of central limit theorems for dependent processes,
 many of which are fairly technical.
 We won't enter into details (see Hamilton,
 Chapter 7 if you're interested).
 A main requirement for use of standard asymptotics for a dependent sequence 
\begin_inset Formula 
\[
\{s_{t}\}=\{\frac{1}{n}\sum_{t=1}^{n}z_{t}\}
\]

\end_inset

 to converge in probability to a finite limit is that 
\begin_inset Formula $z_{t}$
\end_inset

 be 
\emph on
stationary
\emph default
,
 in some sense.
\end_layout

\begin_layout Itemize
Strong stationarity requires that the joint distribution of the set 
\begin_inset Formula 
\[
\{z_{t},z_{t+s},z_{t-q},...\}
\]

\end_inset

 not depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Itemize
Covariance (weak) stationarity requires that the first and second moments of this set not depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Itemize
An example of a sequence that doesn't satisfy this is an AR(1) process with a unit root (a 
\emph on
random walk)
\emph default
:
 
\begin_inset Formula 
\begin{eqnarray*}
x_{t} & = & x_{t-1}+\varepsilon_{t}\\
\varepsilon_{t} & \sim & IIN(0,\sigma^{2})
\end{eqnarray*}

\end_inset

 One can show that the variance of 
\begin_inset Formula $x_{t}$
\end_inset

 depends upon 
\begin_inset Formula $t$
\end_inset

 in this case,
 so it's not weakly stationary.
\end_layout

\begin_layout Itemize
The series 
\begin_inset Formula $\sin t+\epsilon_{t}$
\end_inset

 has a first moment that depends upon 
\begin_inset Formula $t$
\end_inset

,
 so it's not weakly stationary either.
\end_layout

\begin_layout Standard
Stationarity prevents the process from trending off to plus or minus infinity,
 and prevents cyclical behavior which would allow correlations between far removed 
\begin_inset Formula $z_{t}$
\end_inset

 znd 
\begin_inset Formula $z_{s}$
\end_inset

 to be high.
 
\emph on
Draw a picture here.
\end_layout

\begin_layout Itemize
In summary,
 the assumptions are reasonable when the stochastic conditioning variables have variances that are finite,
 and are not too strongly dependent.
 The AR(1) model with unit root is an example of a case where the dependence is too strong for standard asymptotics to apply.
\end_layout

\begin_layout Itemize
The study of nonstationary processes is an important part of econometrics,
 but it isn't in the scope of this course.
 
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Show that for two random variables 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B,$
\end_inset

 if 
\begin_inset Formula $E(A|B)=0,$
\end_inset

 then 
\begin_inset Formula $E\left(Af(B)\right)=0$
\end_inset

.
 How is this used in the proof of the Gauss-Markov theorem?
\end_layout

\begin_layout Enumerate
Is it possible for an AR(1) model for time series data,
 
\emph on
e.g.,

\emph default
 
\begin_inset Formula $y_{t}=0+0.9y_{t-1}+\varepsilon_{t}$
\end_inset

 satisfy weak exogeneity?
 Strong exogeneity?
 Discuss.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Data problems
\end_layout

\begin_layout Standard
In this section we'll consider problems associated with the regressor matrix:
 collinearity,
 missing observations and measurement error.
\end_layout

\begin_layout Section
Collinearity
\end_layout

\begin_layout Subsection
Motivation:
 Data on Mortality and Related Factors
\begin_inset CommandInset label
LatexCommand label
name "sec:Motivation:-Data-on"

\end_inset


\end_layout

\begin_layout Standard
The data set 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Data/mortality.data}{mortality.data} 
\end_layout

\end_inset

 contains annual data from 1947 - 1980 on death rates in the U.S.,
 along with data on factors like smoking and consumption of alcohol.
 The data description is:
\end_layout

\begin_layout Standard
DATA4-7:
 Death rates in the U.S.
 due to coronary heart disease and their
\end_layout

\begin_layout Standard
determinants.
 Data compiled by Jennifer Whisenand
\end_layout

\begin_layout Itemize
chd = death rate per 100,000 population (Range 321.2 - 375.4)
\end_layout

\begin_layout Itemize
cal = Per capita consumption of calcium per day in grams (Range 0.9 - 1.06)
\end_layout

\begin_layout Itemize
unemp = Percent of civilian labor force unemployed in 1,000 of persons 16 years and older (Range 2.9 - 8.5)
\end_layout

\begin_layout Itemize
cig = Per capita consumption of cigarettes in pounds of tobacco by persons 18 years and older–approx.
 339 cigarettes per pound of tobacco (Range 6.75 - 10.46)
\end_layout

\begin_layout Itemize
edfat = Per capita intake of edible fats and oil in pounds–includes lard,
 margarine and butter (Range 42 - 56.5)
\end_layout

\begin_layout Itemize
meat = Per capita intake of meat in pounds–includes beef,
 veal,
 pork,
 lamb and mutton (Range 138 - 194.8)
\end_layout

\begin_layout Itemize
spirits = Per capita consumption of distilled spirits in taxed gallons for individuals 18 and older (Range 1 - 2.9)
\end_layout

\begin_layout Itemize
beer = Per capita consumption of malted liquor in taxed gallons for individuals 18 and older (Range 15.04 - 34.9)
\end_layout

\begin_layout Itemize
wine = Per capita consumption of wine measured in taxed gallons for individuals 18 and older (Range 0.77 - 2.65)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Consider estimation results for two models:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
newcommand{
\backslash
subsize}[1]{
\backslash
footnotesize{#1}}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center} OLS estimates
\backslash

\backslash
 Dependent variable:
 chd 
\backslash

\backslash
 
\backslash
vspace{1em}
\end_layout

\begin_layout Plain Layout


\backslash
begin{longtable}{lcc}  & (1)  & (2) 
\backslash

\backslash
  [6pt]  const & 226.0 & $
\backslash
,
\backslash
,
\backslash
,$498.3$^{***}$ 
\backslash

\backslash
 & 
\backslash
subsize{(146.8)} & 
\backslash
subsize{(88.97)} 
\backslash

\backslash
 [4pt]  cal & $-$69.98 & $
\backslash
,
\backslash
,$$-$165.9$^{**}$ 
\backslash

\backslash
 & 
\backslash
subsize{(78.56)} & 
\backslash
subsize{(69.93)} 
\backslash

\backslash
 [4pt]  unemp & $-$0.6134 & $-$0.9523 
\backslash

\backslash
 & 
\backslash
subsize{(1.586)} & 
\backslash
subsize{(1.650)} 
\backslash

\backslash
 [4pt]  cig & $
\backslash
,$10.12$^{*}$ & 6.652 
\backslash

\backslash
 & 
\backslash
subsize{(5.071)} & 
\backslash
subsize{(5.083)} 
\backslash

\backslash
 [4pt]  edfat & 2.810 & 
\backslash

\backslash
 & 
\backslash
subsize{(1.668)} & 
\backslash

\backslash
 [4pt]  meat & 0.1116 & 
\backslash

\backslash
 & 
\backslash
subsize{(0.2430)} & 
\backslash

\backslash
 [4pt]  spirits & $
\backslash
,
\backslash
,$21.72$^{**}$ & $
\backslash
,
\backslash
,
\backslash
,$25.04$^{***}$ 
\backslash

\backslash
 & 
\backslash
subsize{(8.457)} & 
\backslash
subsize{(8.591)} 
\backslash

\backslash
 [4pt]  beer & $
\backslash
,
\backslash
,$$-$3.467$^{**}$ & $
\backslash
,
\backslash
,
\backslash
,$$-$4.338$^{***}$ 
\backslash

\backslash
 & 
\backslash
subsize{(1.298)} & 
\backslash
subsize{(1.304)} 
\backslash

\backslash
 [4pt]  wine & $-$4.562 & 11.70 
\backslash

\backslash
 & 
\backslash
subsize{(16.25)} & 
\backslash
subsize{(13.54)} 
\backslash

\backslash
 [4pt]  $n$ & 34 & 34 
\backslash

\backslash
 $
\backslash
bar R^2$ & 0.6454 & 0.6041 
\backslash

\backslash
 $
\backslash
ell$& $-$117.3 & $-$120.5 
\backslash

\backslash
 
\backslash
end{longtable}
\end_layout

\begin_layout Plain Layout


\backslash
vspace{1em} Standard errors in parentheses
\backslash

\backslash
 {}* significant at the 10 percent level
\backslash

\backslash
 {}** significant at the 5 percent level
\backslash

\backslash
 {}*** significant at the 1 percent level
\backslash

\backslash
 
\backslash
end{center} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The parameter estimates are highly sensitive to the particular model we estimate.
\end_layout

\begin_layout Itemize
For example,
 calcium is significant in one model,
 and not in the other.
 The sign of the coefficient of wine changes.
 
\end_layout

\begin_layout Itemize
Why?
 We'll see that the problem is that the data exhibit 
\emph on
collinearity
\emph default
.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Collinearity:
 definition
\end_layout

\begin_layout Standard
Collinearity is the existence of linear relationships amongst the regressors.
 We can always write 
\begin_inset Formula 
\[
\lambda_{1}\mathbf{x}_{1}+\lambda_{2}\mathbf{x}_{2}+\cdots+\lambda_{K}\mathbf{x}_{K}+v=0
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 column of the regressor matrix 
\begin_inset Formula $X,$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector.
 In the case that there exists collinearity,
 the variation in 
\begin_inset Formula $v$
\end_inset

 is relatively small,
 so that there is an approximately exact linear relation between the regressors.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

relative
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

approximate
\begin_inset Quotes erd
\end_inset

 are imprecise,
 so it's difficult to define when collinearilty exists.
 
\end_layout

\begin_layout Standard
In the extreme,
 if there are exact linear relationships (every element of 
\begin_inset Formula $v$
\end_inset

 equal) then 
\begin_inset Formula $\rho(X)<K,$
\end_inset

 so 
\begin_inset Formula $\rho(X^{\prime}X)<K,$
\end_inset

 so 
\begin_inset Formula $X^{\prime}X$
\end_inset

 is not invertible and the OLS estimator is not uniquely defined.
 For example,
 if the model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \beta_{1}+\beta_{2}x_{2t}+\beta_{3}x_{3t}+\varepsilon_{t}\\
x_{2t} & = & \alpha_{1}+\alpha_{2}x_{3t}
\end{eqnarray*}

\end_inset

 then we can write 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \beta_{1}+\beta_{2}\left(\alpha_{1}+\alpha_{2}x_{3t}\right)+\beta_{3}x_{3t}+\varepsilon_{t}\\
 & = & \beta_{1}+\beta_{2}\alpha_{1}+\beta_{2}\alpha_{2}x_{3t}+\beta_{3}x_{3t}+\varepsilon_{t}\\
 & = & \left(\beta_{1}+\beta_{2}\alpha_{1}\right)+\left(\beta_{2}\alpha_{2}+\beta_{3}\right)x_{3t}\\
 & = & \gamma_{1}+\gamma_{2}x_{3t}+\varepsilon_{t}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\gamma^{\prime}s$
\end_inset

 can be consistently estimated,
 but since the 
\begin_inset Formula $\gamma^{\prime}$
\end_inset

s define two equations in three 
\begin_inset Formula $\beta^{\prime}s,$
\end_inset

 the 
\begin_inset Formula $\beta^{\prime}s$
\end_inset

 can't be consistently estimated (there are multiple values of 
\begin_inset Formula $\beta$
\end_inset

 that solve the first order conditions).
 The 
\begin_inset Formula $\beta^{\prime}s$
\end_inset

 are 
\emph on
unidentified
\emph default
 in the case of perfect collinearity.
\end_layout

\begin_layout Itemize
Perfect collinearity is unusual,
 except in the case of an error in construction of the regressor matrix,
 such as including the same regressor twice.
 
\end_layout

\begin_layout Standard
Another case where perfect collinearity may be encountered is with models with indicator variables,
 if one is not careful.
 Consider a model of rental price 
\begin_inset Formula $(y_{i})$
\end_inset

 of an apartment.
 This could depend factors such as size,
 quality etc.,
 collected in 
\begin_inset Formula $x_{i},$
\end_inset

 as well as on the location of the apartment.
 Let 
\begin_inset Formula $B_{i}=1$
\end_inset

 if the 
\begin_inset Formula $i^{th}$
\end_inset

 apartment is in Barcelona,
 
\begin_inset Formula $B_{i}=0$
\end_inset

 otherwise.
 Similarly,
 define 
\begin_inset Formula $G_{i},$
\end_inset

 
\begin_inset Formula $T_{i}$
\end_inset

 and 
\begin_inset Formula $L_{i}$
\end_inset

 for Girona,
 Tarragona and Lleida.
 One could use a model such as 
\begin_inset Formula 
\[
y_{i}=\beta_{1}+\beta_{2}B_{i}+\beta_{3}G_{i}+\beta_{4}T_{i}+\beta_{5}L_{i}+x_{i}^{\prime}\gamma+\varepsilon_{i}
\]

\end_inset

 In this model,
 
\begin_inset Formula $B_{i}+G_{i}+T_{i}+L_{i}=1,$
\end_inset

 
\begin_inset Formula $\forall i,$
\end_inset

 so there is an exact relationship between these variables and the column of ones corresponding to the constant.
 One must either drop the constant,
 or one of the qualitative variables.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
A brief aside on indicator variables
\end_layout

\begin_layout Standard

\series bold
indicator variable
\series default
:
 A indicator variable is a binary-valued variable that indicates whether or not some condition is true.
 It is customary to assign the value 1 if the condition is true,
 and 0 if the condition is false.
\end_layout

\begin_layout Standard
indicator variables are used essentially like any other regressor.
 Use 
\begin_inset Formula $d$
\end_inset

 to indicate that a variable is a indicator,
 so that variables like 
\begin_inset Formula $d_{t}$
\end_inset

 and 
\begin_inset Formula $d_{t2}$
\end_inset

 are understood to be indicator variables.
 Variables like 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $x_{t3}$
\end_inset

 are ordinary continuous regressors.
 You know how to interpret the following models:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}d_{t}+\beta_{2}(1-d_{t})+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\beta_{3}x_{t}+\epsilon_{t}
\]

\end_inset


\series bold
Interaction terms:

\series default
 an interaction term is the product of two variables,
 so that the effect of one variable on the dependent variable depends on the value of the other.
 The following model has an interaction term.
 Note that 
\begin_inset Formula $\frac{\partial E(y|x)}{\partial x}=\beta_{3}+\beta_{4}d_{t}$
\end_inset

.
 The slope depends on the value of 
\begin_inset Formula $d_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\beta_{3}x_{t}+\beta_{4}d_{t}x_{t}+\epsilon_{t}
\]

\end_inset


\series bold
Multiple indicator variables:
 
\series default
we can use more than one indicator variable in a model.
 We will study models of the form
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t1}+\beta_{3}d_{t2}+\beta_{4}x_{t}+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t1}+\beta_{3}d_{t2}+\beta_{4}d_{t1}d_{t2}+\beta_{5}x_{t}+\epsilon_{t}
\]

\end_inset


\series bold
Incorrect usage:
 
\series default
You should understand why the following models are not correct usages of indicator variables:
\end_layout

\begin_layout Enumerate
overparameterization:
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\beta_{3}(1-d_{t})+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Enumerate
multiple values assigned to multiple categories.
 Suppose that we a condition that defines 4 possible categories,
 and we create a variable 
\begin_inset Formula $d=1$
\end_inset

 if the observation is in the first category,
 
\begin_inset Formula $d=2$
\end_inset

 if in the second,
 etc.
 (This is not strictly speaking a indicator variable,
 according to our definition).
 Why is the following model not a good one?
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d+\epsilon
\]

\end_inset

What is the correct way to deal with this situation?
\end_layout

\begin_layout Standard

\series bold
Multiple parameterizations.
 
\series default
To formulate a model that conditions on a given set of categorical information,
 there are multiple ways to use indicator variables.
 For example,
 the two models 
\begin_inset Formula 
\[
y_{t}=\beta_{1}d_{t}+\beta_{2}(1-d_{t})+\beta_{3}x_{t}+\beta_{4}d_{t}x_{t}+\epsilon_{t}
\]

\end_inset

and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\alpha_{1}+\alpha_{2}d_{t}+\alpha_{3}x_{t}d_{t}+\alpha_{4}x_{t}(1-d_{t})+\epsilon_{t}
\]

\end_inset

are equivalent.
 You should know what are the 4 equations that relate the 
\begin_inset Formula $\beta_{j}$
\end_inset

 parameters to the 
\begin_inset Formula $\alpha_{j}$
\end_inset

 parameters,
 
\begin_inset Formula $j=1,2,3,4.$
\end_inset

 You should know how to interpret the parameters of both models.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Back to collinearity
\end_layout

\begin_layout Standard
The more common case,
 if one doesn't make mistakes such as these,
 is the existence of inexact linear relationships,
 
\emph on
i.e.
\emph default
,
 correlations between the regressors that are less than one in absolute value,
 but not zero.
 The basic problem is that when two (or more) variables move together,
 it is difficult to determine their separate influences.
\end_layout

\begin_layout Example
Two children are in a room,
 along with a broken lamp.
 Both say 
\begin_inset Quotes sld
\end_inset

I didn't do it!
\begin_inset Quotes srd
\end_inset

.
 This is like two regressors giving exactly the same information.
 How can we tell who broke the lamp?
\end_layout

\begin_layout Standard
Lack of knowledge about the separate influences of variables is reflected in imprecise estimates,
 
\emph on
i.e
\emph default
.,
 estimates with high variances.
 
\emph on
With economic data,
 collinearity is commonly encountered,
 and is often a severe problem.
\end_layout

\begin_layout Standard
When there is collinearity,
 the minimizing point of the objective function that defines the OLS estimator (
\begin_inset Formula $s(\beta)$
\end_inset

,
 the sum of squared errors) is relatively poorly defined.
 This is seen in Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "nocollin"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "collin"
nolink "false"

\end_inset

.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "nocollin"

\end_inset


\begin_inset Formula $s(\beta)$
\end_inset

 when there is no collinearity
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/nocollin.pdf

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "collin"

\end_inset


\begin_inset Formula $s(\beta)$
\end_inset

 when there is collinearity
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/collin.pdf

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To see the effect of collinearity on variances,
 partition the regressor matrix as 
\begin_inset Formula 
\[
X=\left[\begin{array}{cc}
\mathbf{x} & W\end{array}\right]
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the first column of 
\begin_inset Formula $X$
\end_inset

 (note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

we can interchange the columns of 
\begin_inset Formula $X$
\end_inset

 isf we like,
 so there's no loss of generality in considering the first column).
 Now,
 the variance of 
\begin_inset Formula $\hat{\beta},$
\end_inset

 under the classical assumptions,
 is 
\begin_inset Formula 
\[
V(\hat{\beta})=\left(X^{\prime}X\right)^{-1}\sigma^{2}
\]

\end_inset

 Using the partition,
 
\begin_inset Formula 
\[
X^{\prime}X=\left[\begin{array}{cc}
\mathbf{x}^{\prime}\mathbf{x} & \mathbf{x}^{\prime}W\\
W^{\prime}\mathbf{x} & W^{\prime}W
\end{array}\right]
\]

\end_inset

 and following a rule for partitioned inversion,
 
\begin_inset Formula 
\begin{eqnarray*}
\left(X^{\prime}X\right)_{1,1}^{-1} & = & \left(\mathbf{x}^{\prime}\mathbf{x}-\mathbf{x}^{\prime}W(W^{\prime}W)^{-1}W^{\prime}\mathbf{x}\right)^{-1}\\
 & = & \left(\mathbf{x}^{\prime}\left(I_{n}-W(W^{\prime}W)^{^{\prime}1}W^{\prime}\right)\mathbf{x}\right)^{-1}\\
 & = & \left(ESS_{\mathbf{x}|W}\right)^{-1}
\end{eqnarray*}

\end_inset

 where by 
\begin_inset Formula $ESS_{\mathbf{x}|W}$
\end_inset

 we mean the error sum of squares obtained from the regression 
\begin_inset Formula 
\[
\mathbf{x}=W\lambda+v.
\]

\end_inset

 Since 
\begin_inset Formula 
\[
R^{2}=1-ESS/TSS,
\]

\end_inset

 we have 
\begin_inset Formula 
\[
ESS=TSS(1-R^{2})
\]

\end_inset

 so the variance of the coefficient corresponding to 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
V(\hat{\beta}_{\mathbf{x}})=\frac{\sigma^{2}}{TSS_{\mathbf{x}}(1-R_{\mathbf{x}|W}^{2})}\label{eq:variance factors}
\end{equation}

\end_inset

 We see three factors influence the variance of this coefficient.
 It will be high if
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sigma^{2}$
\end_inset

 is large
\end_layout

\begin_layout Enumerate
There is little variation in 
\begin_inset Formula $\mathbf{x}.$
\end_inset

 
\emph on
Draw a picture here
\emph default
.
\end_layout

\begin_layout Enumerate
There is a strong linear relationship between 
\begin_inset Formula $x$
\end_inset

 and the other regressors,
 so that 
\begin_inset Formula $W$
\end_inset

 can explain the movement in 
\begin_inset Formula $\mathbf{x}$
\end_inset

 well.
 In this case,
 
\begin_inset Formula $R_{\mathbf{x}|W}^{2}$
\end_inset

 will be close to 1.
 As 
\begin_inset Formula $R_{\mathbf{x}|W}^{2}\rightarrow1,V(\hat{\beta}_{\mathbf{x}})\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Standard
The last of these cases is collinearity.
\end_layout

\begin_layout Standard
Intuitively,
 when there are strong linear relations between the regressors,
 it is difficult to determine the separate influence of the regressors on the dependent variable.
 This can be seen by comparing the OLS objective function in the case of no correlation between regressors with the objective function with correlation between the regressors.
 See the figures nocollin.ps (no correlation) and collin.ps (correlation),
 available on the web site.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The VIFs for the mortality data at the beginning of the section are
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout

\family typewriter
Variance Inflation Factors
\end_layout

\begin_layout Plain Layout

\family typewriter
Minimum possible value = 1.0
\end_layout

\begin_layout Plain Layout

\family typewriter
Values > 10.0 may indicate a collinearity problem
\end_layout

\begin_layout Plain Layout

\family typewriter
cal 5.786
\end_layout

\begin_layout Plain Layout

\family typewriter
unemp 1.967
\end_layout

\begin_layout Plain Layout

\family typewriter
cig 11.756
\end_layout

\begin_layout Plain Layout

\family typewriter
edfat 25.570
\end_layout

\begin_layout Plain Layout

\family typewriter
meat 6.250
\end_layout

\begin_layout Plain Layout

\family typewriter
spirits 16.703
\end_layout

\begin_layout Plain Layout

\family typewriter
beer 36.218
\end_layout

\begin_layout Plain Layout

\family typewriter
wine 49.066
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
We see that collinearity is a serious problem in this data set.
\end_layout

\begin_layout Itemize
For example,
 the VIF for wine is almost 50.
 This perhaps explains why this regressor is not significant in the two models that were estimated.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DataProblems/collinearity.jl}{DataProblems/collinearity.jl} 
\end_layout

\end_inset

 performs a Monte Carlo study with correlated regressors.
 The model is 
\begin_inset Formula $y=1+x_{2}+x_{3}+\epsilon$
\end_inset

,
 where the correlation between 
\begin_inset Formula $x_{2}$
\end_inset

 and 
\begin_inset Formula $x_{3}$
\end_inset

can be set.
 Three estimators are used:
 OLS,
 OLS dropping 
\begin_inset Formula $x_{3}$
\end_inset

 (a false restriction),
 and restricted LS using 
\begin_inset Formula $\beta_{2}=\beta_{3}$
\end_inset

 (a true restriction).
 The output when the correlation between the two regressors is 0.9 is
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/DataProblems/collinearity.out"
literal "true"

\end_inset


\end_layout

\begin_layout Example
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Collinearity:-Monte-Carlo"
nolink "false"

\end_inset

 shows densities for the estimated 
\begin_inset Formula $\beta_{2},$
\end_inset

 for each of the three estimators.
 
\end_layout

\begin_layout Itemize
Check the biases and variances.
\end_layout

\begin_layout Itemize
repeat the experiment with a lower value of rho,
 and note how the standard errors of the OLS estimator change.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Collinearity:-Monte-Carlo"

\end_inset

Collinearity:
 Monte Carlo results
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/collin.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Detection of collinearity
\end_layout

\begin_layout Standard
The best way is simply to regress each explanatory variable in turn on the remaining regressors.
 If any of these auxiliary regressions has a high 
\begin_inset Formula $R^{2},$
\end_inset

 there is a problem of collinearity.
 Furthermore,
 this procedure identifies which parameters are affected.
\end_layout

\begin_layout Itemize
Sometimes,
 we're only interested in certain parameters.
 Collinearity isn't a problem if it doesn't affect what we're interested in estimating.
 
\end_layout

\begin_layout Standard
An alternative is to examine the matrix of correlations between the regressors.
 High correlations are sufficient but not necessary for severe collinearity.
\end_layout

\begin_layout Standard
Also indicative of collinearity is that the model fits well (high 
\begin_inset Formula $R^{2}),$
\end_inset

 but none of the variables is significantly different from zero (e.g.,
 their separate influences aren't well determined).
\end_layout

\begin_layout Standard
In summary,
 the artificial regressions are the best approach if one wants to be careful.
\end_layout

\begin_layout Example
Nerlove data and collinearity.
 The simple Nerlove model is 
\begin_inset Formula 
\[
\ln C=\beta_{1}+\beta_{2}\ln Q+\beta_{3}\ln P_{L}+\beta_{4}\ln P_{F}+\beta_{5}\ln P_{K}+\epsilon
\]

\end_inset

When this model is estimated by OLS,
 some coefficients are not significant (see subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"
nolink "false"

\end_inset

).
 Maybe this is due to collinearity?
 The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DataProblems/NerloveCollinearity.jl}{DataProblems/NerloveCollinearity.jl} 
\end_layout

\end_inset

 checks the regressors for collinearity.
 If you run this,
 you will see that collinearity is not a problem with this data.
 Why is the coefficient of 
\begin_inset Formula $\ln P_{K}$
\end_inset

 not significantly different from zero?
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Dealing with collinearity
\end_layout

\begin_layout Subsubsection
More information
\end_layout

\begin_layout Standard
Collinearity is a problem of an uninformative sample.
 The first question is:
 is all the available information being used?
 Is more data available?
 Are there coefficient restrictions that have been neglected?
 
\emph on
Picture illustrating how a restriction can solve problem of perfect collinearity.
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Stochastic-restrictions-and"

\end_inset

Stochastic restrictions and ridge regression
\end_layout

\begin_layout Standard
Note:
 here's a nice introduction to ridge regression:
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Supposing that there is no more data or neglected restrictions,
 one possibility is to change perspectives,
 to Bayesian econometrics.
 One can express prior beliefs regarding the coefficients using stochastic restrictions.
 A stochastic linear restriction would be something of the form 
\begin_inset Formula 
\[
R\beta=r+v
\]

\end_inset

 where 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 are as in the case of exact linear restrictions,
 but 
\begin_inset Formula $v$
\end_inset

 is a random vector.
 For example,
 the model could be 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
R\beta & = & r+v\\
\left(\begin{array}{c}
\varepsilon\\
v
\end{array}\right) & \sim & N\left(\begin{array}{c}
0\\
0
\end{array}\right),\left(\begin{array}{cc}
\sigma_{\varepsilon}^{2}I_{n} & 0_{n\times q}\\
0_{q\times n} & \sigma_{v}^{2}I_{q}
\end{array}\right)
\end{eqnarray*}

\end_inset

 This sort of model isn't in line with the classical interpretation of parameters as constants:
 according to this interpretation the left hand side of 
\begin_inset Formula $R\beta=r+v$
\end_inset

 is constant but the right is random.
 This model does fit the Bayesian perspective:
 we combine information coming from the model and the data,
 summarized in 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
\varepsilon & \sim & N(0,\sigma_{\varepsilon}^{2}I_{n})
\end{eqnarray*}

\end_inset

 with prior beliefs regarding the distribution of the parameter,
 summarized in
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
R\beta\sim N(r,\sigma_{v}^{2}I_{q})
\]

\end_inset

 Since the sample is random it is reasonable to suppose that 
\begin_inset Formula $\mathcal{E}(\varepsilon v^{\prime})=0,$
\end_inset

 which is the last piece of information in the specification.
 How can you estimate using this model?
 The solution is to treat the restrictions as artificial data.
 Write 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y\\
r
\end{array}\right]=\left[\begin{array}{c}
X\\
R
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon\\
v
\end{array}\right]
\]

\end_inset

 This model is heteroscedastic,
 since 
\begin_inset Formula $\sigma_{\varepsilon}^{2}\neq\sigma_{v}^{2}.$
\end_inset

 Define the 
\emph on
prior precision
\emph default
 
\begin_inset Formula $k=\sigma_{\varepsilon}/\sigma_{v}.$
\end_inset

 This expresses the degree of belief in the restriction relative to the variability of the data.
 Supposing that we specify 
\begin_inset Formula $k,$
\end_inset

 then the model 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y\\
kr
\end{array}\right]=\left[\begin{array}{c}
X\\
kR
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon\\
kv
\end{array}\right]
\]

\end_inset

 is homoscedastic and can be estimated by OLS.
 Note that this estimator is biased.
 It is consistent,
 however,
 given that 
\begin_inset Formula $k$
\end_inset

 is a fixed constant,
 even if the restriction is false (this is in contrast to the case of false exact restrictions).
 To see this,
 note that there are 
\begin_inset Formula $Q$
\end_inset

 restrictions,
 where 
\begin_inset Formula $Q$
\end_inset

 is the number of rows of 
\begin_inset Formula $R.$
\end_inset

 As 
\begin_inset Formula $n\rightarrow\infty,$
\end_inset

 these 
\begin_inset Formula $Q$
\end_inset

 artificial observations have no weight in the objective function,
 so the estimator has the same limiting objective function as the OLS estimator,
 and is therefore consistent.
\end_layout

\begin_layout Standard
To motivate the use of stochastic restrictions,
 consider the expectation of the squared length of 
\begin_inset Formula $\hat{\beta}$
\end_inset

:
 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\hat{\beta}^{\prime}\hat{\beta}) & = & \mathcal{E}\left\{ \left(\beta+\left(X^{\prime}X\right)^{-1}X^{\prime}\varepsilon\right)^{\prime}\left(\beta+\left(X^{\prime}X\right)^{-1}X^{\prime}\varepsilon\right)\right\} \\
 & = & \beta^{\prime}\beta+\mathcal{E}\left(\varepsilon^{\prime}X(X^{\prime}X)^{-1}(X^{\prime}X)^{-1}X^{\prime}\varepsilon\right)\\
 & = & \beta^{\prime}\beta+Tr\left(X^{\prime}X\right)^{-1}\sigma^{2}\\
 & = & \beta^{\prime}\beta+\sigma^{2}\sum_{i=1}^{K}\lambda_{i}\text{(the\:\ trace\:\ is\:\ the\:\ sum\:\ of\:\ eigenvalues)}\\
 & > & \beta^{\prime}\beta+\lambda_{\max\left(X^{\prime}X^{-1}\right)}\sigma^{2}\text{(the\:\ eigenvalues\:\ are\:\ all\:\ positive,\:\ since}X^{\prime}X\text{\:\ is\:\ p.d.}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
\mathcal{E}(\hat{\beta}^{\prime}\hat{\beta})>\beta^{\prime}\beta+\frac{\sigma^{2}}{\lambda_{\min\left(X^{\prime}X\right)}}
\]

\end_inset

 where 
\begin_inset Formula $\lambda_{\min\left(X^{\prime}X\right)}$
\end_inset

 is the minimum eigenvalue of 
\begin_inset Formula $X^{\prime}X$
\end_inset

 (which is the inverse of the maximum eigenvalue of 
\begin_inset Formula $(X^{\prime}X)^{-1}).$
\end_inset

 As collinearity becomes worse and worse,
 
\begin_inset Formula $X^{\prime}X$
\end_inset

 becomes more nearly singular,
 so 
\begin_inset Formula $\lambda_{\min\left(X^{\prime}X\right)}$
\end_inset

 tends to zero (recall that the determinant is the product of the eigenvalues) and 
\begin_inset Formula $\mathcal{E}(\hat{\beta}^{\prime}\hat{\beta})$
\end_inset

 tends to infinite.
 On the other hand,
 
\begin_inset Formula $\beta^{\prime}\beta$
\end_inset

 is finite.
\end_layout

\begin_layout Standard
Now considering the restriction 
\begin_inset Formula $I_{K}\beta=0+v.$
\end_inset

 With this restriction the model becomes 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y\\
0
\end{array}\right]=\left[\begin{array}{c}
X\\
kI_{K}
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon\\
kv
\end{array}\right]
\]

\end_inset

 and the estimator is 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{ridge} & = & \left(\left[\begin{array}{cc}
X^{\prime} & kI_{K}\end{array}\right]\left[\begin{array}{c}
X\\
kI_{K}
\end{array}\right]\right)^{-1}\left[\begin{array}{cc}
X^{\prime} & I_{K}\end{array}\right]\left[\begin{array}{c}
y\\
0
\end{array}\right]\\
 & = & \left(X^{\prime}X+k^{2}I_{K}\right)^{-1}X^{\prime}y
\end{eqnarray*}

\end_inset

 This is the ordinary 
\emph on
ridge regression
\emph default
 estimator.
 The ridge regression estimator can be seen to add 
\begin_inset Formula $k^{2}I_{K},$
\end_inset

 which is nonsingular,
 to 
\begin_inset Formula $X^{\prime}X,$
\end_inset

 which is more and more nearly singular as collinearity becomes worse and worse.
 As 
\begin_inset Formula $k\rightarrow\infty,$
\end_inset

 the restrictions tend to 
\begin_inset Formula $\beta=0,$
\end_inset

 that is,
 the coefficients are shrunken toward zero.
 Also,
 the estimator tends to 
\begin_inset Formula 
\[
\hat{\beta}_{ridge}=\left(X^{\prime}X+k^{2}I_{K}\right)^{-1}X^{\prime}y\rightarrow\left(k^{2}I_{K}\right)^{-1}X^{\prime}y=\frac{X^{\prime}y}{k^{2}}\rightarrow0
\]

\end_inset

 so 
\begin_inset Formula $\hat{\beta}_{ridge}^{\prime}\hat{\beta}_{ridge}\rightarrow0.$
\end_inset

 This is clearly a false restriction in the limit,
 if our original model is at all sensible.
\end_layout

\begin_layout Standard
There should be some amount of shrinkage that is in fact a true restriction.
 The problem is to determine the 
\begin_inset Formula $k$
\end_inset

 such that the restriction is correct.
 The interest in ridge regression centers on the fact that it can be shown that there exists a 
\begin_inset Formula $k$
\end_inset

 such that 
\begin_inset Formula $MSE(\hat{\beta}_{ridge})<\hat{\beta}_{OLS}.$
\end_inset

 The problem is that this 
\begin_inset Formula $k$
\end_inset

 depends on 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\sigma^{2},$
\end_inset

 which are unknown.
\end_layout

\begin_layout Standard
The ridge trace method plots 
\begin_inset Formula $\hat{\beta}_{ridge}^{\prime}\hat{\beta}_{ridge}$
\end_inset

 as a function of 
\begin_inset Formula $k,$
\end_inset

 and chooses the value of 
\begin_inset Formula $k$
\end_inset

 that 
\begin_inset Quotes eld
\end_inset

artistically
\begin_inset Quotes erd
\end_inset

 seems appropriate (e.g.,
 where the effect of increasing 
\begin_inset Formula $k$
\end_inset

 dies off).
 
\emph on
Draw picture here.

\emph default
 This means of choosing 
\begin_inset Formula $k$
\end_inset

 is obviously subjective.
 This is not a problem from the Bayesian perspective:
 the choice of 
\begin_inset Formula $k$
\end_inset

 reflects prior beliefs about the length of 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Standard
In summary,
 the ridge estimator offers some hope,
 but it is impossible to guarantee that it will outperform the OLS estimator.
 Collinearity is a fact of life in econometrics,
 and there is no clear solution to the problem.
\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DataProblems/RidgeRegression.jl}{DataProblems/RidgeRegression.jl} 
\end_layout

\end_inset

 does a Monte Carlo study that shows that ridge regression can help to deal with collinearity.
 This script generates the following figures,
 which show the Monte Carlo sampling frequency of the OLS and ridge estimators,
 after subtracting the true parameter values.
 You can see that the ridge estimator has much lower RMSE:
 both histograms are centered close to zero,
 but the ridge histogram is much tighter.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
OLS and Ridge regression
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/RidgeExample.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Measurement error
\end_layout

\begin_layout Standard
Measurement error is exactly what it says,
 either the dependent variable or the regressors are measured with error.
 Thinking about the way economic data are reported,
 measurement error is probably quite prevalent.
 For example,
 estimates of growth of GDP,
 inflation,
 etc.
 are commonly revised several times.
 Why should the last revision necessarily be correct?
\end_layout

\begin_layout Subsection
Error of measurement of the dependent variable
\end_layout

\begin_layout Standard
Measurement errors in the dependent variable and the regressors have important differences.
 First consider error in measurement of the dependent variable.
 The data generating process is presumed to be 
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & X\beta+\varepsilon\\
y & = & y^{*}+v\\
v_{t} & \sim & iid(0,\sigma_{v}^{2})
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $y^{*}=y+v$
\end_inset

 is the unobservable true dependent variable,
 and 
\begin_inset Formula $y$
\end_inset

 is what is observed.
 We assume that 
\begin_inset Formula $\varepsilon$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

 are independent and that 
\begin_inset Formula $y^{*}=X\beta+\varepsilon$
\end_inset

 satisfies the classical assumptions.
 Given this,
 we have 
\begin_inset Formula 
\[
y+v=X\beta+\varepsilon
\]

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon-v\\
 & = & X\beta+\omega\\
\omega_{t} & \sim & iid(0,\sigma_{\varepsilon}^{2}+\sigma_{v}^{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
As long as 
\begin_inset Formula $v$
\end_inset

 is uncorrelated with 
\begin_inset Formula $X,$
\end_inset

 this model satisfies the classical assumptions and can be estimated by OLS.
 This type of measurement error isn't a problem,
 then,
 except in that the increased variability of the error term causes an increase in the variance of the OLS estimator (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:variance factors"
nolink "false"

\end_inset

).
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Error of measurement of the regressors
\end_layout

\begin_layout Standard
The situation isn't so good in this case.
 The DGP is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{*\prime}\beta+\varepsilon_{t}\\
x_{t} & = & x_{t}^{*}+v_{t}\\
v_{t} & \sim & iid(0,\Sigma_{v})
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\Sigma_{v}$
\end_inset

 is a 
\begin_inset Formula $K\times K$
\end_inset

 matrix.
 Now 
\begin_inset Formula $X^{*}$
\end_inset

 contains the true,
 unobserved regressors,
 and 
\begin_inset Formula $X$
\end_inset

 is what is observed.
 Again assume that 
\begin_inset Formula $v$
\end_inset

 is independent of 
\begin_inset Formula $\varepsilon,$
\end_inset

 and that the model 
\begin_inset Formula $y=X^{*}\beta+\varepsilon$
\end_inset

 satisfies the classical assumptions.
 Now we have 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \left(x_{t}-v_{t}\right)^{\prime}\beta+\varepsilon_{t}\\
 & = & x_{t}^{\prime}\beta-v_{t}^{\prime}\beta+\varepsilon_{t}\\
 & = & x_{t}^{\prime}\beta+\omega_{t}
\end{eqnarray*}

\end_inset

 The problem is that now there is a correlation between 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $\omega_{t},$
\end_inset

 since 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(x_{t}\omega_{t}) & = & \mathcal{E}\left(\left(x_{t}^{*}+v_{t}\right)\left(-v_{t}^{\prime}\beta+\varepsilon_{t}\right)\right)\\
 & = & -\Sigma_{v}\beta
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
\Sigma_{v}=\mathcal{E}\left(v_{t}v_{t}^{\prime}\right).
\]

\end_inset

 Because of this correlation,
 the OLS estimator is biased and inconsistent,
 just as in the case of autocorrelated errors with lagged dependent variables.
 In matrix notation,
 write the estimated model as 
\begin_inset Formula 
\[
y=X\beta+\omega
\]

\end_inset

 We have that 
\begin_inset Formula 
\[
\hat{\beta}=\left(\frac{X^{\prime}X}{n}\right)^{-1}\left(\frac{X^{\prime}y}{n}\right)
\]

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
plim\left(\frac{X^{\prime}X}{n}\right)^{-1} & = & plim\frac{\left(X^{*\prime}+V^{\prime}\right)\left(X^{*}+V\right)}{n}\\
 & = & \left(Q_{X^{*}}+\Sigma_{v}\right)^{-1}
\end{eqnarray*}

\end_inset

 since 
\begin_inset Formula $X^{*}$
\end_inset

 and 
\begin_inset Formula $V$
\end_inset

 are independent,
 and 
\begin_inset Formula 
\begin{eqnarray*}
plim\frac{V^{\prime}V}{n} & = & \lim\mathcal{E}\frac{1}{n}\sum_{t=1}^{n}v_{t}v_{t}^{\prime}\\
 & = & \Sigma_{v}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Likewise,
 
\begin_inset Formula 
\begin{eqnarray*}
plim\left(\frac{X^{\prime}y}{n}\right) & = & plim\frac{\left(X^{*\prime}+V^{\prime}\right)\left(X^{*}\beta+\varepsilon\right)}{n}\\
 & = & Q_{X^{*}}\beta
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
plim\hat{\beta}=\left(Q_{X^{*}}+\Sigma_{v}\right)^{-1}Q_{X^{*}}\beta
\]

\end_inset

 So we see that the least squares estimator is inconsistent when the regressors are measured with error.
\end_layout

\begin_layout Itemize
A potential solution to this problem is the instrumental variables (IV) estimator,
 which we'll discuss shortly.
 
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:Measurement-error-in"

\end_inset

Measurement error in a dynamic model.
 Consider the model 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}^{*} & = & \alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}\\
y_{t} & = & y_{t}^{*}+\upsilon_{t}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 are independent Gaussian white noise errors.
 Suppose that 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is not observed,
 and instead we observe 
\begin_inset Formula $y_{t}$
\end_inset

.
 What are the properties of the OLS regression on the equation
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\eta_{t}
\]

\end_inset

?
 The error is 
\begin_inset Formula 
\begin{align*}
\eta_{t} & =y_{t}-\alpha-\rho y_{t-1}-\beta x_{t}\\
 & =y_{t}^{*}+\upsilon_{t}-\alpha-\rho y_{t-1}^{*}-\rho\upsilon_{t-1}-\beta x_{t}\\
 & =\alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}+\upsilon_{t}-\alpha-\rho y_{t-1}^{*}-\rho\upsilon_{t-1}-\beta x_{t}\\
 & =\epsilon_{t}+\upsilon_{t}-\rho{\color{blue}\upsilon_{t-1}}
\end{align*}

\end_inset

So the error term is autocorrelated.
 Note that 
\begin_inset Formula 
\[
y_{t-1}=\alpha+\rho y_{t-2}+\beta x_{t-1}+\eta_{t-1}
\]

\end_inset

and 
\begin_inset Formula 
\[
\eta_{t-1}=\epsilon_{t-1}+{\color{blue}\upsilon}_{{\color{blue}t-1}}-\rho\upsilon_{t-2},
\]

\end_inset

so the error 
\begin_inset Formula $\eta_{t}$
\end_inset

 and the regressor 
\begin_inset Formula $y_{t-1}$
\end_inset

 are correlated,
 because they share the common term 
\begin_inset Formula $\upsilon_{t-1}.$
\end_inset

 This means that the equation 
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\eta_{t}
\]

\end_inset

 does not satisfy weak exogeneity,
 and the OLS estimator will be biased and inconsistent.
\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DataProblems/MeasurementError.jl}{DataProblems/MeasurementError.jl} 
\end_layout

\end_inset

 does a Monte Carlo study.
 The sample size is 
\begin_inset Formula $n=100$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:measurement error"
nolink "false"

\end_inset

 gives the results.
 The first panel shows a histogram for 1000 replications of 
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

,
 when 
\begin_inset Formula $\sigma_{\nu}=1$
\end_inset

,
 so that there is significant measurement error.
 The second panel repeats this with 
\begin_inset Formula $\sigma_{\nu}=0,$
\end_inset

 so that there is not measurement error.
 Note that there is much more bias with measurement error.
 There is also bias without measurement error.
 This is due to the same reason that we saw bias in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-biasedness"
nolink "false"

\end_inset

:
 one of the classical assumptions (nonstochastic regressors) that guarantees unbiasedness of OLS does not hold for this model.
 Without measurement error,
 the OLS estimator 
\emph on
is 
\emph default
consistent.
 By re-running the script with larger 
\begin_inset Formula $n$
\end_inset

,
 you can verify that the bias disappears when 
\begin_inset Formula $\sigma_{\nu}=0$
\end_inset

,
 but not when 
\begin_inset Formula $\sigma_{\nu}>0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:measurement error"

\end_inset


\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

 with and without measurement error
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
with measurement error:
 
\begin_inset Formula $\sigma_{\nu}=1$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/ylag_n100.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
without measurement error:
 
\begin_inset Formula $\sigma_{\nu}=0$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/ylag_n100_no_error.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Missing observations
\end_layout

\begin_layout Standard
Missing observations occur quite frequently:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

time series data may not be gathered in a certain year,
 or respondents to a survey may not answer all questions.
 We'll consider two cases:
 missing observations on the dependent variable and missing observations on the regressors.
\end_layout

\begin_layout Subsection
Missing observations on the dependent variable
\end_layout

\begin_layout Standard
In this case,
 we have 
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

 or 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y_{1}\\
y_{2}
\end{array}\right]=\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]
\]

\end_inset

 where 
\begin_inset Formula $y_{2}$
\end_inset

 is not observed.
 Otherwise,
 we assume the classical assumptions hold.
\end_layout

\begin_layout Itemize
A clear alternative is to simply estimate using the compete observations 
\begin_inset Formula 
\[
y_{1}=X_{1}\beta+\varepsilon_{1}
\]

\end_inset

 Since these observations satisfy the classical assumptions,
 one could estimate by OLS.
\end_layout

\begin_layout Itemize
The question remains whether or not one could somehow replace the unobserved 
\begin_inset Formula $y_{2}$
\end_inset

 by a predictor,
 and improve over OLS in some sense.
 Let 
\begin_inset Formula $\hat{y}_{2}$
\end_inset

 be the predictor of 
\begin_inset Formula $y_{2}.$
\end_inset

 Now 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \left\{ \left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]^{\prime}\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]\right\} ^{-1}\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]^{\prime}\left[\begin{array}{c}
y_{1}\\
\hat{y}_{2}
\end{array}\right]\\
 & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}\left[X_{1}^{\prime}y_{1}+X_{2}^{\prime}\hat{y}_{2}\right]
\end{eqnarray*}

\end_inset

 Recall that the OLS fonc are 
\begin_inset Formula 
\[
X^{\prime}X\hat{\beta}=X^{\prime}y
\]

\end_inset

 so if we regressed using only the first (complete) observations,
 we would have 
\begin_inset Formula 
\[
X_{1}^{\prime}X_{1}\hat{\beta}_{1}=X_{1}^{\prime}y_{1.}
\]

\end_inset

 Likewise,
 an OLS regression using only the second (filled in) observations would give 
\begin_inset Formula 
\[
X_{2}^{\prime}X_{2}\hat{\beta}_{2}=X_{2}^{\prime}\hat{y}_{2}.
\]

\end_inset

 Substituting these into the equation for the overall combined estimator gives 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}\left[X_{1}^{\prime}X_{1}\hat{\beta}_{1}+X_{2}^{\prime}X_{2}\hat{\beta}_{2}\right]\\
 & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{1}^{\prime}X_{1}\hat{\beta}_{1}+\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{2}^{\prime}X_{2}\hat{\beta}_{2}\\
 & \equiv & A\hat{\beta}_{1}+(I_{K}-A)\hat{\beta}_{2}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
A\equiv\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{1}^{\prime}X_{1}
\]

\end_inset

 and we use 
\begin_inset Formula 
\begin{eqnarray*}
\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{2}^{\prime}X_{2} & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}\left[\left(X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right)-X_{1}^{\prime}X_{1}\right]\\
 & = & I_{K}-\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{1}^{\prime}X_{1}\\
 & = & I_{K}-A.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Now,
 
\begin_inset Formula 
\[
\mathcal{E}(\hat{\beta})=A\beta+(I_{K}-A)\mathcal{E}\left(\hat{\beta}_{2}\right)
\]

\end_inset

 and this will be unbiased only if 
\begin_inset Formula $\mathcal{E}\left(\hat{\beta}_{2}\right)=\beta.$
\end_inset


\end_layout

\begin_layout Itemize
The conclusion is that the filled in observations alone would need to define an unbiased estimator.
 This will be the case only if 
\begin_inset Formula 
\[
\hat{y}_{2}=X_{2}\beta+\hat{\varepsilon}_{2}
\]

\end_inset

 where 
\begin_inset Formula $\hat{\varepsilon}_{2}$
\end_inset

 has mean zero.
 Clearly,
 it is difficult to satisfy this condition without knowledge of 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Itemize
Note that putting 
\begin_inset Formula $\hat{y}_{2}=\bar{y}_{1}$
\end_inset

 does not satisfy the condition and therefore leads to a biased estimator.
 
\end_layout

\begin_layout Exercise
Formally prove this last statement.
\end_layout

\begin_layout Subsection
The sample selection problem
\end_layout

\begin_layout Standard
In the above discussion we assumed that the missing observations are random.
 The sample selection problem is a case where the missing observations are not random.
 Consider the model 
\begin_inset Formula 
\[
y_{t}^{*}=x_{t}^{\prime}\beta+\varepsilon_{t}
\]

\end_inset

 which is assumed to satisfy the classical assumptions.
 However,
 
\begin_inset Formula $y_{t}^{*\;}$
\end_inset

 is not always observed.
 What is observed is 
\begin_inset Formula $y_{t}$
\end_inset

 defined as 
\begin_inset Formula 
\[
y_{t}=y_{t}^{*}\text{\textnormal{ }if }y_{t}^{*}\geq0
\]

\end_inset

 Or,
 in other words,
 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is missing when it is less than zero.
\end_layout

\begin_layout Standard
The difference in this case is that the missing values are not random:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

they are correlated with the 
\begin_inset Formula $x_{t}.$
\end_inset

 Consider the case 
\begin_inset Formula 
\[
y^{*}=x+\varepsilon
\]

\end_inset

 with 
\begin_inset Formula $V(\varepsilon)=25$
\end_inset

,
 but using only the observations for which 
\begin_inset Formula $y^{*}>0$
\end_inset

 to estimate.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Sample-selection-bias"
nolink "false"

\end_inset

 illustrates the bias.
 The Julia program is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DataProblems/sampsel.jl}{sampsel.jl} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Sample-selection-bias"

\end_inset

Sample selection bias
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/sampsel.png

\end_inset


\end_layout

\end_inset

There are means of dealing with sample selection bias,
 but we will not go into it here.
 One should at least be aware that nonrandom selection of the sample will normally lead to bias and inconsistency if the problem is not taken into account.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Missing observations on the regressors
\end_layout

\begin_layout Standard
Again the model is 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y_{1}\\
y_{2}
\end{array}\right]=\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]
\]

\end_inset

 but we assume now that each row of 
\begin_inset Formula $X_{2}$
\end_inset

 has an unobserved component(s).
 Again,
 one could just estimate using the complete observations,
 but it may seem frustrating to have to drop observations simply because of a single missing variable.
 In general,
 if the unobserved 
\begin_inset Formula $X_{2}$
\end_inset

 is replaced by some prediction,
 
\begin_inset Formula $X_{2}^{*},$
\end_inset

 then we are in the case of errors of observation.
 As before,
 this means that the OLS estimator is biased when 
\begin_inset Formula $X_{2}^{*}$
\end_inset

 is used instead of 
\begin_inset Formula $X_{2}.$
\end_inset

 Consistency is salvaged,
 however,
 as long as the number of missing observations doesn't increase with 
\begin_inset Formula $n.$
\end_inset


\end_layout

\begin_layout Itemize
Including observations that have missing values replaced by 
\emph on
ad hoc
\emph default
 values can be interpreted as introducing false stochastic restrictions.
 In general,
 this introduces bias.
 It is difficult to determine whether MSE increases or decreases.
 Monte Carlo studies suggest that it is dangerous to simply substitute the mean,
 for example.
\end_layout

\begin_layout Itemize
In the case that there is only one regressor other than the constant,
 substitution of 
\begin_inset Formula $\bar{x}$
\end_inset

 for the missing 
\begin_inset Formula $x_{t}$
\end_inset

 
\emph on
does not lead to bias
\emph default
.
 This is a special case that doesn't hold for 
\begin_inset Formula $K>2.$
\end_inset


\end_layout

\begin_layout Exercise
Prove this last statement.
\end_layout

\begin_layout Itemize
In summary,
 if one is strongly concerned with bias,
 it is best to drop observations that have missing components.
 There is potential for reduction of MSE through filling in missing elements with intelligent guesses,
 but this could also increase MSE.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Missing-regressors"

\end_inset

Missing regressors
\end_layout

\begin_layout Standard
Suppose that the model 
\begin_inset Formula $y=X\beta+W\gamma+\epsilon$
\end_inset

 satisfies the classical assumptions,
 so OLS would be a consistent estimator.
 However,
 let's suppose that the regressors 
\begin_inset Formula $W$
\end_inset

 are not available in the sample.
 What are the properties of the OLS estimator of the model 
\begin_inset Formula $y=X\beta+\omega?$
\end_inset

 We can think of this as a case of imposing false restrictions:
 
\begin_inset Formula $\gamma=0$
\end_inset

 when in fact 
\begin_inset Formula $\gamma\ne0$
\end_inset

.
 We know that the restricted least squares estimator is biased and inconsistent,
 in general,
 when we impose false restrictions.
 Another way of thinking of this is to look to see if 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

 are correlated.
 We have
\begin_inset Formula 
\begin{align*}
E(X_{t}\omega_{t}) & =E\left(X_{t}\left(W_{t}^{\prime}\gamma+\epsilon_{t}\right)\right)\\
 & =E(X_{t}W_{t}^{\prime}\gamma)+E(X_{t}\epsilon_{t})\\
 & =E(X_{t}W_{t}^{\prime}\gamma)
\end{align*}

\end_inset

where the last line follows because 
\begin_inset Formula $E(X_{t}\epsilon_{t})=0$
\end_inset

 by assumption.
 So,
 there will be correlation between the error and the regressors if there is collinearity between the included regressors 
\begin_inset Formula $X_{t}$
\end_inset

 and the missing regressors 
\begin_inset Formula $W_{t}$
\end_inset

.
 If there is not,
 the OLS estimator will be consistent.
 Because the normal thing is to have collinearity between regressors,
 we expect that missing regressors will lead to bias and inconsistency of the OLS estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Consider the simple Nerlove model 
\begin_inset Formula 
\[
\ln C=\beta_{1}+\beta_{2}\ln Q+\beta_{3}\ln P_{L}+\beta_{4}\ln P_{F}+\beta_{5}\ln P_{K}+\epsilon
\]

\end_inset

When this model is estimated by OLS,
 some coefficients are not significant.
 We have seen that collinearity is not an important problem.
 Why is 
\begin_inset Formula $\beta_{5}$
\end_inset

 not significantly different from zero?
 Give an economic explanation.
\end_layout

\begin_layout Enumerate
For the model 
\begin_inset Formula $y=\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon,$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
verify that the level sets of the OLS criterion function (defined in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:OLS criterion function"
nolink "false"

\end_inset

) are straight lines when there is perfect collinearity
\end_layout

\begin_layout Enumerate
For this model with perfect collinearity,
 the OLS estimator does not exist.
 Depict what this statement means using a drawing.
\end_layout

\begin_layout Enumerate
Show how a restriction 
\begin_inset Formula $R_{1}\beta_{1}+R_{2}\beta_{2}=r$
\end_inset

 causes the restricted least squares estimator to exist,
 using a drawing.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Functional form and nonnested tests
\end_layout

\begin_layout Standard
Though theory often suggests which conditioning variables should be included,
 and suggests the signs of certain derivatives,
 it is usually silent regarding the functional form of the relationship between the dependent variable and the regressors.
 For example,
 considering a cost function,
 one could have a Cobb-Douglas model 
\begin_inset Formula 
\[
c=Aw_{1}^{\beta_{1}}w_{2}^{\beta_{2}}q^{\beta_{q}}e^{\varepsilon}
\]

\end_inset

 This model,
 after taking logarithms,
 gives 
\begin_inset Formula 
\[
\ln c=\beta_{0}+\beta_{1}\ln w_{1}+\beta_{2}\ln w_{2}+\beta_{q}\ln q+\varepsilon
\]

\end_inset

 where 
\begin_inset Formula $\beta_{0}=\ln A.$
\end_inset

 Theory suggests that 
\begin_inset Formula $A>0,\beta_{1}>0,\beta_{2}>0,\beta_{3}>0.$
\end_inset

 This model isn't compatible with a fixed cost of production since 
\begin_inset Formula $c=0$
\end_inset

 when 
\begin_inset Formula $q=0.$
\end_inset

 Homogeneity of degree one in input prices suggests that 
\begin_inset Formula $\beta_{1}+\beta_{2}=1,$
\end_inset

 while constant returns to scale implies 
\begin_inset Formula $\beta_{q}=1.$
\end_inset


\end_layout

\begin_layout Standard
While this model may be reasonable in some cases,
 an alternative 
\begin_inset Formula 
\[
\sqrt{c}=\beta_{0}+\beta_{1}\sqrt{w_{1}}+\beta_{2}\sqrt{w_{2}}+\beta_{q}\sqrt{q}+\varepsilon
\]

\end_inset

 may be just as plausible.
 Note that 
\begin_inset Formula $\sqrt{x}$
\end_inset

 and 
\begin_inset Formula $\ln(x)$
\end_inset

 look quite alike,
 for certain values of the regressors,
 and up to a linear transformation,
 so it may be difficult to choose between these models.
\end_layout

\begin_layout Standard
The basic point is that many functional forms are compatible with the linear-in-parameters model,
 since this model can incorporate a wide variety of nonlinear transformations of the dependent variable and the regressors.
 For example,
 suppose that 
\begin_inset Formula $g(\cdot)$
\end_inset

 is a real valued function and that 
\begin_inset Formula $x(\cdot)$
\end_inset

 is a 
\begin_inset Formula $K-$
\end_inset

 vector-valued function.
 The following model is linear in the parameters but nonlinear in the variables:
 
\begin_inset Formula 
\begin{eqnarray*}
x_{t} & = & x(z_{t})\\
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}
\end{eqnarray*}

\end_inset

 There may be 
\begin_inset Formula $P$
\end_inset

 fundamental conditioning variables 
\begin_inset Formula $z_{t}$
\end_inset

,
 but there may be 
\begin_inset Formula $K$
\end_inset

 regressors,
 where 
\begin_inset Formula $K$
\end_inset

 may be smaller than,
 equal to or larger than 
\begin_inset Formula $P.$
\end_inset

 For example,
 
\begin_inset Formula $x_{t}$
\end_inset

 could include squares and cross products of the conditioning variables in 
\begin_inset Formula $z_{t}.$
\end_inset


\end_layout

\begin_layout Section
Flexible functional forms
\end_layout

\begin_layout Standard
Given that the functional form of the relationship between the dependent variable and the regressors is in general unknown,
 one might wonder if there exist parametric models that can closely approximate a wide variety of functional relationships.
 A 
\begin_inset Quotes eld
\end_inset

Diewert-Flexible
\begin_inset Quotes erd
\end_inset

 functional form is defined as one such that the function,
 the vector of first derivatives and the matrix of second derivatives can take on an arbitrary value 
\emph on
at a single data point.

\emph default
 Flexibility in this sense clearly requires that there be at least 
\begin_inset Formula 
\[
K=1+P+\left(P^{2}-P\right)/2+P
\]

\end_inset

 free parameters:
 one for each independent effect that we wish to model.
\end_layout

\begin_layout Standard
Suppose that the model is 
\begin_inset Formula 
\[
y=g(x)+\varepsilon
\]

\end_inset

 A second-order Taylor's series expansion (with remainder term) of the function 
\begin_inset Formula $g(x)$
\end_inset

 about the point 
\begin_inset Formula $x=0$
\end_inset

 is 
\begin_inset Formula 
\[
g(x)=g(0)+x^{\prime}D_{x}g(0)+\frac{x^{\prime}D_{x}^{2}g(0)x}{2}+R
\]

\end_inset

 Use the approximation,
 which simply drops the remainder term,
 as an approximation to 
\begin_inset Formula $g(x):$
\end_inset


\begin_inset Formula 
\[
g(x)\simeq g_{K}(x)=g(0)+x^{\prime}D_{x}g(0)+\frac{x^{\prime}D_{x}^{2}g(0)x}{2}
\]

\end_inset

 As 
\begin_inset Formula $x\rightarrow0,$
\end_inset

 the approximation becomes more and more exact,
 in the sense that 
\begin_inset Formula $g_{K}(x)\rightarrow g(x),$
\end_inset

 
\begin_inset Formula $D_{x}g_{K}(x)\rightarrow D_{x}g(x)$
\end_inset

 and 
\begin_inset Formula $D_{x}^{2}g_{K}(x)\rightarrow D_{x}^{2}g(x).$
\end_inset

 For 
\begin_inset Formula $x=0,$
\end_inset

 the approximation is exact,
 up to the second order.
 The idea behind many flexible functional forms is to note that 
\begin_inset Formula $g(0),$
\end_inset

 
\begin_inset Formula $D_{x}g(0)$
\end_inset

 and 
\begin_inset Formula $D_{x}^{2}g(0)$
\end_inset

 are all constants.
 If we treat them as parameters,
 the approximation will have exactly enough free parameters to approximate the function 
\begin_inset Formula $g(x),$
\end_inset

 which is of unknown form,
 exactly,
 up to second order,
 at the point 
\begin_inset Formula $x=0.$
\end_inset

 The model is 
\begin_inset Formula 
\[
g_{K}(x)=\alpha+x^{\prime}\beta+1/2x^{\prime}\Gamma x
\]

\end_inset

 so the regression model to fit is 
\begin_inset Formula 
\[
y=\alpha+x^{\prime}\beta+1/2x^{\prime}\Gamma x+\varepsilon
\]

\end_inset


\end_layout

\begin_layout Itemize
While the regression model has enough free parameters to be Diewert-flexible,
 the question remains:
 is 
\begin_inset Formula $plim\hat{\alpha}=g(0)?$
\end_inset

 Is 
\begin_inset Formula $plim\hat{\beta}=D_{x}g(0)?$
\end_inset

 Is 
\begin_inset Formula $plim\hat{\Gamma}=D_{x}^{2}g(0)?$
\end_inset


\end_layout

\begin_layout Itemize
The answer is no,
 in general.
 The reason is that if we treat the true values of the parameters as these derivatives,
 then 
\begin_inset Formula $\varepsilon$
\end_inset

 is forced to play the part of the remainder term,
 which is a function of 
\begin_inset Formula $x,$
\end_inset

 so that 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are correlated in this case.
 As before,
 the estimator is biased in this case.
\end_layout

\begin_layout Itemize
A simpler example would be to consider a first-order T.S.
 approximation to a quadratic function.
 
\emph on
Draw picture.
\end_layout

\begin_layout Itemize
The conclusion is that 
\begin_inset Quotes eld
\end_inset

flexible functional forms
\begin_inset Quotes erd
\end_inset

 aren't really flexible in a useful statistical sense,
 in that neither the function itself nor its derivatives are consistently estimated,
 unless the function belongs to the parametric family of the specified functional form.
 In order to lead to consistent inferences,
 the regression model must be correctly specified.
 
\end_layout

\begin_layout Subsection
The translog form
\end_layout

\begin_layout Standard
In spite of the fact that FFF's aren't really flexible for the purposes of econometric estimation and inference,
 they are useful,
 and they are certainly subject to less bias due to misspecification of the functional form than are many popular forms,
 such as the Cobb-Douglas or the simple linear in the variables model.
 The translog model is probably the most widely used FFF.
 This model is as above,
 except that the variables are subjected to a logarithmic tranformation.
 Also,
 the expansion point is usually taken to be the sample mean of the data,
 after the logarithmic transformation.
 The model is defined by 
\begin_inset Formula 
\begin{eqnarray*}
y & = & \ln(c)\\
x & = & \ln\left(\frac{z}{\bar{z}}\right)\\
 & = & \ln(z)-\ln(\bar{z})\\
y & = & \alpha+x^{\prime}\beta+1/2x^{\prime}\Gamma x+\varepsilon
\end{eqnarray*}

\end_inset

 In this presentation,
 the 
\begin_inset Formula $t$
\end_inset

 subscript that distinguishes observations is suppressed for simplicity.
 Note that 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial y}{\partial x} & = & \beta+\Gamma x\\
 & = & \frac{\partial\ln(c)}{\partial\ln(z)}\text{\:(the\:\ other\:\ part\:\ of\:}x\,\text{is\:\ constant)}\\
 & = & \frac{\partial c}{\partial z}\frac{z}{c}
\end{eqnarray*}

\end_inset

 which is the elasticity of 
\begin_inset Formula $c$
\end_inset

 with respect to 
\begin_inset Formula $z.$
\end_inset

 This is a convenient feature of the translog model.
 Note that at the means of the conditioning variables,
 
\begin_inset Formula $\bar{z}$
\end_inset

,
 
\begin_inset Formula $x=0,$
\end_inset

 so 
\begin_inset Formula 
\[
\left.\frac{\partial y}{\partial x}\right|_{z=\bar{z}}=\beta
\]

\end_inset

 so the 
\begin_inset Formula $\beta$
\end_inset

 are the first-order elasticities,
 at the means of the data.
\end_layout

\begin_layout Standard
To illustrate,
 consider that 
\begin_inset Formula $y$
\end_inset

 is cost of production:
 
\begin_inset Formula 
\[
y=c(w,q)
\]

\end_inset

 where 
\begin_inset Formula $w$
\end_inset

 is a vector of input prices and 
\begin_inset Formula $q$
\end_inset

 is output.
 We could add other variables by extending 
\begin_inset Formula $q$
\end_inset

 in the obvious manner,
 but this is supressed for simplicity.
 By Shephard's lemma,
 the conditional factor demands are 
\begin_inset Formula 
\[
x=\frac{\partial c(w,q)}{\partial w}
\]

\end_inset

 and the cost shares of the factors are therefore 
\begin_inset Formula 
\[
s=\frac{wx}{c}=\frac{\partial c(w,q)}{\partial w}\frac{w}{c}
\]

\end_inset

 which is simply the vector of elasticities of cost with respect to input prices.
 If the cost function is modeled using a translog function,
 we have 
\begin_inset Formula 
\begin{eqnarray*}
\ln(c) & = & \alpha+x^{\prime}\beta+z^{\prime}\delta+1/2\left[\begin{array}{cc}
x^{\prime} & z\end{array}\right]\left[\begin{array}{cc}
\Gamma_{11} & \Gamma_{12}\\
\Gamma_{12}^{\prime} & \Gamma_{22}
\end{array}\right]\left[\begin{array}{c}
x\\
z
\end{array}\right]\\
 & = & \alpha+x^{\prime}\beta+z^{\prime}\delta+1/2x^{\prime}\Gamma_{11}x+x^{\prime}\Gamma_{12}z+1/2z^{2}\gamma_{22}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $x=\ln(w/\bar{w})$
\end_inset

 (element-by-element division) and 
\begin_inset Formula $z=\ln(q/\bar{q}),$
\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
\Gamma_{11} & = & \left[\begin{array}{ll}
\gamma_{11} & \gamma_{12}\\
\gamma_{12} & \gamma_{22}
\end{array}\right]\\
\Gamma_{12} & = & \left[\begin{array}{l}
\gamma_{13}\\
\gamma_{23}
\end{array}\right]\\
\Gamma_{22} & = & \gamma_{33}.
\end{eqnarray*}

\end_inset

 Note that symmetry of the second derivatives has been imposed.
\end_layout

\begin_layout Standard
Then the share equations are just 
\begin_inset Formula 
\[
s=\beta+\left[\begin{array}{cc}
\Gamma_{11} & \Gamma_{12}\end{array}\right]\left[\begin{array}{c}
x\\
z
\end{array}\right]
\]

\end_inset

 Therefore,
 the share equations and the cost equation have parameters in common.
 By pooling the equations together and imposing the (true) restriction that the parameters of the equations be the same,
 we can gain efficiency.
\end_layout

\begin_layout Standard
To illustrate in more detail,
 consider the case of two inputs,
 so 
\begin_inset Formula 
\[
x=\left[\begin{array}{l}
x_{1}\\
x_{2}
\end{array}\right].
\]

\end_inset

 In this case the translog model of the logarithmic cost function is 
\begin_inset Formula 
\[
\ln c=\alpha+\beta_{1}x_{1}+\beta_{2}x_{2}+\delta z+\frac{\gamma_{11}}{2}x_{1}^{2}+\frac{\gamma_{22}}{2}x_{2}^{2}+\frac{\gamma_{33}}{2}z^{2}+\gamma_{12}x_{1}x_{2}+\gamma_{13}x_{1}z+\gamma_{23}x_{2}z
\]

\end_inset

 The two cost shares of the inputs are the derivatives of 
\begin_inset Formula $\ln c$
\end_inset

 with respect to 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

:
 
\begin_inset Formula 
\begin{eqnarray*}
s_{1} & = & \beta_{1}+\gamma_{11}x_{1}+\gamma_{12}x_{2}+\gamma_{13}z\\
s_{2} & = & \beta_{2}+\gamma_{12}x_{1}+\gamma_{22}x_{2}+\gamma_{13}z
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that the share equations and the cost equation have parameters in common.
 One can do a pooled estimation of the three equations at once,
 imposing that the parameters are the same.
 In this way we're using more observations and therefore more information,
 which will lead to imporved efficiency.
 Note that this does assume that the cost equation is correctly specified (
\emph on
i.e.,

\emph default
 not an approximation),
 since otherwise the derivatives would not be the true derivatives of the log cost function,
 and would then be misspecified for the shares.
 To pool the equations,
 write the model in matrix form (adding in error terms) 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\ln c\\
s_{1}\\
s_{2}
\end{array}\right]=\left[\begin{array}{llllllllll}
1 & x_{1} & x_{2} & z & \frac{x_{1}^{2}}{2} & \frac{x_{2}^{2}}{2} & \frac{z^{2}}{2} & x_{1}x_{2} & x_{1}z & x_{2}z\\
0 & 1 & 0 & 0 & x_{1} & 0 & 0 & x_{2} & z & 0\\
0 & 0 & 1 & 0 & 0 & x_{2} & 0 & x_{1} & 0 & z
\end{array}\right]\left[\begin{array}{l}
\alpha\\
\beta_{1}\\
\beta_{2}\\
\delta\\
\gamma_{11}\\
\gamma_{22}\\
\gamma_{33}\\
\gamma_{12}\\
\gamma_{13}\\
\gamma_{23}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\varepsilon_{3}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
This is 
\emph on
one
\emph default
 observation on the three equations.
 With the appropriate notation,
 a single observation can be written as 
\begin_inset Formula 
\[
y_{t}={X_{t}\theta}+\varepsilon_{t}
\]

\end_inset

 The overall model would stack 
\begin_inset Formula $n$
\end_inset

 observations on the three equations for a total of 
\begin_inset Formula $3n$
\end_inset

 observations:
 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right]=\left[\begin{array}{l}
X_{1}\\
X_{2}\\
\vdots\\
X_{n}
\end{array}\right]\theta+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{n}
\end{array}\right]
\]

\end_inset

 Next we need to consider the errors.
 For observation 
\begin_inset Formula $t$
\end_inset

 the errors can be placed in a vector 
\begin_inset Formula 
\[
\varepsilon_{t}=\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}\\
\varepsilon_{3t}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
First consider the covariance matrix of this vector:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the shares are certainly correlated since they must sum to one.
 (In fact,
 with 2 shares the variances are equal and the covariance is -1 times the variance.
 General notation is used to allow easy extension to the case of more than 2 inputs).
 Also,
 it's likely that the shares and the cost equation have different variances.
 Supposing that the model is covariance stationary,
 the variance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 won
\begin_inset Formula $^{\prime}$
\end_inset

t depend upon 
\begin_inset Formula $t$
\end_inset

:
 
\begin_inset Formula 
\[
Var\varepsilon_{t}=\Sigma_{0}=\left[\begin{array}{lll}
\sigma_{11} & \sigma_{12} & \sigma_{13}\\
\cdot & \sigma_{22} & \sigma_{23}\\
\cdot & \cdot & \sigma_{33}
\end{array}\right]
\]

\end_inset

 Note that this matrix is singular,
 since the shares sum to 1.
 Assuming that there is no autocorrelation,
 the overall covariance matrix has the 
\emph on
seemingly unrelated regressions
\emph default
 (SUR) structure.
 
\begin_inset Formula 
\begin{eqnarray*}
Var\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{n}
\end{array}\right] & = & \Sigma\\
 & = & \left[\begin{array}{llll}
\Sigma_{0} & 0 & \cdots & 0\\
0 & \Sigma_{0} & \ddots & \vdots\\
\vdots & \ddots &  & 0\\
0 & \cdots & 0 & \Sigma_{0}
\end{array}\right]\\
 & = & I_{n}\otimes\Sigma_{0}
\end{eqnarray*}

\end_inset

 where the symbol 
\begin_inset Formula $\otimes$
\end_inset

 indicates the 
\emph on
Kronecker product
\emph default
.
 The Kronecker product of two matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is 
\begin_inset Formula 
\[
A\otimes B=\left[\begin{array}{llll}
a_{11}B & a_{12}B & \cdots & a_{1q}B\\
a_{21}B & \ddots &  & \vdots\\
\vdots\\
a_{pq}B & \cdots &  & a_{pq}B
\end{array}\right].
\]

\end_inset

 
\end_layout

\begin_layout Subsection
FGLS estimation of a translog model
\end_layout

\begin_layout Standard
So,
 this model has heteroscedasticity and autocorrelation,
 so OLS won't be efficient.
 The next question is:
 how do we estimate efficiently using FGLS?
 FGLS is based upon inverting the estimated error covariance 
\begin_inset Formula $\hat{\Sigma}.$
\end_inset

 So we need to estimate 
\begin_inset Formula $\Sigma.$
\end_inset


\end_layout

\begin_layout Standard
An asymptotically efficient procedure is (supposing normality of the errors)
\end_layout

\begin_layout Enumerate
Estimate each equation by OLS
\end_layout

\begin_layout Enumerate
Estimate 
\begin_inset Formula $\Sigma_{0}$
\end_inset

 using 
\begin_inset Formula 
\[
\hat{\Sigma}_{0}=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}^{\prime}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Next we need to account for the singularity of 
\begin_inset Formula $\Sigma_{0}.$
\end_inset

 It can be shown that 
\begin_inset Formula $\hat{\Sigma}_{0}$
\end_inset

 will be singular when the shares sum to one,
 so FGLS won't work.
 The solution is to drop one of the share equations,
 for example the second.
 The model becomes 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\ln c\\
s_{1}
\end{array}\right]=\left[\begin{array}{llllllllll}
1 & x_{1} & x_{2} & z & \frac{x_{1}^{2}}{2} & \frac{x_{2}^{2}}{2} & \frac{z^{2}}{2} & x_{1}x_{2} & x_{1}z & x_{2}z\\
0 & 1 & 0 & 0 & x_{1} & 0 & 0 & x_{2} & z & 0
\end{array}\right]\left[\begin{array}{l}
\alpha\\
\beta_{1}\\
\beta_{2}\\
\delta\\
\gamma_{11}\\
\gamma_{22}\\
\gamma_{33}\\
\gamma_{12}\\
\gamma_{13}\\
\gamma_{23}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]
\]

\end_inset

 or in matrix notation for the observation:
 
\begin_inset Formula 
\[
y_{t}^{\ast}=X_{t}^{\ast}\theta+\varepsilon_{t}^{\ast}
\]

\end_inset

 and in stacked notation for all observations we have the 
\begin_inset Formula $2n$
\end_inset

 observations:
 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}^{\ast}\\
y_{2}^{\ast}\\
\vdots\\
y_{n}^{\ast}
\end{array}\right]=\left[\begin{array}{l}
X_{1}^{\ast}\\
X_{2}^{\ast}\\
\vdots\\
X_{n}^{\ast}
\end{array}\right]\theta+\left[\begin{array}{l}
\varepsilon_{1}^{\ast}\\
\varepsilon_{2}^{\ast}\\
\vdots\\
\varepsilon_{n}^{\ast}
\end{array}\right]
\]

\end_inset

 or,
 finally in matrix notation for all observations:
 
\begin_inset Formula 
\[
y^{\ast}=X^{\ast}\theta+\varepsilon^{\ast}
\]

\end_inset

 Considering the error covariance,
 we can define 
\begin_inset Formula 
\begin{eqnarray*}
\Sigma_{0}^{\ast} & = & Var\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]\\
\Sigma^{\ast} & = & I_{n}\otimes\Sigma_{0}^{\ast}
\end{eqnarray*}

\end_inset

 Define 
\begin_inset Formula $\hat{\Sigma}_{0}^{\ast}$
\end_inset

 as the leading 
\begin_inset Formula $2\times2$
\end_inset

 block of 
\begin_inset Formula $\hat{\Sigma}_{0}$
\end_inset

 ,
 and form 
\begin_inset Formula 
\[
\hat{\Sigma}^{\ast}=I_{n}\otimes\hat{\Sigma}_{0}^{\ast}.
\]

\end_inset

 This is a consistent estimator,
 following the consistency of OLS and applying a LLN.
\end_layout

\begin_layout Enumerate
Next compute the Cholesky factorization 
\begin_inset Formula 
\[
\hat{P}_{0}=Chol\left(\hat{\Sigma}_{0}^{\ast}\right)^{-1}
\]

\end_inset

 (I am assuming this is defined as an upper triangular matrix,
 which is consistent with the way Octave does it) and the Cholesky factorization of the overall covariance matrix of the 2 equation model,
 which can be calculated as 
\begin_inset Formula 
\[
\hat{P}=Chol\hat{\Sigma}^{\ast}=I_{n}\otimes\hat{P}_{0}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Finally the FGLS estimator can be calculated by applying OLS to the transformed model 
\begin_inset Formula 
\[
\hat{P}^{\prime}y^{\ast}=\hat{P}^{\prime}X^{\ast}\theta+\hat{\hat{P}^{\prime}}\varepsilon^{\ast}
\]

\end_inset

 or by directly using the GLS formula 
\begin_inset Formula 
\[
\hat{\theta}_{FGLS}=\left(X^{\ast\prime}\left(\hat{\Sigma}_{0}^{\ast}\right)^{-1}X^{\ast}\right)^{-1}X^{\ast\prime}\left(\hat{\Sigma}_{0}^{\ast}\right)^{-1}y^{\ast}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
It is equivalent to transform each observation individually:
 
\begin_inset Formula 
\[
\hat{P}_{0}^{\prime}y_{y}^{\ast}=\hat{P}_{0}^{\prime}X_{t}^{\ast}\theta+\hat{P}_{0}^{\prime}\varepsilon^{\ast}
\]

\end_inset

 and then apply OLS.
 This is probably the simplest approach.
\end_layout

\end_deeper
\begin_layout Standard
A few last comments.
\end_layout

\begin_layout Enumerate
We have assumed no autocorrelation across time.
 This is clearly restrictive.
 It is relatively simple to relax this,
 but we won't go into it here.
\end_layout

\begin_layout Enumerate
Also,
 we have only imposed symmetry of the second derivatives.
 Another restriction that the model should satisfy is that the estimated shares should sum to 1.
 This can be accomplished by imposing 
\begin_inset Formula 
\begin{eqnarray*}
\beta_{1}+\beta_{2} & = & 1\\
\sum_{i=1}^{3}\gamma_{ij} & = & 0,\textnormal{ }j=1,2,3.
\end{eqnarray*}

\end_inset

 These are linear parameter restrictions,
 so they are easy to impose and will improve efficiency if they are true.
\end_layout

\begin_layout Enumerate
The estimation procedure outlined above can be 
\emph on
iterated.

\emph default
 That is,
 estimate 
\begin_inset Formula $\hat{\theta}_{FGLS}$
\end_inset

 as above,
 then re-estimate 
\begin_inset Formula $\Sigma_{0}^{\ast}$
\end_inset

 using errors calculated as 
\begin_inset Formula 
\[
\hat{\varepsilon}=y-X\hat{\theta}_{FGLS}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
These might be expected to lead to a better estimate than the estimator based on 
\begin_inset Formula $\hat{\theta}_{OLS},$
\end_inset

 since FGLS is asymptotically more efficient.
 Then re-estimate 
\begin_inset Formula $\theta$
\end_inset

 using the new estimated error covariance.
 It can be shown that if this is repeated until the estimates don't change (
\emph on
i.e.,

\emph default
 iterated to convergence) then the resulting estimator is the MLE.
 At any rate,
 the asymptotic properties of the iterated and uniterated estimators are the same,
 since both are based upon a consistent estimator of the error covariance.
 
\end_layout

\end_deeper
\begin_layout Section
Testing nonnested hypotheses
\end_layout

\begin_layout Standard
Given that the choice of functional form isn't perfectly clear,
 in that many possibilities exist,
 how can one choose between forms?
 When one form is a parametric restriction of another,
 the previously studied tests such as Wald,
 LR,
 score or 
\begin_inset Formula $qF$
\end_inset

 are all possibilities.
 For example,
 the Cobb-Douglas model is a parametric restriction of the translog:
 The translog is 
\begin_inset Formula 
\[
y_{t}=\alpha+x_{t}^{\prime}\beta+1/2x_{t}^{\prime}\Gamma x_{t}+\varepsilon
\]

\end_inset

 where the variables are in logarithms,
 while the Cobb-Douglas is 
\begin_inset Formula 
\[
y_{t}=\alpha+x_{t}^{\prime}\beta+\varepsilon
\]

\end_inset

 so a test of the Cobb-Douglas versus the translog is simply a test that 
\begin_inset Formula $\Gamma=0.$
\end_inset


\end_layout

\begin_layout Standard
The situation is more complicated when we want to test 
\emph on
non-nested hypotheses.

\emph default
 If the two functional forms are linear in the parameters,
 and use the same transformation of the dependent variable,
 then they may be written as 
\begin_inset Formula 
\begin{eqnarray*}
M_{1}:y & = & X\beta+\varepsilon\\
\varepsilon_{t} & \sim & iid(0,\sigma_{\varepsilon}^{2})\\
M_{2}:y & = & Z\gamma+\eta\\
\eta & \sim & iid(0,\sigma_{\eta}^{2})
\end{eqnarray*}

\end_inset

 We wish to test hypotheses of the form:
 
\begin_inset Formula $H_{0}:M_{i}$
\end_inset

 
\emph on
is correctly specified
\emph default
 versus 
\begin_inset Formula $H_{A}:M_{i}$
\end_inset

 
\emph on
is misspecified
\emph default
,
 for 
\begin_inset Formula $i=1,2.$
\end_inset


\end_layout

\begin_layout Itemize
One could account for non-iid errors,
 but we'll suppress this for simplicity.
\end_layout

\begin_layout Itemize
There are a number of ways to proceed.
 We'll consider the 
\begin_inset Formula $J$
\end_inset

 test,
 proposed by Davidson and MacKinnon,
 
\emph on
Econometrica
\emph default
 (1981).
 The idea is to artificially nest the two models,
 e.g.,
 
\begin_inset Formula 
\[
y=(1-\alpha)X\beta+\alpha(Z\gamma)+\omega
\]

\end_inset

 If the first model is correctly specified,
 then the true value of 
\begin_inset Formula $\alpha$
\end_inset

 is zero.
 On the other hand,
 if the second model is correctly specified then 
\begin_inset Formula $\alpha=1.$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The problem is that this model is not identified in general.
 For example,
 if the models share some regressors,
 as in 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
M_{1}:y_{t} & = & \beta_{1}+\beta_{2}x_{2t}+\beta_{3}x_{3t}+\varepsilon_{t}\\
M_{2}:y_{t} & = & \gamma_{1}+\gamma_{2}x_{2t}+\gamma_{3}x_{4t}+\eta_{t}
\end{eqnarray*}

\end_inset

 then the composite model is 
\begin_inset Formula 
\[
y_{t}=(1-\alpha)\beta_{1}+(1-\alpha)\beta_{2}x_{2t}+(1-\alpha)\beta_{3}x_{3t}+\alpha\gamma_{1}+\alpha\gamma_{2}x_{2t}+\alpha\gamma_{3}x_{4t}+\omega_{t}
\]

\end_inset

 Combining terms we get 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \left((1-\alpha)\beta_{1}+\alpha\gamma_{1}\right)+\left((1-\alpha)\beta_{2}+\alpha\gamma_{2}\right)x_{2t}+(1-\alpha)\beta_{3}x_{3t}+\alpha\gamma_{3}x_{4t}+\omega_{t}\\
 & = & \delta_{1}+\delta_{2}x_{2t}+\delta_{3}x_{3t}+\delta_{4}x_{4t}+\omega_{t}
\end{eqnarray*}

\end_inset

 The four 
\begin_inset Formula $\delta^{\prime}s$
\end_inset

 are consistently estimable,
 but 
\begin_inset Formula $\alpha$
\end_inset

 is not,
 since we have four equations in 7 unknowns,
 so one can't test the hypothesis that 
\begin_inset Formula $\alpha=0.$
\end_inset


\end_layout

\begin_layout Standard
The idea of the 
\begin_inset Formula $J$
\end_inset

 test is to substitute 
\begin_inset Formula $\hat{\gamma}$
\end_inset

 in place of 
\begin_inset Formula $\gamma.$
\end_inset

 This is a consistent estimator supposing that the second model is correctly specified.
 It will tend to a finite probability limit even if the second model is misspecified.
 Then estimate the model 
\begin_inset Formula 
\begin{eqnarray*}
y & = & (1-\alpha)X\beta+\alpha(Z\hat{\gamma})+\omega\\
 & = & X\theta+\alpha\hat{y}+\omega
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\hat{y}=Z(Z^{\prime}Z)^{-1}Z^{\prime}y=P_{Z}y.$
\end_inset

 In this model,
 
\begin_inset Formula $\alpha$
\end_inset

 is consistently estimable,
 and one can show that,
 under the hypothesis that the first model is correct,
 
\begin_inset Formula $\alpha\overset{p}{\rightarrow}0$
\end_inset

 and that the ordinary 
\begin_inset Formula $t$
\end_inset

 -statistic for 
\begin_inset Formula $\alpha=0$
\end_inset

 is asymptotically normal:
 
\begin_inset Formula 
\[
t=\frac{\hat{\alpha}}{\hat{\sigma}_{\hat{\alpha}}}\overset{a}{\sim}N(0,1)
\]

\end_inset


\end_layout

\begin_layout Itemize
If the second model is correctly specified,
 then 
\begin_inset Formula $t\overset{p}{\rightarrow}\infty,$
\end_inset

 since 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 tends in probability to 1,
 while it's estimated standard error tends to zero.
 Thus the test will always reject the false null model,
 asymptotically,
 since the statistic will eventually exceed any critical value with probability one.
\end_layout

\begin_layout Itemize
We can reverse the roles of the models,
 testing the second against the first.
\end_layout

\begin_layout Itemize
It may be the case that 
\emph on
neither
\emph default
 model is correctly specified.
 In this case,
 the test will still reject the null hypothesis,
 asymptotically,
 if we use critical values from the 
\begin_inset Formula $N(0,1)$
\end_inset

 distribution,
 since as long as 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 tends to something different from zero,
 
\begin_inset Formula $|t|\overset{p}{\rightarrow}\infty.$
\end_inset

 Of course,
 when we switch the roles of the models the other will also be rejected asymptotically.
\end_layout

\begin_layout Itemize
In summary,
 there are 4 possible outcomes when we test two models,
 each against the other.
 Both may be rejected,
 neither may be rejected,
 or one of the two may be rejected.
\end_layout

\begin_layout Itemize
There are other tests available for non-nested models.
 The 
\begin_inset Formula $J-$
\end_inset

 test is simple to apply when both models are linear in the parameters.
 The 
\begin_inset Formula $P$
\end_inset

-test is similar,
 but easier to apply when 
\begin_inset Formula $M_{1}$
\end_inset

 is nonlinear.
\end_layout

\begin_layout Itemize
The above presentation assumes that the same transformation of the dependent variable is used by both models.
 MacKinnon,
 White and Davidson,
 
\emph on
Journal of Econometrics
\emph default
,
 (1983) shows how to deal with the case of different transformations.
 
\end_layout

\begin_layout Itemize
Monte-Carlo evidence shows that these tests often over-reject a correctly specified model.
 Can use bootstrap critical values to get better-performing tests.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Generalized least squares
\end_layout

\begin_layout Standard
Recall the assumptions of the classical linear regression model,
 in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-classical-linear"
nolink "false"

\end_inset

.
 One of the assumptions we've made up to now is that 
\begin_inset Formula 
\[
\varepsilon_{t}\sim IID(0,\sigma^{2})
\]

\end_inset

or occasionally 
\begin_inset Formula 
\[
\varepsilon_{t}\sim IIN(0,\sigma^{2}).
\]

\end_inset

Now we'll investigate the consequences of non-identically and/or dependently distributed errors.
 We'll assume fixed regressors for now,
 to keep the presentation simple,
 and later we'll look at the consequences of relaxing this admittedly unrealistic assumption.
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
\mathcal{E}(\varepsilon) & = & 0\\
V(\varepsilon) & = & \Sigma
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\Sigma$
\end_inset

 is a general symmetric positive definite matrix (we'll write 
\begin_inset Formula $\beta$
\end_inset

 in place of 
\begin_inset Formula $\beta_{0}$
\end_inset

 to simplify the typing of these notes).
\end_layout

\begin_layout Itemize
The case where 
\begin_inset Formula $\Sigma$
\end_inset

 is a diagonal matrix gives uncorrelated,
 non-identically distributed errors.
 This is known as 
\emph on
heteroscedasticity
\emph default
:
 
\begin_inset Formula $\exists i,j\,s.t.\,V(\epsilon_{i})\ne V(\epsilon_{j})$
\end_inset


\end_layout

\begin_layout Itemize
The case where 
\begin_inset Formula $\Sigma$
\end_inset

 has the same number on the main diagonal but nonzero elements off the main diagonal gives identically (assuming higher moments are also the same) dependently distributed errors.
 This is known as 
\emph on
autocorrelation
\emph default
:
 
\begin_inset Formula $\exists i\ne j\,s.t.\,E(\epsilon_{i}\epsilon_{j})\ne0)$
\end_inset


\end_layout

\begin_layout Itemize
The general case combines heteroscedasticity and autocorrelation.
 This is known as 
\begin_inset Quotes eld
\end_inset

non-spherical
\begin_inset Quotes erd
\end_inset

 disturbances,
 though why this term is used,
 I have no idea.
 Perhaps it's because under the classical assumptions,
 a joint confidence region for 
\begin_inset Formula $\varepsilon$
\end_inset

 would be an 
\begin_inset Formula $n-$
\end_inset

 dimensional hypersphere.
 
\end_layout

\begin_layout Section
Effects of non-spherical disturbances on the OLS estimator
\end_layout

\begin_layout Standard
The least square estimator is 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & (X^{\prime}X)^{-1}X^{\prime}y\\
 & = & \beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
We have unbiasedness,
 as before.
\end_layout

\begin_layout Itemize
The variance of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray}
\mathcal{E}\left[(\hat{\beta}-\beta)(\hat{\beta}-\beta)^{\prime}\right] & = & \mathcal{E}\left[(X^{\prime}X)^{-1}X^{\prime}\varepsilon\varepsilon^{\prime}X(X^{\prime}X)^{-1}\right]\nonumber \\
 & = & (X^{\prime}X)^{-1}X^{\prime}\Sigma X(X^{\prime}X)^{-1}\label{OLS covariance with nonspaerical}
\end{eqnarray}

\end_inset

 Due to this,
 any test statistic that is based upon an estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is invalid,
 since there 
\emph on
isn't
\emph default
 any 
\begin_inset Formula $\sigma^{2}$
\end_inset

,
 it doesn't exist as a feature of the true d.g.p.
 In particular,
 the formulas for the 
\begin_inset Formula $t,$
\end_inset

 
\begin_inset Formula $F,\chi^{2}$
\end_inset

 based tests given above do not lead to statistics with these distributions.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}$
\end_inset

 is still consistent,
 following exactly the same argument given before.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed,
 then
\begin_inset Formula 
\[
\hat{\beta}\sim N\left(\beta,(X^{\prime}X)^{-1}X^{\prime}\Sigma X(X^{\prime}X)^{-1}\right)
\]

\end_inset

 The problem is that 
\begin_inset Formula $\Sigma$
\end_inset

 is unknown in general,
 so this distribution won't be useful for testing hypotheses.
\end_layout

\begin_layout Itemize
Without normality,
 and with stochastic 
\begin_inset Formula $X$
\end_inset

 (e.g.,
 weakly exogenous regressors) we still have 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}\left(\hat{\beta}-\beta\right) & = & \sqrt{n}(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & \left(\frac{X^{\prime}X}{n}\right)^{-1}n^{-1/2}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Define the limiting variance of 
\begin_inset Formula $n^{-1/2}X^{\prime}\varepsilon$
\end_inset

 (supposing a CLT applies) as 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)=\Omega,\,\textrm{a.s.}
\]

\end_inset

 so we obtain 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right).\label{eq: as. dist. OLS with het/aut}
\end{equation}

\end_inset

 Note that the true asymptotic distribution of the OLS has changed with respect to the results under the classical assumptions.
 If we neglect to take this into account,
 the Wald and score tests will not be asymptotically valid.
 So we need to figure out 
\emph on
how 
\emph default
to take it into account.
\end_layout

\begin_layout Standard
To see the invalidity of test procedures that are correct under the classical assumptions,
 when we have non-spherical errors,
 consider the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GLS/EffectsOLS.jl}{GLS/EffectsOLS.jl}
\end_layout

\end_inset

.
 This script does a Monte Carlo study,
 generating data that are either heteroscedastic or homoscedastic,
 and then computes the empirical rejection frequency of a nominally 10% t-test.
 When the data are heteroscedastic,
 we obtain something like what we see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rejection-frequency-of"
nolink "false"

\end_inset

.
 This sort of heteroscedasticity causes us to reject a true null hypothesis regarding the slope parameter much too often.
 You can experiment with the script to look at the effects of other sorts of HET,
 and to vary the sample size.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Rejection-frequency-of"

\end_inset

Rejection frequency of 10% t-test,
 H0 is true.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/EffectsOLS.png
	lyxscale 50
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
\begin_inset Newpage newpage
\end_inset

Summary
\series default
:
 OLS with heteroscedasticity and/or autocorrelation is:
\end_layout

\begin_layout Itemize
unbiased with fixed or strongly exogenous regressors
\end_layout

\begin_layout Itemize
biased with weakly exogenous regressors
\end_layout

\begin_layout Itemize
has a different variance than before,
 so the previous test statistics aren't valid 
\end_layout

\begin_layout Itemize
is consistent
\end_layout

\begin_layout Itemize
is asymptotically normally distributed,
 but with a different limiting covariance matrix.
 Previous test statistics aren't valid in this case for this reason.
\end_layout

\begin_layout Itemize
is inefficient,
 as is shown below.
 
\end_layout

\begin_layout Section
The GLS estimator
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $\Sigma$
\end_inset

 were known.
 Then one could form the Cholesky decomposition
\begin_inset Formula 
\[
P^{\prime}P=\Sigma^{-1}
\]

\end_inset

 Here,
 
\begin_inset Formula $P$
\end_inset

 is an upper triangular matrix.
 We have 
\begin_inset Formula 
\[
P^{\prime}P\Sigma=I_{n}
\]

\end_inset

 so 
\begin_inset Formula 
\[
P^{\prime}P\Sigma P^{\prime}=P^{\prime},
\]

\end_inset

 which implies that 
\begin_inset Formula 
\[
P\Sigma P^{\prime}=I_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
Let's take some time to play with the Cholesky decomposition.
 Try out the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GLS/cholesky.jl}{GLS/cholesky.jl}
\end_layout

\end_inset

 Julia script to see that the above claims are true,
 and also to see how one can generate data from a 
\begin_inset Formula $N(0,V)$
\end_inset

 distribution.
\end_layout

\begin_layout Standard
Consider the model 
\begin_inset Formula 
\[
Py=PX\beta+P\varepsilon,
\]

\end_inset

 or,
 making the obvious definitions,
 
\begin_inset Formula 
\[
y^{*}=X^{*}\beta+\varepsilon^{*}.
\]

\end_inset

 This variance of 
\begin_inset Formula $\varepsilon^{*}=P\varepsilon$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(P\varepsilon\varepsilon^{\prime}P^{\prime}) & = & P\Sigma P^{\prime}\\
 & = & I_{n}
\end{eqnarray*}

\end_inset

 Therefore,
 the model 
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & X^{*}\beta+\varepsilon^{*}\\
\mathcal{E}(\varepsilon^{*}) & = & 0\\
V(\varepsilon^{*}) & = & I_{n}
\end{eqnarray*}

\end_inset

 satisfies the classical assumptions.
 The GLS estimator is simply OLS applied to the transformed model:
 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{GLS} & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}y^{*}\\
 & = & (X^{\prime}P'PX)^{-1}X^{\prime}P'Py\\
 & = & (X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The GLS estimator is unbiased in the same circumstances under which the OLS estimator is unbiased.
 For example,
 assuming 
\begin_inset Formula $X$
\end_inset

 is nonstochastic 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\hat{\beta}_{GLS}) & = & \mathcal{E}\left\{ (X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}y\right\} \\
 & = & \mathcal{E}\left\{ (X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}(X\beta+\varepsilon\right\} \\
 & = & \beta.
\end{eqnarray*}

\end_inset

To get the variance of the estimator,
 we have 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{GLS} & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}y^{*}\\
 & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}\left(X^{*}\beta+\varepsilon^{*}\right)\\
 & = & \beta+(X^{*\prime}X^{*})^{-1}X^{*\prime}\varepsilon^{*}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}\left\{ \left(\hat{\beta}_{GLS}-\beta\right)\left(\hat{\beta}_{GLS}-\beta\right)^{\prime}\right\}  & = & \mathcal{E}\left\{ (X^{*\prime}X^{*})^{-1}X^{*\prime}\varepsilon^{*}\varepsilon^{*\prime}X^{*}(X^{*\prime}X^{*})^{-1}\right\} \\
 & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}X^{*}(X^{*\prime}X^{*})^{-1}\\
 & = & (X^{*\prime}X^{*})^{-1}\\
 & = & (X^{\prime}\Sigma^{-1}X)^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Either of these last formulas can be used.
\end_layout

\begin_layout Itemize
All the previous results regarding the desirable properties of the least squares estimator hold,
 when dealing with the transformed model,
 since the transformed model satisfies the classical assumptions..
\end_layout

\begin_layout Itemize
Tests are valid,
 using the previous formulas,
 as long as we substitute 
\begin_inset Formula $X^{\ast}$
\end_inset

 in place of 
\begin_inset Formula $X.$
\end_inset

 Furthermore,
 any test that involves 
\begin_inset Formula $\sigma^{2}$
\end_inset

 can set it to 
\begin_inset Formula $1.$
\end_inset

 This is preferable to re-deriving the appropriate formulas.
\end_layout

\begin_layout Itemize
The GLS estimator is more efficient than the OLS estimator.
 This is a consequence of the Gauss-Markov theorem,
 since the GLS estimator is based on a model that satisfies the classical assumptions but the OLS estimator is not.
 To see this directly,
 note that
\begin_inset Formula 
\begin{eqnarray*}
Var(\hat{\beta})-Var(\hat{\beta}_{GLS}) & = & (X'X)^{-1}X'\Sigma X(X'X)^{-1}-(X'\Sigma^{-1}X)^{-1}\\
 & = & A\Sigma A^{'}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $A=\left[\left(X^{\prime}X\right)^{-1}X^{\prime}-(X'\Sigma^{-1}X)^{-1}X'\Sigma^{-1}\right].$
\end_inset

 This may not seem obvious,
 but it is true,
 as you can verify for yourself.
 Then noting that 
\begin_inset Formula $A\Sigma A^{'}$
\end_inset

 is a quadratic form in a positive definite matrix,
 we conclude that 
\begin_inset Formula $A\Sigma A^{'}$
\end_inset

 is positive semi-definite,
 and that GLS is efficient relative to OLS.
\end_layout

\begin_layout Itemize
As one can verify by calculating first order conditions,
 the GLS estimator is the solution to the minimization problem 
\begin_inset Formula 
\[
\hat{\beta}_{GLS}=\arg\min(y-X\beta)^{\prime}\Sigma^{-1}(y-X\beta)
\]

\end_inset

 so the 
\emph on
metric
\emph default
 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 is used to weight the residuals.
 
\end_layout

\begin_layout Section
Feasible GLS
\end_layout

\begin_layout Standard
The problem is that 
\begin_inset Formula $\Sigma$
\end_inset

 ordinarily isn't known,
 so this estimator isn't available.
\end_layout

\begin_layout Itemize
Consider the dimension of 
\begin_inset Formula $\Sigma$
\end_inset

 :
 it's an 
\begin_inset Formula $n\times n$
\end_inset

 matrix with 
\begin_inset Formula $\left(n^{2}-n\right)/2+n=\left(n^{2}+n\right)/2$
\end_inset

 unique elements (remember - it is symmetric,
 because it's a covariance matrix).
\end_layout

\begin_layout Itemize
The number of parameters to estimate is larger than 
\begin_inset Formula $n$
\end_inset

 and increases faster than 
\begin_inset Formula $n.$
\end_inset

 There's no way to devise an estimator that satisfies a LLN without adding restrictions.
\end_layout

\begin_layout Itemize
The 
\emph on
feasible GLS estimator
\emph default
 is based upon making sufficient assumptions regarding the form of 
\begin_inset Formula $\Sigma$
\end_inset

 so that a consistent estimator can be devised.
 
\end_layout

\begin_layout Standard
Suppose that we 
\emph on
parameterize
\emph default
 
\begin_inset Formula $\Sigma$
\end_inset

 as a function of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

,
 where 
\begin_inset Formula $\theta$
\end_inset

 may include 
\begin_inset Formula $\beta$
\end_inset

 as well as other parameters,
 so that 
\begin_inset Formula 
\[
\Sigma=\Sigma(X,\theta)
\]

\end_inset

 where 
\begin_inset Formula $\theta$
\end_inset

 is of fixed dimension.
 If we can consistently estimate 
\begin_inset Formula $\theta,$
\end_inset

 we can consistently estimate 
\begin_inset Formula $\Sigma,$
\end_inset

 as long as the elements of 
\begin_inset Formula $\Sigma(X,\theta)$
\end_inset

 are continuous functions of 
\begin_inset Formula $\theta$
\end_inset

 (by the Slutsky theorem).
 In this case,
 
\begin_inset Formula 
\[
\widehat{\Sigma}=\Sigma(X,\hat{\theta})\overset{p}{\rightarrow}\Sigma(X,\theta)
\]

\end_inset

 If we replace 
\begin_inset Formula $\Sigma$
\end_inset

 in the formulas for the GLS estimator with 
\begin_inset Formula $\widehat{\Sigma},$
\end_inset

 we obtain the FGLS estimator.
 
\series bold
The FGLS estimator shares the same asymptotic properties as GLS.
 These are
\end_layout

\begin_layout Enumerate
Consistency
\end_layout

\begin_layout Enumerate
Asymptotic normality
\end_layout

\begin_layout Enumerate
Asymptotic efficiency 
\emph on
if
\emph default
 the errors are normally distributed.
 (Cramér-Rao).
\end_layout

\begin_layout Enumerate
Test procedures are asymptotically valid.
 
\end_layout

\begin_layout Standard

\series bold
In practice,
 the usual way to proceed is
\end_layout

\begin_layout Enumerate
Define a consistent estimator of 
\begin_inset Formula $\theta.$
\end_inset

 This is a case-by-case proposition,
 depending on the parameterization 
\begin_inset Formula $\Sigma(\theta).$
\end_inset

 We'll see examples below.
\end_layout

\begin_layout Enumerate
Form 
\begin_inset Formula $\widehat{\Sigma}=\Sigma(X,\hat{\theta})$
\end_inset


\end_layout

\begin_layout Enumerate
Calculate the Cholesky factorization 
\begin_inset Formula $\widehat{P}=Chol(\hat{\Sigma}^{-1})$
\end_inset

.
\end_layout

\begin_layout Enumerate
Transform the model using 
\begin_inset Formula 
\[
\hat{P}y=\hat{P}X\beta+\hat{P}\varepsilon
\]

\end_inset


\end_layout

\begin_layout Enumerate
Estimate using OLS on the transformed model.
 
\end_layout

\begin_layout Section
Heteroscedasticity
\end_layout

\begin_layout Standard
Heteroscedasticity is the case where 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon\varepsilon^{\prime})=\Sigma
\]

\end_inset

 is a diagonal matrix,
 so that the errors are uncorrelated,
 but have different variances.
 Heteroscedasticity is usually thought of as associated with cross sectional data,
 though there is absolutely no reason why time series data cannot also be heteroscedastic.
 Actually,
 the popular ARCH (autoregressive conditionally heteroscedastic) models explicitly assume that a time series is conditionally heteroscedastic.
\end_layout

\begin_layout Standard
Consider a supply function 
\begin_inset Formula 
\[
q_{i}=\beta_{1}+\beta_{p}P_{i}+\beta_{s}S_{i}+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula $P_{i}$
\end_inset

 is price and 
\begin_inset Formula $S_{i}$
\end_inset

 is some measure of size of the 
\begin_inset Formula $i^{th}$
\end_inset

 firm.
 One might suppose that unobservable factors (e.g.,
 talent of managers,
 degree of coordination between production units,
 
\emph on
etc.
\emph default
) account for the error term 
\begin_inset Formula $\varepsilon_{i}.$
\end_inset

 If there is more variability in these factors for large firms than for small firms,
 then 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 may have a higher variance when 
\begin_inset Formula $S_{i}$
\end_inset

 is high than when it is low.
\end_layout

\begin_layout Standard
Another example,
 individual demand.
 
\begin_inset Formula 
\[
q_{i}=\beta_{1}+\beta_{p}P_{i}+\beta_{m}M_{i}+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula $P$
\end_inset

 is price and 
\begin_inset Formula $M$
\end_inset

 is income.
 In this case,
 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 can reflect variations in preferences.
 There are more possibilities for expression of preferences when one is rich,
 so it is possible that the variance of 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 could be higher when 
\begin_inset Formula $M$
\end_inset

 is high.
\end_layout

\begin_layout Standard

\emph on
Add example of group means.
\end_layout

\begin_layout Subsection
OLS with heteroscedastic consistent varcov estimation
\end_layout

\begin_layout Standard
Eicker (1967) and White (1980) showed how to modify test statistics to account for heteroscedasticity of unknown form.
 The OLS estimator has asymptotic distribution 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right)
\]

\end_inset

 as we've already seen.
 Recall that we defined 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)=\Omega
\]

\end_inset

 This matrix has dimension 
\begin_inset Formula $K\times K$
\end_inset

 and can be consistently estimated,
 even if we can't estimate 
\begin_inset Formula $\Sigma$
\end_inset

 consistently.
 The consistent estimator,
 under heteroscedasticity but no autocorrelation is 
\begin_inset Formula 
\[
\widehat{\Omega}=\frac{1}{n}\sum_{t=1}^{n}x_{t}x_{t}^{\prime}\hat{\varepsilon}_{t}^{2}
\]

\end_inset

 One can then modify the previous test statistics to obtain tests that are valid when there is heteroscedasticity of unknown form.
 For example,
 the Wald test for 
\begin_inset Formula $H_{0}:R\beta-r=0$
\end_inset

 would be 
\begin_inset Formula 
\[
n\left(R\hat{\beta}-r\right)^{\prime}\left(R\left(\frac{X^{\prime}X}{n}\right)^{-1}\hat{\Omega}\left(\frac{X^{\prime}X}{n}\right)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{a}{\sim}\chi^{2}(q)
\]

\end_inset


\end_layout

\begin_layout Subsection
Detection
\end_layout

\begin_layout Standard
There exist many tests for the presence of heteroscedasticity.
 We'll discuss three methods.
\end_layout

\begin_layout Paragraph
Goldfeld-Quandt
\end_layout

\begin_layout Standard
The sample is divided in to three parts,
 with 
\begin_inset Formula $n_{1},n_{2}$
\end_inset

 and 
\begin_inset Formula $n_{3}$
\end_inset

 observations,
 where 
\begin_inset Formula $n_{1}+n_{2}+n_{3}=n$
\end_inset

.
 The model is estimated using the first and third parts of the sample,
 separately,
 so that 
\begin_inset Formula $\hat{\beta}^{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}^{3}$
\end_inset

 will be independent.
 Then we have 
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}^{1\prime}\hat{\varepsilon}^{1}}{\sigma^{2}}=\frac{\varepsilon^{1^{\prime}}M^{1}\varepsilon^{1}}{\sigma^{2}}\overset{d}{\rightarrow}\chi^{2}(n_{1}-K)
\]

\end_inset

 and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}^{3\prime}\hat{\varepsilon}^{3}}{\sigma^{2}}=\frac{\varepsilon^{3^{\prime}}M^{3}\varepsilon^{3}}{\sigma^{2}}\overset{d}{\rightarrow}\chi^{2}(n_{3}-K)
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}^{1\prime}\hat{\varepsilon}^{1}/(n_{1}-K)}{\hat{\varepsilon}^{3\prime}\hat{\varepsilon}^{3}/(n_{3}-K)}\overset{d}{\rightarrow}F(n_{1}-K,n_{3}-K).
\]

\end_inset

 The distributional result is exact if the errors are normally distributed.
 This test is a two-tailed test.
 Alternatively,
 and probably more conventionally,
 if one has prior ideas about the possible magnitudes of the variances of the observations,
 one could order the observations accordingly,
 from largest to smallest.
 In this case,
 one would use a conventional one-tailed F-test.
 
\emph on
Draw picture.
\end_layout

\begin_layout Itemize
Ordering the observations is an important step if the test is to have any power.
\end_layout

\begin_layout Itemize
The motive for dropping the middle observations is to increase the difference between the average variance in the subsamples,
 supposing that there exists heteroscedasticity.
 This can increase the power of the test.
 On the other hand,
 dropping too many observations will substantially increase the variance of the statistics 
\begin_inset Formula $\hat{\varepsilon}^{1\prime}\hat{\varepsilon}^{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\varepsilon}^{3\prime}\hat{\varepsilon}^{3}.$
\end_inset

 A rule of thumb,
 based on Monte Carlo experiments is to drop around 25% of the observations.
\end_layout

\begin_layout Itemize
If one doesn't have any ideas about the form of the het.
 the test will probably have low power since a sensible data ordering isn't available.
 
\end_layout

\begin_layout Paragraph
White's test
\end_layout

\begin_layout Standard
When one has little idea if there exists heteroscedasticity,
 and no idea of its potential form,
 the White test is a possibility.
 The idea is that if there is homoscedasticity,
 then 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon_{t}^{2}|x_{t})=\sigma^{2},\forall t
\]

\end_inset

 so that 
\begin_inset Formula $x_{t}$
\end_inset

 or functions of 
\begin_inset Formula $x_{t}$
\end_inset

 shouldn't help to explain 
\begin_inset Formula $\mathcal{E}(\varepsilon_{t}^{2}).$
\end_inset

 The test works as follows:
\end_layout

\begin_layout Enumerate
Since 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 isn't available,
 use the consistent estimator 
\begin_inset Formula $\hat{\varepsilon}_{t}$
\end_inset

 instead.
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}^{2}=\sigma^{2}+z_{t}^{\prime}\gamma+v_{t}
\]

\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 is a 
\begin_inset Formula $P$
\end_inset

-vector.
 
\begin_inset Formula $z_{t}$
\end_inset

 may include some or all of the variables in 
\begin_inset Formula $x_{t},$
\end_inset

 as well as other variables.
 White's original suggestion was to use 
\begin_inset Formula $x_{t}$
\end_inset

,
 plus the set of all unique squares and cross products of variables in 
\begin_inset Formula $x_{t}.$
\end_inset


\end_layout

\begin_layout Enumerate
Test the hypothesis that 
\begin_inset Formula $\gamma=0.$
\end_inset

 The 
\begin_inset Formula $qF$
\end_inset

 statistic in this case is 
\begin_inset Formula 
\[
qF=\frac{P\left(ESS_{R}-ESS_{U}\right)/P}{ESS_{U}/\left(n-P-1\right)}
\]

\end_inset

 Note that 
\begin_inset Formula $ESS_{R}=TSS_{U},$
\end_inset

 so dividing both numerator and denominator by this we get 
\begin_inset Formula 
\[
qF=\left(n-P-1\right)\frac{R^{2}}{1-R^{2}}
\]

\end_inset

 Note that this is the 
\begin_inset Formula $R^{2}$
\end_inset

 of the artificial regression used to test for heteroscedasticity,
 not the 
\begin_inset Formula $R^{2}$
\end_inset

 of the original model.
 
\end_layout

\begin_layout Standard
An asymptotically equivalent statistic,
 under the null of no heteroscedasticity (so that 
\begin_inset Formula $R^{2}$
\end_inset

 should tend to zero),
 is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
nR^{2}\overset{a}{\sim}\chi^{2}(P).
\]

\end_inset

 This doesn't require normality of the errors,
 though it does assume that the fourth moment of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is constant,
 under the null.
 
\series bold
Question
\series default
:
 why is this necessary?
\end_layout

\begin_layout Itemize
The White test has the disadvantage that it may not be very powerful unless the 
\begin_inset Formula $z_{t}$
\end_inset

 vector is chosen well,
 and this is hard to do without knowledge of the form of heteroscedasticity.
\end_layout

\begin_layout Itemize
It also has the problem that specification errors other than heteroscedasticity may lead to rejection.
\end_layout

\begin_layout Itemize
Note:
 the null hypothesis of this test may be interpreted as 
\begin_inset Formula $\theta=0$
\end_inset

 for the variance model 
\begin_inset Formula $V(\varepsilon_{t}^{2})=h(\alpha+z_{t}^{\prime}\theta),$
\end_inset

 where 
\begin_inset Formula $h(\cdot)$
\end_inset

 is an arbitrary function of unknown form.
 The test is more general than is may appear from the regression that is used.
\end_layout

\begin_layout Paragraph
Plotting the residuals
\end_layout

\begin_layout Standard
A very simple method is to simply plot the residuals (or their squares).
 
\emph on
Draw pictures here
\emph default
.
 Like the Goldfeld-Quandt test,
 this will be more informative if the observations are ordered according to the suspected form of the heteroscedasticity.
\end_layout

\begin_layout Subsection
Correction
\end_layout

\begin_layout Standard
Correcting for heteroscedasticity requires that a parametric form for 
\begin_inset Formula $\Sigma(\theta)$
\end_inset

 be supplied,
 and that a means for estimating 
\begin_inset Formula $\theta$
\end_inset

 consistently be determined.
 The estimation method will be specific to the assumed form 
\begin_inset Formula $\Sigma(\theta).$
\end_inset

 In recent years,
 there has been a trend toward simply estimating by OLS,
 and using robust standard errors.
 This may be somewhat unfortunate,
 as the weighted least squares estimator (GLS when there is only HET) is still consistent even if the specification of 
\begin_inset Formula $\Sigma(\theta)$
\end_inset

 is incorrect,
 and it may be a good deal more efficient than OLS.
 Also,
 robust standard errors don't always work so well.
 
\end_layout

\begin_layout Example
The GRETL script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GLS/Heteroscedasticity.inp}{GLS/Heteroscedasticity.inp}
\end_layout

\end_inset

 illustrates these points.
 
\end_layout

\begin_layout Standard
Perhaps a middle ground is to attempt to use GLS when severe HET is detected,
 but to continue to use robust standard errors,
 to account for misspecifications in the modeling of 
\begin_inset Formula $\Sigma(\theta).$
\end_inset


\end_layout

\begin_layout Standard
We'll consider two examples.
 Before this,
 let's consider the general nature of GLS when there is heteroscedasticity.
 When we have HET but no AUT,
 
\begin_inset Formula $\Sigma$
\end_inset

 is a diagonal matrix:
\begin_inset Formula 
\[
\Sigma=\left[\begin{array}{cccc}
\sigma_{1}^{2} & 0 & \ldots & 0\\
\vdots & \sigma_{2}^{2} &  & \vdots\\
 &  & \ddots & 0\\
0 & \cdots & 0 & \sigma_{n}^{2}
\end{array}\right]
\]

\end_inset

Likewise,
 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 is diagonal
\begin_inset Formula 
\[
\Sigma^{-1}=\left[\begin{array}{cccc}
\frac{1}{\sigma_{1}^{2}} & 0 & \ldots & 0\\
\vdots & \frac{1}{\sigma_{2}^{2}} &  & \vdots\\
 &  & \ddots & 0\\
0 & \cdots & 0 & \frac{1}{\sigma_{n}^{2}}
\end{array}\right]
\]

\end_inset

and so is the Cholesky decomposition 
\begin_inset Formula $P=chol(\Sigma^{-1}$
\end_inset

)
\begin_inset Formula 
\[
P=\left[\begin{array}{cccc}
\frac{1}{\sigma_{1}} & 0 & \ldots & 0\\
\vdots & \frac{1}{\sigma_{2}} &  & \vdots\\
 &  & \ddots & 0\\
0 & \cdots & 0 & \frac{1}{\sigma_{n}}
\end{array}\right]
\]

\end_inset

 We need to transform the model,
 just as before,
 in the general case:
 
\begin_inset Formula 
\[
Py=PX\beta+P\varepsilon,
\]

\end_inset

 or,
 making the obvious definitions,
 
\begin_inset Formula 
\[
y^{*}=X^{*}\beta+\varepsilon^{*}.
\]

\end_inset

 Note that multiplying by 
\begin_inset Formula $P$
\end_inset

 just divides the data for each observation (
\begin_inset Formula $y_{i},x_{i})$
\end_inset

 by the corresponding standard error of the error term,
 
\begin_inset Formula $\sigma_{i}$
\end_inset

.
 That is,
 
\begin_inset Formula $y_{i}^{*}=y_{i}/\sigma_{i}$
\end_inset

 and 
\begin_inset Formula $x_{i}^{*}=x_{i}/\sigma_{i}$
\end_inset

 (note that 
\begin_inset Formula $x_{i}$
\end_inset

 is a 
\begin_inset Formula $K$
\end_inset

-vector:
 we divided each element,
 including the 1 corresponding to the constant).
\end_layout

\begin_layout Standard
This makes sense.
 Consider Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Motivation-for-GLS"
nolink "false"

\end_inset

,
 which shows a true regression line with heteroscedastic errors.
 Which sample is more informative about the location of the line?
 The ones with observations with smaller variances.
 So,
 the GLS solution is equivalent to OLS on the transformed data.
 By the transformed data is the original data,
 weighted by the inverse of the standard error of the observation's error term.
 When the standard error is small,
 the weight is high,
 and vice versa.
 The GLS correction for the case of HET is also known as weighted least squares,
 for this reason.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Motivation-for-GLS"

\end_inset

Motivation for GLS correction when there is HET
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/wls.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Multiplicative heteroscedasticity
\end_layout

\begin_layout Standard
Suppose the model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\sigma_{t}^{2} & = & \mathcal{E}(\varepsilon_{t}^{2})=\left(z_{t}^{\prime}\gamma\right)^{\delta}
\end{eqnarray*}

\end_inset

 but the other classical assumptions hold.
 In this case 
\begin_inset Formula 
\[
\varepsilon_{t}^{2}=\left(z_{t}^{\prime}\gamma\right)^{\delta}+v_{t}
\]

\end_inset

 and 
\begin_inset Formula $v_{t}$
\end_inset

 has mean zero.
 Nonlinear least squares could be used to estimate 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\delta$
\end_inset

 consistently,
 were 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 observable.
 The solution is to substitute the squared OLS residuals 
\begin_inset Formula $\hat{\varepsilon}_{t}^{2}$
\end_inset

 in place of 
\begin_inset Formula $\varepsilon_{t}^{2},$
\end_inset

 since it is consistent by the Slutsky theorem.
 Once we have 
\begin_inset Formula $\hat{\gamma}$
\end_inset

 and 
\begin_inset Formula $\hat{\delta},$
\end_inset

 we can estimate 
\begin_inset Formula $\sigma_{t}^{2}$
\end_inset

 consistently using 
\begin_inset Formula 
\[
\hat{\sigma}_{t}^{2}=\left(z_{t}^{\prime}\hat{\gamma}\right)^{\hat{\delta}}\overset{p}{\rightarrow\sigma_{t}^{2}}.
\]

\end_inset

 In the second step,
 we transform the model by dividing by the standard deviation:
 
\begin_inset Formula 
\[
\frac{y_{t}}{\hat{\sigma}_{t}}=\frac{x_{t}^{\prime}\beta}{\hat{\sigma}_{t}}+\frac{\varepsilon_{t}}{\hat{\sigma}_{t}}
\]

\end_inset

 or 
\begin_inset Formula 
\[
y_{t}^{*}=x_{t}^{*\prime}\beta+\varepsilon_{t}^{*}.
\]

\end_inset

 Asymptotically,
 this model satisfies the classical assumptions.
\end_layout

\begin_layout Itemize
This model is a bit complex in that NLS is required to estimate the model of the variance.
 A simpler version would be 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\sigma_{t}^{2} & =\mathcal{E}(\varepsilon_{t}^{2})= & \sigma^{2}z_{t}^{\delta}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 is a single variable.
 There are still two parameters to be estimated,
 and the model of the variance is still nonlinear in the parameters.
 However,
 the 
\emph on
search method
\emph default
 can be used in this case to reduce the estimation problem to repeated applications of OLS.
\end_layout

\begin_layout Itemize
First,
 we define an interval of reasonable values for 
\begin_inset Formula $\delta,$
\end_inset

 e.g.,
 
\begin_inset Formula $\delta\in[0,3].$
\end_inset


\end_layout

\begin_layout Itemize
Partition this interval into 
\begin_inset Formula $M$
\end_inset

 equally spaced values,
 e.g.,
 
\begin_inset Formula $\{0,.1,.2,...,2.9,3\}.$
\end_inset


\end_layout

\begin_layout Itemize
For each of these values,
 calculate the variable 
\begin_inset Formula $z_{t}^{\delta_{m}}.$
\end_inset


\end_layout

\begin_layout Itemize
The regression 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}^{2}=\sigma^{2}z_{t}^{\delta_{m}}+v_{t}
\]

\end_inset

 is linear in the parameters,
 conditional on 
\begin_inset Formula $\delta_{m},$
\end_inset

 so one can estimate 
\begin_inset Formula $\sigma^{2}$
\end_inset

 by OLS.
\end_layout

\begin_layout Itemize
Save the pairs (
\begin_inset Formula $\sigma_{m}^{2},\delta_{m}),$
\end_inset

 and the corresponding 
\begin_inset Formula $ESS_{m}.$
\end_inset

 Choose the pair with the minimum 
\begin_inset Formula $ESS_{m}$
\end_inset

 as the estimate.
 
\end_layout

\begin_layout Itemize
Next,
 divide the model by the estimated standard deviations.
\end_layout

\begin_layout Itemize
Can refine.
 
\emph on
Draw picture.
 
\end_layout

\begin_layout Itemize
Works well when the parameter to be searched over is low dimensional,
 as in this case.
 
\end_layout

\begin_layout Subsubsection
Groupwise heteroscedasticity
\end_layout

\begin_layout Standard
A common case is where we have repeated observations on each of a number of economic agents:
 e.g.,
 10 years of macroeconomic data on each of a set of countries or regions,
 or daily observations of transactions of 200 banks.
 This sort of data is a 
\emph on
pooled cross-section time-series model.

\emph default
 It may be reasonable to presume that the variance is constant over time within the cross-sectional units,
 but that it differs across them (e.g.,
 firms or countries of different sizes...).
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{it} & = & x_{it}^{\prime}\beta+\varepsilon_{it}\\
\mathcal{E}(\varepsilon_{it}^{2}) & = & \sigma_{i}^{2},\forall t
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $i=1,2,...,G$
\end_inset

 are the agents,
 and 
\begin_inset Formula $t=1,2,...,n$
\end_inset

 are the observations on each agent.
\end_layout

\begin_layout Itemize
The other classical assumptions are presumed to hold.
\end_layout

\begin_layout Itemize
In this case,
 the variance 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 is specific to each agent,
 but constant over the 
\begin_inset Formula $n$
\end_inset

 observations for that agent.
\end_layout

\begin_layout Itemize
In this model,
 we assume that 
\begin_inset Formula $\mathcal{E}(\varepsilon_{it}\varepsilon_{is})=0.$
\end_inset

 This is a strong assumption that we'll relax later.
 
\end_layout

\begin_layout Standard
To correct for heteroscedasticity,
 just estimate each 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 using the natural estimator:
 
\begin_inset Formula 
\[
\hat{\sigma}_{i}^{2}=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{it}^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that we use 
\begin_inset Formula $1/n$
\end_inset

 here since it's possible that there are more than 
\begin_inset Formula $n$
\end_inset

 regressors,
 so 
\begin_inset Formula $n-K$
\end_inset

 could be negative.
 Asymptotically the difference is unimportant.
\end_layout

\begin_layout Itemize
With each of these,
 transform the model as usual:
 
\begin_inset Formula 
\[
\frac{y_{it}}{\hat{\sigma}_{i}}=\frac{x_{it}^{\prime}\beta}{\hat{\sigma}_{i}}+\frac{\varepsilon_{it}}{\hat{\sigma}_{i}}
\]

\end_inset

 Do this for each cross-sectional group.
 This transformed model satisfies the classical assumptions,
 asymptotically.
 
\end_layout

\begin_layout Subsection
Example:
 the Nerlove model
\end_layout

\begin_layout Itemize
Here's the data in Gretl format:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Data/nerlove.gdt}{nerlove.gdt} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
estimate the basic Nerlove model by OLS,
 using Gretl,
 and plot the residuals:
 evidence of HET and AUT
\end_layout

\begin_layout Itemize
include square of 
\begin_inset Formula $\ln Q$
\end_inset

,
 now there's no AUT,
 but still HET.
 Conclusion:
 apparent AUT may be evidence of misspecification,
 rather than true autocorrelation.
 
\end_layout

\begin_layout Itemize
estimate using HET correction,
 and compare standard error estimates.
\end_layout

\begin_layout Section
Autocorrelation
\end_layout

\begin_layout Standard
Autocorrelation,
 which is the serial correlation of the error term,
 is a problem that is usually associated with time series data,
 but also can affect cross-sectional data.
 For example,
 a shock to oil prices will simultaneously affect all countries,
 so one could expect contemporaneous correlation of macroeconomic variables across countries.
\end_layout

\begin_layout Subsection
Example
\end_layout

\begin_layout Standard
Consider the Keeling-Whorf data on atmospheric CO2 concentrations an Mauna Loa,
 Hawaii (see 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Keeling_Curve
\end_layout

\end_inset

 and 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://cdiac.ornl.gov/ftp/ndp001/maunaloa.txt
\end_layout

\end_inset

).
\end_layout

\begin_layout Standard
From the file maunaloa.txt:
 
\begin_inset Quotes sld
\end_inset

THE DATA FILE PRESENTED IN THIS SUBDIRECTORY CONTAINS MONTHLY AND ANNUAL ATMOSPHERIC CO2 CONCENTRATIONS DERIVED FROM THE SCRIPPS INSTITUTION OF OCEANOGRAPHY'S (SIO's) CONTINUOUS MONITORING PROGRAM AT MAUNA LOA OBSERVATORY,
 HAWAII.
 THIS RECORD CONSTITUTES THE LONGEST CONTINUOUS RECORD OF ATMOSPHERIC CO2 CONCENTRATIONS AVAILABLE IN THE WORLD.
 MONTHLY AND ANNUAL AVERAGE MOLE FRACTIONS OF CO2 IN WATER-VAPOR-FREE AIR ARE GIVEN FROM MARCH 1958 THROUGH DECEMBER 2003,
 EXCEPT FOR A FEW INTERRUPTIONS.
\begin_inset Quotes srd
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The data is available in Octave format at 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Data/CO2.data}{CO2.data} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
If we fit,
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GLS/CO2Example.jl}{using this script} 
\end_layout

\end_inset

,
 the model 
\begin_inset Formula $CO2_{t}=\beta_{1}+\beta_{2}t+\epsilon_{t}$
\end_inset

,
 we get the results
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GLS/CO2Example.out"
literal "true"

\end_inset

It seems pretty clear that CO2 concentrations have been going up in the last 50 years,
 surprise,
 surprise.
 Let's look at a residual plot for the last 3 years of the data,
 see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Residuals-from-time"
nolink "false"

\end_inset

.
 Note that there is a very predictable pattern.
 This is pretty strong evidence that the errors of the model are not independent of one another,
 which means there seems to be autocorrelation.
 
\end_layout

\begin_layout Itemize
this data is clearly nonstationary.
 The very large t-statistics that you get from OLS are suspicious,
 no?
\end_layout

\begin_layout Itemize
What is the limit of 
\begin_inset Formula $X^{\prime}X/n$
\end_inset

 when there is a time trend in the regressor matrix?
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Residuals-from-time"

\end_inset

Residuals from time trend for CO2 data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/CO2Residuals.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Causes
\end_layout

\begin_layout Standard
Autocorrelation is the existence of correlation across the error term:
 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon_{t}\varepsilon_{s})\neq0,t\neq s.
\]

\end_inset

 Why might this occur?
 Plausible explanations include
\end_layout

\begin_layout Enumerate
Lags in adjustment to shocks.
 In a model such as 
\begin_inset Formula 
\[
y_{t}=x_{t}^{\prime}\beta+\varepsilon_{t},
\]

\end_inset

 one could interpret 
\begin_inset Formula $x_{t}^{\prime}\beta$
\end_inset

 as the equilibrium value.
 Suppose 
\begin_inset Formula $x_{t}$
\end_inset

 is constant over a number of observations.
 One can interpret 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 as a shock that moves the system away from equilibrium.
 If the time needed to return to equilibrium is long with respect to the observation frequency,
 one could expect 
\begin_inset Formula $\varepsilon_{t+1}$
\end_inset

 to be positive,
 conditional on 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 positive,
 which induces a correlation.
 
\end_layout

\begin_layout Enumerate
Unobserved factors that are correlated over time.
 The error term is often assumed to correspond to unobservable factors.
 If these factors are correlated over time,
 there will be autocorrelation.
\end_layout

\begin_layout Enumerate
Misspecification of the model.
 Suppose that the DGP is 
\begin_inset Formula 
\[
y_{t}=\beta_{0}+\beta_{1}x_{t}+\beta_{2}x_{t}^{2}+\varepsilon_{t}
\]

\end_inset

 but we estimate 
\begin_inset Formula 
\[
y_{t}=\beta_{0}+\beta_{1}x_{t}+\varepsilon_{t}
\]

\end_inset

 The effects are illustrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Autocorrelation-induced-by"
nolink "false"

\end_inset

.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Autocorrelation-induced-by"

\end_inset

Autocorrelation induced by misspecification
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/MisspecCausesAutcorrelation.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Neglecting to include dynamics in a model.
 Lags of the dependent variable may be relevant regressors,
 and if they are omitted,
 their effects go into the error term,
 which will be autocorrelated.
 
\end_layout

\begin_layout Subsection
Effects on the OLS estimator
\end_layout

\begin_layout Standard
The variance of the OLS estimator is the same as in the case of heteroscedasticity - the standard formula does not apply.
 The correct formula is given in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "OLS covariance with nonspaerical"
nolink "false"

\end_inset

.
 Next we discuss two GLS corrections for OLS.
 This sort of solution may lead to inconsistent estimation of betas in some cases,
 and it has definitely gone completely out of style.
 The standard procedure is to include enough lags of the dependent variable so that detectable AUT disappears,
 and then to use robust covariance estimation to take care of residual effects (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Asymptotically-valid-inferences"
nolink "false"

\end_inset

).
 For reference,
 a couple of examples of the old-fashioned GLS corrections follow,
 but I will not discuss this in class.
\end_layout

\begin_layout Subsection
AR(1)
\end_layout

\begin_layout Standard
There are many types of autocorrelation.
 We'll consider two examples.
 The first is the most commonly encountered case:
 autoregressive order 1 (AR(1) errors.
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\varepsilon_{t} & = & \rho\varepsilon_{t-1}+u_{t}\\
u_{t} & \sim & iid(0,\sigma_{u}^{2})\\
\mathcal{E}(\varepsilon_{t}u_{s}) & = & 0,t<s
\end{eqnarray*}

\end_inset

 We assume that the model satisfies the other classical assumptions.
\end_layout

\begin_layout Itemize
We need a stationarity assumption:
 
\begin_inset Formula $|\rho|<1.$
\end_inset

 Otherwise the variance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 explodes as 
\begin_inset Formula $t$
\end_inset

 increases,
 so standard asymptotics will not apply.
\end_layout

\begin_layout Itemize
By recursive substitution we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\varepsilon_{t} & = & \rho\varepsilon_{t-1}+u_{t}\\
 & = & \rho\left(\rho\varepsilon_{t-2}+u_{t-1}\right)+u_{t}\\
 & = & \rho^{2}\varepsilon_{t-2}+\rho u_{t-1}+u_{t}\\
 & = & \rho^{2}\left(\rho\varepsilon_{t-3}+u_{t-2}\right)+\rho u_{t-1}+u_{t}
\end{eqnarray*}

\end_inset

 In the limit the lagged 
\begin_inset Formula $\varepsilon$
\end_inset

 drops out,
 since 
\begin_inset Formula $\rho^{m}\rightarrow0$
\end_inset

 as 
\begin_inset Formula $m\rightarrow\infty,$
\end_inset

 so we obtain 
\begin_inset Formula 
\[
\varepsilon_{t}=\sum_{m=0}^{\infty}\rho^{m}u_{t-m}
\]

\end_inset

 With this,
 the variance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is found as 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\varepsilon_{t}^{2}) & = & \sigma_{u}^{2}\sum_{m=0}^{\infty}\rho^{2m}\\
 & = & \frac{\sigma_{u}^{2}}{1-\rho^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
If we had directly assumed that 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 were covariance stationary,
 we could obtain this using 
\begin_inset Formula 
\begin{eqnarray*}
V(\varepsilon_{t}) & = & \rho^{2}\mathcal{E}(\varepsilon_{t-1}^{2})+2\rho\mathcal{E}(\varepsilon_{t-1}u_{t})+\mathcal{E}(u_{t}^{2})\\
 & = & \rho^{2}V(\varepsilon_{t})+\sigma_{u}^{2},
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
V(\varepsilon_{t})=\frac{\sigma_{u}^{2}}{1-\rho^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
The variance is the 
\begin_inset Formula $0^{th}$
\end_inset

 order autocovariance:
 
\begin_inset Formula $\gamma_{0}=V(\varepsilon_{t})$
\end_inset


\end_layout

\begin_layout Itemize
Note that the variance does not depend on 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Standard
Likewise,
 the first order autocovariance 
\begin_inset Formula $\gamma_{1}$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
Cov(\varepsilon_{t},\varepsilon_{t-1}) & =\gamma_{s}= & \mathcal{E}(\left(\rho\varepsilon_{t-1}+u_{t}\right)\varepsilon_{t-1})\\
 & = & \rho V(\varepsilon_{t})\\
 & = & \frac{\rho\sigma_{u}^{2}}{1-\rho^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Using the same method,
 we find that for 
\begin_inset Formula $s<t$
\end_inset


\begin_inset Formula 
\[
Cov(\varepsilon_{t},\varepsilon_{t-s})=\gamma_{s}=\frac{\rho^{s}\sigma_{u}^{2}}{1-\rho^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
The autocovariances don't depend on 
\begin_inset Formula $t$
\end_inset

:
 the process 
\begin_inset Formula $\{\varepsilon_{t}\}$
\end_inset

 is 
\emph on
covariance stationary
\end_layout

\begin_layout Standard
The 
\emph on
correlation (
\emph default
in general,
 for r.v.'s 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

) is defined as 
\begin_inset Formula 
\[
\text{corr}(x,y)=\frac{\text{cov}(x,y)}{\text{se}(x)\text{se}(y)}
\]

\end_inset

 but in this case,
 the two standard errors are the same,
 so the 
\begin_inset Formula $s$
\end_inset

-order autocorrelation 
\begin_inset Formula $\rho_{s}$
\end_inset

 is 
\begin_inset Formula 
\[
\rho_{s}=\rho^{s}
\]

\end_inset


\end_layout

\begin_layout Itemize
All this means that the overall matrix 
\begin_inset Formula $\Sigma$
\end_inset

 has the form 
\begin_inset Formula 
\[
\Sigma=\underbrace{\frac{\sigma_{u}^{2}}{1-\rho^{2}}}_{\textrm{this is the variance}}\underbrace{\left[\begin{array}{ccccc}
1 & \rho & \rho^{2} & \cdots & \rho^{n-1}\\
\rho & 1 & \rho & \cdots & \rho^{n-2}\\
\vdots &  & \ddots &  & \vdots\\
 &  &  & \ddots & \rho\\
\rho^{n-1} & \cdots &  &  & 1
\end{array}\right]}_{\textrm{this is the correlation matrix}}
\]

\end_inset

 So we have homoscedasticity,
 but elements off the main diagonal are not zero.
 All of this depends only on two parameters,
 
\begin_inset Formula $\rho$
\end_inset

 and 
\begin_inset Formula $\sigma_{u}^{2}.$
\end_inset

 If we can estimate these consistently,
 we can apply FGLS.
 
\end_layout

\begin_layout Standard
It turns out that it's easy to estimate these consistently.
 The steps are
\end_layout

\begin_layout Enumerate
Estimate the model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\varepsilon_{t}$
\end_inset

 by OLS.
\end_layout

\begin_layout Enumerate
Take the residuals,
 and estimate the model 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}=\rho\hat{\varepsilon}_{t-1}+u_{t}^{*}
\]

\end_inset

 Since 
\begin_inset Formula $\hat{\varepsilon}_{t}\overset{p}{\rightarrow}\varepsilon_{t},$
\end_inset

 this regression is asymptotically equivalent to the regression 
\begin_inset Formula 
\[
\varepsilon_{t}=\rho\varepsilon_{t-1}+u_{t}
\]

\end_inset

 which satisfies the classical assumptions.
 Therefore,
 
\begin_inset Formula $\hat{\rho}$
\end_inset

 obtained by applying OLS to 
\begin_inset Formula $\hat{\varepsilon}_{t}=\rho\hat{\varepsilon}_{t-1}+u_{t}^{*}$
\end_inset

 is consistent.
 Also,
 since 
\begin_inset Formula $u_{t}^{*}\overset{p}{\rightarrow}u_{t}$
\end_inset

,
 the estimator 
\begin_inset Formula 
\[
\hat{\sigma}_{u}^{2}=\frac{1}{n}\sum_{t=2}^{n}\left(\hat{u}_{t}^{*}\right)^{2}\overset{p}{\rightarrow}\sigma_{u}^{2}
\]

\end_inset


\end_layout

\begin_layout Enumerate
With the consistent estimators 
\begin_inset Formula $\hat{\sigma}_{u}^{2}$
\end_inset

 and 
\begin_inset Formula $\hat{\rho},$
\end_inset

 form 
\begin_inset Formula $\hat{\Sigma}=\Sigma(\hat{\sigma}_{u}^{2},\hat{\rho})$
\end_inset

 using the previous structure of 
\begin_inset Formula $\Sigma,$
\end_inset

 and estimate by FGLS.
 Actually,
 one can omit the factor 
\begin_inset Formula $\hat{\sigma}_{u}^{2}/(1-\rho^{2}),$
\end_inset

 since it cancels out in the formula 
\begin_inset Formula 
\[
\hat{\beta}_{FGLS}=\left(X^{\prime}\hat{\Sigma}^{-1}X\right)^{-1}(X^{\prime}\hat{\Sigma}^{-1}y).
\]

\end_inset


\end_layout

\begin_layout Itemize
One can iterate the process,
 by taking the first FGLS estimator of 
\begin_inset Formula $\beta,$
\end_inset

 re-estimating 
\begin_inset Formula $\rho$
\end_inset

 and 
\begin_inset Formula $\sigma_{u}^{2},$
\end_inset

 etc.
 If one iterates to convergences it's equivalent to MLE (supposing normal errors).
\end_layout

\begin_layout Itemize
An asymptotically equivalent approach is to simply estimate the transformed model 
\begin_inset Formula 
\[
y_{t}-\hat{\rho}y_{t-1}=(x_{t}-\hat{\rho}x_{t-1})^{\prime}\beta+u_{t}^{*}
\]

\end_inset

 using 
\begin_inset Formula $n-1$
\end_inset

 observations (since 
\begin_inset Formula $y_{0}$
\end_inset

 and 
\begin_inset Formula $x_{0}$
\end_inset

 aren't available).
 This is the method of Cochrane and Orcutt.
 Dropping the first observation is asymptotically irrelevant,
 but 
\emph on
it can be very important in small samples.

\emph default
 One can recuperate the first observation by putting 
\begin_inset Formula 
\begin{eqnarray*}
y_{1}^{*} & =y_{1} & \sqrt{1-\hat{\rho}^{2}}\\
x_{1}^{*} & =x_{1} & \sqrt{1-\hat{\rho}^{2}}
\end{eqnarray*}

\end_inset

 This somewhat odd-looking result is related to the Cholesky factorization of 
\begin_inset Formula $\Sigma^{-1}.$
\end_inset

 See Davidson and MacKinnon,
 pg.
 348-49 for more discussion.
 Note that the variance of 
\begin_inset Formula $y_{1}^{*}$
\end_inset

 is 
\begin_inset Formula $\sigma_{u}^{2},$
\end_inset

 asymptotically,
 so we see that the transformed model will be homoscedastic (and nonautocorrelated,
 since the 
\begin_inset Formula $u^{\prime}s$
\end_inset

 are uncorrelated with the 
\begin_inset Formula $y^{\prime}s,$
\end_inset

 in different time periods.
 
\end_layout

\begin_layout Subsection
MA(1)
\end_layout

\begin_layout Standard
The linear regression model with moving average order 1 errors is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\varepsilon_{t} & = & u_{t}+\phi u_{t-1}\\
u_{t} & \sim & iid(0,\sigma_{u}^{2})\\
\mathcal{E}(\varepsilon_{t}u_{s}) & = & 0,t<s
\end{eqnarray*}

\end_inset

 In this case,
 
\begin_inset Formula 
\begin{eqnarray*}
V(\varepsilon_{t}) & =\gamma_{0}= & \mathcal{E}\left[\left(u_{t}+\phi u_{t-1}\right)^{2}\right]\\
 & = & \sigma_{u}^{2}+\phi^{2}\sigma_{u}^{2}\\
 & = & \sigma_{u}^{2}(1+\phi^{2})
\end{eqnarray*}

\end_inset

 Similarly 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{1} & = & \mathcal{E}\left[\left(u_{t}+\phi u_{t-1}\right)\left(u_{t-1}+\phi u_{t-2}\right)\right]\\
 & = & \phi\sigma_{u}^{2}
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{2} & = & \left[\left(u_{t}+\phi u_{t-1}\right)\left(u_{t-2}+\phi u_{t-3}\right)\right]\\
 & = & 0
\end{eqnarray*}

\end_inset

 so in this case 
\begin_inset Formula 
\[
\Sigma=\sigma_{u}^{2}\left[\begin{array}{ccccc}
1+\phi^{2} & \phi & 0 & \cdots & 0\\
\phi & 1+\phi^{2} & \phi\\
0 & \phi & \ddots &  & \vdots\\
\vdots &  &  & \ddots & \phi\\
0 & \cdots &  & \phi & 1+\phi^{2}
\end{array}\right]
\]

\end_inset

 Note that the first order autocorrelation is 
\begin_inset Formula 
\begin{eqnarray*}
\rho_{1} & =\frac{\phi\sigma_{u}^{2}}{\sigma_{u}^{2}(1+\phi^{2})} & =\frac{\gamma_{1}}{\gamma_{0}}\\
 & = & \frac{\phi}{(1+\phi^{2})}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
This achieves a maximum at 
\begin_inset Formula $\phi=1$
\end_inset

 and a minimum at 
\begin_inset Formula $\phi=-1,$
\end_inset

 and the maximal and minimal autocorrelations are 1/2 and -1/2.
 Therefore,
 series that are more strongly autocorrelated can't be MA(1) processes.
 
\end_layout

\begin_layout Standard
Again the covariance matrix has a simple structure that depends on only two parameters.
 The problem in this case is that one can't estimate 
\begin_inset Formula $\phi$
\end_inset

 using OLS on 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}=u_{t}+\phi u_{t-1}
\]

\end_inset

 because the 
\begin_inset Formula $u_{t}$
\end_inset

 are unobservable and they can't be estimated consistently.
 However,
 there is a simple way to estimate the parameters.
\end_layout

\begin_layout Itemize
Since the model is homoscedastic,
 we can estimate 
\begin_inset Formula 
\[
V(\varepsilon_{t})=\sigma_{\varepsilon}^{2}=\sigma_{u}^{2}(1+\phi^{2})
\]

\end_inset

 using the typical estimator:
 
\begin_inset Formula 
\[
\widehat{\sigma_{\varepsilon}^{2}}=\widehat{\sigma_{u}^{2}(1+\phi^{2})}=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
By the Slutsky theorem,
 we can interpret this as defining an (unidentified) estimator of both 
\begin_inset Formula $\sigma_{u}^{2}$
\end_inset

 and 
\begin_inset Formula $\phi,$
\end_inset

 e.g.,
 use this as 
\begin_inset Formula 
\[
\widehat{\sigma_{u}^{2}}(1+\widehat{\phi}^{2})=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}
\]

\end_inset

 However,
 this isn't sufficient to define consistent estimators of the parameters,
 since it's unidentified - two unknowns,
 one equation.
\end_layout

\begin_layout Itemize
To solve this problem,
 estimate the covariance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{t-1}$
\end_inset

 using 
\begin_inset Formula 
\[
\widehat{Cov}(\varepsilon_{t},\varepsilon_{t-1})=\widehat{\phi\sigma_{u}^{2}}=\frac{1}{n}\sum_{t=2}^{n}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t-1}
\]

\end_inset

 This is a consistent estimator,
 following a LLN (and given that the epsilon hats are consistent for the epsilons).
 As above,
 this can be interpreted as defining an unidentified estimator of the two parameters:
 
\begin_inset Formula 
\[
\hat{\phi}\widehat{\sigma_{u}^{2}}=\frac{1}{n}\sum_{t=2}^{n}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t-1}
\]

\end_inset


\end_layout

\begin_layout Itemize
Now solve these two equations to obtain identified (and therefore consistent) estimators of both 
\begin_inset Formula $\phi$
\end_inset

 and 
\begin_inset Formula $\sigma_{u}^{2}.$
\end_inset

 Define the consistent estimator 
\begin_inset Formula 
\[
\hat{\Sigma}=\Sigma(\hat{\phi},\widehat{\sigma_{u}^{2}})
\]

\end_inset

 following the form we've seen above,
 and transform the model using the Cholesky decomposition.
 The transformed model satisfies the classical assumptions asymptotically.
\end_layout

\begin_layout Itemize
Note:
 there is no guarantee that 
\begin_inset Formula $\Sigma$
\end_inset

 estimated using the above method will be positive definite,
 which may pose a problem.
 Another method would be to use ML estimation,
 if one is willing to make distributional assumptions regarding the white noise errors.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Monte-Carlo-example:"

\end_inset

Monte Carlo example:
 AR1
\end_layout

\begin_layout Standard
(too lazy to convert the code to Julia)
\end_layout

\begin_layout Standard
Let's look at a Monte Carlo study that compares OLS and GLS when we have AR1 errors.
 The model is 
\begin_inset Formula 
\begin{align*}
y_{t} & =1+x_{t}+\epsilon_{t}\\
\epsilon_{t} & =\rho\epsilon_{t-1}+u_{t}
\end{align*}

\end_inset

with 
\begin_inset Formula $\rho=0.9$
\end_inset

.
 The sample size is 
\begin_inset Formula $n=30,$
\end_inset

 and 1000 Monte Carlo replications are done.
 The Octave script is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GLS/AR1Errors.jl}{GLS/AR1Errors.jl}
\end_layout

\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Efficiency-of-OLS"
nolink "false"

\end_inset

 shows histograms of the estimated coefficient of 
\begin_inset Formula $x$
\end_inset

 (the true value is 1) .
 We can see that the GLS histogram is much more concentrated about 1,
 which is indicative of the efficiency of GLS relative to OLS.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Efficiency-of-OLS"

\end_inset

Efficiency of OLS and FGLS,
 AR1 errors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/AR1errors_OLSvsGLS.png
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Asymptotically-valid-inferences"

\end_inset

Asymptotically valid inferences with autocorrelation of unknown form
\end_layout

\begin_layout Standard
See Hamilton Ch.
 10,
 pp.
 261-2 and 280-84.
\end_layout

\begin_layout Standard
When the form of autocorrelation is unknown,
 one may decide to use the OLS estimator,
 without correction.
 We've seen that this estimator has the limiting distribution 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right)
\]

\end_inset

 where,
 as before,
 
\begin_inset Formula $\Omega$
\end_inset

 is 
\begin_inset Formula 
\[
\Omega=\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)
\]

\end_inset

 We need a consistent estimate of 
\begin_inset Formula $\Omega$
\end_inset

.
 Define 
\begin_inset Formula $m_{t}=x_{t}\varepsilon_{t}$
\end_inset

 (recall that 
\begin_inset Formula $x_{t}$
\end_inset

 is defined as a 
\begin_inset Formula $K\times1$
\end_inset

 vector).
 Note that 
\begin_inset Formula 
\begin{eqnarray*}
X^{\prime}\varepsilon & = & \left[\begin{array}{cccc}
x_{1} & x_{2} & \cdots & x_{n}\end{array}\right]\left[\begin{array}{c}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{n}
\end{array}\right]\\
 & = & \sum_{t=1}^{n}x_{t}\varepsilon_{t}\\
 & = & \sum_{t=1}^{n}m_{t}
\end{eqnarray*}

\end_inset

 so that 
\begin_inset Formula 
\[
\Omega=\lim_{n\rightarrow\infty}\frac{1}{n}\mathcal{E}\left[\left(\sum_{t=1}^{n}m_{t}\right)\left(\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]
\]

\end_inset

 We assume that 
\begin_inset Formula $m_{t}$
\end_inset

 is covariance stationary (so that the covariance between 
\begin_inset Formula $m_{t}$
\end_inset

 and 
\begin_inset Formula $m_{t-s}$
\end_inset

 does not depend on 
\begin_inset Formula $t).$
\end_inset


\end_layout

\begin_layout Standard
Define the 
\begin_inset Formula $v-th$
\end_inset

 autocovariance of 
\begin_inset Formula $m_{t}$
\end_inset

 as 
\begin_inset Formula 
\[
\Gamma_{v}=\mathcal{E}(m_{t}m_{t-v}^{\prime}).
\]

\end_inset

 Note that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+v}^{\prime})=\Gamma_{v}^{\prime}.$
\end_inset

 
\emph on
(show this with an example).

\emph default
 In general,
 we expect that:
\end_layout

\begin_layout Itemize
\begin_inset Formula $m_{t}$
\end_inset

 will be autocorrelated,
 since 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is potentially autocorrelated:
 
\begin_inset Formula 
\[
\Gamma_{v}=\mathcal{E}(m_{t}m_{t-v}^{\prime})\neq0
\]

\end_inset

 Note that this autocovariance does not depend on 
\begin_inset Formula $t,$
\end_inset

 due to covariance stationarity.
\end_layout

\begin_layout Itemize
contemporaneously correlated ( 
\begin_inset Formula $\mathcal{E}(m_{it}m_{jt})\neq0$
\end_inset

 ),
 since the regressors in 
\begin_inset Formula $x_{t}$
\end_inset

 will in general be correlated (more on this later).
\end_layout

\begin_layout Itemize
and heteroscedastic (
\begin_inset Formula $\mathcal{E}(m_{it}^{2})=\sigma_{i}^{2}$
\end_inset

 ,
 which depends upon 
\begin_inset Formula $i$
\end_inset

 ),
 again since the regressors will have different variances.
 
\end_layout

\begin_layout Standard
While one could estimate 
\begin_inset Formula $\Omega$
\end_inset

 parametrically,
 we in general have little information upon which to base a parametric specification.
 Recent research has focused on consistent nonparametric estimators of 
\begin_inset Formula $\Omega.$
\end_inset


\end_layout

\begin_layout Standard
Now define 
\begin_inset Formula 
\[
\Omega_{n}=\mathcal{E}\frac{1}{n}\left[\left(\sum_{t=1}^{n}m_{t}\right)\left(\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]
\]

\end_inset

 We have (
\emph on
show that the following is true,
 by expanding sum and shifting rows to left)
\emph default

\begin_inset Formula 
\[
\Omega_{n}=\Gamma_{0}+\frac{n-1}{n}\left(\Gamma_{1}+\Gamma_{1}^{\prime}\right)+\frac{n-2}{n}\left(\Gamma_{2}+\Gamma_{2}^{\prime}\right)\cdots+\frac{1}{n}\left(\Gamma_{n-1}+\Gamma_{n-1}^{\prime}\right)
\]

\end_inset

 The natural,
 consistent estimator of 
\begin_inset Formula $\Gamma_{v}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\Gamma_{v}}=\frac{1}{n}\sum_{t=v+1}^{n}\hat{m}_{t}\hat{m}_{t-v}^{\prime}.
\]

\end_inset

 where 
\begin_inset Formula 
\[
\hat{m}_{t}=x_{t}\hat{\varepsilon}_{t}
\]

\end_inset

 (note:
 one could put 
\begin_inset Formula $1/(n-v)$
\end_inset

 instead of 
\begin_inset Formula $1/n$
\end_inset

 here).
 So,
 a natural,
 but inconsistent,
 estimator of 
\begin_inset Formula $\Omega_{n}$
\end_inset

 would be 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Omega}_{n} & = & \widehat{\Gamma_{0}}+\frac{n-1}{n}\left(\widehat{\Gamma_{1}}+\widehat{\Gamma_{1}^{\prime}}\right)+\frac{n-2}{n}\left(\widehat{\Gamma_{2}}+\widehat{\Gamma_{2}^{\prime}}\right)+\cdots+\frac{1}{n}\left(\widehat{\Gamma_{n-1}}+\widehat{\Gamma_{n-1}^{\prime}}\right)\\
 & = & \widehat{\Gamma_{0}}+\sum_{v=1}^{n-1}\frac{n-v}{n}\left(\widehat{\Gamma_{v}}+\widehat{\Gamma_{v}^{\prime}}\right).
\end{eqnarray*}

\end_inset

 This estimator is inconsistent in general,
 since the number of parameters to estimate is more than the number of observations,
 and increases more rapidly than 
\begin_inset Formula $n$
\end_inset

,
 so information does not build up as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Standard
On the other hand,
 supposing that 
\begin_inset Formula $\Gamma_{v}$
\end_inset

 tends to zero sufficiently rapidly as 
\begin_inset Formula $v$
\end_inset

 tends to 
\begin_inset Formula $\infty,$
\end_inset

 a modified estimator 
\begin_inset Formula 
\[
\hat{\Omega}_{n}=\widehat{\Gamma_{0}}+\sum_{v=1}^{q(n)}\left(\widehat{\Gamma_{v}}+\widehat{\Gamma_{v}^{\prime}}\right),
\]

\end_inset

 where 
\begin_inset Formula $q(n)\overset{p}{\rightarrow}\infty$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 will be consistent,
 provided 
\begin_inset Formula $q(n)$
\end_inset

 grows sufficiently slowly.
\end_layout

\begin_layout Itemize
The assumption that autocorrelations die off is reasonable in many cases.
 For example,
 the AR(1) model with 
\begin_inset Formula $|\rho|<1$
\end_inset

 has autocorrelations that die off.
\end_layout

\begin_layout Itemize
The term 
\begin_inset Formula $\frac{n-v}{n}$
\end_inset

 can be dropped because it tends to one for 
\begin_inset Formula $v<q(n)$
\end_inset

,
 given that 
\begin_inset Formula $q(n)$
\end_inset

 increases slowly relative to 
\begin_inset Formula $n.$
\end_inset


\end_layout

\begin_layout Itemize
A disadvantage of this estimator is that is may not be positive definite.
 This could cause one to calculate a negative 
\begin_inset Formula $\chi^{2}$
\end_inset

 statistic,
 for example!
\end_layout

\begin_layout Itemize
Newey and West proposed and estimator (
\emph on
Econometrica
\emph default
,
 1987) that solves the problem of possible nonpositive definiteness of the above estimator.
 Their estimator is 
\begin_inset Formula 
\[
\hat{\Omega}_{n}=\widehat{\Gamma_{0}}+\sum_{v=1}^{q(n)}\left[1-\frac{v}{q+1}\right]\left(\widehat{\Gamma_{v}}+\widehat{\Gamma_{v}^{\prime}}\right).
\]

\end_inset

 This estimator is p.d.
 by construction.
 The condition for consistency is that 
\begin_inset Formula $n^{-1/4}q(n)\rightarrow0.$
\end_inset

 Note that this is a very slow rate of growth for 
\begin_inset Formula $q.$
\end_inset

 This estimator is nonparametric - we've placed no parametric restrictions on the form of 
\begin_inset Formula $\Omega.$
\end_inset

 It is an example of a 
\emph on
kernel
\emph default
 estimator.
 
\end_layout

\begin_layout Standard
Finally,
 since 
\begin_inset Formula $\Omega_{n}$
\end_inset

 has 
\begin_inset Formula $\Omega$
\end_inset

 as its limit,
 
\begin_inset Formula $\hat{\Omega}_{n}\overset{p}{\rightarrow}\Omega.$
\end_inset

 We can now use 
\begin_inset Formula $\hat{\Omega}_{n}$
\end_inset

 and 
\begin_inset Formula $\widehat{Q_{X}}=\frac{1}{n}X^{\prime}X$
\end_inset

 to consistently estimate the limiting distribution of the OLS estimator under heteroscedasticity and autocorrelation of unknown form.
 With this,
 asymptotically valid tests are constructed in the usual way.
\end_layout

\begin_layout Subsection
Testing for autocorrelation
\end_layout

\begin_layout Standard

\series bold
Breusch-Godfrey test
\end_layout

\begin_layout Standard
This test uses an auxiliary regression,
 as does the White test for heteroscedasticity.
 The regression is 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}=x_{t}^{\prime}\delta+\gamma_{1}\hat{\varepsilon}_{t-1}+\gamma_{2}\hat{\varepsilon}_{t-2}+\cdots+\gamma_{P}\hat{\varepsilon}_{t-P}+v_{t}
\]

\end_inset

 and the test statistic is the 
\begin_inset Formula $nR^{2}$
\end_inset

 statistic,
 just as in the White test.
 There are 
\begin_inset Formula $P$
\end_inset

 restrictions,
 so the test statistic is asymptotically distributed as a 
\begin_inset Formula $\chi^{2}(P).$
\end_inset


\end_layout

\begin_layout Itemize
The intuition is that the lagged errors shouldn't contribute to explaining the current error if there is no autocorrelation.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{t}$
\end_inset

 is included as a regressor to account for the fact that the 
\begin_inset Formula $\hat{\varepsilon}_{t}$
\end_inset

 are not independent even if the 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 are.
 This is a technicality that we won't go into here.
\end_layout

\begin_layout Itemize
This test is valid even if the regressors are stochastic and contain lagged dependent variables,
 so it is considerably more useful than the DW test for typical time series data.
\end_layout

\begin_layout Itemize
The alternative is not that the model is an AR(P),
 following the argument above.
 The alternative is simply that some or all of the first 
\begin_inset Formula $P\;$
\end_inset

autocorrelations are different from zero.
 This is compatible with many specific forms of autocorrelation.
 
\end_layout

\begin_layout Standard

\series bold
Durbin-Watson test
\end_layout

\begin_layout Standard
The Durbin-Watson test is not strictly valid in most situations where we would like to use it.
 Nevertheless,
 it is encountered often enough so that one should know something about it (perhaps:
 I'm no longer teaching this,
 but I'll leave it in the notes for reference).
 The Durbin-Watson test statistic is 
\begin_inset Formula 
\begin{eqnarray*}
DW & = & \frac{\sum_{t=2}^{n}\left(\hat{\varepsilon}_{t}-\hat{\varepsilon}_{t-1}\right)^{2}}{\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}}\\
 & = & \frac{\sum_{t=2}^{n}\left(\hat{\varepsilon}_{t}^{2}-2\hat{\varepsilon}_{t}\hat{\varepsilon}_{t-1}+\hat{\varepsilon}_{t-1}^{2}\right)}{\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The null hypothesis is that the first order autocorrelation of the errors is zero:
 
\begin_inset Formula $H_{0}:\rho_{1}=0.$
\end_inset

 The alternative is of course 
\begin_inset Formula $H_{A}:\rho_{1}\neq0.$
\end_inset

 Note that the alternative is not that the errors are AR(1),
 since many general patterns of autocorrelation will have the first order autocorrelation different than zero.
 For this reason the test is useful for detecting autocorrelation in general.
 For the same reason,
 one shouldn't just assume that an AR(1) model is appropriate when the DW test rejects the null.
\end_layout

\begin_layout Itemize
Under the null,
 the middle term tends to zero,
 and the other two tend to one,
 so 
\begin_inset Formula $DW\overset{p}{\rightarrow}2.$
\end_inset


\end_layout

\begin_layout Itemize
Supposing that we had an AR(1) error process with 
\begin_inset Formula $\rho=1.$
\end_inset

 In this case the middle term tends to 
\begin_inset Formula $-2,$
\end_inset

 so 
\begin_inset Formula $DW\overset{p}{\rightarrow}0$
\end_inset


\end_layout

\begin_layout Itemize
Supposing that we had an AR(1) error process with 
\begin_inset Formula $\rho=-1.$
\end_inset

 In this case the middle term tends to 
\begin_inset Formula $2,$
\end_inset

 so 
\begin_inset Formula $DW\overset{p}{\rightarrow}4$
\end_inset


\end_layout

\begin_layout Itemize
These are the extremes:
 
\begin_inset Formula $DW$
\end_inset

 always lies between 0 and 4.
\end_layout

\begin_layout Itemize
The distribution of the test statistic depends on the matrix of regressors,
 
\begin_inset Formula $X,$
\end_inset

 so tables can't give exact critical values.
 The give upper and lower bounds,
 which correspond to the extremes that are possible.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Durbin-Watson-critical-values"
nolink "false"

\end_inset

.
 There are means of determining exact critical values conditional on 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Itemize
Note that DW can be used to test for nonlinearity (add discussion).
 
\end_layout

\begin_layout Itemize
The DW test is based upon the assumption that the matrix 
\begin_inset Formula $X$
\end_inset

 is fixed in repeated samples.
 This is often unreasonable in the context of economic time series,
 which is precisely the context where the test would have application.
 It is possible to relate the DW test to other test statistics which are valid without strict exogeneity.
\end_layout

\begin_layout Verse
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Durbin-Watson-critical-values"

\end_inset

Durbin-Watson critical values
\end_layout

\end_inset


\begin_inset Graphics
	filename Examples/Figures/DurbinWatson.pdf
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Lagged dependent variables and autocorrelation
\end_layout

\begin_layout Standard
We've seen that the OLS estimator is consistent under autocorrelation,
 as long as 
\begin_inset Formula $plim\frac{X^{\prime}\varepsilon}{n}=0.$
\end_inset

 This will be the case when 
\begin_inset Formula $\mathcal{E}(X^{\prime}\varepsilon)$
\end_inset

 
\begin_inset Formula $=0,$
\end_inset

 following a LLN.
 An important exception is the case where 
\begin_inset Formula $X$
\end_inset

 contains lagged 
\begin_inset Formula $y^{\prime}s$
\end_inset

 and the errors are autocorrelated.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:-Dynamic-model"

\end_inset

Dynamic model with MA1 errors.
 Consider the model 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \alpha+\rho y_{t-1}+\beta x_{t}+\epsilon_{t}\\
\epsilon_{t} & = & \upsilon_{t}+\phi\upsilon_{t-1}
\end{eqnarray*}

\end_inset

We can easily see that a regressor is not weakly exogenous:
 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(y_{t-1}\varepsilon_{t}) & = & \mathcal{E}\left\{ (\alpha+\rho y_{t-2}+\beta x_{t-1}+\upsilon_{t-1}+\phi\upsilon_{t-2})(\upsilon_{t}+\phi\upsilon_{t-1})\right\} \\
 & \neq & 0
\end{eqnarray*}

\end_inset

 since one of the terms is 
\begin_inset Formula $\mathcal{E}(\phi\upsilon_{t-1}^{2})$
\end_inset

 which is clearly nonzero.
 In this case 
\begin_inset Formula $\mathcal{E}(\mathbf{x}_{t}\varepsilon_{t})\neq0,$
\end_inset

 and therefore 
\begin_inset Formula $plim\frac{X^{\prime}\varepsilon}{n}\neq0.$
\end_inset

 Since
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
plim\hat{\beta}=\beta+plim\frac{X^{\prime}\varepsilon}{n}
\]

\end_inset

 the OLS estimator is inconsistent in this case.
 One needs to estimate by instrumental variables (IV),
 which we'll get to later
\end_layout

\begin_layout Standard
The Octave (reminder to self:
 translate this?) script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GLS/DynamicMA.m}{GLS/DynamicMA.m} 
\end_layout

\end_inset

 does a Monte Carlo study.
 The sample size is 
\begin_inset Formula $n=100$
\end_inset

.
 The true coefficients are 
\begin_inset Formula $\alpha=1$
\end_inset

 
\begin_inset Formula $\rho=0.9$
\end_inset

 and 
\begin_inset Formula $\beta=1$
\end_inset

.
 The MA parameter is 
\begin_inset Formula $\phi=-0.95$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dynamic-model-with"
nolink "false"

\end_inset

 gives the results.
 You can see that the constant and the autoregressive parameter have a lot of bias.
 By re-running the script with 
\begin_inset Formula $\phi=0$
\end_inset

,
 you will see that much of the bias disappears (not all - why?).
\begin_inset Newpage newpage
\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dynamic-model-with"

\end_inset

Dynamic model with MA(1) errors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $\hat{\alpha}-\alpha$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/constant_n100.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/ylag_n100.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $\hat{\beta}-\beta$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/x_n100.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

Consider the following model with $n$ observations and only one explanatory variable:
\end_layout

\begin_layout Plain Layout


\backslash
begin{equation*} y_{i}=
\backslash
beta x_{i}+u_{i}
\backslash
text{ (1)} 
\backslash
end{equation*}
\end_layout

\begin_layout Plain Layout

which satisfies the basic OLS
\backslash
 assumptions except for the homoskedasticity assumption.
 More precisely,
 the variance of the error term for the first $n/2$ observations is $Var(u_{i})=2$,
 and for the remaining observations $Var(u_{i})=
\backslash
sigma ^{2}$,
 where $
\backslash
sigma ^{2}$ is unknown.
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

a) How can you get an asymptotically efficient estimator of $
\backslash
beta$?
 Provide an expression for the estimator.
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

b) How can you test the hypothesis that the explanatory variable ($x_{i})$ is relevant in explaining $y_{i}$ in the model in equation (1)?
 Provide the statistic to be used in the test as well as its distribution.
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Comparing the variances of the OLS and GLS estimators,
 I claimed that the following holds:
\begin_inset Formula 
\begin{eqnarray*}
Var(\hat{\beta})-Var(\hat{\beta}_{GLS}) & = & A\Sigma A^{'}
\end{eqnarray*}

\end_inset

Verify that this is true.
\end_layout

\begin_layout Enumerate
Show that the GLS estimator can be defined as
\begin_inset Formula 
\[
\hat{\beta}_{GLS}=\arg\min(y-X\beta)^{\prime}\Sigma^{-1}(y-X\beta)
\]

\end_inset


\end_layout

\begin_layout Enumerate
The limiting distribution of the OLS estimator with heteroscedasticity of unknown form is
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right),
\]

\end_inset

 where
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)=\Omega
\]

\end_inset

 Explain why
\begin_inset Formula 
\[
\widehat{\Omega}=\frac{1}{n}\sum_{t=1}^{n}x_{t}x_{t}^{\prime}\hat{\varepsilon}_{t}^{2}
\]

\end_inset

 is a consistent estimator of this matrix.
\end_layout

\begin_layout Enumerate
Define the 
\begin_inset Formula $v-th$
\end_inset

 autocovariance of a covariance stationary process 
\begin_inset Formula $m_{t}$
\end_inset

,
 where 
\begin_inset Formula $E(m_{t})=0$
\end_inset

 as 
\begin_inset Formula 
\[
\Gamma_{v}=\mathcal{E}(m_{t}m_{t-v}^{\prime}).
\]

\end_inset

 Show that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+v}^{\prime})=\Gamma_{v}^{\prime}.$
\end_inset


\end_layout

\begin_layout Enumerate
Perhaps we can be a little more parsimonious with the Nerlove data (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/nerlove.data}{nerlove.data} 
\end_layout

\end_inset

),
 rather than using so many parameters to account for non-constant returns to scale,
 and to account for heteroscedasticity.
 Consider the original model 
\begin_inset Formula 
\[
\ln C=\beta+\beta_{Q}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Estimate by OLS,
 plot the residuals,
 and test for autocorrelation and heteroscedasticity.
 Explain your findings.
\end_layout

\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\[
\ln C=\beta+\beta_{Q}\ln Q+\gamma_{Q}\left(\ln Q\right)^{2}+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Explain how this model can account for non-constant returns to scale.
 
\end_layout

\begin_layout Enumerate
estimate this model,
 and test for autocorrelation and heteroscedasticity.
 You should find that there is HET,
 but no strong evidence of AUT.
 Why is this the case?
\end_layout

\begin_layout Enumerate
Do a GLS correction where it is assumed that 
\begin_inset Formula $V(\epsilon_{i})=\frac{\sigma^{2}}{\left(\ln Q_{i}\right)^{2}}$
\end_inset

.
 In GRETL,
 there is a weighted least squares option that you can use.
 Why does this assumed form of HET make sense?
\end_layout

\begin_layout Enumerate
plot the weighted residuals versus output.
 Is there evidence of HET,
 or has the correction eliminated the problem?
\end_layout

\begin_layout Enumerate
plot the fitted values for returns to scale,
 for all of the firms.
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
The 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{./Examples/Data/hall.csv}{hall.csv} 
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/hall.gdt}{hall.gdt} 
\end_layout

\end_inset

 dataset contains monthly observation on 3 variables:
 the consumption ratio 
\begin_inset Formula $c_{t}/c_{t-1}$
\end_inset

;
 the gross return of an equally weighted index of assets 
\begin_inset Formula $ewr_{t}$
\end_inset

;
 and the gross return of the same index,
 but weighted by value,
 
\begin_inset Formula $vwr_{t}$
\end_inset

.
 The idea is that a representative consumer may finance consumption by investing in assets.
 Present wealth is used for two things:
 consumption and investment.
 The return on investment defines wealth in the next period,
 and the process repeats.
 For the moment,
 explore the properties of the variables.
\end_layout

\begin_deeper
\begin_layout Enumerate
Are the variances constant over time?
\end_layout

\begin_layout Enumerate
Do the variables appear to be autocorrelated?
 Hint:
 regress a variable on its own lags.
\end_layout

\begin_layout Enumerate
Do the variable seem to be normally distributed?
\end_layout

\begin_layout Enumerate
Look at the properties of the growth rates of the variables:
 repeat a-c for growth rates.
 The growth rate of a variable 
\begin_inset Formula $x_{t}$
\end_inset

 is given by 
\begin_inset Formula $\log\left(x_{t}/x_{t-1}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\begin{align*}
y_{t} & =C+A_{1}y_{t-1}+\epsilon_{t}\\
E(\epsilon_{t}\epsilon_{t}^{\prime}) & =\Sigma\\
E(\epsilon_{t}\epsilon_{s}^{\prime}) & =0,t\ne s
\end{align*}

\end_inset

where 
\begin_inset Formula $y_{t}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 are 
\begin_inset Formula $G\times1$
\end_inset

 vectors,
 
\begin_inset Formula $C$
\end_inset

 is a 
\begin_inset Formula $G\times1$
\end_inset

 of constants,
 and 
\begin_inset Formula $A_{1}$
\end_inset

is a 
\begin_inset Formula $G\times G$
\end_inset

 matrix of parameters.
 The matrix 
\begin_inset Formula $\Sigma$
\end_inset

 is a 
\begin_inset Formula $G\times G$
\end_inset

 covariance matrix.
 Assume that we have 
\begin_inset Formula $n$
\end_inset

 observations.
 This is a 
\emph on
vector autoregressive
\emph default
 model,
 of order 1 - commonly referred to as a VAR(1) model.
\end_layout

\begin_deeper
\begin_layout Enumerate
Show how the model can be written in the form 
\begin_inset Formula $Y=X\beta+\nu$
\end_inset

,
 where 
\begin_inset Formula $Y$
\end_inset

 is a 
\begin_inset Formula $Gn\times1$
\end_inset

 vector,
 
\begin_inset Formula $\beta$
\end_inset

 is a 
\begin_inset Formula $(G+G^{2})\times$
\end_inset

1 parameter vector,
 and the other items are conformable.
 What is the structure of 
\begin_inset Formula $X$
\end_inset

?
 What is the structure of the covariance matrix of 
\begin_inset Formula $\nu$
\end_inset

?
\end_layout

\begin_layout Enumerate
This model has HET and AUT.
 Verify this statement.
\end_layout

\begin_layout Enumerate
Set 
\begin_inset Formula $G=2,$
\end_inset


\begin_inset Formula $C=(0\,0)^{\prime}$
\end_inset


\begin_inset Formula $A=\left[\begin{array}{cc}
0.8 & -0.1\\
0.2 & 0.5
\end{array}\right]$
\end_inset

,
 
\begin_inset Formula $\Sigma=\left[\begin{array}{cc}
1 & 0.5\\
0.5 & 1
\end{array}\right]$
\end_inset

.
 Simulate data from this model,
 then estimate the model using OLS and feasible GLS.
 You should find that the two estimators are identical,
 which might seem surprising,
 given that there is HET and AUT.
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:(advanced).-Prove-analytically"

\end_inset

(optional,
 and advanced).
 Prove analytically that the OLS and GLS estimators are identical.
 Hint:
 this model is of the form of 
\emph on
seemingly unrelated regressions
\emph default
.
 
\end_layout

\end_deeper
\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\begin{align*}
y_{t} & =\alpha+\rho_{1}y_{t-1}+\rho_{2}y_{t-2}+\epsilon_{t}
\end{align*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is a 
\begin_inset Formula $N(0,1)$
\end_inset

 white noise error.
 This is an autogressive model of order 2 (AR2) model.
 Suppose that data is generated from the AR2 model,
 but the econometrician mistakenly decides to estimate an AR1 model (
\begin_inset Formula $y_{t}=\alpha+\rho_{1}y_{t-1}+\epsilon_{t}$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Enumerate
simulate data from the AR2 model,
 setting 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset

 and 
\begin_inset Formula $\rho_{2}=0.4,$
\end_inset

 using a sample size of 
\begin_inset Formula $n=30.$
\end_inset

 
\end_layout

\begin_layout Enumerate
Estimate the AR1 model by OLS,
 using the simulated data
\end_layout

\begin_layout Enumerate
test the hypothesis that 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset


\end_layout

\begin_layout Enumerate
test for autocorrelation using the test of your choice
\end_layout

\begin_layout Enumerate
repeat the above steps 10000 times.
\end_layout

\begin_deeper
\begin_layout Enumerate
What percentage of the time does a t-test reject the hypothesis that 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset

?
\end_layout

\begin_layout Enumerate
What percentage of the time is the hypothesis of no autocorrelation rejected?
\end_layout

\end_deeper
\begin_layout Enumerate
discuss your findings.
 Include a residual plot for a representative sample.
\end_layout

\end_deeper
\begin_layout Enumerate
Modify the script given in Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Monte-Carlo-example:"
nolink "false"

\end_inset

 so that the first observation is dropped,
 rather than given special treatment.
 This corresponds to using the Cochrane-Orcutt method,
 whereas the script as provided implements the Prais-Winsten method.
 Check if there is an efficiency loss when the first observation is dropped.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Endogeneity and simultaneity
\end_layout

\begin_layout Standard
Several times we've encountered cases where correlation between regressors and the error term lead to biasedness and inconsistency of the OLS estimator.
 Cases include autocorrelation with lagged dependent variables (Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:-Dynamic-model"
nolink "false"

\end_inset

),
 measurement error in the regressors (Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"
nolink "false"

\end_inset

) and missing regressors (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Missing-regressors"
nolink "false"

\end_inset

).
 Another important case we have not seen yet is that of simultaneous equations.
 The cause is different,
 but the effect is the same:
 bias and inconsistency when OLS is applied to a single equation.
 The basic idea is presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Exogeneity-and-Endogeneity"
nolink "false"

\end_inset

.
 A simple regression will estimate the overall effect of x on y.
 If we're interested in the direct effect,
 
\begin_inset Formula $\beta$
\end_inset

,
 then we have a problem when the overall effect and the direct effect differ.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Exogeneity-and-Endogeneity"

\end_inset

Exogeneity and Endogeneity (adapted from Cameron and Trivedi)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/EndogExog.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Simultaneous-equations"

\end_inset

Simultaneous equations
\end_layout

\begin_layout Standard
Up until now our model is 
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

where we assume weak exogeneity of the regressors,
 so that 
\begin_inset Formula $E(x_{t}\epsilon_{t})=0$
\end_inset

.
 With weak exogeneity,
 the OLS estimator has desirable large sample properties (consistency,
 asymptotic normality).
\end_layout

\begin_layout Standard
Simultaneous equations is a different prospect.
 An example of a simultaneous equation system is a simple supply-demand system:
 
\begin_inset Formula 
\begin{eqnarray}
\text{Demand:\;\ }q_{t} & = & \alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}y_{t}+\varepsilon_{1t}\label{eq:demand function}\\
\text{Supply:\;\ }q_{t} & = & \beta_{1}+\beta_{2}p_{t}+\varepsilon_{2t}\nonumber \\
\mathcal{E}\left(\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}
\end{array}\right]\left[\begin{array}{ll}
\varepsilon_{1t} & \varepsilon_{2t}\end{array}\right]\right) & = & \left[\begin{array}{ll}
\sigma_{11} & \sigma_{12}\\
\cdot & \sigma_{22}
\end{array}\right]\nonumber \\
 & \equiv & \Sigma,\forall t\nonumber 
\end{eqnarray}

\end_inset

 The presumption is that 
\begin_inset Formula $q_{t}$
\end_inset

 and 
\begin_inset Formula $p_{t}$
\end_inset

 are jointly determined at the same time by the intersection of these equations.
 We'll assume that 
\begin_inset Formula $y_{t}$
\end_inset

 is determined by some unrelated process.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

It's easy to see that we have correlation between regressors and errors.
 Solving for 
\begin_inset Formula $p_{t}$
\end_inset

 :
 
\begin_inset Formula 
\begin{eqnarray*}
\alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}y_{t}+\varepsilon_{1t} & = & \beta_{1}+\beta_{2}p_{t}+\varepsilon_{2t}\\
\beta_{2}p_{t}-\alpha_{2}p_{t} & = & \alpha_{1}-\beta_{1}+\alpha_{3}y_{t}+\varepsilon_{1t}-\varepsilon_{2t}\\
p_{t} & = & \frac{\alpha_{1}-\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Now consider whether 
\begin_inset Formula $p_{t}$
\end_inset

 is uncorrelated with 
\begin_inset Formula $\varepsilon_{1t}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(p_{t}\varepsilon_{1t}) & = & \mathcal{E}\left\{ \left(\frac{\alpha_{1}-\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\varepsilon_{1t}\right\} \\
 & = & \frac{\sigma_{11}-\sigma_{12}}{\beta_{2}-\alpha_{2}}
\end{eqnarray*}

\end_inset

 Because of this correlation,
 weak exogeneity does not hold,
 and OLS estimation of the demand equation will be biased and inconsistent.
 The same applies to the supply equation,
 for the same reason.
\end_layout

\begin_layout Standard
A GRETL script which generates data according to this simple supply-demand system is here:
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/Simeq/simeq.inp}{Simeq/simeq.inp}
\end_layout

\end_inset

.
 It does a Monte Carlo study which verifies that the OLS estimator is inconsistent,
 and that the IV estimator is consistent.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
In this model,
 
\begin_inset Formula $q_{t}$
\end_inset

 and 
\begin_inset Formula $p_{t}$
\end_inset

 are the 
\emph on
endogenous
\emph default
 variables (endogs),
 that are determined within the system.
 
\begin_inset Formula $y_{t}$
\end_inset

 is an 
\emph on
exogenous
\emph default
 variable (exogs).
 These concepts are a bit tricky,
 and we'll return to it in a minute.
 First,
 some notation.
 Suppose we group together current endogs in the vector 
\begin_inset Formula $Y_{t}.$
\end_inset

 If there are 
\begin_inset Formula $G$
\end_inset

 endogs,
 
\begin_inset Formula $Y_{t}$
\end_inset

 is 
\begin_inset Formula $G\times1.$
\end_inset

 Group current and lagged exogs,
 as well as lagged endogs in the vector 
\begin_inset Formula $X_{t}$
\end_inset

 ,
 which is 
\begin_inset Formula $K\times1.$
\end_inset

 Stack the errors of the 
\begin_inset Formula $G$
\end_inset

 equations into the error vector 
\begin_inset Formula $E_{t}.$
\end_inset

 The model,
 with additional assumptions,
 can be written as 
\begin_inset Formula 
\begin{eqnarray}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}^{\prime}\nonumber \\
E_{t} & \sim & N(0,\Sigma),\forall t\label{eq:SIMEQ structural form}\\
\mathcal{E}(E_{t}E_{s}^{\prime}) & = & 0,t\neq s\nonumber 
\end{eqnarray}

\end_inset

 There are 
\begin_inset Formula $G$
\end_inset

 equations here,
 and the parameters that enter into each equation are contained in the 
\emph on
columns
\emph default
 of the matrices 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

We can stack all 
\begin_inset Formula $n$
\end_inset

 observations and write the model as 
\begin_inset Formula 
\begin{eqnarray*}
Y\Gamma & = & XB+E\\
\mathcal{E}(X^{\prime}E) & = & 0_{(K\times G)}\\
\mathrm{vec}(E) & \sim & N(0,\Psi)
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
Y=\left[\begin{array}{l}
Y_{1}^{\prime}\\
Y_{2}^{\prime}\\
\vdots\\
Y_{n}^{\prime}
\end{array}\right],X=\left[\begin{array}{l}
X_{1}^{\prime}\\
X_{2}^{\prime}\\
\vdots\\
X_{n}^{\prime}
\end{array}\right],E=\left[\begin{array}{l}
E_{1}^{\prime}\\
E_{2}^{\prime}\\
\vdots\\
E_{n}^{\prime}
\end{array}\right]
\]

\end_inset

 
\begin_inset Formula $Y$
\end_inset

 is 
\begin_inset Formula $n\times G,$
\end_inset

 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula $n\times K,$
\end_inset

 and 
\begin_inset Formula $E$
\end_inset

 is 
\begin_inset Formula $n\times G.$
\end_inset

 Recall that the 
\begin_inset Formula $\mathrm{vec(})$
\end_inset

 operator just stacks the columns of a matrix on top of one another into a vector.
\end_layout

\begin_layout Itemize
This system is 
\emph on
complete
\emph default
,
 in that there are as many equations as endogs.
\end_layout

\begin_layout Itemize
There is a normality assumption.
 This isn't necessary for obtaining consistent estimates,
 but,
 if it holds,
 it allows us to consider the relationship between least squares and ML estimators.
\end_layout

\begin_layout Itemize
Since there is no autocorrelation of the 
\begin_inset Formula $E_{t}$
\end_inset

 's,
 and since the columns of 
\begin_inset Formula $E$
\end_inset

 are individually homoscedastic,
 then 
\begin_inset Formula 
\begin{eqnarray*}
\Psi & = & \left[\begin{array}{llll}
\sigma_{11}I_{n} & \sigma_{12}I_{n} & \cdots & \sigma_{1G}I_{n}\\
 & \sigma_{22}I_{n} &  & \vdots\\
 &  & \ddots & \vdots\\
\cdot &  &  & \sigma_{GG}I_{n}
\end{array}\right]\\
 & = & I_{n}\otimes\Sigma
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $X$
\end_inset

 may contain lagged endogenous and exogenous variables.
 These variables are 
\emph on
predetermined.
 
\end_layout

\begin_layout Itemize
We need to define what is meant by 
\begin_inset Quotes eld
\end_inset

endogenous
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

exogenous
\begin_inset Quotes erd
\end_inset

 when classifying the current period variables.
 Remember the definition of weak exogeneity Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "ass:Weakly-exogenous-regressors:"
nolink "false"

\end_inset

,
 the regressors are weakly exogenous if 
\begin_inset Formula $E(E_{t}|X_{t})=0.$
\end_inset

 Endogenous regressors are those for which this assumption does not hold.
 As long as there is no autocorrelation,
 lagged endogenous variables are weakly exogenous.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Reduced-form"

\end_inset

Reduced form
\end_layout

\begin_layout Standard
Recall that the model is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}^{\prime}\\
V(E_{t}) & = & \Sigma
\end{eqnarray*}

\end_inset

 This is the model in 
\emph on
structural form.
\end_layout

\begin_layout Definition
[Structural form] 
\begin_inset CommandInset label
LatexCommand label
name "Structural form"

\end_inset

An equation is in structural form when more than one current period endogenous variable is included.
\end_layout

\begin_layout Standard
The solution for the current period endogs is easy to find.
 It is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}^{\prime}
\end{eqnarray*}

\end_inset

 Now only one current period endog appears in each equation.
 This is the 
\emph on
reduced form.
\end_layout

\begin_layout Definition
[Reduced form] 
\begin_inset CommandInset label
LatexCommand label
name "Reduced form"

\end_inset

An equation is in reduced form if only one current period endog is included.
\end_layout

\begin_layout Standard
An example is our supply/demand system.
 The reduced form for quantity is obtained by solving the supply equation for price and substituting into demand:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
q_{t} & = & \alpha_{1}+\alpha_{2}\left(\frac{q_{t}-\beta_{1}-\varepsilon_{2t}}{\beta_{2}}\right)+\alpha_{3}y_{t}+\varepsilon_{1t}\\
\beta_{2}q_{t}-\alpha_{2}q_{t} & = & \beta_{2}\alpha_{1}-\alpha_{2}\left(\beta_{1}+\varepsilon_{2t}\right)+\beta_{2}\alpha_{3}y_{t}+\beta_{2}\varepsilon_{1t}\\
q_{t} & = & \frac{\beta_{2}\alpha_{1}-\alpha_{2}\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\beta_{2}\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\\
 & = & \pi_{11}+\pi_{21}y_{t}+V_{1t}
\end{eqnarray*}

\end_inset

 Similarly,
 the rf for price,
 as we've seen above,
 is 
\begin_inset Formula 
\begin{eqnarray*}
p_{t} & = & \frac{\alpha_{1}-\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\\
 & = & \pi_{12}+\pi_{22}y_{t}+V_{2t}
\end{eqnarray*}

\end_inset

 The interesting thing about the rf is that the equations individually satisfy the classical assumptions,
 since 
\begin_inset Formula $y_{t}$
\end_inset

 is uncorrelated with 
\begin_inset Formula $\varepsilon_{1t}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{2t}$
\end_inset

 by assumption,
 and therefore 
\begin_inset Formula $\mathcal{E}(y_{t}V_{it})=0,$
\end_inset

 i=1,2,
 
\begin_inset Formula $\forall t.$
\end_inset

 The errors of the rf are 
\begin_inset Formula 
\[
\left[\begin{array}{l}
V_{1t}\\
V_{2t}
\end{array}\right]=\left[\begin{array}{l}
\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\\
\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The variance of 
\begin_inset Formula $V_{1t}$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
V(V_{1t}) & = & \mathcal{E}\left[\left(\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\left(\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\right]\\
 & = & \frac{\beta_{2}^{2}\sigma_{11}-2\beta_{2}\alpha_{2}\sigma_{12}+\alpha_{2}\sigma_{22}}{\left(\beta_{2}-\alpha_{2}\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
This is constant over time,
 so the first rf equation is homoscedastic.
\end_layout

\begin_layout Itemize
Likewise,
 since the 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 are independent over time,
 so are the 
\begin_inset Formula $V_{t}.$
\end_inset


\end_layout

\begin_layout Standard
The variance of the second rf error is 
\begin_inset Formula 
\begin{eqnarray*}
V(V_{2t}) & = & \mathcal{E}\left[\left(\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\left(\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\right]\\
 & = & \frac{\sigma_{11}-2\sigma_{12}+\sigma_{22}}{\left(\beta_{2}-\alpha_{2}\right)^{2}}
\end{eqnarray*}

\end_inset

 and the contemporaneous covariance of the errors across equations is 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(V_{1t}V_{2t}) & = & \mathcal{E}\left[\left(\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\left(\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\right]\\
 & = & \frac{\beta_{2}\sigma_{11}-\left(\beta_{2}+\alpha_{2}\right)\sigma_{12}+\sigma_{22}}{\left(\beta_{2}-\alpha_{2}\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
In summary the rf equations individually satisfy the classical assumptions,
 under the assumptions we've made,
 but they are contemporaneously correlated.
 
\end_layout

\begin_layout Standard
The general form of the rf is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}^{\prime}
\end{eqnarray*}

\end_inset

 so we have that 
\begin_inset Formula 
\[
V_{t}=\left(\Gamma^{-1}\right)^{\prime}E_{t}\sim N\left(0,\left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}\right),\forall t
\]

\end_inset

 and that the 
\begin_inset Formula $V_{t}$
\end_inset

 are timewise independent (note that this wouldn't be the case if the 
\begin_inset Formula $E_{t}$
\end_inset

 were autocorrelated).
\end_layout

\begin_layout Standard
From the reduced form,
 we can easily see that the endogenous variables are correlated with the structural errors:
\begin_inset Formula 
\begin{align}
E(E_{t}Y_{t}^{\prime}) & =E\left(E_{t}\left(X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\right)\right)\nonumber \\
 & =E\left(E_{t}X_{t}^{\prime}B\Gamma^{-1}+E_{t}E_{t}^{\prime}\Gamma^{-1}\right)\nonumber \\
 & =\Sigma\Gamma^{-1}\label{eq:correlation between endogs and structural error}
\end{align}

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:EstimationRF"

\end_inset

Estimation of the reduced form equations
\end_layout

\begin_layout Standard
From above,
 the RF equations are 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}^{\prime}
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\[
V_{t}\sim N\left(0,\Xi\right),\forall t
\]

\end_inset

where we define 
\begin_inset Formula $\Xi\equiv\left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}$
\end_inset

.
 The rf parameter estimator 
\begin_inset Formula $\hat{\Pi},$
\end_inset

 is simply OLS applied to this model,
 equation by equation::
 
\begin_inset Formula 
\[
\hat{\Pi}=(X^{\prime}X)^{-1}X^{\prime}Y
\]

\end_inset

 which is simply 
\begin_inset Formula 
\[
\hat{\Pi}=(X^{\prime}X)^{-1}X^{\prime}\left[\begin{array}{llll}
y_{1} & y_{2} & \cdots & y_{G}\end{array}\right]
\]

\end_inset

 that is,
 OLS equation by equation using 
\emph on
all
\emph default
 the exogs in the estimation of each column of 
\begin_inset Formula $\Pi.$
\end_inset


\end_layout

\begin_layout Standard
It may seem odd that we use OLS on the reduced form,
 since the rf equations are correlated,
 because 
\begin_inset Formula $\Xi\equiv\left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}$
\end_inset

 is a full matrix.
 Why don't we do GLS to improve efficiency of estimation of the RF parameters?
\end_layout

\begin_layout Standard
OLS equation by equation to get the rf is equivalent to 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{G}
\end{array}\right]=\left[\begin{array}{llll}
X & 0 & \cdots & 0\\
0 & X &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & X
\end{array}\right]\left[\begin{array}{l}
\pi_{1}\\
\pi_{2}\\
\vdots\\
\pi_{G}
\end{array}\right]+\left[\begin{array}{l}
v_{1}\\
v_{2}\\
\vdots\\
v_{G}
\end{array}\right]
\]

\end_inset

 where 
\begin_inset Formula $y_{i}$
\end_inset

 is the 
\begin_inset Formula $n\times1$
\end_inset

 vector of observations of the 
\begin_inset Formula $i^{th}$
\end_inset

 endog,
 
\begin_inset Formula $X$
\end_inset

 is the entire 
\begin_inset Formula $n\times K$
\end_inset

 matrix of exogs,
 
\begin_inset Formula $\pi_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 column of 
\begin_inset Formula $\Pi,$
\end_inset

 and 
\begin_inset Formula $v_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 column of 
\begin_inset Formula $V.$
\end_inset

 Use the notation 
\begin_inset Formula 
\[
y=\mathbf{X}\pi+v
\]

\end_inset

 to indicate the pooled model.
 Following this notation,
 the error covariance matrix is 
\begin_inset Formula 
\[
V(v)=\Xi\otimes I_{n}
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a special case of a type of model known as a set of 
\emph on
seemingly unrelated equations (SUR)
\emph default
 since the parameter vector of each equation is different.
 The important feature of this special case is that 
\emph on
the regressors are the same in each equation
\emph default
.
 The equations are contemporanously correlated,
 because of the non-zero off diagonal elements in 
\begin_inset Formula $\Xi$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that each equation of the system individually satisfies the classical assumptions.
\end_layout

\begin_layout Itemize
Normally when doing SUR,
 one simply does GLS on the whole system 
\begin_inset Formula $y=\mathbf{X}\pi+v$
\end_inset

,
 where 
\begin_inset Formula $V(v)=\Xi\otimes I_{n}$
\end_inset

,
 which is in general more efficient than OLS on each equation.
\end_layout

\begin_layout Itemize
However,
 when the regressors are the same in all equations,
 as is true in the present case of estimation of the RF parameters,
 SUR 
\begin_inset Formula $\equiv$
\end_inset

OLS.
 To show this note that in this case 
\begin_inset Formula $\mathbf{X}=I_{n}\otimes X.$
\end_inset

 Using the rules
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $(A\otimes B)^{-1}=(A^{-1}\otimes B^{-1})$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $(A\otimes B)^{\prime}=(A^{\prime}\otimes B^{\prime})$
\end_inset

 and
\end_layout

\begin_layout Enumerate
\begin_inset Formula $(A\otimes B)(C\otimes D)=(AC\otimes BD),$
\end_inset

 we get 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\pi}_{SUR} & = & \left(\left(I_{n}\otimes X\right)^{\prime}\left(\Xi\otimes I_{n}\right)^{-1}\left(I_{n}\otimes X\right)\right)^{-1}\left(I_{n}\otimes X\right)^{\prime}\left(\Xi\otimes I_{n}\right)^{-1}y\\
 & = & \left(\left(\Xi^{-1}\otimes X^{\prime}\right)\left(I_{n}\otimes X\right)\right)^{-1}\left(\Xi^{-1}\otimes X^{\prime}\right)y\\
 & = & \left(\Xi\otimes(X^{\prime}X)^{-1}\right)\left(\Xi^{-1}\otimes X^{\prime}\right)y\\
 & = & \left[I_{G}\otimes(X^{\prime}X)^{-1}X^{\prime}\right]y\\
 & = & \left[\begin{array}{l}
\hat{\pi}_{1}\\
\hat{\pi}_{2}\\
\vdots\\
\hat{\pi}_{G}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Note that this provides the answer to the exercise 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:(advanced).-Prove-analytically"
nolink "false"

\end_inset

 in the chapter on GLS.
\end_layout

\begin_layout Itemize
So the unrestricted rf coefficients can be estimated efficiently (assuming normality) by OLS,
 even if the equations are correlated.
\end_layout

\begin_layout Itemize
We have ignored any potential zeros in the matrix 
\begin_inset Formula $\Pi,$
\end_inset

 which if they exist could potentially increase the efficiency of estimation of the rf.
\end_layout

\begin_layout Itemize
Another example where SUR
\begin_inset Formula $\equiv$
\end_inset

OLS is in estimation of vector autoregressions which is discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:VAR-models"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Bias and inconsistency of OLS estimation of a structural equation
\end_layout

\begin_layout Standard
Considering the first equation (this is without loss of generality,
 since we can always reorder the equations) we can partition the 
\begin_inset Formula $Y$
\end_inset

 matrix as 
\begin_inset Formula 
\[
Y=\left[\begin{array}{lll}
y & Y_{1} & Y_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $y$
\end_inset

 is the first column
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y_{1}$
\end_inset

 are the other endogenous variables that enter the first equation
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y_{2}$
\end_inset

 are endogs that are excluded from this equation 
\end_layout

\begin_layout Standard
Similarly,
 partition 
\begin_inset Formula $X$
\end_inset

 as 
\begin_inset Formula 
\[
X=\left[\begin{array}{ll}
X_{1} & X_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $X_{1}$
\end_inset

 are the included exogs,
 and 
\begin_inset Formula $X_{2}$
\end_inset

 are the excluded exogs.
 
\end_layout

\begin_layout Standard
Finally,
 partition the error matrix as 
\begin_inset Formula 
\[
E=\left[\begin{array}{ll}
\varepsilon & E_{12}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Assume that 
\begin_inset Formula $\Gamma$
\end_inset

 has ones on the main diagonal.
 These are normalization restrictions that simply scale the remaining coefficients on each equation,
 and which scale the variances of the error terms.
\end_layout

\begin_layout Standard
Given this scaling and our partitioning,
 the coefficient matrices can be written as 
\begin_inset Formula 
\begin{eqnarray*}
\Gamma & = & \left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}
\end{array}\right]\\
B & = & \left[\begin{array}{ll}
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
With this,
 the first equation can be written as 
\begin_inset Formula 
\begin{eqnarray}
y & = & Y_{1}\gamma_{1}+X_{1}\beta_{1}+\varepsilon\label{eq:single equation from system}\\
 & = & Z\delta+\varepsilon\nonumber 
\end{eqnarray}

\end_inset

 The problem,
 as we've seen,
 is that the columns of 
\begin_inset Formula $Z$
\end_inset

 corresponding to 
\begin_inset Formula $Y_{1}$
\end_inset

 are correlated with 
\begin_inset Formula $\varepsilon,$
\end_inset

 because these are endogenous variables,
 and as we saw in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:correlation between endogs and structural error"
nolink "false"

\end_inset

,
 the endogenous variables are correlated with the structural errors,
 so they don't satisfy weak exogeneity.
 So,
 
\color green

\begin_inset Formula $E(Z^{\prime}\epsilon)\ne0$
\end_inset


\color inherit
.
 What are the properties of the OLS estimator in this situation?
\begin_inset Formula 
\begin{eqnarray*}
\hat{\delta} & = & \left(Z^{\prime}Z\right)^{-1}Z^{\prime}y\\
 & = & \left(Z^{\prime}Z\right)^{-1}Z^{\prime}\left(Z\delta^{0}+\varepsilon\right)\\
 & = & \delta^{0}+\left(Z^{\prime}Z\right)^{-1}{\color{green}Z^{\prime}\epsilon}
\end{eqnarray*}

\end_inset

It's clear that the OLS estimator is biased in general.
 Also,
 
\begin_inset Formula 
\[
\hat{\delta}-\delta^{0}=\left(\frac{Z^{\prime}Z}{n}\right)^{-1}{\color{green}\frac{Z^{\prime}\epsilon}{n}}
\]

\end_inset

Say that 
\begin_inset Formula $\lim\frac{Z^{\prime}\epsilon}{n}=A,$
\end_inset

a.s.,
 and 
\begin_inset Formula $\lim\frac{Z^{\prime}Z}{n}=Q_{Z},\,a.s.$
\end_inset

 Then
\begin_inset Formula 
\[
\lim\left(\hat{\delta}-\delta^{0}\right)=Q_{Z}^{-1}A\ne0,\,a.s.
\]

\end_inset

So the OLS estimator of a structural equation is inconsistent.
 In general,
 correlation between regressors and errors leads to this problem,
 whether due to measurement error,
 simultaneity,
 or omitted regressors.
\end_layout

\begin_layout Standard
A GRETL script which generates data according to a simple supply-demand system is here:
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/Simeq/simeq.inp}{Simeq/simeq.inp}
\end_layout

\end_inset

.
 It does a Monte Carlo study which verifies that the OLS estimator is inconsistent,
 and that the IV estimator is consistent.
\end_layout

\begin_layout Section
Note about the rest of this chapter
\end_layout

\begin_layout Standard
In class,
 I will not teach the material in the rest of this chapter at this time,
 but instead we will go on to GMM.
 The material that follows is easier to understand in the context of GMM,
 where we get a nice unified theory.
\end_layout

\begin_layout Section
Identification by exclusion restrictions
\end_layout

\begin_layout Standard
The material in the rest of this chapter is no longer used in classes,
 but I'm leaving it in the notes for reference.
\end_layout

\begin_layout Standard
The identification problem in simultaneous equations is in fact of the same nature as the identification problem in any estimation setting:
 does the limiting objective function have the proper curvature so that there is a unique global minimum or maximum at the true parameter value?
 In the context of IV estimation,
 this is the case if the limiting covariance of the IV estimator is positive definite and 
\begin_inset Formula $plim\frac{1}{n}W^{\prime}\varepsilon=0$
\end_inset

.
 This matrix is 
\begin_inset Formula 
\[
V_{\infty}(\hat{\beta}_{IV})=(Q_{XW}Q_{WW}^{-1}Q_{XW}^{\prime})^{-1}\sigma^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
The necessary and sufficient condition for identification is simply that this matrix be positive definite,
 and that the instruments be (asymptotically) uncorrelated with 
\begin_inset Formula $\varepsilon$
\end_inset

.
\end_layout

\begin_layout Itemize
For this matrix to be positive definite,
 we need that the conditions noted above hold:
 
\begin_inset Formula $Q_{WW}$
\end_inset

 must be positive definite and 
\begin_inset Formula $Q_{XW}$
\end_inset

 must be of full rank ( 
\begin_inset Formula $K$
\end_inset

 ).
\end_layout

\begin_layout Itemize
These identification conditions are not that intuitive nor is it very obvious how to check them.
 
\end_layout

\begin_layout Subsection
Necessary conditions
\end_layout

\begin_layout Standard
If we use IV estimation for a single equation of the system,
 the equation can be written as 
\begin_inset Formula 
\[
y=Z\delta+\varepsilon
\]

\end_inset

 where 
\begin_inset Formula 
\[
Z=\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Notation:
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $K$
\end_inset

 be the total numer of weakly exogenous variables.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $K^{\ast}=cols(X_{1})\;$
\end_inset

be the number of included exogs,
 and let 
\begin_inset Formula $K^{\ast\ast}=K-K^{\ast}$
\end_inset

 be the number of excluded exogs (in this equation).
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $G^{\ast}=cols(Y_{1})+1$
\end_inset

 be the total number of included endogs,
 and let 
\begin_inset Formula $G^{\ast\ast}=G-G^{\ast}$
\end_inset

 be the number of excluded endogs.
 
\end_layout

\begin_layout Standard
Using this notation,
 consider the selection of instruments.
\end_layout

\begin_layout Itemize
Now the 
\begin_inset Formula $X_{1}$
\end_inset

 are weakly exogenous and can serve as their own instruments.
\end_layout

\begin_layout Itemize
It turns out that 
\begin_inset Formula $X$
\end_inset

 exhausts the set of possible instruments,
 in that if the variables in 
\begin_inset Formula $X$
\end_inset

 don't lead to an identified model then no other instruments will identify the model either.
 Assuming this is true (we'll prove it in a moment),
 then a necessary condition for identification is that 
\begin_inset Formula $cols(X_{2})\geq cols(Y_{1})$
\end_inset

 since if not then at least one instrument must be used twice,
 so 
\begin_inset Formula $W$
\end_inset

 will not have full column rank:
 
\begin_inset Formula 
\[
\rho(W)<K^{\ast}+G^{\ast}-1\Rightarrow\rho(Q_{ZW})<K^{\ast}+G^{\ast}-1
\]

\end_inset

 This is the 
\emph on
order condition
\emph default
 for identification in a set of simultaneous equations.
 When the only identifying information is exclusion restrictions on the variables that enter an equation,
 then the number of excluded exogs must be greater than or equal to the number of included endogs,
 minus 1 (the normalized lhs endog),
 e.g.,
 
\begin_inset Formula 
\[
K^{\ast\ast}\geq G^{\ast}-1
\]

\end_inset


\end_layout

\begin_layout Itemize
To show that this is in fact a necessary condition consider some arbitrary set of instruments 
\begin_inset Formula $W.$
\end_inset

 A necessary condition for identification is that 
\begin_inset Formula 
\[
\rho\left(plim\frac{1}{n}W^{\prime}Z\right)=K^{\ast}+G^{\ast}-1
\]

\end_inset

 where 
\begin_inset Formula 
\[
Z=\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right]
\]

\end_inset

 Recall that we've partitioned the model 
\begin_inset Formula 
\[
Y\Gamma=XB+E
\]

\end_inset

 as 
\begin_inset Formula 
\[
Y=\left[\begin{array}{lll}
y & Y_{1} & Y_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X=\left[\begin{array}{ll}
X_{1} & X_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Given the reduced form 
\begin_inset Formula 
\[
Y=X\Pi+V
\]

\end_inset

 we can write the reduced form using the same partition 
\begin_inset Formula 
\[
\left[\begin{array}{lll}
y & Y_{1} & Y_{2}\end{array}\right]=\left[\begin{array}{ll}
X_{1} & X_{2}\end{array}\right]\left[\begin{array}{lll}
\pi_{11} & \Pi_{12} & \Pi_{13}\\
\pi_{21} & \Pi_{22} & \Pi_{23}
\end{array}\right]+\left[\begin{array}{lll}
v & V_{1} & V_{2}\end{array}\right]
\]

\end_inset

 so we have 
\begin_inset Formula 
\[
Y_{1}=X_{1}\Pi_{12}+X_{2}\Pi_{22}+V_{1}
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{1}{n}W^{\prime}Z=\frac{1}{n}W^{\prime}\left[\begin{array}{ll}
X_{1}\Pi_{12}+X_{2}\Pi_{22}+V_{1} & X_{1}\end{array}\right]
\]

\end_inset

 Because the 
\begin_inset Formula $W$
\end_inset

 's are uncorrelated with the 
\begin_inset Formula $V_{1}$
\end_inset

 's,
 by assumption,
 the cross between 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $V_{1}$
\end_inset

 converges in probability to zero,
 so 
\begin_inset Formula 
\[
plim\frac{1}{n}W^{\prime}Z=plim\frac{1}{n}W^{\prime}\left[\begin{array}{ll}
X_{1}\Pi_{12}+X_{2}\Pi_{22} & X_{1}\end{array}\right]
\]

\end_inset

 Since the far rhs term is formed only of linear combinations of columns of 
\begin_inset Formula $X,$
\end_inset

 the rank of this matrix can never be greater than 
\begin_inset Formula $K,$
\end_inset

 regardless of the choice of instruments.
 If 
\begin_inset Formula $Z$
\end_inset

 has more than 
\begin_inset Formula $K$
\end_inset

 columns,
 then it is not of full column rank.
 When 
\begin_inset Formula $Z$
\end_inset

 has more than 
\begin_inset Formula $K\;$
\end_inset

columns we have 
\begin_inset Formula 
\[
G^{*}-1+K^{*}>K
\]

\end_inset

 or noting that 
\begin_inset Formula $K^{**}=K-K^{*},$
\end_inset


\begin_inset Formula 
\[
G^{*}-1>K^{**}
\]

\end_inset

 In this case,
 the limiting matrix is not of full column rank,
 and the identification condition fails.
\end_layout

\begin_layout Subsection
Sufficient conditions
\end_layout

\begin_layout Standard
Identification essentially requires that the structural parameters be recoverable from the data.
 This won't be the case,
 in general,
 unless the structural model is subject to some restrictions.
 We've already identified necessary conditions.
 Turning to sufficient conditions (again,
 we're only considering identification through zero restricitions on the parameters,
 for the moment).
\end_layout

\begin_layout Standard
The model is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}\\
V(E_{t}) & = & \Sigma
\end{eqnarray*}

\end_inset

 This leads to the reduced form 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}\\
V(V_{t}) & = & \left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}\\
 & = & \Omega
\end{eqnarray*}

\end_inset

 The reduced form parameters are consistently estimable,
 but none of them are known 
\begin_inset Formula $\emph{a}$
\end_inset

 
\emph on
priori,

\emph default
 and there are no restrictions on their values.
 The problem is that more than one structural form has the same reduced form,
 so knowledge of the reduced form parameters alone isn't enough to determine the structural parameters.
 To see this,
 consider the model 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma F & = & X_{t}^{\prime}BF+E_{t}F\\
V(E_{t}F) & = & F^{\prime}\Sigma F
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $F$
\end_inset

 is some arbirary nonsingular 
\begin_inset Formula $G\times G$
\end_inset

 matrix.
 The rf of this new model is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}BF\left(\Gamma F\right)^{-1}+E_{t}F\left(\Gamma F\right)^{-1}\\
 & = & X_{t}^{\prime}BFF^{-1}\Gamma^{-1}+E_{t}FF^{-1}\Gamma^{-1}\\
 & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}
\end{eqnarray*}

\end_inset

 Likewise,
 the covariance of the rf of the transformed model is 
\begin_inset Formula 
\begin{eqnarray*}
V(E_{t}F\left(\Gamma F\right)^{-1}) & = & V(E_{t}\Gamma^{-1})\\
 & = & \Omega
\end{eqnarray*}

\end_inset

 Since the two structural forms lead to the same rf,
 and the rf is all that is directly estimable,
 the models are said to be 
\emph on
observationally equivalent.

\emph default
 What we need for identification are restrictions on 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 such that the only admissible 
\begin_inset Formula $F$
\end_inset

 is an identity matrix (if all of the equations are to be identified).
 Take the coefficient matrices as partitioned before:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma\\
B
\end{array}\right]=\left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}\\
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]
\]

\end_inset

 The coefficients of the first equation of the transformed model are simply these coefficients multiplied by the first column of 
\begin_inset Formula $F$
\end_inset

.
 This gives 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma\\
B
\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=\left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}\\
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]
\]

\end_inset

 For identification of the first equation we need that there be enough restrictions so that the only admissible 
\begin_inset Formula 
\[
\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]
\]

\end_inset

 be the leading column of an identity matrix,
 so that 
\begin_inset Formula 
\[
\left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}\\
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=\left[\begin{array}{l}
1\\
-\gamma_{1}\\
0\\
\beta_{1}\\
0
\end{array}\right]
\]

\end_inset

 Note that the third and fifth rows are 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]F_{2}=\left[\begin{array}{l}
0\\
0
\end{array}\right]
\]

\end_inset

 Supposing that the leading matrix is of full column rank,
 e.g.,
 
\begin_inset Formula 
\[
\rho\left(\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]\right)=cols\left(\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]\right)=G-1
\]

\end_inset

 then the only way this can hold,
 without additional restrictions on the model's parameters,
 is if 
\begin_inset Formula $F_{2}$
\end_inset

 is a vector of zeros.
 Given that 
\begin_inset Formula $F_{2}$
\end_inset

 is a vector of zeros,
 then the first equation 
\begin_inset Formula 
\[
\left[\begin{array}{ll}
1 & \Gamma_{12}\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=1\Rightarrow f_{11}=1
\]

\end_inset

 Therefore,
 as long as 
\begin_inset Formula 
\[
\rho\left(\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]\right)=G-1
\]

\end_inset

 then 
\begin_inset Formula 
\[
\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=\left[\begin{array}{l}
1\\
0_{G-1}
\end{array}\right]
\]

\end_inset

The first equation is identified in this case,
 so the condition is sufficient for identification.
 It is also necessary,
 since the condition implies that this submatrix must have at least 
\begin_inset Formula $G-1$
\end_inset

 rows.
 Since this matrix has 
\begin_inset Formula 
\[
G^{\ast\ast}+K^{\ast\ast}=G-G^{\ast}+K^{\ast\ast}
\]

\end_inset

 rows,
 we obtain 
\begin_inset Formula 
\[
G-G^{\ast}+K^{\ast\ast}\geq G-1
\]

\end_inset

 or 
\begin_inset Formula 
\[
K^{\ast\ast}\geq G^{\ast}-1
\]

\end_inset

 which is the previously derived necessary condition.
\end_layout

\begin_layout Standard
The above result is fairly intuitive (draw picture here).
 The necessary condition ensures that there are enough variables not in the equation of interest to potentially move the other equations,
 so as to trace out the equation of interest.
 The sufficient condition ensures that those other equations in fact do move around as the variables change their values.
 Some points:
\end_layout

\begin_layout Itemize
When an equation has 
\begin_inset Formula $K^{\ast\ast}=G^{\ast}-1,$
\end_inset

 is is 
\emph on
exactly identified
\emph default
,
 in that omission of an identifiying restriction is not possible without loosing consistency.
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $K^{\ast\ast}>G^{\ast}-1,$
\end_inset

 the equation is 
\emph on
overidentified
\emph default
,
 since one could drop a restriction and still retain consistency.
 Overidentifying restrictions are therefore testable.
 When an equation is overidentified we have more instruments than are strictly necessary for consistent estimation.
 Since estimation by IV with more instruments is more efficient asymptotically,
 one should employ overidentifying restrictions if one is confident that they're true.
\end_layout

\begin_layout Itemize
We can repeat this partition for each equation in the system,
 to see which equations are identified and which aren't.
\end_layout

\begin_layout Itemize
These results are valid assuming that the only identifying information comes from knowing which variables appear in which equations,
 e.g.,
 by exclusion restrictions,
 and through the use of a normalization.
 There are other sorts of identifying information that can be used.
 These include
\end_layout

\begin_deeper
\begin_layout Enumerate
Cross equation restrictions
\end_layout

\begin_layout Enumerate
Additional restrictions on parameters within equations (as in the Klein model discussed below)
\end_layout

\begin_layout Enumerate
Restrictions on the covariance matrix of the errors
\end_layout

\begin_layout Enumerate
Nonlinearities in variables 
\end_layout

\end_deeper
\begin_layout Itemize
When these sorts of information are available,
 the above conditions aren't necessary for identification,
 though they are of course still sufficient.
 
\end_layout

\begin_layout Standard
To give an example of how other information can be used,
 consider the model 
\begin_inset Formula 
\[
Y\Gamma=XB+E
\]

\end_inset

 where 
\begin_inset Formula $\Gamma$
\end_inset

 is an upper triangular matrix with 1's on the main diagonal.
 This is a 
\emph on
triangular system
\emph default
 of equations.
 In this case,
 the first equation is 
\begin_inset Formula 
\[
y_{1}=XB_{\cdot1}+E_{\cdot1}
\]

\end_inset

 Since only exogs appear on the rhs,
 this equation is identified.
\end_layout

\begin_layout Standard
The second equation is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{2}=-\gamma_{21}y_{1}+XB_{\cdot2}+E_{\cdot2}
\]

\end_inset

 This equation has 
\begin_inset Formula $K^{**}=0$
\end_inset

 excluded exogs,
 and 
\begin_inset Formula $G^{*}=2$
\end_inset

 included endogs,
 so it fails the order (necessary) condition for identification.
\end_layout

\begin_layout Itemize
However,
 suppose that we have the restriction 
\begin_inset Formula $\Sigma_{21}=0,$
\end_inset

 so that the first and second structural errors are uncorrelated.
 In this case 
\begin_inset Formula 
\[
\mathcal{E}(y_{1t}\varepsilon_{2t})=\mathcal{E}\left\{ (X_{t}^{\prime}B_{\cdot1}+\varepsilon_{1t})\varepsilon_{2t}\right\} =0
\]

\end_inset

 so there's no problem of simultaneity.
 If the entire 
\begin_inset Formula $\Sigma$
\end_inset

 matrix is diagonal,
 then following the same logic,
 all of the equations are identified.
 This is known as a 
\emph on
fully recursive
\emph default
 model.
 
\end_layout

\begin_layout Section
2SLS
\end_layout

\begin_layout Standard
When we have no information regarding cross-equation restrictions or the structure of the error covariance matrix,
 one can estimate the parameters of a single equation of the system without regard to the other equations.
\end_layout

\begin_layout Itemize
This isn't always efficient,
 as we'll see,
 but it has the advantage that misspecifications in other equations will not affect the consistency of the estimator of the parameters of the equation of interest.
\end_layout

\begin_layout Itemize
Also,
 estimation of the equation won't be affected by identification problems in other equations.
 
\end_layout

\begin_layout Standard
The 2SLS estimator is very simple:
 it is the GIV estimator,
 using all of the weakly exogenous variables as instruments.
 In the first stage,
 each column of 
\begin_inset Formula $Y_{1}$
\end_inset

 is regressed on 
\emph on
all
\emph default
 the weakly exogenous variables in the system,
 e.g.,
 the entire 
\begin_inset Formula $X$
\end_inset

 matrix.
 The fitted values are 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Y}_{1} & = & X(X^{\prime}X)^{-1}X^{\prime}Y_{1}\\
 & = & P_{X}Y_{1}\\
 & = & X\hat{\Pi}_{1}
\end{eqnarray*}

\end_inset

 Since these fitted values are the projection of 
\begin_inset Formula $Y_{1}$
\end_inset

 on the space spanned by 
\begin_inset Formula $X,$
\end_inset

 and since any vector in this space is uncorrelated with 
\begin_inset Formula $\varepsilon$
\end_inset

 by assumption,
 
\begin_inset Formula $\hat{Y}_{1}$
\end_inset

 is uncorrelated with 
\begin_inset Formula $\varepsilon.$
\end_inset

 Since 
\begin_inset Formula $\hat{Y}_{1}$
\end_inset

 is simply the reduced-form prediction,
 it is correlated with 
\begin_inset Formula $Y_{1},$
\end_inset

 The only other requirement is that the instruments be linearly independent.
 This should be the case when the order condition is satisfied,
 since there are more columns in 
\begin_inset Formula $X_{2}$
\end_inset

 than in 
\begin_inset Formula $Y_{1}$
\end_inset

 in this case.
\end_layout

\begin_layout Standard
The second stage substitutes 
\begin_inset Formula $\hat{Y}_{1}$
\end_inset

 in place of 
\begin_inset Formula $Y_{1},$
\end_inset

 and estimates by OLS.
 This original model is 
\begin_inset Formula 
\begin{eqnarray*}
y & = & Y_{1}\gamma_{1}+X_{1}\beta_{1}+\varepsilon\\
 & = & Z\delta+\varepsilon
\end{eqnarray*}

\end_inset

 and the second stage model is 
\begin_inset Formula 
\[
y=\hat{Y_{1}}\gamma_{1}+X_{1}\beta_{1}+\varepsilon.
\]

\end_inset

 Since 
\begin_inset Formula $X_{1}$
\end_inset

 is in the space spanned by 
\begin_inset Formula $X,$
\end_inset

 
\begin_inset Formula $P_{X}X_{1}=X_{1},$
\end_inset

 so we can write the second stage model as 
\begin_inset Formula 
\begin{eqnarray*}
y & = & P_{X}Y_{1}\gamma_{1}+P_{X}X_{1}\beta_{1}+\varepsilon\\
 & \equiv & P_{X}Z\delta+\varepsilon
\end{eqnarray*}

\end_inset

 The OLS estimator applied to this model is 
\begin_inset Formula 
\[
\hat{\delta}=(Z^{\prime}P_{X}Z)^{-1}Z^{\prime}P_{X}y
\]

\end_inset

 which is exactly what we get if we estimate using IV,
 with the reduced form predictions of the endogs used as instruments.
 Note that if we define 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Z} & = & P_{X}Z\\
 & = & \left[\begin{array}{cc}
\hat{Y}_{1} & X_{1}\end{array}\right]
\end{eqnarray*}

\end_inset

 so that 
\begin_inset Formula $\hat{Z}$
\end_inset

 are the instruments for 
\begin_inset Formula $Z,$
\end_inset

 then we can write 
\begin_inset Formula 
\[
\hat{\delta}=(\hat{Z}^{\prime}Z)^{-1}\hat{Z}^{\prime}y
\]

\end_inset


\end_layout

\begin_layout Itemize
Important note:
 OLS on the transformed model can be used to calculate the 2SLS estimate of 
\begin_inset Formula $\delta,$
\end_inset

 since we see that it's equivalent to IV using a particular set of instruments.
 However 
\emph on
the OLS covariance formula is not valid.

\emph default
 We need to apply the IV covariance formula already seen above.
 
\end_layout

\begin_layout Standard
Actually,
 there is also a simplification of the general IV variance formula.
 Define 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Z} & = & P_{X}Z\\
 & = & \left[\begin{array}{ll}
\hat{Y} & X\end{array}\right]
\end{eqnarray*}

\end_inset

 The IV covariance estimator would ordinarily be 
\begin_inset Formula 
\[
\hat{V}(\hat{\delta})=\left(\hat{Z}^{\prime}Z\right)^{-1}\left(\hat{Z}^{\prime}\hat{Z}\right)\left(Z^{\prime}\hat{Z}\right)^{-1}\hat{\sigma}_{IV}^{2}
\]

\end_inset

 However,
 looking at the first term in parentheses
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\hat{Z}^{\prime}Z=\left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]^{\prime}\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right]=\left[\begin{array}{ll}
Y_{1}^{\prime}(P_{X})Y_{1} & Y_{1}^{\prime}(P_{X})X_{1}\\
X_{1}^{\prime}Y_{1} & X_{1}^{\prime}X_{1}
\end{array}\right]
\]

\end_inset

 but since 
\begin_inset Formula $P_{X}$
\end_inset

 is idempotent and since 
\begin_inset Formula $P_{X}X=X,$
\end_inset

 we can write 
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]^{\prime}\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right] & = & \left[\begin{array}{ll}
Y_{1}^{\prime}P_{X}P_{X}Y_{1} & Y_{1}^{\prime}P_{X}X_{1}\\
X_{1}^{\prime}P_{X}Y_{1} & X_{1}^{\prime}X_{1}
\end{array}\right]\\
 & = & \left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]^{\prime}\left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]\\
 & = & \hat{Z}^{\prime}\hat{Z}
\end{eqnarray*}

\end_inset

 Therefore,
 the first and second terms in the variance formula cancel,
 so the 2SLS varcov estimator simplifies to 
\begin_inset Formula 
\[
\hat{V}(\hat{\delta})=\left(Z^{\prime}\hat{Z}\right)^{-1}\hat{\sigma}_{IV}^{2}
\]

\end_inset

 which,
 following some algebra similar to the above,
 can also be written as 
\begin_inset Formula 
\begin{equation}
\hat{V}(\hat{\delta})=\left(\hat{Z}^{\prime}\hat{Z}\right)^{-1}\hat{\sigma}_{IV}^{2}\label{eq:2sls varcov}
\end{equation}

\end_inset

Finally,
 recall that though this is presented in terms of the first equation,
 it is general,
 since any equation can be placed first.
\end_layout

\begin_layout Standard

\series bold
Properties of 2SLS:
\end_layout

\begin_layout Enumerate
Consistent
\end_layout

\begin_layout Enumerate
Asymptotically normal
\end_layout

\begin_layout Enumerate
Biased when the mean esists (the existence of moments is a technical issue we won't go into here).
\end_layout

\begin_layout Enumerate
Asymptotically inefficient,
 except in special circumstances (more on this later).
 
\end_layout

\begin_layout Section
Testing the overidentifying restrictions
\end_layout

\begin_layout Standard
The selection of which variables are endogs and which are exogs 
\emph on
is part of the specification of the model
\emph default
.
 As such,
 there is room for error here:
 one might erroneously classify a variable as exog when it is in fact correlated with the error term.
 A general test for the specification on the model can be formulated as follows:
\end_layout

\begin_layout Standard
The IV estimator can be calculated by applying OLS to the transformed model,
 so the IV objective function at the minimized value is 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\left(y-X\hat{\beta}_{IV}\right)^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right),
\]

\end_inset

 but 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\varepsilon}_{IV} & = & y-X\hat{\beta}_{IV}\\
 & = & y-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}y\\
 & = & \left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)y\\
 & = & \left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\left(X\beta+\varepsilon\right)\\
 & = & A\left(X\beta+\varepsilon\right)
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
A\equiv I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}
\]

\end_inset

 so 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\left(\varepsilon^{\prime}+\beta^{\prime}X^{\prime}\right)A^{\prime}P_{W}A\left(X\beta+\varepsilon\right)
\]

\end_inset

 Moreover,
 
\begin_inset Formula $A^{\prime}P_{W}A$
\end_inset

 is idempotent,
 as can be verified by multiplication:
 
\begin_inset Formula 
\begin{eqnarray*}
A^{\prime}P_{W}A & = & \left(I-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}\right)P_{W}\left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\\
 & = & \left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\\
 & = & \left(I-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}\right)P_{W}.
\end{eqnarray*}

\end_inset

 Furthermore,
 
\begin_inset Formula $A$
\end_inset

 is orthogonal to 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
AX & = & \left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)X\\
 & = & X-X\\
 & = & 0
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\varepsilon^{\prime}A^{\prime}P_{W}A\varepsilon
\]

\end_inset

 Supposing the 
\begin_inset Formula $\varepsilon$
\end_inset

 are normally distributed,
 with variance 
\begin_inset Formula $\sigma^{2},$
\end_inset

 then the random variable 
\begin_inset Formula 
\[
\frac{s(\hat{\beta}_{IV})}{\sigma^{2}}=\frac{\varepsilon^{\prime}A^{\prime}P_{W}A\varepsilon}{\sigma^{2}}
\]

\end_inset

 is a quadratic form of a 
\begin_inset Formula $N(0,1)$
\end_inset

 random variable with an idempotent matrix in the middle,
 so 
\begin_inset Formula 
\[
\frac{s(\hat{\beta}_{IV})}{\sigma^{2}}\sim\chi^{2}(\rho(A^{\prime}P_{W}A))
\]

\end_inset

 This isn't available,
 since we need to estimate 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Substituting a consistent estimator,
 
\begin_inset Formula 
\[
\frac{s(\hat{\beta}_{IV})}{\widehat{\sigma^{2}}}\overset{a}{\sim}\chi^{2}(\rho(A^{\prime}P_{W}A))
\]

\end_inset


\end_layout

\begin_layout Itemize
Even if the 
\begin_inset Formula $\varepsilon$
\end_inset

 aren't normally distributed,
 the asymptotic result still holds.
 The last thing we need to determine is the rank of the idempotent matrix.
 We have 
\begin_inset Formula 
\[
A^{\prime}P_{W}A=\left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)
\]

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\rho(A^{\prime}P_{W}A) & = & Tr\left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\\
 & = & TrP_{W}-TrX^{\prime}P_{W}P_{W}X(X^{\prime}P_{W}X)^{-1}\\
 & = & TrW(W^{\prime}W)^{-1}W^{\prime}-K_{X}\\
 & = & TrW^{\prime}W(W^{\prime}W)^{-1}-K_{X}\\
 & = & K_{W}-K_{X}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $K_{W}$
\end_inset

 is the number of columns of 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $K_{X}$
\end_inset

 is the number of columns of 
\begin_inset Formula $X.$
\end_inset

 The degrees of freedom of the test is simply the number of overidentifying restrictions:
 the number of instruments we have beyond the number that is strictly necessary for consistent estimation.
\end_layout

\begin_layout Itemize
This test is an overall specification test:
 the joint null hypothesis is that the model is correctly specified 
\begin_inset Formula $\emph{and}$
\end_inset

 that the 
\begin_inset Formula $W$
\end_inset

 form valid instruments (e.g.,
 that the variables classified as exogs really are uncorrelated with 
\begin_inset Formula $\varepsilon.$
\end_inset

 Rejection can mean that either the model 
\begin_inset Formula $y=Z\delta+\varepsilon$
\end_inset

 is misspecified,
 or that there is correlation between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\varepsilon.$
\end_inset


\end_layout

\begin_layout Itemize
This is a particular case of the GMM criterion test,
 which is covered in the second half of the course.
 See Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:A-specification-test"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Itemize
Note that since 
\begin_inset Formula 
\[
\hat{\varepsilon}_{IV}=A\varepsilon
\]

\end_inset

 and 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\varepsilon^{\prime}A^{\prime}P_{W}A\varepsilon
\]

\end_inset

 we can write 
\begin_inset Formula 
\begin{eqnarray*}
\frac{s(\hat{\beta}_{IV})}{\widehat{\sigma^{2}}} & = & \frac{\left(\hat{\varepsilon}^{\prime}W(W^{\prime}W)^{-1}W^{\prime}\right)\left(W(W^{\prime}W)^{-1}W^{\prime}\hat{\varepsilon}\right)}{\hat{\varepsilon}^{\prime}\hat{\varepsilon}/n}\\
 & = & n(RSS_{\hat{\varepsilon}_{IV}|W}/TSS_{\hat{\varepsilon}_{IV}})\\
 & = & nR_{u}^{2}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $R_{u}^{2}$
\end_inset

 is the uncentered 
\begin_inset Formula $R^{2}$
\end_inset

 from a regression of the 
\begin_inset Formula $IV$
\end_inset

 residuals on all of the instruments 
\begin_inset Formula $W$
\end_inset

.
 This is a convenient way to calculate the test statistic.
 
\end_layout

\begin_layout Standard
On an aside,
 consider IV estimation of a just-identified model,
 using the standard notation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 is the matrix of instruments.
 If we have exact identification then 
\begin_inset Formula $cols(W)=cols(X)$
\end_inset

,
 so 
\begin_inset Formula $W^{'}X$
\end_inset

 is a square matrix.
 The transformed model is 
\begin_inset Formula 
\[
P_{W}y=P_{W}X\beta+P_{W}\varepsilon
\]

\end_inset

 and the fonc are 
\begin_inset Formula 
\[
X^{\prime}P_{W}(y-X\hat{\beta}_{IV})=0
\]

\end_inset

 The IV estimator is 
\begin_inset Formula 
\[
\hat{\beta}_{IV}=\left(X^{\prime}P_{W}X\right)^{-1}X^{\prime}P_{W}y
\]

\end_inset

 Considering the inverse here 
\begin_inset Formula 
\begin{eqnarray*}
\left(X^{\prime}P_{W}X\right)^{-1} & = & \left(X^{\prime}W(W^{\prime}W)^{-1}W^{\prime}X\right)^{-1}\\
 & = & (W^{\prime}X)^{-1}\left(X^{\prime}W(W^{\prime}W)^{-1}\right)^{-1}\\
 & = & (W^{\prime}X)^{-1}(W^{\prime}W)\left(X^{\prime}W\right)^{-1}
\end{eqnarray*}

\end_inset

 Now multiplying this by 
\begin_inset Formula $X^{\prime}P_{W}y,$
\end_inset

 we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{IV} & = & (W^{\prime}X)^{-1}(W^{\prime}W)\left(X^{\prime}W\right)^{-1}X^{\prime}P_{W}y\\
 & = & (W^{\prime}X)^{-1}(W^{\prime}W)\left(X^{\prime}W\right)^{-1}X^{\prime}W(W^{\prime}W)^{-1}W^{\prime}y\\
 & = & (W^{\prime}X)^{-1}W^{\prime}y
\end{eqnarray*}

\end_inset

 The objective function for the generalized IV estimator is 
\begin_inset Formula 
\begin{eqnarray*}
s(\hat{\beta}_{IV}) & = & \left(y-X\hat{\beta}_{IV}\right)^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)-\hat{\beta}_{IV}^{\prime}X^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)-\hat{\beta}_{IV}^{\prime}X^{\prime}P_{W}y+\hat{\beta}_{IV}^{\prime}X^{\prime}P_{W}X\hat{\beta}_{IV}\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)-\hat{\beta}_{IV}^{\prime}\left(X^{\prime}P_{W}y+X^{\prime}P_{W}X\hat{\beta}_{IV}\right)\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)
\end{eqnarray*}

\end_inset

 by the fonc for generalized IV.
 However,
 when we're in the just indentified case,
 this is 
\begin_inset Formula 
\begin{eqnarray*}
s(\hat{\beta}_{IV}) & = & y^{\prime}P_{W}\left(y-X(W^{\prime}X)^{-1}W^{\prime}y\right)\\
 & = & y^{\prime}P_{W}\left(I-X(W^{\prime}X)^{-1}W^{\prime}\right)y\\
 & = & y^{\prime}\left(W(W^{\prime}W)^{-1}W^{\prime}-W(W^{\prime}W)^{-1}W^{\prime}X(W^{\prime}X)^{-1}W^{\prime}\right)y\\
 & = & 0
\end{eqnarray*}

\end_inset

 
\emph on
The value of the objective function of the IV estimator is zero in the just identified case.

\emph default
 This makes sense,
 since we've already shown that the objective function after dividing by 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is asymptotically 
\begin_inset Formula $\chi^{2}$
\end_inset

 with degrees of freedom equal to the number of overidentifying restrictions.
 In the present case,
 there are no overidentifying restrictions,
 so we have a 
\begin_inset Formula $\chi^{2}(0)$
\end_inset

 rv,
 which has mean 0 and variance 0,
 e.g.,
 it's simply 0.
 This means we're not able to test the identifying restrictions in the case of exact identification.
\end_layout

\begin_layout Section
System methods of estimation
\end_layout

\begin_layout Standard
2SLS is a single equation method of estimation,
 as noted above.
 The advantage of a single equation method is that it's unaffected by the other equations of the system,
 so they don't need to be specified (except for defining what are the exogs,
 so 2SLS can use the complete set of instruments).
 The disadvantage of 2SLS is that it's inefficient,
 in general.
\end_layout

\begin_layout Itemize
Recall that overidentification improves efficiency of estimation,
 since an overidentified equation can use more instruments than are necessary for consistent estimation.
\end_layout

\begin_layout Itemize
Secondly,
 the assumption is that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
Y\Gamma & = & XB+E\\
\mathcal{E}(X^{\prime}E) & = & 0_{(K\times G)}\\
vec(E) & \sim & N(0,\Psi)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Since there is no autocorrelation of the 
\begin_inset Formula $E_{t}$
\end_inset

 's,
 and since the columns of 
\begin_inset Formula $E$
\end_inset

 are individually homoscedastic,
 then 
\begin_inset Formula 
\begin{eqnarray*}
\Psi & = & \left[\begin{array}{llll}
\sigma_{11}I_{n} & \sigma_{12}I_{n} & \cdots & \sigma_{1G}I_{n}\\
 & \sigma_{22}I_{n} &  & \vdots\\
 &  & \ddots & \vdots\\
\cdot &  &  & \sigma_{GG}I_{n}
\end{array}\right]\\
 & = & \Sigma\otimes I_{n}
\end{eqnarray*}

\end_inset

 This means that the structural equations are heteroscedastic and correlated with one another
\end_layout

\begin_layout Itemize
In general,
 ignoring this will lead to inefficient estimation,
 following the section on GLS.
 When equations are correlated with one another estimation should account for the correlation in order to obtain efficiency.
\end_layout

\begin_layout Itemize
Also,
 since the equations are correlated,
 information about one equation is implicitly information about all equations.
 Therefore,
 overidentification restrictions in any equation improve efficiency for 
\emph on
all
\emph default
 equations,
 even the just identified equations.
\end_layout

\begin_layout Itemize
Single equation methods can't use these types of information,
 and are therefore inefficient (in general).
 
\end_layout

\begin_layout Subsection
3SLS
\end_layout

\begin_layout Standard
Note:
 It is easier and more practical to treat the 3SLS estimator as a generalized method of moments estimator (see Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "cha:Generalized-method-of"
nolink "false"

\end_inset

).
 I no longer teach the following section,
 but it is retained for its possible historical interest.
 Another alternative is to use FIML (Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:FIML"
nolink "false"

\end_inset

),
 if you are willing to make distributional assumptions on the errors.
 This is computationally feasible with modern computers.
 
\end_layout

\begin_layout Standard
Following our above notation,
 each structural equation can be written as 
\begin_inset Formula 
\begin{eqnarray*}
y_{i} & = & Y_{i}\gamma_{1}+X_{i}\beta_{1}+\varepsilon_{i}\\
 & = & Z_{i}\delta_{i}+\varepsilon_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Grouping the 
\begin_inset Formula $G$
\end_inset

 equations together we get 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{G}
\end{array}\right]=\left[\begin{array}{llll}
Z_{1} & 0 & \cdots & 0\\
0 & Z_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & Z_{G}
\end{array}\right]\left[\begin{array}{l}
\delta_{1}\\
\delta_{2}\\
\vdots\\
\delta_{G}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{G}
\end{array}\right]
\]

\end_inset

 or 
\begin_inset Formula 
\[
y=Z\delta+\varepsilon
\]

\end_inset

 where we already have that 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\varepsilon\varepsilon^{\prime}) & = & \Psi\\
 & = & \Sigma\otimes I_{n}
\end{eqnarray*}

\end_inset

 The 3SLS estimator is just 2SLS combined with a GLS correction that takes advantage of the structure of 
\begin_inset Formula $\Psi.$
\end_inset

 Define 
\begin_inset Formula $\hat{Z}$
\end_inset

 as 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Z} & = & \left[\begin{array}{llll}
X(X^{\prime}X)^{-1}X^{\prime}Z_{1} & 0 & \cdots & 0\\
0 & X(X^{\prime}X)^{-1}X^{\prime}Z_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & X(X^{\prime}X)^{-1}X^{\prime}Z_{G}
\end{array}\right]\\
 & = & \left[\begin{array}{llll}
\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array} & 0 & \cdots & 0\\
0 & \begin{array}{ll}
\hat{Y}_{2} & X_{2}\end{array} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \begin{array}{ll}
\hat{Y}_{G} & X_{G}\end{array}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
These instruments are simply the 
\emph on
unrestricted
\emph default
 rf predicitions of the endogs,
 combined with the exogs.
 The distinction is that if the model is overidentified,
 then 
\begin_inset Formula 
\[
\Pi=B\Gamma^{-1}
\]

\end_inset

 may be subject to some zero restrictions,
 depending on the restrictions on 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B,$
\end_inset

 and 
\begin_inset Formula $\hat{\Pi}$
\end_inset

 does not impose these restrictions.
 Also,
 note that 
\begin_inset Formula $\hat{\Pi}$
\end_inset

 is calculated using OLS equation by equation,
 as was discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:EstimationRF"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard
The 2SLS estimator would be 
\begin_inset Formula 
\[
\hat{\delta}=(\hat{Z}^{\prime}Z)^{-1}\hat{Z}^{\prime}y
\]

\end_inset

 as can be verified by simple multiplication,
 and noting that the inverse of a block-diagonal matrix is just the matrix with the inverses of the blocks on the main diagonal.
 This IV estimator still ignores the covariance information.
 The natural extension is to add the GLS transformation,
 putting the inverse of the error covariance into the formula,
 which gives the 3SLS estimator 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\delta}_{3SLS} & = & \left(\hat{Z}^{\prime}\left(\Sigma\otimes I_{n}\right)^{-1}Z\right)^{-1}\hat{Z}^{\prime}\left(\Sigma\otimes I_{n}\right)^{-1}y\\
 & = & \left(\hat{Z}^{\prime}\left(\Sigma^{-1}\otimes I_{n}\right)Z\right)^{-1}\hat{Z}^{\prime}\left(\Sigma^{-1}\otimes I_{n}\right)y
\end{eqnarray*}

\end_inset

 This estimator requires knowledge of 
\begin_inset Formula $\Sigma.$
\end_inset

 The solution is to define a feasible estimator using a consistent estimator of 
\begin_inset Formula $\Sigma.$
\end_inset

 The obvious solution is to use an estimator based on the 2SLS residuals:
 
\begin_inset Formula 
\[
\hat{\varepsilon}_{i}=y_{i}-Z_{i}\hat{\delta}_{i,2SLS}
\]

\end_inset

 
\series bold
(IMPORTANT NOTE
\series default
:
 this is calculated using 
\begin_inset Formula $Z_{i},$
\end_inset

 not 
\begin_inset Formula $\hat{Z}_{i}).$
\end_inset

 Then the element 
\begin_inset Formula $i,j$
\end_inset

 of 
\begin_inset Formula $\Sigma$
\end_inset

 is estimated by 
\begin_inset Formula 
\[
\hat{\sigma}_{ij}=\frac{\hat{\varepsilon}_{i}^{\prime}\hat{\varepsilon}_{j}}{n}
\]

\end_inset

 Substitute 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 into the formula above to get the feasible 3SLS estimator.
\end_layout

\begin_layout Standard
Analogously to what we did in the case of 2SLS,
 the asymptotic distribution of the 3SLS estimator can be shown to be 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\delta}_{3SLS}-\delta\right)\overset{a}{\sim}N\left(0,\lim_{n\rightarrow\infty}\mathcal{E}\left\{ \left(\frac{\hat{Z}^{\prime}\left(\Sigma\otimes I_{n}\right)^{-1}\hat{Z}}{n}\right)^{-1}\right\} \right)
\]

\end_inset

 A formula for estimating the variance of the 3SLS estimator in finite samples (cancelling out the powers of 
\begin_inset Formula $n)$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{V}\left(\hat{\delta}_{3SLS}\right)=\left(\hat{Z}^{\prime}\left(\hat{\Sigma}^{-1}\otimes I_{n}\right)\hat{Z}\right)^{-1}
\]

\end_inset


\end_layout

\begin_layout Itemize
This is analogous to the 2SLS formula in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2sls varcov"
nolink "false"

\end_inset

),
 combined with the GLS correction.
\end_layout

\begin_layout Itemize
In the case that all equations are just identified,
 3SLS is numerically equivalent to 2SLS.
 Proving this is easiest if we use a GMM interpretation of 2SLS and 3SLS.
 GMM is presented in the next econometrics course.
 For now,
 take it on faith.
 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Subsection
FIML
\begin_inset CommandInset label
LatexCommand label
name "subsec:FIML"

\end_inset


\end_layout

\begin_layout Standard
Full information maximum likelihood is an alternative estimation method.
 FIML will be asymptotically efficient,
 since ML estimators based on a given information set are asymptotically efficient w.r.t.
 all other estimators that use the same information set,
 and in the case of the full-information ML estimator we use the entire information set.
 The 2SLS and 3SLS estimators don't require distributional assumptions,
 while FIML of course does.
 Our model is,
 recall 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}^{\prime}\\
E_{t} & \sim & N(0,\Sigma),\forall t\\
\mathcal{E}(E_{t}E_{s}^{\prime}) & = & 0,t\neq s
\end{eqnarray*}

\end_inset

 The joint normality of 
\begin_inset Formula $E_{t}$
\end_inset

 means that the density for 
\begin_inset Formula $E_{t}$
\end_inset

 is the multivariate normal,
 which is 
\begin_inset Formula 
\[
(2\pi)^{-g/2}\left(\det\Sigma^{-1}\right)^{-1/2}\exp\left(-\frac{1}{2}E_{t}^{\prime}\Sigma^{-1}E_{t}\right)
\]

\end_inset

 The transformation from 
\begin_inset Formula $E_{t}$
\end_inset

 to 
\begin_inset Formula $Y_{t}$
\end_inset

 requires the Jacobian 
\begin_inset Formula 
\[
|\det\frac{dE_{t}}{dY_{t}'}|=|\det\Gamma|
\]

\end_inset

 so the density for 
\begin_inset Formula $Y_{t}$
\end_inset

 is 
\begin_inset Formula 
\[
(2\pi)^{-G/2}|\det\Gamma|\left(\det\Sigma^{-1}\right)^{-1/2}\exp\left(-\frac{1}{2}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)\Sigma^{-1}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)^{\prime}\right)
\]

\end_inset

 Given the assumption of independence over time,
 the joint log-likelihood function is 
\begin_inset Formula 
\[
\ln L(B,\Gamma,\Sigma)=-\frac{nG}{2}\ln(2\pi)+n\ln(|\det\Gamma|)-\frac{n}{2}\ln\det\Sigma^{-1}-\frac{1}{2}\sum_{t=1}^{n}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)\Sigma^{-1}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)^{\prime}
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a nonlinear in the parameters objective function.
 Maximixation of this can be done using iterative numeric methods.
 We'll see how to do this in the next section.
\end_layout

\begin_layout Itemize
It turns out that the asymptotic distribution of 3SLS and FIML are the same,
 
\emph on
assuming normality of the errors
\emph default
.
\end_layout

\begin_layout Itemize
One can calculate the FIML estimator by iterating the 3SLS estimator,
 thus avoiding the use of a nonlinear optimizer.
 The steps are
\end_layout

\begin_deeper
\begin_layout Enumerate
Calculate 
\begin_inset Formula $\hat{\Gamma}_{3SLS}$
\end_inset

 and 
\begin_inset Formula $\hat{B}_{3SLS}$
\end_inset

 as normal.
\end_layout

\begin_layout Enumerate
Calculate 
\begin_inset Formula $\hat{\Pi}=\hat{B}_{3SLS}\hat{\Gamma}_{3SLS}^{-1}.$
\end_inset

 This is new,
 we didn't estimate 
\begin_inset Formula $\Pi$
\end_inset

 in this way before.
 This estimator may have some zeros in it.
 When Greene says iterated 3SLS doesn't lead to FIML,
 he means this for a procedure that doesn't update 
\begin_inset Formula $\hat{\Pi},$
\end_inset

 but only updates 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 and 
\begin_inset Formula $\hat{B}$
\end_inset

 and 
\begin_inset Formula $\hat{\Gamma}.$
\end_inset

 If you update 
\begin_inset Formula $\hat{\Pi}$
\end_inset

 you 
\emph on
do
\emph default
 converge to FIML.
\end_layout

\begin_layout Enumerate
Calculate the instruments 
\begin_inset Formula $\hat{Y}=X\hat{\Pi}$
\end_inset

 and calculate 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 using 
\begin_inset Formula $\hat{\Gamma}$
\end_inset

 and 
\begin_inset Formula $\hat{B}$
\end_inset

 to get the estimated errors,
 applying the usual estimator.
\end_layout

\begin_layout Enumerate
Apply 3SLS using these new instruments and the estimate of 
\begin_inset Formula $\Sigma.$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat steps 2-4 until there is no change in the parameters.
 
\end_layout

\end_deeper
\begin_layout Itemize
FIML is fully efficient,
 since it's an ML estimator that uses all information.
 This implies that 3SLS is fully efficient 
\emph on
when the errors are normally distributed.

\emph default
 Also,
 if each equation is just identified and the errors are normal,
 then 2SLS will be fully efficient,
 since in this case 2SLS
\begin_inset Formula $\equiv$
\end_inset

3SLS.
\end_layout

\begin_layout Itemize
When the errors aren't normally distributed,
 the likelihood function is of course different than what's written above.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Example:-Klein's-Model"

\end_inset

Example:
 Klein's Model 1
\end_layout

\begin_layout Standard
To give a practical example,
 consider the following (old-fashioned,
 but illustrative) macro model (this is the widely known Klein's Model 1) 
\begin_inset Formula 
\begin{eqnarray*}
\text{Consumption:\;\ }C_{t} & = & \alpha_{0}+\alpha_{1}P_{t}+\alpha_{2}P_{t-1}+\alpha_{3}(W_{t}^{p}+W_{t}^{g})+\varepsilon_{1t}\\
\text{Investment:\;\ }I_{t} & = & \beta_{0}+\beta_{1}P_{t}+\beta_{2}P_{t-1}+\beta_{3}K_{t-1}+\varepsilon_{2t}\\
\text{Private\:\ Wages:\;\ }W_{t}^{p} & = & \gamma_{0}+\gamma_{1}X_{t}+\gamma_{2}X_{t-1}+\gamma_{3}A_{t}+\varepsilon_{3t}\\
\text{Output:\;\ }X_{t} & = & C_{t}+I_{t}+G_{t}\\
\text{Profits:\;\ }P_{t} & = & X_{t}-T_{t}-W_{t}^{p}\\
\text{Capital\:\ Stock:\;\ }K_{t} & = & K_{t-1}+I_{t}\\
\left(\begin{array}{c}
\epsilon_{1t}\\
\epsilon_{2t}\\
\epsilon_{3t}
\end{array}\right) & \sim & IID\left(\left(\begin{array}{c}
0\\
0\\
0
\end{array}\right),\left(\begin{array}{ccc}
\sigma_{11} & \sigma_{12} & \sigma_{13}\\
 & \sigma_{22} & \sigma_{23}\\
 &  & \sigma_{33}
\end{array}\right)\right)
\end{eqnarray*}

\end_inset

 The other variables are the government wage bill,
 
\begin_inset Formula $W_{t}^{g},$
\end_inset

 taxes,
 
\begin_inset Formula $T_{t},$
\end_inset

 government nonwage spending,
 
\begin_inset Formula $G_{t},$
\end_inset

and a time trend,
 
\begin_inset Formula $A_{t}.$
\end_inset

 The endogenous variables are the lhs variables,
 
\begin_inset Formula 
\[
Y_{t}^{\prime}=\left[\begin{array}{cccccc}
C_{t} & I_{t} & W_{t}^{p} & X_{t} & P_{t} & K_{t}\end{array}\right]
\]

\end_inset

 and the predetermined variables are all others:
 
\begin_inset Formula 
\[
X_{t}^{\prime}=\left[\begin{array}{cccccccc}
1 & W_{t}^{g} & G_{t} & T_{t} & A_{t} & P_{t-1} & K_{t-1} & X_{t-1}\end{array}\right].
\]

\end_inset

 The model assumes that the errors of the equations are contemporaneously correlated,
 but nonautocorrelated.
 The model written as 
\begin_inset Formula $Y\Gamma=XB+E$
\end_inset

 gives 
\begin_inset Formula 
\[
\Gamma=\left[\begin{array}{llllll}
1 & 0 & 0 & -1 & 0 & 0\\
0 & 1 & 0 & -1 & 0 & -1\\
-\alpha_{3} & 0 & 1 & 0 & 1 & 0\\
0 & 0 & -\gamma_{1} & 1 & -1 & 0\\
-\alpha_{1} & -\beta_{1} & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0 & 1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
B=\left[\begin{array}{llllll}
\alpha_{0} & \beta_{0} & \gamma_{0} & 0 & 0 & 0\\
\alpha_{3} & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & -1 & 0\\
0 & 0 & \gamma_{3} & 0 & 0 & 0\\
\alpha_{2} & \beta_{2} & 0 & 0 & 0 & 0\\
0 & \beta_{3} & 0 & 0 & 0 & 1\\
0 & 0 & \gamma_{2} & 0 & 0 & 0
\end{array}\right]
\]

\end_inset

 To check this identification of the consumption equation,
 we need to extract 
\begin_inset Formula $\Gamma_{32}$
\end_inset

 and 
\begin_inset Formula $B_{22},$
\end_inset

 the submatrices of coefficients of endogs and exogs that 
\emph on
don't
\emph default
 appear in this equation.
 These are the rows that have zeros in the first column,
 and we need to drop the first column.
 We get 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]=\left[\begin{array}{lllll}
1 & 0 & -1 & 0 & -1\\
0 & -\gamma_{1} & 1 & -1 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & \gamma_{3} & 0 & 0 & 0\\
\beta_{3} & 0 & 0 & 0 & 1\\
0 & \gamma_{2} & 0 & 0 & 0
\end{array}\right]
\]

\end_inset

 We need to find a set of 5 rows of this matrix gives a full-rank 5
\begin_inset Formula $\times5$
\end_inset

 matrix.
 For example,
 selecting rows 3,4,5,6,
 and 7 we obtain the matrix 
\begin_inset Formula 
\[
A=\left[\begin{array}{lllll}
0 & 0 & 0 & 0 & 1\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & \gamma_{3} & 0 & 0 & 0\\
\beta_{3} & 0 & 0 & 0 & 1
\end{array}\right]
\]

\end_inset

 This matrix is of full rank,
 so the sufficient condition for identification is met.
 Counting included endogs,
 
\begin_inset Formula $G^{\ast}=3,$
\end_inset

 and counting excluded exogs,
 
\begin_inset Formula $K^{\ast\ast}=5,$
\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
K^{\ast\ast}-L & =G^{\ast}-1\\
5-L & =3-1\\
L & =3
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The equation is over-identified by three restrictions,
 according to the counting rules,
 which are correct when the only identifying information are the exclusion restrictions.
 However,
 there is additional information in this case.
 Both 
\begin_inset Formula $W_{t}^{p}$
\end_inset

 and 
\begin_inset Formula $W_{t}^{g}$
\end_inset

 enter the consumption equation,
 and their coefficients are restricted to be the same.
 For this reason the consumption equation is in fact overidentified by four restrictions.
 
\end_layout

\begin_layout Standard
The Octave program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Simeq/Klein2SLS.m}{Simeq/Klein2SLS.m}
\end_layout

\end_inset

 performs 2SLS estimation for the 3 equations of Klein's model 1,
 assuming nonautocorrelated errors,
 so that lagged endogenous variables can be used as instruments.
 The results are:
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/Simeq/Klein.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The above results are not valid (specifically,
 they are inconsistent) if the errors are autocorrelated,
 since lagged endogenous variables will not be valid instruments in that case.
 You might consider eliminating the lagged endogenous variables as instruments,
 and re-estimating by 2SLS,
 to obtain consistent parameter estimates in this more complex case.
 Standard errors will still be estimated inconsistently,
 unless use a Newey-West type covariance estimator.
 Food for thought...
\end_layout

\begin_layout Standard
Here's a Gretl script to estimate Klein's model 1:
 
\begin_inset CommandInset href
LatexCommand href
name "http://gretl.sourceforge.net/gretl-help/scripts/klein.inp"
target "http://gretl.sourceforge.net/gretl-help/scripts/klein.inp"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Numeric-optimization-methods"

\end_inset

Numeric optimization methods
\end_layout

\begin_layout Standard

\series bold
Readings:

\series default
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Ch.
 10;
 Hamilton,
 ch.
 5,
 section 7 (pp.
 133-139)
\begin_inset Formula $^{*};$
\end_inset

 Gourieroux and Monfort,
 Vol.
 1,
 ch.
 13,
 pp.
 443-60
\begin_inset Formula $^{*}$
\end_inset

;
 Goffe,
 et.
 al.
 (1994).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The next chapter introduces extremum estimators,
 which are minimizers or maximizers of objective functions.
 If we're going to be applying extremum estimators,
 we'll need to know how to find an extremum.
 This section gives a very brief introduction to what is a large literature on numeric optimization methods.
 We'll consider a few well-known techniques,
 and one of the global optimization methods that may allow one to solve difficult problems.
\end_layout

\begin_layout Standard
The main objectives are 
\end_layout

\begin_layout Itemize
to become familiar with the issues,
 which should lead you to take a cautious attitude
\end_layout

\begin_layout Itemize
to learn how to use gradient-based local minimizers such as 
\family typewriter
fminunc
\family default
 and 
\family typewriter
fmincon
\family default
 at the practical level.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The general problem we consider is how to find the maximizing element 
\begin_inset Formula $\hat{\theta}$
\end_inset

 (a 
\begin_inset Formula $K$
\end_inset

 -vector) of a function 
\begin_inset Formula $s(\theta).$
\end_inset

 This function may not be continuous,
 and it may not be differentiable.
 Even if it is twice continuously differentiable,
 it may not be globally concave,
 so 
\begin_inset CommandInset href
LatexCommand href
name "local maxima, minima"
target "https://en.wikipedia.org/wiki/Maxima_and_minima"
literal "false"

\end_inset

 and 
\begin_inset CommandInset href
LatexCommand href
name "saddlepoints"
target "https://en.wikipedia.org/wiki/Saddle_point"
literal "false"

\end_inset

 may all exist.
 Supposing 
\begin_inset Formula $s(\theta)$
\end_inset

 were a quadratic function of 
\begin_inset Formula $\theta,$
\end_inset

 
\emph on
i.e
\emph default
.,
 
\begin_inset Formula 
\[
s(\theta)=a+b^{\prime}\theta+\frac{1}{2}\theta^{\prime}C\theta,
\]

\end_inset

the first order conditions would be linear:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{\theta}s(\theta)=b+C\theta
\]

\end_inset

so the maximizing (minimizing) element would be 
\begin_inset Formula $\hat{\theta}=-C^{-1}b.$
\end_inset

 This is the sort of problem we have with linear models estimated by OLS.
 It's also the case for feasible GLS,
 since conditional on the estimate of the varcov matrix,
 we have a quadratic objective function in the remaining parameters.
 
\end_layout

\begin_layout Standard
More general problems will not have linear f.o.c.,
 and we will not be able to solve for the maximizer analytically.
 This is when we need a numeric optimization method.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Search
\end_layout

\begin_layout Standard
The idea is to create a grid over the parameter space and evaluate the function at each point on the grid.
 Select the best point.
 Then refine the grid in the neighborhood of the best point,
 and continue until the accuracy is 
\begin_inset Quotes sld
\end_inset

good enough
\begin_inset Quotes srd
\end_inset

.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Search-method"
nolink "false"

\end_inset

.
 One has to be careful that the grid is fine enough in relationship to the irregularity of the function to ensure that sharp peaks are not missed entirely.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Search-method"

\end_inset

Search method
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/Search.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
To check 
\begin_inset Formula $q$
\end_inset

 values in each dimension of a 
\begin_inset Formula $K$
\end_inset

 dimensional parameter space,
 we need to check 
\begin_inset Formula $q^{K}$
\end_inset

 points.
 For example,
 if 
\begin_inset Formula $q=100$
\end_inset

 and 
\begin_inset Formula $K=10,$
\end_inset

 there would be 
\begin_inset Formula $100^{10}$
\end_inset

 points to check.
 If 1000 points can be checked in a second,
 it would take 
\begin_inset Formula $3.\,171\times10^{9}$
\end_inset

 years to perform the calculations,
 which is approximately 2/3 the age of the earth.
 The search method is a very reasonable choice if 
\begin_inset Formula $K$
\end_inset

 is small,
 but it quickly becomes infeasible if 
\begin_inset Formula $K$
\end_inset

 is moderate or large.
 
\end_layout

\begin_layout Standard
The Julia function 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{./Examples/NonlinearOptimization/GridExample.jl}{GridExample.jl} 
\end_layout

\end_inset

 allows you to play around with a simple one dimensional grid search,
 selecting the number of evenly spaced values to try.
 Try running GridExample(5) and GridExample(10).
 The result of GridExample(10) is in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Grid-search,-one"
nolink "false"

\end_inset

.
 In this example,
 we're in the neighborhood of the minimizer,
 but still not too close to the minimizer.
 However,
 we're close enough so refinement will lead us to converge to the global minimizer.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Grid-search,-one"

\end_inset

Grid search,
 one dimension
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/gridsearch.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Derivative-based methods
\end_layout

\begin_layout Standard

\emph on
In the following,
 the superscript 
\begin_inset Formula $k$
\end_inset

 is used as the index of the iterations of a given method.
 It is not an exponent,
 and it is not the dimension of the parameter vector.
\end_layout

\begin_layout Standard
We assume that the objective function is at least one time differentiable.
 Otherwise,
 these methods are not applicable,
 obviously.
 Derivative-based methods are defined by
\end_layout

\begin_layout Enumerate
the method for choosing the initial value,
 
\begin_inset Formula $\theta^{1}$
\end_inset


\end_layout

\begin_layout Enumerate
the iteration method for choosing 
\begin_inset Formula $\theta^{k+1}$
\end_inset

,
 given that we're at 
\begin_inset Formula $\theta^{k}$
\end_inset

 at iteration 
\begin_inset Formula $k$
\end_inset

 (based upon derivatives)
\end_layout

\begin_layout Enumerate
the stopping criterion.
 
\end_layout

\begin_layout Standard
The iteration method can be broken into two problems:
 choosing the stepsize 
\begin_inset Formula $a^{k}$
\end_inset

 (a scalar) and choosing the direction of movement,
 
\begin_inset Formula $d^{k},$
\end_inset

 which is of the same dimension of 
\begin_inset Formula $\theta,$
\end_inset

 so that 
\begin_inset Formula 
\[
\theta^{(k+1)}=\theta^{(k)}+a^{k}d^{k}.
\]

\end_inset


\end_layout

\begin_layout Standard

\emph on
\begin_inset Newpage newpage
\end_inset


\emph default
At the point 
\begin_inset Formula $\theta$
\end_inset

,
 a
\emph on
 locally increasing direction of search
\emph default
,
 
\begin_inset Formula $d$
\end_inset

,
 is a direction such that 
\begin_inset Formula 
\[
\frac{\partial s(\theta+ad)}{\partial a}>0.
\]

\end_inset

 That is,
 if we go from 
\begin_inset Formula $\theta$
\end_inset

 in direction 
\begin_inset Formula $d$
\end_inset

,
 we will improve on the objective function 
\color blue
(Note:
 I'm assuming we are maximizing)
\color inherit
,
 at least if we don't go too far.
\end_layout

\begin_layout Itemize
As long as the gradient at 
\begin_inset Formula $\theta^{k}$
\end_inset

 is not zero,
 there exist increasing directions,
 and they can all be represented as 
\begin_inset Formula $Q^{k}g(\theta^{k})$
\end_inset

 where 
\begin_inset Formula $Q^{k}$
\end_inset

 is a symmetric positive definite matrix and 
\begin_inset Formula $g\left(\theta\right)=D_{\theta}s(\theta)$
\end_inset

 is the gradient at 
\begin_inset Formula $\theta$
\end_inset

.
 Every increasing direction can be represented in this way (p.d.
 matrices are those such that the angle between 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $Qg(\theta)$
\end_inset

 is less than 90 degrees).
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Increasing directions"
nolink "false"

\end_inset

.
\begin_inset Newpage newpage
\end_inset

 
\begin_inset Float figure
placement H
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Increasing directions"

\end_inset

Increasing directions of search
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/IncreasingDirections.pdf
	width 5in

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
With this,
 the iteration rule becomes 
\begin_inset Formula 
\[
\theta^{(k+1)}=\theta^{(k)}+a^{k}Q^{k}g(\theta^{k})
\]

\end_inset

and we keep going until the gradient becomes zero,
 so that there is no increasing direction.
 
\end_layout

\begin_layout Itemize
The problem is now 
\emph on
how to choose 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $Q.$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Conditional on
\series default
 
\begin_inset Formula $Q$
\end_inset

,
 choosing 
\begin_inset Formula $a$
\end_inset

 is fairly straightforward.
 A simple line (1 dimensional grid) search is an attractive possibility,
 since 
\begin_inset Formula $a$
\end_inset

 is a scalar.
 But there are other methods that may be better (bisection,
 golden,
 etc.).
 Optimization packages will generally do this well for you,
 so you usually don't need to worry too much about it.
\end_layout

\begin_layout Itemize
The remaining problem is how to choose 
\begin_inset Formula $Q.$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Note also that this gives no guarantees to find a global maximum.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Steepest ascent
\end_layout

\begin_layout Standard
Steepest ascent (descent if we're minimizing) just sets 
\begin_inset Formula $Q$
\end_inset

 to an identity matrix,
 since the gradient provides the direction of maximum rate of increase of the objective function.
\end_layout

\begin_layout Itemize
Advantages:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

fast,
 per iteration - doesn't require anything more than first derivatives.
\end_layout

\begin_layout Itemize
Disadvantages:
 May not be fast after all,
 as we may need many iterations:
 see the Rosenbrock,
 or 
\begin_inset Quotes sld
\end_inset

banana
\begin_inset Quotes srd
\end_inset

 function:
 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Rosenbrock_function
\end_layout

\end_inset

.
 (note for lectures:
 click on the image in the web page to enlarge it)
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Newton's method
\end_layout

\begin_layout Standard
Newton's method uses information about the slope and curvature of the objective function to determine which direction and how far to move from an initial point.
 Supposing we're trying to maximize 
\begin_inset Formula $s_{n}(\theta).$
\end_inset

 Take a second order Taylor's series approximation of 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 about 
\begin_inset Formula $\theta^{k}$
\end_inset

 (an initial guess).
 
\begin_inset Formula 
\[
s_{n}(\theta)\approx s_{n}(\theta^{k})+g(\theta^{k})^{\prime}\left(\theta-\theta^{k}\right)+1/2\left(\theta-\theta^{k}\right)^{\prime}H(\theta^{k})\left(\theta-\theta^{k}\right)
\]

\end_inset

(
\begin_inset Formula $g$
\end_inset

 is the gradient vector and 
\begin_inset Formula $H$
\end_inset

 is the Hessian matrix).
 To attempt to maximize 
\begin_inset Formula $s_{n}(\theta),$
\end_inset

 we can maximize the portion of the right-hand side that depends on 
\begin_inset Formula $\theta,$
\end_inset

 
\emph on
i.e.
\emph default
,
 we can maximize 
\begin_inset Formula 
\[
\tilde{s}(\theta)=g(\theta^{k})^{\prime}\theta+1/2\left(\theta-\theta^{k}\right)^{\prime}H(\theta^{k})\left(\theta-\theta^{k}\right)
\]

\end_inset

 with respect to 
\begin_inset Formula $\theta.$
\end_inset

 This is a much easier problem,
 since it is a quadratic function in 
\begin_inset Formula $\theta,$
\end_inset

 so it has linear first order conditions.
 These are
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{\theta}\tilde{s}(\theta)=g(\theta^{k})+H(\theta^{k})\left(\theta-\theta^{k}\right)
\]

\end_inset

 So the solution for the next round estimate is 
\begin_inset Formula 
\[
\theta^{k+1}=\theta^{k}-H(\theta^{k})^{-1}g(\theta^{k})
\]

\end_inset

So,
 the 
\begin_inset Formula $Q^{k}$
\end_inset

 from above is set to 
\begin_inset Formula $-H^{-1}(\theta^{k})$
\end_inset

 when we use Newton's method.
 See 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization
\end_layout

\end_inset

 for more information.
 This is illustrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Newton-iteration"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Newton-iteration"

\end_inset

Newton iteration
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/newton.png
	lyxscale 50
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
However,
 it's good to include a stepsize,
 since the approximation to 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 may be bad far away from the maximizer 
\begin_inset Formula $\hat{\theta},$
\end_inset

 so the actual iteration formula is 
\begin_inset Formula 
\[
\theta^{k+1}=\theta^{k}-a^{k}H(\theta^{k})^{-1}g(\theta^{k})
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
Quasi-Newton methods
\end_layout

\begin_layout Itemize
A potential problem is that the Hessian may not be negative definite when we're far from the maximizing point.
 So 
\begin_inset Formula $-H(\theta^{k})^{-1}$
\end_inset

 may not be positive definite,
 and 
\begin_inset Formula $-H(\theta^{k})^{-1}g(\theta^{k})$
\end_inset

 may not define an increasing direction of search.
 This can happen when the objective function has flat regions,
 in which case the Hessian matrix is very ill-conditioned (e.g.,
 is nearly singular),
 or when we're in the vicinity of a local minimum,
 
\begin_inset Formula $H(\theta^{k})$
\end_inset

 is positive definite,
 and our direction is a 
\emph on
decreasing
\emph default
 direction of search.
 Matrix inverses by computers are subject to large errors when the matrix is ill-conditioned.
 Also,
 we certainly don't want to go in the direction of a minimum when we're maximizing.
 To solve this problem,
 
\emph on
Quasi-Newton
\emph default
 methods simply add a positive definite component to 
\begin_inset Formula $H(\theta)$
\end_inset

 to ensure that the resulting matrix is positive definite,
 
\emph on
e.g.,

\emph default
 
\begin_inset Formula $Q=-H(\theta)+b\mathbf{I},$
\end_inset

 where 
\begin_inset Formula $b$
\end_inset

 is chosen large enough so that 
\begin_inset Formula $Q$
\end_inset

 is well-conditioned and positive definite.
 This has the benefit that improvement in the objective function is guaranteed.
 See 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Quasi-Newton_method
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Another variation of quasi-Newton methods is to approximate the Hessian by using successive gradient evaluations,
 which makes sense,
 because the Hessian is the derivative of the gradient,
 and we observe different gradients during the course of the iterations.
 This avoids actual calculation of the Hessian,
 which is an order of magnitude (in the dimension of the parameter vector) more costly than calculation of the gradient.
 They can be done to ensure that the approximation is positive definite.
 DFP and BFGS are two well-known examples.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
BFGS minimization:
 cut and paste the following code into julia to see some BFGS minimization of the Rosenbrock function
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

using Optim
\end_layout

\begin_layout Plain Layout

rosenbrock(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
\end_layout

\begin_layout Plain Layout

result = optimize(rosenbrock,
 zeros(2),
 BFGS(),
 Optim.Options(iterations=1000))
\end_layout

\end_inset

Try replacing 
\family typewriter
BFGS()
\family default
 with 
\family typewriter
GradientDescent()
\family default
 and see what happens.
 
\end_layout

\begin_layout Standard
Also,
 run the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/NonlinearOptimization/RosenbrockTrace.jl}{RosenbrockTrace.jl} 
\end_layout

\end_inset

 code to see how the path to find the optimizer changes between steepest descent and Newton's method.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

 
\end_layout

\begin_layout Standard

\series bold
Stopping criteria
\end_layout

\begin_layout Standard
The last thing we need is to decide when to stop.
 A digital computer is subject to limited machine precision and round-off errors.
 For these reasons,
 it is unreasonable to hope that a program can 
\series bold
exactly
\series default
 find the point that maximizes a function.
 We need to define acceptable tolerances.
 Some stopping criteria are:
\end_layout

\begin_layout Itemize
Negligible change in parameters:
 
\begin_inset Formula 
\[
|\theta_{j}^{k}-\theta_{j}^{k-1}|<\varepsilon_{1},\forall j
\]

\end_inset


\end_layout

\begin_layout Itemize
Negligible relative change:
 
\begin_inset Formula 
\[
|\frac{\theta_{j}^{k}-\theta_{j}^{k-1}}{\theta_{j}^{k-1}}|<\varepsilon_{2},\forall j
\]

\end_inset


\end_layout

\begin_layout Itemize
Negligible change of function:
 
\begin_inset Formula 
\[
|s(\theta^{k})-s(\theta^{k-1})|<\varepsilon_{3}
\]

\end_inset


\end_layout

\begin_layout Itemize
Gradient negligibly different from zero:
 
\begin_inset Formula 
\[
|g_{j}(\theta^{k})|<\varepsilon_{4},\forall j
\]

\end_inset


\end_layout

\begin_layout Itemize
Or,
 even better,
 check all of these.
 Observe that the BFGS snippet from above checks a number of criteria.
\end_layout

\begin_layout Itemize
Also,
 it's good to check that the last round (real,
 not approximate) Hessian has the appropriate definiteness.
 This can be done by computing the Hessian at the final estimates,
 and then computing its eigenvalues.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Starting values
\end_layout

\begin_layout Standard
The Newton-Raphson and related algorithms work well if the objective function is concave (when maximizing),
 but not so well if there are convex regions and local minima or multiple local maxima.
 The algorithm may converge to a local minimum or to a local maximum that is not optimal.
 The algorithm may also have difficulties converging at all.
\end_layout

\begin_layout Itemize
The usual way to 
\begin_inset Quotes eld
\end_inset

ensure
\begin_inset Quotes erd
\end_inset

 that a global maximum has been found is to use many different starting values,
 and choose the solution that returns the highest objective function value.
 
\series bold
\color red
THIS IS IMPORTANT in practice.

\series default
\color inherit
 More on this later.
\end_layout

\begin_layout Itemize
an alternative is to use a global optimization algorithm,
 e.g.,
 simulated annealing or others,
 which may or may not be gradient based.
 This may be slow,
 but is more likely to give you the correct answer,
 if your problem is difficult.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Calculating derivatives
\end_layout

\begin_layout Standard
Gradient-based methods obviously require first and possibly second derivatives.
 It is often difficult to calculate derivatives (especially the Hessian) analytically if the function 
\begin_inset Formula $s_{n}(\cdot)$
\end_inset

 is complicated.
 Fortunately,
 there are some good options:
\end_layout

\begin_layout Itemize
computing derivatives by hand:
 this works well,
 if you're able to do it,
 and you don't make mistakes.
 I don't recommend it for complex problems,
 as you are likely to make errors and/or program your results incorrectly.
\end_layout

\begin_layout Itemize
symbolic differentiation:
 Mathematica and many other similar packages can give analytic solutions,
 in many cases,
 and they can output the solution in code form,
 to be pasted in to your programs.
\end_layout

\begin_layout Itemize
automatic differentiation:
 this is now a common way to compute derivatives,
 and should probably be your default way to go.
\end_layout

\begin_layout Itemize
numeric derivatives based on finite differences were historically widely used.
 They are less accurate than analytic derivatives,
 and are usually more costly to evaluate.
 Both factors usually cause optimization programs to be less successful when numeric derivatives are used.
 However,
 numeric derivatives provide a reasonably reliable fall-back option if other methods don't work,
 for some reason.
 They can be used to check derivatives computed by hand.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Computing some derivatives.
 Here's an example of computing the exact gradient using automatic differentiation,
 and the approximate gradient using finite differences.
 You will see that the finite difference version has some error.
 If you replace 
\begin_inset Quotes sld
\end_inset

gradient
\begin_inset Quotes srd
\end_inset

 with 
\begin_inset Quotes sld
\end_inset

hessian
\begin_inset Quotes srd
\end_inset

,
 you can get the Hessian matrix.
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

using ForwardDiff,
 Calculus
\end_layout

\begin_layout Plain Layout

rosenbrock(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
\end_layout

\begin_layout Plain Layout

ForwardDiff.gradient(rosenbrock,
 2.0*ones(2))
\end_layout

\begin_layout Plain Layout

Calculus.gradient(rosenbrock,
 2.0*ones(2)) 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Scaling your data
\end_layout

\begin_layout Itemize
Gradient based optimization methods are much more likely to be successful and give accurate results if the data are scaled so that the elements of the gradient are of the same order of magnitude.
 Example:
 if the model is 
\begin_inset Formula $y_{t}=h(\alpha x_{t}+\beta z_{t})+\varepsilon_{t},$
\end_inset

 and estimation is by NLS.
 Let 
\begin_inset Formula $g()$
\end_inset

 be the derivative of 
\begin_inset Formula $h().$
\end_inset


\begin_inset Formula 
\begin{align*}
s_{n}(\theta)= & \frac{1}{n}\sum_{t}\left(y_{t}-h(\alpha x_{t}+\beta z_{t})\right)^{2}\\
D_{\alpha}s_{n}(\cdot)= & \frac{-1}{n}\sum_{t}{\color{teal}{\color{blue}2\left(y_{t}-h(\alpha x_{t}+\beta z_{t})\right)g(\alpha x_{t}+\beta z_{t})}}x_{t}\\
D_{\beta}s_{n}(\cdot)= & \frac{-1}{n}\sum_{t}2\left(y_{t}-h(\alpha x_{t}+\beta z_{t})\right)g(\alpha x_{t}+\beta z_{t})z_{t}
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
suppose that 
\begin_inset Formula $D_{\alpha}s_{n}(\cdot)=1000$
\end_inset

 and 
\begin_inset Formula $D_{\beta}s_{n}(\cdot)=0.001.$
\end_inset

 One could define 
\begin_inset Formula $\alpha^{\ast}=1000\alpha;$
\end_inset

 
\begin_inset Formula $x_{t}^{\ast}=x_{t}/1000$
\end_inset

;
\begin_inset Formula $\beta^{\ast}=\beta/1000;z_{t}^{\ast}=1000z_{t}.$
\end_inset

 
\end_layout

\begin_layout Itemize
then 
\begin_inset Formula 
\begin{align*}
D_{\alpha^{*}}s_{n}(\cdot) & =\frac{-1}{n}\sum_{t}2\left(y_{t}-h(\alpha^{*}x_{t}^{*}+\beta^{*}z_{t}^{*})\right)g(\alpha^{*}x_{t}^{*}+\beta^{*}z_{t}^{*})x_{t}^{*}\\
 & =\frac{-1}{n}\sum_{t}{\color{blue}2\left(y_{t}-h(\alpha x_{t}+\beta z_{t})\right)g(\alpha x_{t}+\beta z_{t})}{\color{red}x_{t}^{*}}
\end{align*}

\end_inset

Everything is the same as before,
 because the 1000s cancel out (compare the blue highlighted parts),
 except there is an 
\begin_inset Formula $x_{t}^{*}$
\end_inset

 at the end,
 instead of 
\begin_inset Formula $x_{t}$
\end_inset

,
 which causes the derivative to be 1 now,
 where it was 1000 before.
\end_layout

\begin_layout Itemize
the same is true for the other derivative,
 it will be 1.
\end_layout

\begin_layout Itemize
this scaling causes the derivatives to be of the same order.
\end_layout

\begin_layout Standard
In general,
 estimation programs always work better if data is scaled in this way,
 regardless of what type of derivatives you are using (analytic,
 automatic,
 finite difference) since roundoff errors are less likely to become important.
 
\emph on
This is important in practice.
 
\emph default
It is easy to loose precision in calculation if you don't take care.
 In the future,
 if you start to do empirical work and get results that seem meaningless or crazy,
 try to remember this point.

\color red
 This is really a very common stumbling block,
 and many people have wasted a lot of time due to forgetting about it.
\color inherit

\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Example
Here are some Hessian computations,
 using the same Rosenbrock function,
 but with 
\begin_inset Formula $x_{1}$
\end_inset

 scaled differently from the previous example .
\end_layout

\begin_layout Example
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

using ForwardDiff,
 Calculus
\end_layout

\begin_layout Plain Layout

rosenbrock(x) =  (1.0 - 1000x[1])^2 + 100.0 * (x[2] - (1000x[1])^2)^2
\end_layout

\begin_layout Plain Layout

ForwardDiff.hessian(rosenbrock,
 2.0*ones(2))
\end_layout

\begin_layout Plain Layout

Calculus.hessian(rosenbrock,
 2.0*ones(2)) 
\end_layout

\end_inset


\end_layout

\begin_layout Example
Look at the (2,2) element:
 the finite difference version is off by 6 orders of magnitude!
 The off diagonal elements are not too good,
 either.
 If this were part of a larger program,
 the results that build on this would be meaningless garbage!
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Global methods
\end_layout

\begin_layout Standard
Global optimization methods seek to find the global optimum,
 over a defined parameter space.
 They can (hopefully) find an optimum in the presence of nonconcavities,
 discontinuities and multiple local minima/maxima.
 There are methods backup up by theory that gives conditions under which there is a guarantee of finding the global optimum (perhaps using an infeasible amount of time),
 and there are so-called heuristic methods,
 which may perform well,
 but which may offer no guarantees of working.
 A well known and popular heuristic method is the Nelder-Mead (simplex) method.
\end_layout

\begin_layout Standard
Simulated annealing (SA) is an example of an algorithm which has theoretical support.
 There are many others,
 so we'll just focus on this one to get the basic idea.
 Essentially,
 the SA algorithm 
\end_layout

\begin_layout Itemize
randomly selects evaluation points within pre-established limits
\end_layout

\begin_layout Itemize
accepts all points that yield an increase in the objective function,
 but also accepts some points that decrease the objective function.
 This allows the algorithm to escape from local minima.
\end_layout

\begin_layout Itemize
As more and more points are tried,
 periodically the algorithm focuses on the best point so far,
 and reduces the range over which random points are generated.
 Also,
 the probability that a negative move is accepted reduces.
 
\end_layout

\begin_layout Itemize
The algorithm relies on many evaluations,
 as in the search method,
 but focuses in on promising areas,
 which reduces function evaluations with respect to the search method.
 It does not require derivatives to be evaluated.
 
\end_layout

\begin_layout Itemize
The theoretical support for SA requires that the region being search contracts very slowly,
 so slowly that it becomes impractical for most problems.
 In practice,
 this is usually done more quickly than what theory asks for,
 so the method as used is actually heuristic.
 One can try to check that it is working by running it more than once,
 to see if the same solution is found.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An example is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/NonlinearOptimization/OLSviaSA.jl}{OLSviaSA.jl} 
\end_layout

\end_inset

,
 which uses SA to compute the OLS estimator,
 and plots the trace of the improvements as they are found.
 You can see how SA narrows in on the solution,
 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Trace-of-SA"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Trace-of-SA"

\end_inset

Trace of SA path to minimize sum of squared errors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/OLSviaSA.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-Nerlove-model"

\end_inset

The Nerlove model via numeric minimization
\end_layout

\begin_layout Standard
The Nerlove model we studied previously (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "simple nerlove model"
nolink "false"

\end_inset

),
 when estimated by OLS,
 gives results that do not satisfy homogeneity of degree one,
 and one of the estimated cost shares is negative.
 Theory says those things should not happen.
 Let's impose the restrictions (note,
 this is not a case of simple linear restrictions,
 because the restriction that shares are in the 
\begin_inset Formula $[0,1]$
\end_inset

 interval is not a linear equality restriction).
 Here's the code:
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/NonlinearOptimization/EstimateRestrictedNerlove.jl}{EstimateRestrictedNerlove.jl} 
\end_layout

\end_inset

 which shows how to use unconstrained and constrained minimization to estimate the simple Nerlove model with and without parameter restrictions.
 You should use the objective function values to compute the 
\begin_inset Formula $qF$
\end_inset

 test.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Multiple optima
\end_layout

\begin_layout Standard
Multiple optima (one global,
 others local) can complicate life,
 since we have limited means of determining if there is a higher maximum than the one we're at.
 Think of climbing a mountain in an unknown range,
 in a very foggy place.
 A nice picture is Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "foggy mountain-1"
nolink "false"

\end_inset

,
 panel (a),
 but try to imagine the scene if the clouds were 2000m thicker.
 A mathematical representation of a similar problem is in the second panel.
 You can go up until there's nowhere else to go up,
 but since you're in the fog you don't know if the true summit is across the gap that's at your feet.
 Do you claim victory (convergence) and go home,
 or do you trudge down the gap and explore the other side?
 (example inspired by H.W.
 Tilman 
\emph on
Snow on the Equator).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "foggy mountain-1"

\end_inset

Multiple local maxima:
 Mountains with low fog
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/mountain.jpg
	width 4in

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/FoggySurface.png
	width 4in

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/FMresults.png
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The best way to avoid stopping at a local maximum is to use many starting values,
 for example on a grid,
 or randomly generated.
 Or perhaps one might have priors about possible values for the parameters (
\emph on
e.g.,

\emph default
 from previous studies of similar data).
\end_layout

\begin_layout Standard
Let's try to find the true minimizer of minus 1 times the foggy mountain function (since the algorithms are set up to minimize).
 From the picture,
 you can see it's close to 
\begin_inset Formula $(0.1,0.1)$
\end_inset

,
 but let's pretend there is fog,
 and that we don't know that.
 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/NonlinearOptimization/FoggyMountain.jl}{FoggyMountain.jl}
\end_layout

\end_inset

 shows that poor start values can lead to problems.
 It uses SA,
 which finds the true global minimum,
 and it shows that BFGS using a battery of random start values can also find the global minimum help.
 The output of one run is in panel (c) of the figure:
 In that run,
 the single BFGS run with bad start values converged to a point far from the true minimizer,
 while simulated annealing found the true minimizer.
 BFGS using multiple start values also gets the correct solution,
 and if you check,
 you'll find that it's faster than SA.
 The moral of the story is to be cautious and don't publish your results too quickly.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical Summary
\end_layout

\begin_layout Standard
The practical summary for the Chapter is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./PracticalSummaries/12-Optimization.jl}{here}
\end_layout

\end_inset

.
 A video that works through the summary 
\begin_inset CommandInset href
LatexCommand href
name "is here"
target "https://www.youtube.com/watch?v=u7XQOusjZ3c"
literal "false"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Numerically minimize the function 
\begin_inset Formula $\sin(x)+0.01\left(x-a\right)^{2}$
\end_inset

,
 setting 
\begin_inset Formula $a=0$
\end_inset

,
 using the software of your choice.
 Plot the function over the interval 
\begin_inset Formula $\left(-2\pi,2\pi\right)$
\end_inset

.
 Does the software find the global minimum?
 Does this depend on the starting value you use?
 Outline a strategy that would allow you to find the minimum reliably,
 when 
\begin_inset Formula $a$
\end_inset

 can take on any given value in the interval 
\begin_inset Formula $\left(-\pi,\pi\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Numerically compute the OLS estimator of the Nerlove model 
\begin_inset Formula 
\[
\ln C=\beta+\beta_{Q}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon
\]

\end_inset

by using the fminunc function in the 
\family typewriter
Econometrics
\family default
 package to minimize the sum of squared residuals.
 See Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-model"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 for a really good hint.
 The data is at the link 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/nerlove.data}{nerlove.data} 
\end_layout

\end_inset

 .
 Verify that the results coincide with those given in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"
nolink "false"

\end_inset

,
 or with what you get from GRETL,
 i.e.:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

%%% the following needs the amsmath LaTeX package
\end_layout

\begin_layout Plain Layout


\backslash
begin{gather} 
\backslash
begin{split} 
\backslash
widehat{
\backslash
rm l
\backslash
_cost} &= -
\backslash
underset{(1.7744)}{3.52650} +
\backslash
underset{(0.017466)}{0.720394}
\backslash
,
\backslash
mbox{l
\backslash
_output} +
\backslash
underset{(0.29105)}{0.436341}
\backslash
,
\backslash
mbox{l
\backslash
_labor} +
\backslash
underset{(0.10037)}{0.426517}
\backslash
,
\backslash
mbox{l
\backslash
_fuel}
\backslash

\backslash
 & -
\backslash
underset{(0.33943)}{0.219888}
\backslash
,
\backslash
mbox{l
\backslash
_capital} 
\backslash
end{split} 
\backslash
notag 
\backslash

\backslash
 T = 145 
\backslash
quad 
\backslash
bar{R}^2 = 0.9238 
\backslash
quad F(4,140) = 437.69 
\backslash
quad 
\backslash
hat{
\backslash
sigma} = 0.39236 
\backslash
notag 
\backslash

\backslash
 
\backslash
centerline{(standard errors in parentheses)} 
\backslash
notag 
\backslash
end{gather} 
\end_layout

\end_inset

The important part of this problem is to learn how to minimize a function that depends on both parameters and data.
 Try to write your function so that it is re-usable,
 with a different data set.
 
\end_layout

\begin_layout Enumerate
Take the code from the previous problem,
 and modify it to estimate the model 
\begin_inset Formula 
\[
\ln W=\beta_{0}+\beta_{EDUC}EDUC+\beta_{X}EXP+\beta_{EXP^{2}}\frac{EXP^{2}}{100}+\beta_{BLACK}BLACK+\beta_{SMSA}SMSA+\beta_{SOUTH}SOUTH+\epsilon
\]

\end_inset

 using the Card data,
 which was presented in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Exploratory-analysis-and"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Enumerate
Suppose we have an 
\begin_inset Formula $AR(1)$
\end_inset

 model 
\begin_inset Formula $y_{t}=\rho y_{t-1}+u_{t}$
\end_inset

.
 Suppose that 
\begin_inset Formula $y_{t}$
\end_inset

 is stationary,
 and that the error 
\begin_inset Formula $u_{t}$
\end_inset

 is white noise.
 Explain how one could compute an estimator of 
\begin_inset Formula $\rho$
\end_inset

 using the grid search method.
 You must define your criterion function and explain how to implement the grid search.
 
\end_layout

\begin_layout Enumerate
Consider the data generating process 
\begin_inset Formula $y_{t}=\beta_{1}+\beta_{2}x_{t}+u_{t}$
\end_inset

,
 where 
\begin_inset Formula $x_{t}\sim U(0,5)$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}\sim N(0,1)$
\end_inset

.
 Suppose we have 
\begin_inset Formula $n$
\end_inset

 observations on 
\begin_inset Formula $\left(y_{t},x_{t}\right).$
\end_inset

 Suppose that the data satisfies the assumptions of the classical linear regression model.
 Suppose that we know that 
\begin_inset Formula $\beta_{1}=1$
\end_inset

 and that 
\begin_inset Formula $0<\beta_{2}<5$
\end_inset

.
 Generate 
\begin_inset Formula $n=100$
\end_inset

 observations from this model,
 and compute an estimator of 
\begin_inset Formula $\beta_{2}$
\end_inset

 
\emph on
using the grid search method
\emph default
.
 Your answer should include the program that you wrote,
 and the estimate of 
\begin_inset Formula $\beta_{2}.$
\end_inset


\end_layout

\begin_layout Enumerate
In Julia (with the 
\family typewriter
Econometrics
\family default
 package installed),
 type 
\family typewriter
fminunc()
\family default
,
 to learn a bit more about the 
\family typewriter
fminunc
\family default
 function for unconstrained minimization,
 and to see a simple example.
 This is a convenience function,
 for Matlab users,
 and uses the Julia Optim.jl package in the background.
 If you get into Julia,
 it's better to use Optim.jl directly.
\end_layout

\begin_layout Enumerate
In Julia (with the 
\family typewriter
Econometrics
\family default
 package installed),
 type 
\family typewriter
fmincon()
\family default
,
 to learn a bit more about the 
\family typewriter
fmincon
\family default
 function for unconstrained minimization,
 and to see a simple example.
 This is a convenience function,
 for Matlab users,
 and uses the Julia NLopt.jl package in the background.
 If you get into Julia,
 it's better to use NLopt.jl directly.
\end_layout

\begin_layout Enumerate
In Julia (with the 
\family typewriter
Econometrics
\family default
 package installed),
 type 
\family typewriter
samin()
\family default
,
 to learn a bit more about the 
\family typewriter
samin
\family default
 function for minimization by simulated annealing,
 and to see a simple example.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Asymptotic-properties-of"

\end_inset

Asymptotic properties of extremum estimators
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
:
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Ch.
 5;
 Hayashi (2000),
 Ch.
 7;
 Gourieroux and Monfort (1995),
 Vol.
 2,
 Ch.
 24;
 Amemiya,
 Ch.
 4 section 4.1;
 Davidson and MacKinnon,
 pp.
 591-96;
 Gallant,
 Ch.
 3;
 
\begin_inset CommandInset citation
LatexCommand cite
key "NeweyMcfadden"
literal "true"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Extremum estimators
\end_layout

\begin_layout Standard
We'll begin with study of 
\emph on
extremum estimators
\emph default
 in general.
 
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\mathbf{Z}_{n}=\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

 be the available data,
 based on a sample of size 
\begin_inset Formula $n$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Suppose there are 
\begin_inset Formula $p$
\end_inset

 variables,
 so each 
\begin_inset Formula $z_{i},\,\text{\ensuremath{i=1,2,...,n},}$
\end_inset

 is a 
\begin_inset Formula $p$
\end_inset

-vector.
 
\end_layout

\begin_deeper
\begin_layout Itemize
so,
 we have 
\begin_inset Formula $n$
\end_inset

 observations on 
\begin_inset Formula $p$
\end_inset

 variables.
\end_layout

\begin_layout Itemize
\begin_inset Formula $n$
\end_inset

 could be a number of economic agents in a cross sectional sample,
 or the number of time periods in a time series sample.
\end_layout

\begin_layout Itemize
it's probably most natural to organize the data in a 
\begin_inset Formula $n\times p$
\end_inset

 array,
 like what you would have in a spreadsheet.
 That is not important here,
 but it may help to fix the ideas.
\end_layout

\end_deeper
\begin_layout Itemize
Our paradigm is that data are generated as a draw from a joint density 
\begin_inset Formula $f_{Z_{n}}(z)$
\end_inset

.
 This density may not be known,
 but it exists in principle.
 It is multivariate,
 with high dimension.
\end_layout

\begin_layout Itemize
The draw from the density may be thought of as the outcome of a random experiment that is characterized by the probability space 
\begin_inset Formula $\left\{ \Omega,\mathcal{F},P\right\} $
\end_inset

.
 When the experiment is performed,
 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 is the result,
 and 
\begin_inset Formula $\mathbf{Z}_{n}(\omega)=\left\{ Z_{1}(\omega),Z_{2}(\omega),...,Z_{n}(\omega)\right\} =\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

 is the realized data.
 
\end_layout

\begin_layout Itemize
The probability space is rich enough to allow us to consider events defined in terms of an infinite sequence of data 
\begin_inset Formula $\mathbf{Z}=\left\{ z_{1},z_{2},...,\right\} $
\end_inset

,
 so,
 we can consider what happens as the sample size becomes arbitrarily large.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
[Extremum estimator] 
\begin_inset CommandInset label
LatexCommand label
name "Extremum estimator"

\end_inset

An extremum estimator
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
extremum estimator
\end_layout

\end_inset

 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the optimizing element of an objective function 
\begin_inset Formula $s_{n}(\mathbf{Z}_{n},\theta)$
\end_inset

 over a compact set 
\begin_inset Formula $\overline{\Theta}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
OLS.
 Let the d.g.p.
 be 
\begin_inset Formula $y_{t}=\mathbf{x}_{t}^{\prime}\theta_{0}+\varepsilon_{t},\,t=1,2,...,n,\,\theta_{0}\in\Theta.$
\end_inset

 Stacking observations vertically,
 
\begin_inset Formula $\mathbf{y}_{n}=\mathbf{X}_{n}\theta_{0}+\varepsilon_{n},$
\end_inset

 where 
\begin_inset Formula $\mathbf{X}_{n}=\left(\begin{array}{llll}
\boldsymbol{x}_{1} & \boldsymbol{x}_{2} & \cdots & \boldsymbol{x}_{n}\end{array}\right)^{\prime}.$
\end_inset

 Let 
\begin_inset Formula $\mathbf{Z}_{n}=[\mathbf{y}_{n}\,\mathbf{X}_{n}]$
\end_inset

.
 The least squares estimator is defined as 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}s_{n}(\mathbf{Z}_{n},\theta)
\]

\end_inset

where 
\begin_inset Formula 
\[
s_{n}(\mathbf{Z}_{n},\theta)=1/n\sum_{t=1}^{n}\left(y_{t}-\boldsymbol{x}_{t}^{\prime}\theta\right)^{2}
\]

\end_inset

 As you already know,
 
\begin_inset Formula $\hat{\theta}=(\mathbf{X}^{\prime}\mathbf{X})^{-1}\mathbf{X}^{\prime}\mathbf{y},$
\end_inset

 as this is a case where we can solve the f.o.c.
 analytically.
\end_layout

\begin_layout Itemize
if you have some data,
 you can use this formula to get the OLS estimates,
 which are just numbers.
\end_layout

\begin_layout Itemize
if you want to study the properties of the OLS estimator,
 you're really thinking about 
\begin_inset Formula $\hat{\theta}(\omega)=(\mathbf{X(\omega)}^{\prime}\mathbf{X(\omega)})^{-1}\mathbf{X(\omega)}^{\prime}\mathbf{y(\omega)}$
\end_inset

,
 and how it behaves for 
\begin_inset Formula $\omega\in\Omega$
\end_inset

,
 even though we wouldn't usually write it that way.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The contours of the OLS objective function are plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:OLS-objective-function"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

,
 based on the Julia script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{./Examples/NonlinearOptimization/OLScontours.jl}{OLScontours.jl} 
\end_layout

\end_inset

.
 This illustrates the idea that an extremum estimator minimizes or maximizes a function,
 and that the estimate and parameters that we are trying to estimate are distinct points.
 As the sample gets large,
 the two points will get close together,
 as we will see,
 if the estimator is 
\emph on
consistent
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:OLS-objective-function"

\end_inset

OLS objective function contours
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/OLScontours.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:Maximum-likelihood.-Suppose"

\end_inset

Maximum likelihood.
 Suppose that the continuous random variables 
\begin_inset Formula $Y_{t}\sim IIN(\mu^{0},\sigma_{0}^{2}),\,t=1,2,...,n$
\end_inset

.
 Suppose we have a sample 
\begin_inset Formula $\{y_{1},y_{2},...,y_{n}\}$
\end_inset

.
 The density at the realization 
\begin_inset Formula $y_{t}$
\end_inset

 is
\begin_inset Formula 
\[
f_{Y}(y_{t};\theta)=\left(2\pi\right)^{-1/2}(1/\sigma)\exp\left(-\frac{1}{2}\left(\frac{y_{t}-\mu}{\sigma}\right)^{2}\right).
\]

\end_inset

where 
\begin_inset Formula $\theta=(\mu,\sigma$
\end_inset

).
 The maximum likelihood estimator maximizes the joint density of the sample.
 Because the data are i.i.d.,
 the joint density of the sample 
\begin_inset Formula $\left\{ y_{1},y_{2},...,y_{n}\right\} $
\end_inset

 is the product of the densities of each observation,
 and the ML estimator is 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\max_{\Theta}\,\,\mathcal{L}_{n}(\theta)=\prod_{t=1}^{n}f_{Y}(y_{t};\theta)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Because the natural logarithm is strictly increasing on 
\begin_inset Formula $(0,\infty)$
\end_inset

,
 maximization of the average logarithmic likelihood function is achieved at the same 
\begin_inset Formula $\hat{\theta}$
\end_inset

 as for the likelihood function.
 So,
 the ML estimator 
\begin_inset Formula $\hat{\theta}=\arg\max_{\Theta}s_{n}(\theta)$
\end_inset

 where 
\begin_inset Formula 
\begin{align*}
s_{n}(\theta) & =\left(1/n\right)\ln\mathcal{L}_{n}(\theta)=\left(1/n\right)\sum_{t=1}^{n}\ln f_{Y}(y_{t};\theta)\\
 & =-\ln\sqrt{2\pi}-\text{\log}\sigma-\left(1/n\right)\sum_{t=1}^{n}\left(\frac{y_{t}-\mu}{\sigma}\right)^{2}
\end{align*}

\end_inset

Solution of the f.o.c.
 leads to the familiar result that 
\begin_inset Formula $\hat{\mu}=\bar{\mathbf{y}}.$
\end_inset

 We'll come back to this in more detail later.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Bayesian estimator
\end_layout

\begin_layout Example
(reminder to self in lectures:
 that squiggle is a 
\begin_inset Quotes sld
\end_inset

zeta
\begin_inset Quotes srd
\end_inset

) Bayesian point estimators such as the posterior mode,
 median or mean can be expressed as extremum estimators.
 For example,
 the posterior mean 
\begin_inset Formula $E(\theta|Z_{n})$
\end_inset

 is the minimizer (with respect to 
\begin_inset Formula $\zeta$
\end_inset

) of the function
\begin_inset Formula 
\[
s_{n}(\zeta)=\int_{\Theta}\left(\theta-\zeta\right)^{2}f(Z_{n};\theta)\pi(\theta)/f(Z_{n})d\theta
\]

\end_inset

where 
\begin_inset Formula $f(Z_{n};\theta)$
\end_inset

 is the likelihood function,
 
\begin_inset Formula $\pi(\theta)$
\end_inset

 is a prior density,
 and 
\begin_inset Formula $f(Z_{n})$
\end_inset

 is the marginal likelihood of the data.
 These concepts are explained later,
 for now the point is that Bayesian point estimators can be thought of as extremum estimators,
 and the theory for extremum estimators will apply.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Note that the objective function 
\begin_inset Formula $s_{n}(\mathbf{Z}_{n},\theta)$
\end_inset

 is a random function,
 because it depends on 
\begin_inset Formula $\mathbf{Z}_{n}(\omega)=\left\{ Z_{1}(\omega),Z_{2}(\omega),...,Z_{n}(\omega)\right\} =\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

.
 
\end_layout

\begin_layout Itemize
We need to consider what happens as different outcomes 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 occur.
 These different outcomes lead to different data being generated,
 and the different data causes the objective function to change.
 
\end_layout

\begin_layout Itemize
Note,
 however,
 that for a fixed 
\begin_inset Formula $\omega\in\Omega$
\end_inset

,
 the data 
\begin_inset Formula $\mathbf{Z}_{n}(\omega)=\left\{ Z_{1}(\omega),Z_{2}(\omega),...,Z_{n}(\omega)\right\} =\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

 are a fixed realization,
 and the objective function 
\begin_inset Formula $s_{n}(\mathbf{Z}_{n},\theta)$
\end_inset

 becomes a non-random function of 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
When actually 
\emph on
computing
\emph default
 an extremum estimator,
 we condition on the observed data,
 and treat it as fixed.
 Then we compute estimators either by solving the f.o.c.,
 as in the case of OLS,
 or if that is not possible,
 by employing algorithms for optimization of nonstochastic functions.
 
\emph on
How
\emph default
 to do this is the topic of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "cha:Numeric-optimization-methods"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Itemize
When 
\emph on
analyzing the properties
\emph default
 of an extremum estimator and deciding 
\emph on
why 
\emph default
to use a particular extremum estimator,
 we need to investigate what happens throughout 
\begin_inset Formula $\Omega$
\end_inset

:
 we do not focus only on the 
\begin_inset Formula $\omega$
\end_inset

 that generated the observed data.
 This is because we would like to find estimators that work well on average,
 for any data set that can result from 
\begin_inset Formula $\omega\in\Omega$
\end_inset

.
 This is the topic of the remainder of the present Chapter.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
We'll often write the objective function suppressing the dependence on 
\begin_inset Formula $\mathbf{Z}_{n},$
\end_inset

 as 
\begin_inset Formula $s_{n}(\omega,\theta)$
\end_inset

 or simply 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

,
 depending on context.
 The first of these emphasizes the fact that the objective function is random,
 and the second is more compact.
 However,
 the data is still in there,
 and because the data is randomly sampled,
 the objective function is random,
 too.
 
\end_layout

\begin_layout Itemize
For lectures,
 
\color red
jump to Ch.
 12 now
\color inherit
,
 and then come back here.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Existence
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is continuous in 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 is compact,
 then a maximizers exists,
 by the Weierstrass maximum theorem (Debreu,
 1959).
\end_layout

\begin_layout Itemize
In some cases of interest,
 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 may not be continuous.
 Nevertheless,
 it may still converge to a continuous function,
 in which case existence will not be a problem,
 at least asymptotically.
 Henceforth in this course,
 we assume that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is continuous,
 unless it is stated otherwise.
\end_layout

\begin_layout Itemize
Compactness is not much of a limitation,
 in most cases.
 Compactness means closed and bounded.
 It doesn't mean we need to know the bounds,
 it only means that they must exist.
 So,
 we will just assume that it holds.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Consistency
\end_layout

\begin_layout Standard
The following theorem is patterned on a proof in 
\begin_inset CommandInset citation
LatexCommand cite
key "gallant_1987"
literal "true"

\end_inset

.
 It is interesting to compare the following proof with 
\begin_inset CommandInset citation
LatexCommand citet
key "amemiya1985advanced"
literal "false"

\end_inset

 Theorem 4.1.1,
 which is done in terms of convergence in probability.
\end_layout

\begin_layout Theorem

\emph on
[Consistency of e.e.]
\emph default
 
\begin_inset CommandInset label
LatexCommand label
name "Consistency of ee"

\end_inset

Suppose that 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is obtained by maximizing 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 over 
\begin_inset Formula $\overline{\Theta}.$
\end_inset


\end_layout

\begin_layout Theorem
Assume
\end_layout

\begin_layout Theorem

\shape italic
(a) Compactness:

\shape default
 The parameter space 
\begin_inset Formula $\Theta$
\end_inset

 is an open bounded subset of Euclidean space 
\begin_inset Formula $\mathbb{R}^{K}.$
\end_inset

 So,
 the closure of 
\begin_inset Formula $\Theta,$
\end_inset

 
\begin_inset Formula $\overline{\Theta}$
\end_inset

,
 is compact.
\end_layout

\begin_layout Theorem

\shape italic
(b) Uniform Convergence:

\shape default
 There is a nonstochastic function 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 that is continuous in 
\begin_inset Formula $\theta$
\end_inset

 on 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{\theta\in\overline{\Theta}}|s_{n}(\omega,\theta)-s_{\infty}(\theta)|=0,\,\text{a.s.}
\]

\end_inset


\end_layout

\begin_layout Theorem

\shape italic
(c) Identification:

\shape default
 
\begin_inset Formula $s_{\infty}(\cdot)$
\end_inset

 has a unique global maximum over 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 at 
\begin_inset Formula $\theta_{0}\in\Theta,$
\end_inset

 
\shape italic
i.e.,

\shape default
 
\begin_inset Formula $s_{\infty}(\theta_{0})>s_{\infty}(\theta),$
\end_inset

 
\begin_inset Formula $\forall\theta\neq\theta_{0},\theta\in\overline{\Theta}$
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset VSpace defskip
\end_inset

Then 
\begin_inset Formula $\hat{\theta}_{n}\stackrel{a.s.}{\rightarrow}\theta_{0}.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Comment:
 
\series default
the ''pseudo-true
\begin_inset Quotes srd
\end_inset

 value 
\begin_inset Formula $\theta_{0}$
\end_inset

 lies in the open set 
\begin_inset Formula $\Theta,$
\end_inset

 and therefore is an interior point of the compact set 
\begin_inset Formula $\overline{\Theta}.$
\end_inset

 We will refer to 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 as the parameter set or parameter space.
\end_layout

\begin_layout Standard

\series bold
Proof:

\series default
 
\end_layout

\begin_layout Itemize
Select a 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 and hold it fixed.
 Then 
\begin_inset Formula $\left\{ s_{n}(\omega,\theta)\right\} $
\end_inset

 is a fixed sequence of functions.
 Suppose that 
\begin_inset Formula $\omega$
\end_inset

 is such that 
\begin_inset Formula $s_{n}(\omega,\theta)$
\end_inset

 converges to 
\begin_inset Formula $s_{\infty}(\theta).$
\end_inset

 
\end_layout

\begin_layout Itemize
The sequence 
\begin_inset Formula $\{\hat{\theta}_{n}\}$
\end_inset

 lies in the compact set 
\begin_inset Formula $\overline{\Theta},$
\end_inset

 by assumption (a) and the fact that maximization is over 
\begin_inset Formula $\overline{\Theta}$
\end_inset

.
 Since every sequence from a compact set has at least one limit point (Bolzano-Weierstrass),
 say that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a limit point of 
\begin_inset Formula $\{\hat{\theta}_{n}\}.$
\end_inset

 As such,
 there is a subsequence 
\begin_inset Formula $\{\hat{\theta}_{n_{m}}\}$
\end_inset

 (
\begin_inset Formula $\{n_{m}\}$
\end_inset

 is simply an increasing sequence of integers) with 
\begin_inset Formula $\lim_{m\rightarrow\infty}\hat{\theta}_{n_{m}}=\hat{\theta}$
\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
By uniform convergence and continuity of the limiting objective function,
 
\begin_inset Formula 
\[
{\color{purple}\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{n_{m}})}={\color{red}s_{\infty}(\hat{\theta})}.
\]

\end_inset

 To see this,
 first of all,
 select an element 
\begin_inset Formula $\hat{\theta}_{t}$
\end_inset

 from the sequence 
\begin_inset Formula $\left\{ \hat{\theta}_{n_{m}}\right\} .$
\end_inset

 Then uniform convergence (assn.
 b) implies 
\begin_inset Formula 
\[
\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{t})=s_{\infty}(\hat{\theta}_{t})
\]

\end_inset

Continuity of 
\begin_inset Formula $s_{\infty}\left(\cdot\right)$
\end_inset

 implies that 
\begin_inset Formula 
\[
\lim_{t\rightarrow\infty}s_{\infty}(\hat{\theta}_{t})=s_{\infty}(\hat{\theta})
\]

\end_inset

 since the limit as 
\begin_inset Formula $t\rightarrow\infty$
\end_inset

 of 
\begin_inset Formula $\left\{ \hat{\theta}_{t}\right\} $
\end_inset

 is 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 So the above claim is true.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Next,
 by maximization 
\begin_inset Formula 
\[
s_{n_{m}}(\hat{\theta}_{n_{m}})\geq s_{n_{m}}(\theta_{0})
\]

\end_inset

 which holds in the limit,
 so
\begin_inset Formula 
\[
{\color{green}{\color{purple}\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{n_{m}})}}\geq{\color{blue}\lim_{m\rightarrow\infty}s_{n_{m}}(\theta_{0})}.
\]

\end_inset

 
\end_layout

\begin_layout Itemize
However,
 for the left hand side,
 the previous slide showed that
\begin_inset Formula 
\[
{\color{purple}\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{n_{m}})}={\color{red}s_{\infty}(\hat{\theta})},
\]

\end_inset

 For the right hand side,
 a similar argument gives
\begin_inset Formula 
\[
{\color{blue}\lim_{m\rightarrow\infty}s_{n_{m}}(\theta_{0})}={\color{magenta}s_{\infty}(\theta_{0})}
\]

\end_inset

 by uniform convergence,
 so the above inequality gives us,
 in the limit:
\begin_inset Formula 
\[
{\color{red}s_{\infty}(\hat{\theta})}\geq{\color{magenta}s_{\infty}(\theta_{0})}.
\]

\end_inset

 
\end_layout

\begin_layout Itemize
But by assumption (c),
 there is a unique global maximum of 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 at 
\begin_inset Formula $\theta_{0},$
\end_inset

 so we must have 
\begin_inset Formula $s_{\infty}(\hat{\theta})=s_{\infty}(\theta_{0}),$
\end_inset

 and,
 therefore,
 
\begin_inset Formula $\hat{\theta}=\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Finally,
 so far we have held 
\begin_inset Formula $\omega$
\end_inset

 fixed,
 but now we need to consider all 
\begin_inset Formula $\omega\in\Omega$
\end_inset

.
 All of the above limits hold almost surely,
 by assumption (b).
  Therefore 
\begin_inset Formula $\{\hat{\theta}_{n}\}$
\end_inset

 has only one limit point,
 
\begin_inset Formula $\theta_{0},$
\end_inset

 except on a set 
\begin_inset Formula $C\subset\Omega$
\end_inset

 with 
\begin_inset Formula $P(C)=0.$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
Discussion of the proof:
\end_layout

\begin_layout Itemize
We assume that 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is in fact a global maximum of 
\begin_inset Formula $s_{n}\left(\theta\right).$
\end_inset

 It is not required to be unique for 
\begin_inset Formula $n$
\end_inset

 finite,
 though the identification assumption requires that the limiting objective function have a unique maximizing argument.
 The previous section on numeric optimization methods showed that actually finding the global maximum of 
\begin_inset Formula $s_{n}\left(\theta\right)$
\end_inset

 may be a non-trivial problem.
\end_layout

\begin_layout Itemize
See 
\begin_inset CommandInset citation
LatexCommand citet
key "amemiya1985advanced"
literal "false"

\end_inset

 Example 4.1.4 for a case where discontinuity leads to breakdown of consistency.
\end_layout

\begin_layout Itemize
uniform convergence is needed,
 so that the maximum of 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is eventually close to 
\begin_inset Formula $\theta_{0}.$
\end_inset

 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Why-uniform-convergence"
nolink "false"

\end_inset

.
 If the objective function is not converging at 
\begin_inset Formula $\theta_{*},$
\end_inset

 there's no guarantee that 
\begin_inset Formula $s_{n}(\theta_{*})$
\end_inset

 will be lower than 
\begin_inset Formula $s_{\infty}(\theta_{0})$
\end_inset

 as 
\begin_inset Formula $n$
\end_inset

 gets large.
 
\emph on

\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\emph on
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Why-uniform-convergence"

\end_inset

Why uniform convergence of 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is needed
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/UniformConvergence.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The assumption that 
\begin_inset Formula $\theta_{0}$
\end_inset

 is in the interior of 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 (part of the identification assumption) has not been used to prove consistency,
 so we could directly assume that 
\begin_inset Formula $\theta_{0}$
\end_inset

 is simply an element of a compact set 
\begin_inset Formula $\overline{\Theta}.$
\end_inset

 The reason that we assume it's in the interior here is that this is necessary for subsequent proof of asymptotic normality,
 and I'd like to maintain a minimal set of simple assumptions,
 for clarity.
 Parameters on the boundary of the parameter set cause theoretical difficulties that we will not deal with in this course.
 Just note that conventional hypothesis testing methods do not apply in this case.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Later,
 when explaining finite mixture models,
 note
\end_layout

\begin_layout Plain Layout
that the collapse of the mixture is a case of a parameter
\end_layout

\begin_layout Plain Layout
on the boundary.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $s_{n}\left(\theta\right)$
\end_inset

 is not required to be continuous,
 though 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 is.
 Arguments involving stochastic equicontinuity can often be applied to get a continuous limiting function,
 even if the small sample objective function is discontinous.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection

\series bold
Sufficient conditions for assumption
\series default
 (b)
\end_layout

\begin_layout Standard
We need a uniform strong law of large numbers in order to verify assumption (2) of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

.
 To verify the uniform convergence assumption,
 it is often feasible to employ the following set of stronger assumptions:
\end_layout

\begin_layout Itemize
the parameter space is compact,
 which is given by assumption (b)
\end_layout

\begin_layout Itemize
the objective function 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is continuous and bounded with probability one on the entire parameter space
\end_layout

\begin_layout Itemize
a standard SLLN can be shown to apply to some point 
\begin_inset Formula $\theta$
\end_inset

 in the parameter space.
 That is,
 we can show that 
\begin_inset Formula $s_{n}(\theta)\stackrel{a.s.}{\rightarrow}s_{\infty}(\theta)$
\end_inset

 for some 
\begin_inset Formula $\theta$
\end_inset

.
 Note that in most cases,
 the objective function will be an average of terms,
 such as 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}s_{t}(\theta)
\]

\end_inset

As long as the 
\begin_inset Formula $s_{t}(\theta)$
\end_inset

 are not too strongly dependent,
 and have finite variances,
 we can usually find a SLLN that will apply.
\end_layout

\begin_layout Standard
With these assumptions,
 it can be shown that assumption (b) holds.
\end_layout

\begin_layout Standard
These are reasonable conditions in many cases,
 and henceforth when dealing abstractly with estimators we'll simply assume that assumption (b) holds.
 In certain cases with discontinuous objective functions,
 we will need to be more careful.
\end_layout

\begin_layout Itemize
an example of a discontinous objective function would be when doing simulation based estimation of models with discrete dependent variables.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example:
 
\begin_inset CommandInset label
LatexCommand label
name "eeolsexample"

\end_inset

Consistency of Least Squares
\end_layout

\begin_layout Standard
Thus example shows how the above theorem can be used to show that the OLS estimator under the classical assumptions is consistent.
 Of course,
 this is not the easiest way to show that.
 The purpose is to show that the theorem gives us a result that we already know to be true.
 This may help you to believe it in the cases where we do not have external confirmation.
\end_layout

\begin_layout Standard
We suppose that data is generated by random sampling of 
\begin_inset Formula $(Y,X)$
\end_inset

,
 where 
\begin_inset Formula $y_{t}=\beta_{0}x_{t}$
\end_inset

 
\begin_inset Formula $+\varepsilon_{t}$
\end_inset

.
 
\begin_inset Formula $\left(X,\varepsilon\right)$
\end_inset

 has the distribution function 
\begin_inset Formula $F_{Z}=\mu_{x}\mu_{\varepsilon}$
\end_inset

 (
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are independent) with support 
\begin_inset Formula $\mathcal{Z}=\mathcal{X}\times\mathcal{E}.$
\end_inset

 Suppose that the variances 
\begin_inset Formula $\sigma_{X}^{2}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\varepsilon}^{2}$
\end_inset

 are finite.
 The sample objective function for a sample size 
\begin_inset Formula $n$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\theta) & = & \frac{1}{n}\sum_{t=1}^{n}\left(y_{t}-\beta x_{t}\right)^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(\beta_{0}x_{t}+\varepsilon_{t}-\beta x_{t}\right)^{2}\\
 & = & \frac{1}{n}\sum_{i=1}^{n}\left((\beta_{0}-\beta)x_{t}+\varepsilon_{t}\right)^{2}\\
 & = & {\color{red}\frac{1}{n}\sum_{t=1}^{n}\left((\beta_{0}-\beta)x_{t}\right)^{2}}+\frac{2}{n}\sum_{t=1}^{n}(\beta_{0}-\beta)x_{t}\varepsilon_{t}+{\color{blue}\frac{1}{n}\sum_{t=1}^{n}\varepsilon_{t}^{2}}
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Itemize
Considering the last term,
 this converges to the variance of the error,
 by the SLLN and the assumption of homoscedasticity and mean zero of the error:
 
\begin_inset Formula 
\[
{\color{blue}\frac{1}{n}\sum_{t=1}^{n}\varepsilon_{t}^{2}}\stackrel{a.s.}{\rightarrow}\int_{\mathcal{X}}\int_{\mathcal{E}}\varepsilon^{2}d\mu_{\mathcal{X}}d\mu_{\mathcal{E}}=\sigma_{\varepsilon}^{2}.
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Considering the second term,
 since 
\begin_inset Formula $E(\varepsilon)=0$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are independent,
 the SLLN implies that it converges to zero.
 
\end_layout

\begin_layout Itemize
Finally,
 for the first term,
 for a given 
\begin_inset Formula $\beta$
\end_inset

,
 we assume that a SLLN applies so that
\begin_inset Formula 
\begin{eqnarray}
{\color{red}\frac{1}{n}\sum_{t=1}^{n}\left((\beta_{0}-\beta)x_{t}\right)^{2}} & \stackrel{a.s.}{\rightarrow} & \int_{\mathcal{X}}\left((\beta_{0}-\beta)x\right)^{2}d\mu_{\mathcal{X}}\label{olslim}\\
 & = & \left(\beta_{0}-\beta\right)^{2}\int_{\mathcal{X}}x^{2}d\mu_{\mathcal{X}}\nonumber \\
 & = & \left(\beta_{0}-\beta\right)^{2}E\left(X^{2}\right)\nonumber 
\end{eqnarray}

\end_inset

 
\end_layout

\begin_layout Standard
Finally,
 the objective function is clearly continuous,
 and the parameter space is assumed to be compact,
 so the convergence is also uniform.
 Thus,
 
\begin_inset Formula 
\[
s_{\infty}(\beta)=\left(\beta_{0}-\beta\right)^{2}E\left(X^{2}\right)+\sigma_{\varepsilon}^{2}
\]

\end_inset

 A minimizer of this is clearly 
\begin_inset Formula $\beta=\beta_{0}.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
\begin_inset CommandInset label
LatexCommand label
name "Identification of OLS"

\end_inset

 Show that in order for the above solution to be unique it is necessary that 
\begin_inset Formula $E(X^{2})\neq0.$
\end_inset

 Interpret this condition.
\end_layout

\begin_layout Standard
This example shows that Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

 can be used to prove strong consistency of the OLS estimator.
 There are easier ways to show this,
 of course - this is only an example of application of the theorem.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
For a more concrete example,
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Consistency-of-OLS"
nolink "false"

\end_inset

 shows computations that illustrate that the OLS estimator is consistent,
 when the true relationship is 
\begin_inset Formula $y=1+1x+\epsilon,$
\end_inset

 
\begin_inset Formula $\epsilon$
\end_inset

 satisfies the classical assumptions,
 and 
\begin_inset Formula $x$
\end_inset

 is distributed uniform
\begin_inset Formula $(0,1).$
\end_inset

 The computations show that the true parameter values satisfy the first order conditions for minimization of the first term of the limiting objective function,
 eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "olslim"
nolink "false"

\end_inset

,
 above.
 You should know how to do this by hand,
 but I use software here,
 just to illustrate that it can do this sort of thing.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Consistency-of-OLS"

\end_inset

Consistency of OLS
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/OLSextremum.png
	width 15cm

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
More on the limiting objective function:
 correctly and incorrectly specified models
\end_layout

\begin_layout Standard
The limiting objective function in assumption (b) is 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

.
 What is the nature of this function and where does it come from?
\end_layout

\begin_layout Itemize
Remember our paradigm - data is presumed to be generated as a draw from 
\color red

\begin_inset Formula $f_{Z_{n}}(z)$
\end_inset


\color inherit
,
 and the objective function is 
\begin_inset Formula $s_{n}(Z_{n},\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
Usually,
 
\begin_inset Formula $s_{n}(Z_{n},\theta)$
\end_inset

 is an average of terms.
 (
\emph on
e.g.,
 
\emph default
sum of squared errors,
 or the log likelihood function of Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Maximum-likelihood.-Suppose"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

)
\end_layout

\begin_layout Itemize
The limiting objective function is found by applying a strong (weak) law of large numbers to 
\begin_inset Formula $s_{n}(Z_{n},\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
A strong (weak) LLN says that an average of terms converges almost surely (in probability) to the limit of the expectation of the average.
\end_layout

\begin_layout Standard
Supposing one holds,
\begin_inset Formula 
\[
s_{\infty}(\theta)=\lim_{n\rightarrow\infty}\mathcal{E}s_{n}(Z_{n},\theta)=\lim_{n\rightarrow\infty}\int_{\mathcal{Z}_{n}}s_{n}(z,\theta){\color{red}f_{Z_{n}}(z)}dz
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Now suppose that the density 
\begin_inset Formula $f_{Z_{n}}(z)$
\end_inset

 that characterizes the DGP is parametric:
 
\begin_inset Formula $f_{Z_{n}}(z;\rho),\,\rho\in\varrho$
\end_inset

,
 and the data is generated as a draw from this density at some true parameter vector 
\begin_inset Formula $\rho_{0}\in\varrho$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Now we have two parameters to worry about,
 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\rho$
\end_inset

.
 
\end_layout

\begin_layout Itemize
We are probably interested in learning about the true DGP,
 which means that 
\begin_inset Formula $\rho_{0}$
\end_inset

 is the item of interest.
 
\end_layout

\begin_layout Standard
When the DGP is parametric,
 the limiting objective function is 
\begin_inset Formula 
\begin{equation}
s_{\infty}(\theta)=\lim_{n\rightarrow\infty}\mathcal{E}s_{n}(Z_{n},\theta)=\lim_{n\rightarrow\infty}\int_{\mathcal{Z}_{n}}s_{n}(z,\theta){\color{red}f_{Z_{n}}(z;\rho_{0})}dz\label{eq:parametric density of data}
\end{equation}

\end_inset

and we can write the limiting objective function as 
\begin_inset Formula $s_{\infty}(\theta,\rho_{0})$
\end_inset

 to emphasize the dependence on the parameter of the DGP.
 
\end_layout

\begin_layout Itemize
From the consistency theorem,
 we know that 
\begin_inset Formula $\hat{\theta}_{n}\stackrel{a.s.}{\rightarrow}\theta_{0}$
\end_inset

 
\emph on
\color blue
What is the relationship between 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\rho_{0}$
\end_inset

?
 Does the econometric estimator tell us something about the true unknown parameter?
\end_layout

\begin_layout Itemize
\begin_inset Formula $\rho$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 may have different dimensions.
 Often,
 the statistical model (with parameter 
\begin_inset Formula $\theta)$
\end_inset

 only partially describes the DGP.
 For example,
 the case of OLS with errors of unknown distribution.
 In some cases,
 the dimension of 
\begin_inset Formula $\theta$
\end_inset

 may be greater than that of 
\begin_inset Formula $\rho.$
\end_inset

 For example,
 fitting a polynomial to an unknown nonlinear function.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
case 1:
 If knowledge of 
\begin_inset Formula $\theta_{0}$
\end_inset

 is sufficient for knowledge of 
\begin_inset Formula $\rho_{0}$
\end_inset

,
 then we have a correctly and fully specified model.
 
\begin_inset Formula $\theta_{0}$
\end_inset

 is referred to as the 
\emph on
true parameter value
\emph default
.
 Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "eeolsexample"
nolink "false"

\end_inset

 illustrates this case.
\end_layout

\begin_layout Itemize
case 2:
 If knowledge of 
\begin_inset Formula $\theta_{0}$
\end_inset

 is sufficient for knowledge of some but not all elements of 
\begin_inset Formula $\rho_{0},$
\end_inset

 we have a correctly specified 
\emph on
semiparametric
\emph default
 model.
 
\begin_inset Formula $\theta_{0}$
\end_inset

 is referred to as the 
\emph on
true parameter value
\emph default
,
 understanding that not all parameters of the DGP are estimated.
 An example would be OLS with heteroscedasticity of unknown form:
 we can learn about the parameters of the conditional mean,
 but not about the conditional variances.
\end_layout

\begin_layout Itemize
case 3:
 If knowledge of 
\begin_inset Formula $\theta_{0}$
\end_inset

 is not sufficient for knowledge of any elements of 
\begin_inset Formula $\rho_{0},$
\end_inset

 or if it causes us to draw false conclusions regarding at least some of the elements of 
\begin_inset Formula $\rho_{0},$
\end_inset

 our model is 
\emph on
misspecified
\emph default
.
 
\begin_inset Formula $\theta_{0}$
\end_inset

 is referred to as the 
\emph on
pseudo-true parameter value
\emph default
.
 The next section provides an example.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example:
 Inconsistency of Misspecified Least Squares
\end_layout

\begin_layout Standard
You already know that the OLS estimator is inconsistent when relevant variables are omitted.
 Let's verify this result in the context of extremum estimators.
 We suppose that data is generated by random sampling of 
\begin_inset Formula $(Y,X)$
\end_inset

,
 where 
\begin_inset Formula 
\[
y_{t}=\beta_{0}x_{t}+\varepsilon_{t}.
\]

\end_inset

 
\begin_inset Formula $\left(X,\varepsilon\right)$
\end_inset

 has the distribution function 
\begin_inset Formula $F_{Z}=\mu_{x}\mu_{\varepsilon}$
\end_inset

 (
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are independent) with support 
\begin_inset Formula $\mathcal{Z}=\mathcal{X}\times\mathcal{E}.$
\end_inset

 Suppose that the variances 
\begin_inset Formula $\sigma_{X}^{2}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\varepsilon}^{2}$
\end_inset

 are finite.
 However,
 the econometrician is unaware of the true DGP,
 and instead proposes the misspecified model 
\begin_inset Formula 
\[
y_{t}=\gamma_{0}w_{t}+\eta_{t}.
\]

\end_inset

 Suppose that 
\begin_inset Formula $E(W\epsilon)=0$
\end_inset

 and that 
\begin_inset Formula $E(WX)\ne0.$
\end_inset


\end_layout

\begin_layout Standard
The sample objective function for a sample size 
\begin_inset Formula $n$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\gamma) & = & 1/n\sum_{t=1}^{n}\left(y_{t}-\gamma w_{t}\right)^{2}=1/n\sum_{i=1}^{n}\left(\beta_{0}x_{t}+\varepsilon_{t}-\gamma w_{t}\right)^{2}\\
 & = & {\color{red}1/n\sum_{t=1}^{n}\left(\beta_{0}x_{t}\right)^{2}}+{\color{blue}1/n\sum_{t=1}^{n}\left(\gamma w_{t}\right)^{2}}+{\color{red}1/n\sum_{t=1}^{n}\varepsilon_{t}^{2}}\\
 &  & +2/n\sum_{t=1}^{n}\beta_{0}x_{t}\varepsilon_{t}{\color{green}{\color{purple}-2/n\sum_{t=1}^{n}\beta_{0}\gamma x_{t}w_{t}}}-2/n\sum_{t=1}^{n}\varepsilon_{t}x_{t}w_{t},
\end{eqnarray*}

\end_inset

which one can verify if armed with patience.
 Using arguments similar to the correctly specified case,
 above,
 
\begin_inset Formula 
\[
s_{\infty}(\gamma)={\color{blue}\gamma^{2}E\left(W^{2}\right)}{\color{green}{\color{purple}-2\beta_{0}\gamma E(WX)}}+{\color{red}C}
\]

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 holds the red terms,
 that do not depend on 
\begin_inset Formula $\gamma$
\end_inset

,
 and the terms that are not given a color converge to 0.
 
\end_layout

\begin_layout Itemize
So,
 finding the minimizer of 
\begin_inset Formula $s_{\infty}(\gamma)$
\end_inset

 with respect to 
\begin_inset Formula $\gamma$
\end_inset

,
 we get 
\begin_inset Formula 
\[
\gamma_{0}=\frac{\beta_{0}E(WX)}{E(W^{2})},
\]

\end_inset

which is the true parameter of the DGP,
 multiplied by the pseudo-true value of a regression of 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $W.$
\end_inset

 The OLS estimator 
\emph on
is not consistent
\emph default
 for the true parameter,
 
\begin_inset Formula $\beta_{0}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Summary
\end_layout

\begin_layout Standard
The theorem for consistency is really quite intuitive.
 It says that,
 with probability one,
 an extremum estimator converges to the value that maximizes the limit of the expectation of the objective function.
 Because the objective function may or may not make sense,
 depending on how good or poor is the econometric model,
 we may or may not be estimating parameters of the DGP.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example:-Linearization-of"

\end_inset

Example:
 Linearization of a nonlinear model
\end_layout

\begin_layout Standard
See 
\begin_inset CommandInset citation
LatexCommand citet
key "white1980using"
literal "false"

\end_inset

 and Gourieroux and Monfort,
 section 8.3.4.
\end_layout

\begin_layout Standard
Suppose we have a nonlinear model 
\begin_inset Formula 
\[
y_{i}=h(x_{i},\theta_{0})+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula 
\[
\varepsilon_{i}\sim iid(0,\sigma^{2})
\]

\end_inset

 The 
\emph on
nonlinear least squares
\emph default
 estimator solves 
\begin_inset Formula 
\[
\hat{\theta}_{n}=\arg\min\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-h(x_{i},\theta)\right)^{2}
\]

\end_inset

 
\begin_inset Newpage newpage
\end_inset

We'll study this more later,
 but for now it is clear that the foc for minimization will require solving a set of nonlinear equations.
 A common approach (at least,
 it was common a while ago,
 like in 1980) to the problem seeks to avoid this difficulty by 
\emph on
linearizing
\emph default
 the model.
 A first order Taylor's series expansion about the point 
\begin_inset Formula $x_{0}$
\end_inset

 with remainder gives 
\begin_inset Formula 
\[
y_{i}=h(x^{0},\theta_{0})+\left(x_{i}-x_{0}\right)^{\prime}\frac{\partial h(x_{0},\theta_{0})}{\partial x}+\nu_{i}
\]

\end_inset

 where 
\begin_inset Formula $\nu_{i}$
\end_inset

 encompasses both 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 and the Taylor's series remainder.
 
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\nu_{i}$
\end_inset

 is no longer a classical error - it is not independent of 
\begin_inset Formula $x$
\end_inset

,
 so weak exogeneity does not hold.
 We should expect problems.
\end_layout

\begin_layout Itemize
Define 
\begin_inset Formula 
\begin{eqnarray*}
\alpha^{*} & = & h(x_{0},\theta_{0})-x_{0}^{\prime}\frac{\partial h(x^{0},\theta_{0})}{\partial x}\\
\beta^{*} & = & \frac{\partial h(x_{0},\theta_{0})}{\partial x}
\end{eqnarray*}

\end_inset

as the intercept and slope of the Taylor's series tangent line.
 So,
 
\begin_inset Formula 
\[
y_{i}=\alpha^{*}+\beta^{*}x_{i}+\nu_{i}
\]

\end_inset


\end_layout

\begin_layout Itemize
Given this,
 one might try to estimate 
\begin_inset Formula $\alpha^{*}$
\end_inset

 and 
\begin_inset Formula $\beta^{*}$
\end_inset

 by applying OLS,
 regression 
\begin_inset Formula $y$
\end_inset

 on 
\begin_inset Formula $x$
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Question,
 will 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}$
\end_inset

 be consistent for 
\begin_inset Formula $\alpha^{*}$
\end_inset

 and 
\begin_inset Formula $\beta^{*}$
\end_inset

?
\end_layout

\begin_layout Itemize
The answer is no,
 as one can see by interpreting 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}$
\end_inset

 as extremum estimators.
 Let 
\begin_inset Formula $\gamma=(\alpha,\beta^{\prime})^{\prime}.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\gamma}=\arg\min s_{n}(\gamma)=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}
\]

\end_inset

 The objective function converges to its expectation 
\begin_inset Formula 
\[
s_{n}(\gamma)\stackrel{u.a.s.}{\rightarrow}s_{\infty}(\gamma)=\mathcal{E}_{X}\mathcal{E}_{Y|X}\left(y-\alpha-\beta x\right)^{2}
\]

\end_inset

 and 
\begin_inset Formula $\hat{\gamma}$
\end_inset

 converges 
\begin_inset Formula $a.s.$
\end_inset

 to the 
\begin_inset Formula $\gamma^{0}$
\end_inset

 that minimizes 
\begin_inset Formula $s_{\infty}(\gamma)$
\end_inset

:
 
\begin_inset Formula 
\[
\gamma^{0}=\arg\min\mathcal{E}_{X}\mathcal{E}_{Y|X}\left(y-\alpha-\beta x\right)^{2}
\]

\end_inset

 Noting that 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}_{X}\mathcal{E}_{Y|X}\left(y-\alpha-x^{\prime}\beta\right)^{2} & = & \mathcal{E}_{X}\mathcal{E}_{Y|X}\left(h(x,\theta_{0})+\varepsilon-\alpha-\beta x\right)^{2}\\
 & = & \sigma^{2}+\mathcal{E}_{X}\left(h(x,\theta_{0})-\alpha-\beta x\right)^{2}
\end{eqnarray*}

\end_inset

 since cross products involving 
\begin_inset Formula $\varepsilon$
\end_inset

 drop out.
 
\begin_inset Formula $\alpha^{0}$
\end_inset

 and 
\begin_inset Formula $\beta_{0}$
\end_inset

 correspond to the hyperplane that is closest to the true regression function 
\begin_inset Formula $h(x,\theta_{0})$
\end_inset

 according to the mean squared error criterion.
 This depends on both the shape of 
\begin_inset Formula $h(\cdot)$
\end_inset

 and the density function of the conditioning variables.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Linear-Approximation"
nolink "false"

\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Linear-Approximation"

\end_inset

Linear Approximation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/LinearApproximation.pdf
	width 12cm

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
It is clear that the tangent line does not minimize MSE,
 since,
 for example,
 if 
\begin_inset Formula $h(x,\theta_{0})$
\end_inset

 is concave,
 all errors between the tangent line and the true function are negative.
\end_layout

\begin_layout Itemize
Note that the true underlying parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

 is not estimated consistently,
 either (it may be of a different dimension than the dimension of the parameter of the approximating model,
 which is 2 in this example).
\end_layout

\begin_layout Itemize
Second order and higher-order approximations suffer from exactly the same problem,
 though to a less severe degree,
 of course.
 For this reason,
 translog,
 Generalized Leontiev and other 
\begin_inset Quotes eld
\end_inset

flexible functional forms
\begin_inset Quotes erd
\end_inset

 based upon second-order approximations in general suffer from bias and inconsistency.
 The bias may not be too important for analysis of conditional means,
 but it can be very important for analyzing first and second derivatives.
 In production and consumer analysis,
 first and second derivatives (
\emph on
e.g.,

\emph default
 elasticities of substitution) are often of interest,
 so in this case,
 one should be cautious of unthinking application of models that impose stong restrictions on second derivatives.
\end_layout

\begin_layout Itemize
This sort of linearization about a long run equilibrium is a common practice in working with dynamic macroeconomic models.
 It is justified for the purposes of theoretical analysis of a model 
\emph on
given
\emph default
 the model's parameters,
 but it will lead to
\emph on
 bias and inconsistency
\emph default
 if it is done before estimation of the parameters of the model using data.
 The section on simulation-based methods offers a means of obtaining consistent estimators of the parameters of dynamic macro models that are too complex for standard methods of analysis.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Asymptotic Normality
\end_layout

\begin_layout Standard
A consistent estimator is oftentimes not very useful unless we know how fast it is likely to be converging to the true value,
 and the probability that it is far away from the true value,
 for a given sample size.
 Establishment of asymptotic normality with a known scaling factor solves these two problems.
 The following theorem is similar to 
\begin_inset CommandInset citation
LatexCommand citet
key "amemiya1985advanced"
literal "false"

\end_inset

 Theorem 4.1.3 (pg.
 111).
\end_layout

\begin_layout Theorem

\emph on
[Asymptotic normality of e.e.]
\emph default
 
\begin_inset CommandInset label
LatexCommand label
name "Normality of ee"

\end_inset

In addition to the assumptions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

,
 assume
\end_layout

\begin_layout Theorem
(a) 
\begin_inset Formula $\mathcal{J}_{n}(\theta)\equiv D_{\theta}^{2}s_{n}(\theta)$
\end_inset

 exists and is continuous in an open,
 convex neighborhood of 
\begin_inset Formula $\theta_{0}.$
\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Theorem
(b) 
\begin_inset Formula $\mathcal{J}_{n}(\theta_{n})\stackrel{a.s.}{\rightarrow}\mathcal{J}_{\infty}(\theta_{0}),$
\end_inset

 a finite negative definite matrix,
 for any sequence of 
\begin_inset Formula $\theta_{n}$
\end_inset

 that converges almost surely to 
\begin_inset Formula $\theta_{0}.$
\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Theorem
(c) 
\begin_inset Formula $\sqrt{n}D_{\theta}s_{n}(\theta_{0})\stackrel{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta_{0})\right],$
\end_inset

 where 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})=\lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta_{0})$
\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Theorem
Then 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Proof:

\series default
 By Taylor expansion:
 
\begin_inset Formula 
\[
D_{\theta}s_{n}(\hat{\theta}_{n})=D_{\theta}s_{n}(\theta_{0})+{\color{blue}D_{\theta}^{2}s_{n}(\theta^{*})}\left(\hat{\theta}-\theta_{0}\right)
\]

\end_inset

 where 
\begin_inset Formula $\theta^{*}=\lambda\hat{\theta}+(1-\lambda)\theta_{0},$
\end_inset

 
\begin_inset Formula $0\leq\lambda\leq1.$
\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 will be in the neighborhood where 
\begin_inset Formula $D_{\theta}^{2}s_{n}(\theta)$
\end_inset

 exists with probability one as 
\begin_inset Formula $n$
\end_inset

 becomes large,
 by consistency.
\end_layout

\begin_layout Itemize
Now the l.h.s.
 of this equation is zero,
 at least asymptotically,
 since 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is a maximizer and the f.o.c.
 must hold exactly since the limiting objective function is strictly concave in a neighborhood of 
\begin_inset Formula $\theta_{0}$
\end_inset

 (assns.
 a and b)
\end_layout

\begin_layout Itemize
Also,
 since 
\begin_inset Formula $\theta^{*}$
\end_inset

 is between 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 and 
\begin_inset Formula $\theta_{0},$
\end_inset

 and since 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 
\begin_inset Formula $\stackrel{a.s.}{\rightarrow}$
\end_inset

 
\begin_inset Formula $\theta_{0}$
\end_inset

 ,
 assumption (b) gives 
\begin_inset Formula 
\[
{\color{blue}D_{\theta}^{2}s_{n}(\theta^{*})}\stackrel{a.s.}{\rightarrow}{\color{red}\mathcal{J}_{\infty}(\theta_{0})}
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
So 
\begin_inset Formula 
\[
0=D_{\theta}s_{n}(\theta_{0})+\left[{\color{red}\mathcal{J}_{\infty}(\theta_{0})}+o_{s}(1)\right]\left(\hat{\theta}-\theta_{0}\right),
\]

\end_inset

and 
\begin_inset Formula 
\[
\sqrt{n}D_{\theta}s_{n}(\theta_{0})=-\left[\mathcal{J}_{\infty}(\theta_{0})+o_{s}(1)\right]\sqrt{n}\left(\hat{\theta}-\theta_{0}\right).
\]

\end_inset

 Now 
\begin_inset Formula $\sqrt{n}D_{\theta}s_{n}(\theta_{0})\stackrel{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta_{0})\right]$
\end_inset

 by assumption c,
 so 
\begin_inset Formula 
\[
-\left[\mathcal{J}_{\infty}(\theta_{0})+o_{s}(1)\right]\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta_{0})\right]
\]

\end_inset

 Also,
 
\begin_inset Formula $\left[\mathcal{J}_{\infty}(\theta_{0})+o_{s}(1)\right]\stackrel{a.s.}{\rightarrow}\mathcal{\mathcal{J}_{\infty}}(\theta_{0}),$
\end_inset

 so 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]
\]

\end_inset

by the Slutsky Theorem (
\begin_inset CommandInset href
LatexCommand href
name "quick version"
target "https://en.wikipedia.org/wiki/Slutsky%27s_theorem"
literal "false"

\end_inset

,
 formal version:
 see 
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "false"

\end_inset

,
 Theorem 4.6).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Effects-of-"
nolink "false"

\end_inset

 shows the effects of the two components,
 the variability of the gradient,
 and the slope of the gradient.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Effects-of-"

\end_inset

Effects of 
\begin_inset Formula $I_{\mbox{\infty}}$
\end_inset

 and 
\begin_inset Formula $J_{\infty}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/IJ.jpg
	scaleBeforeRotation
	rotateAngle 90

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example:
 Classical linear model
\end_layout

\begin_layout Standard
Let's use the results to get the asymptotic distribution of the OLS estimator applied to the classical model,
 to verify that we obtain the results seen before.
 The OLS criterion is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\beta) & = & \frac{1}{n}\left(y-X\beta\right)^{\prime}\left(y-X\beta\right)\\
 & = & \frac{1}{n}\left(X\beta_{0}+\epsilon-X\beta\right)^{\prime}\left(X\beta_{0}+\epsilon-X\beta\right)\\
 & = & \frac{1}{n}\left(X(\beta_{0}-\beta)+\epsilon\right)^{\prime}\left(X(\beta_{0}-\beta)+\epsilon\right)\\
 & = & \frac{1}{n}\left[\left(\beta_{0}-\beta\right)^{\prime}X^{\prime}X\left(\beta_{0}-\beta\right)+2\epsilon^{\prime}X(\beta_{0}-\beta)+\epsilon^{\prime}\epsilon\right]
\end{eqnarray*}

\end_inset

The first derivative is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{\beta}s_{n}(\beta)=\frac{1}{n}\left[-2X^{\prime}X\left(\beta_{0}-\beta\right)-2X^{\prime}\epsilon\right]
\]

\end_inset

so,
 evaluating at 
\begin_inset Formula $\beta_{0},$
\end_inset


\begin_inset Newpage newpage
\end_inset


\begin_inset Formula 
\[
D_{\beta}s_{n}(\beta_{0})=-2\frac{X^{\prime}\epsilon}{n}
\]

\end_inset


\end_layout

\begin_layout Itemize
note that this is an average of terms,
 each of which has expectation zero:
 
\begin_inset Formula $-2\frac{X^{\prime}\epsilon}{n}=-2\frac{1}{n}\sum_{t}x_{t}\epsilon_{t}$
\end_inset


\end_layout

\begin_layout Itemize
thus,
 a LLN tells us this converges almost surely to 0.
\end_layout

\begin_layout Itemize
to keep this from happening,
 we can multiply by something that is converging to infinity.
 It turns out that 
\begin_inset Formula $\sqrt{n}$
\end_inset

 is the right choice,
 because then the asymptotic distribution will be stable,
 and a CLT will apply.
\end_layout

\begin_layout Standard
Now,
 let's get the form of 
\begin_inset Formula $\mathcal{I}_{\infty}$
\end_inset

 of Assumption (c):
 Considering 
\begin_inset Formula $\sqrt{n}D_{\beta}s_{n}(\beta_{0})$
\end_inset

,
 it has expectation 0,
 so the variance is the expectation of the outer product (there's no need to subtract the mean):
\begin_inset Formula 
\begin{eqnarray*}
Var\sqrt{n}D_{\beta}s_{n}(\beta_{0}) & = & E\left[\left(-\sqrt{n}2\frac{X^{\prime}\epsilon}{n}\right)\left(-\sqrt{n}2\frac{X^{\prime}\epsilon}{n}\right)^{\prime}\right]\\
 & = & E4\frac{X^{\prime}\epsilon\epsilon^{\prime}X}{n}\\
 & = & 4\sigma_{\epsilon}^{2}E\left(\frac{X^{\prime}X}{n}\right)
\end{eqnarray*}

\end_inset

(because regressors are independent of errors).
 Therefore
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\beta_{0}) & = & \lim_{n\rightarrow\infty}Var\sqrt{n}D_{\beta}s_{n}(\beta_{0})\\
 & = & 4\sigma_{\epsilon}^{2}Q_{X}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $Q_{X}=\lim E\left(\frac{X^{\prime}X}{n}\right),$
\end_inset

 is a finite p.d.
 matrix (by the classical assumption of no perfect collinearity).
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The second derivative is
\begin_inset Formula 
\[
\mathcal{J}_{n}(\beta)=D_{\beta}^{2}s_{n}(\beta_{0})=\frac{1}{n}\left[2X^{\prime}X\right].
\]

\end_inset

A SLLN tells us that this converges almost surely to the limit of its expectation:
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\beta_{0})=2Q_{X}
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The asymptotic normality theorem (
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

) tells us that
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right) & \stackrel{d}{\rightarrow} & N\left[0,\mathcal{J}_{\infty}(\beta_{0})^{-1}\mathcal{I}_{\infty}(\beta_{0})\mathcal{J}_{\infty}(\beta_{0})^{-1}\right]
\end{eqnarray*}

\end_inset

which is,
 given the above,
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(\frac{Q_{X}^{-1}}{2}\right)\left(4{\color{blue}\sigma_{\epsilon}^{2}Q_{X}}\right)\left(\frac{Q_{X}^{-1}}{2}\right)\right]
\]

\end_inset

or
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,Q_{X}^{-1}\sigma_{\epsilon}^{2}\right].
\]

\end_inset


\end_layout

\begin_layout Itemize
This is the same thing we saw in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:asymp normality OLS"
nolink "false"

\end_inset

,
 of course.
 So,
 the theory seems to work :-)
\end_layout

\begin_layout Itemize
it is easy to adapt this to heteroscedasticity and/or autocorrelation of unknown form.
 The only change would be that the blue term above would be equal to 
\begin_inset Formula $\Omega,$
\end_inset

 say,
 and then we would get 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right]
\]

\end_inset

 which is the same as we saw in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: as. dist. OLS with het/aut"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical Summary
\end_layout

\begin_layout Standard
The practical summary for the Chapter is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./PracticalSummaries/13-ExtremumEstimators.jl}{here}
\end_layout

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
Suppose that 
\begin_inset Formula $x_{i}\sim$
\end_inset

 uniform(0,1),
 and 
\begin_inset Formula $y_{i}=1-x_{i}^{2}+\varepsilon_{i},$
\end_inset

 where 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 is independently and identically distributed 
\begin_inset Formula $N(0,\sigma^{2}).$
\end_inset

 Suppose we estimate the misspecified model 
\begin_inset Formula $y_{i}=\alpha+\beta x_{i}+\eta_{i}$
\end_inset

 by OLS.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Find,
 analytically,
 the numeric values of 
\begin_inset Formula $\alpha^{0}$
\end_inset

 and 
\begin_inset Formula $\beta_{0}$
\end_inset

 that are the probability limits of 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}$
\end_inset

.
 Hint:
 the correct answers are 7/6 and -1.
 To get some help with this exercise,
 you can use a computer algebra program,
 as was done in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Consistency-of-OLS"
nolink "false"

\end_inset

.
 A small modification of that code would solve this problem.
 In particular,
 paste the following into a new Xmaxima session:
 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

integrate((1-x^2-a-b*x)^2,x,0,1);
\end_layout

\begin_layout Plain Layout

diff(%,a);
\end_layout

\begin_layout Plain Layout

diff(%th(2),b);
\end_layout

\begin_layout Plain Layout

solve([%o2=0,%o3=0],[a,b]);
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Verify your results by generating data that follows the above model,
 and calculating the OLS estimator.
 When the sample size is very large,
 the estimator should be very close to the analytical results you obtained in the previous question.
\end_layout

\end_deeper
\begin_layout Enumerate
Use the asymptotic normality theorem to find the asymptotic distribution of the ML estimator of 
\begin_inset Formula $\beta_{0}$
\end_inset

 for the model 
\begin_inset Formula $y=x\beta_{0}+\varepsilon,$
\end_inset

 where 
\begin_inset Formula $\varepsilon\sim N(0,1)$
\end_inset

 and is independent of 
\begin_inset Formula $x.$
\end_inset

 This means finding 
\begin_inset Formula $\frac{\partial^{2}}{\partial\beta\partial\beta^{\prime}}s_{n}(\beta)$
\end_inset

,
 
\begin_inset Formula $\mathcal{J}(\beta_{0}),\left.\frac{\partial s_{n}(\beta)}{\partial\beta}\right|,$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}(\beta_{0}).$
\end_inset

 The expressions may involve the unspecified density of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Application:
 a simple DSGE model
\begin_inset CommandInset label
LatexCommand label
name "chap:Application:-a-simple"

\end_inset


\end_layout

\begin_layout Standard
This short chapter presents a simple DSGE model,
 which will be used to illustrate several estimators:
 ML,
 GMM,
 VARs,
 and Bayesian methods.
 DSGE models are quite widely used by central banks,
 etc.,
 but they are not without their critics:
 see 
\begin_inset CommandInset href
LatexCommand href
name "Paul Romer's WP \"The Trouble with Macroeconomics\""
target "https://paulromer.net/trouble-with-macroeconomics-update/"
literal "false"

\end_inset

,
 for example.
 I like the DSGE model as an example because it allows illustrating a variety of econometric techniques and methods.
 Any other structural nonlinear model could serve the same purpose.
 
\end_layout

\begin_layout Itemize
To build an econometric model and to know how to interpret the results,
 it is very useful to have an economically meaningful model in mind,
 at least vaguely.
 
\end_layout

\begin_deeper
\begin_layout Itemize
selecting variables,
 lags,
 moment conditions,
 instruments,
 etc.
\end_layout

\begin_layout Itemize
here,
 we will have an explicit model as a reference point.
 Often,
 the reference is not so clearly defined.
\end_layout

\end_deeper
\begin_layout Itemize
We will investigate structural estimation methods,
 which attempt to estimate the actual parameters of the data generating process,
 and reduced form methods,
 which characterize the data,
 but which do not recover the parameters of the data generating process.
 
\end_layout

\begin_layout Itemize
Knowing the true DGP will allow us to measure sensibly how well the different methods work.
 This chapter describes the dgp for a fairly simple structural model.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The model
\end_layout

\begin_layout Standard

\series bold
The model is as follows:
 
\end_layout

\begin_layout Itemize
The consumer chooses consumption,
 hours of work,
 and investment to maximize expected discounted utility.
\end_layout

\begin_layout Itemize
Using capital and labor provided by the consumer,
 a competitive firm produces an output to maximize profits,
 and pays the consumer according to the marginal productivity of the inputs.
 
\end_layout

\begin_layout Itemize
The price of the consumption good is normalized to one.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Variables
\end_layout

\begin_layout Standard
There are 9 endogenous variables,
 listed in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Variables"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Variables"

\end_inset

Variables
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
output
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
consumption
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
capital
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
investment
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
hours
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to labor
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $r$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to capital
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\eta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
preference shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $z$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
technology shock
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\series bold
Parameters
\end_layout

\begin_layout Standard
There are 9 parameters,
 listed in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Parameters"
nolink "false"

\end_inset


\begin_inset Float table
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Parameters"

\end_inset

Parameters
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
production
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
discount
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\delta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
depreciation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\gamma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
risk aversion
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\psi$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MRS
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{z}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
persistence technology shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{z}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
variability technology shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{\eta}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
persistence preference shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{\eta}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
variability preference shock
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Consumer's problem
\end_layout

\begin_layout Standard
At the beginning of period 
\begin_inset Formula $t$
\end_inset

,
 the household owns a given amount of capital,
 
\begin_inset Formula $k_{t}$
\end_inset

,
 and chooses 
\begin_inset Formula $c_{t}$
\end_inset

,
 
\begin_inset Formula $i_{t}$
\end_inset

 and 
\begin_inset Formula $n_{t}$
\end_inset

 to maximize expected discounted utility
\begin_inset Formula 
\[
E_{t}\sum_{s=0}^{\infty}\beta^{s}\left(\frac{c_{t+s}^{1-\gamma}}{1-\gamma}+(1-n_{t+s}){\color{blue}\eta_{t}}\psi\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
subject to the budget constraint 
\begin_inset Formula $c_{t}+i_{t}=r_{t}k_{t}+w_{t}n_{t}$
\end_inset


\end_layout

\begin_layout Itemize
available time 
\begin_inset Formula $0\le n_{t}\le$
\end_inset

1
\end_layout

\begin_layout Itemize
and the accumulation of capital 
\begin_inset Formula $k_{t+1}=i_{t}+(1-\delta)k_{t}$
\end_inset

 :
 investment and depreciation
\end_layout

\begin_layout Itemize
There is a shock,
 
\begin_inset Formula ${\color{blue}\eta_{t}}$
\end_inset

,
 that affects the desirability of leisure relative to consumption 
\end_layout

\begin_deeper
\begin_layout Itemize
The shock evolves according to 
\begin_inset Formula $\ln\eta_{t}=\rho_{\eta}\ln\eta_{t-1}+\sigma_{\eta}\epsilon_{t}$
\end_inset

.
\end_layout

\begin_layout Itemize
sometimes,
 people want to work more,
 and sometimes,
 they want to take it easy.
 There is some persistence in this mood.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Firm's problem
\end_layout

\begin_layout Standard
The competitive firm maximizes profits 
\begin_inset Formula 
\[
y_{t}-w_{t}n_{t}-r_{t}k_{t}
\]

\end_inset

 from production of the good 
\begin_inset Formula $y_{t}$
\end_inset

,
 taking 
\begin_inset Formula $w_{t}$
\end_inset

 and 
\begin_inset Formula $r_{t}$
\end_inset

 as given,
 using the constant returns to scale technology
\begin_inset Formula 
\begin{equation}
y_{t}=k_{t}^{\alpha}n_{t}^{1-\alpha}{\color{blue}z_{t}}\label{eq:DSGE production}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
Remember that the price of 
\begin_inset Formula $y$
\end_inset

 is normalized to 1,
 so,
 profit,
 
\begin_inset Formula $y_{t}-w_{t}n_{t}-r_{t}k_{t}$
\end_inset

 is just revenue minus cost of production
\end_layout

\begin_layout Itemize
Technology shocks 
\begin_inset Formula ${\color{blue}z_{t}}$
\end_inset

 also follow an AR(1) process in logarithms:
 
\begin_inset Formula $\ln z_{t}=\rho_{z}\ln z_{t-1}+\sigma_{z}u_{t}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Comments:
\end_layout

\begin_layout Itemize
The innovations to the preference and technology shocks,
 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $u_{t}$
\end_inset

,
 are both i.i.d.
 standard normal random variables,
 and are independent of one another.
 
\end_layout

\begin_layout Itemize
The good 
\begin_inset Formula $y_{t}$
\end_inset

 can be allocated by the consumer to consumption or investment:
 
\begin_inset Formula $y_{t}=c_{t}+i_{t}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The consumer provides capital and labor to the firm,
 and is paid at the rates 
\begin_inset Formula $r_{t}$
\end_inset

 and 
\begin_inset Formula $w_{t}$
\end_inset

,
 respectively.
 
\end_layout

\begin_layout Itemize
The representative agent chooses actions in period 
\begin_inset Formula $t$
\end_inset

 using rational expectations,
 with full information about all variables indexed 
\begin_inset Formula $t-1$
\end_inset

 and earlier.
\end_layout

\begin_layout Itemize
The variables available for estimation are 
\begin_inset Formula $y,c,n,w,$
\end_inset

 and 
\begin_inset Formula $r.$
\end_inset

 We will see that 
\begin_inset Formula $k$
\end_inset

 can be recovered from these.
\end_layout

\begin_layout Itemize
the model is nonlinear in the parameters,
 equations depend on multiple endogenous variables,
 and 4 of the endogenous variables are not observed by the econometrician (the two shocks,
 capital and investment).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
First order conditions
\end_layout

\begin_layout Standard
Five definitions are 
\begin_inset Formula 
\begin{eqnarray*}
\textrm{marginal utility of consumption: }MUC_{t} & := & c_{t}^{-\gamma}\\
\textrm{marginal utility of leisure: }MUL_{t} & := & \psi\eta_{t}\\
\textrm{marginal rate of substitution: }MRS_{t} & := & MUC_{t}/MUL_{t}\\
\textrm{marginal product of labor: }MPL_{t} & := & \left(1-\alpha\right)z_{t}k_{t}^{\alpha}n_{t}^{-\alpha}\\
\textrm{marginal product of capital: }MPK_{t} & := & \alpha z_{t}k_{t}^{\alpha-1}n_{t}^{1-\alpha}
\end{eqnarray*}

\end_inset


\begin_inset Newpage newpage
\end_inset

With these definitions,
 the 9 equations that characterize the solution for the 9 endogenous variables are:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
MUC_{t} & = & E\left(\beta\cdot MUC_{t+1}\left[1+r_{t+1}-\delta\right]\right)\label{eq:Euler}\\
\textrm{ }1/MRS_{t} & = & w_{t}\label{eq:MRS-wage}\\
w_{t} & = & MPL_{t}\label{MPL-wage}\\
r_{t} & = & MPK_{t}\label{eq:-3}\\
\ln\eta_{t} & = & \rho_{\eta}\ln\eta_{t-1}+\sigma_{\eta}\epsilon_{t}\label{eq:}\\
\ln z_{t} & = & \rho_{z}\ln z_{t-1}+\sigma_{z}u_{t}\label{eq:-1}\\
y_{t} & = & z_{t}k_{t}^{\alpha}n_{t}^{(1-\alpha)}\label{eq:output}\\
i_{t} & = & y_{t}-c_{t}\label{eq:investment}\\
k_{t+1} & = & i_{t}+(1-\delta)k_{t}\label{eq:capital}
\end{eqnarray}

\end_inset

where the first two are from utility maximization,
 the second two are from profit maximization,
 and the remaining 5 are directly from the model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
The steady state
\end_layout

\begin_layout Standard
I use a third-order perturbation solution,
 which combines good accuracy with moderate computational demands 
\begin_inset CommandInset citation
LatexCommand cite
key "Aruoba2006"
literal "true"

\end_inset

.
 This is done using 
\begin_inset CommandInset href
LatexCommand href
name "SolveDSGE.jl"
target "https://github.com/RJDennis/SolveDSGE.jl"
literal "false"

\end_inset

,
 a registered Julia package which can be added using 
\family typewriter
]add SolveDSGE.
\end_layout

\begin_layout Itemize
A first step for solving the model is to find the 
\color blue
deterministic steady state
\color inherit
.
 The deterministic steady state is the equilibrium value of each variable that obtains when all shocks are set to 0.
 
\end_layout

\begin_layout Itemize
SolveDSGE.jl can compute the steady state itself,
 given a reasonably good starting value,
 but it is faster to use an analytic solution,
 if one can be computed.
 In this case,
 it can be done:
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\color blue
The following may be of interest to macro people.
 I'm going to skip it in lecture.
\end_layout

\begin_layout Standard
Let variables without the 
\begin_inset Formula $t$
\end_inset

 subscript indicate the deterministic steady state level of the variable.
 The deterministic steady state values of the two shocks 
\begin_inset Formula $\eta$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

 are both 1.
\end_layout

\begin_layout Itemize
For example,
 take 
\begin_inset Formula $z$
\end_inset

.
 The assumption is that 
\begin_inset Formula $\ln z_{t}=\rho_{z}\ln z_{t-1}+\sigma_{z}u_{t}$
\end_inset

.
 If we set 
\begin_inset Formula $u_{t}=0,$
\end_inset

 to make the equation deterministic,
 we get 
\begin_inset Formula $\ln z_{t}=\rho_{z}\ln z_{t-1}$
\end_inset

.
 Drop the subscript to reflect the equilibrium condition:
 
\begin_inset Formula $\ln z=\rho_{z}\ln z$
\end_inset

.
 This only holds when 
\begin_inset Formula $\ln z=0$
\end_inset

,
 for abritrary values of 
\begin_inset Formula $\rho_{z}$
\end_inset

.
 So,
 the steady state value of 
\begin_inset Formula $z=\exp(0)=1.$
\end_inset


\end_layout

\begin_layout Standard
Using equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MRS-wage"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "MPL-wage"
nolink "false"

\end_inset

,
 dropping 
\begin_inset Formula $t$
\end_inset

 subscripts,
 and setting the two shocks to 1,
 we obtain 
\begin_inset Formula 
\begin{eqnarray}
\frac{\psi}{c^{-\gamma}} & = & \left(1-\alpha\right)k^{\alpha}n^{-\alpha}\label{eq:MRS-MPL}\\
n & = & \left(\frac{1-\alpha}{\psi}\right)^{1/\alpha}kc^{-\gamma/\alpha}\nonumber 
\end{eqnarray}

\end_inset

Define 
\begin_inset Formula 
\begin{equation}
\theta:=\left(\frac{1-\alpha}{\psi}\right)^{1/\alpha}\label{theta}
\end{equation}

\end_inset

so
\begin_inset Formula 
\begin{equation}
n=\theta kc^{-\gamma/\alpha}\label{eq:steadystate1}
\end{equation}

\end_inset

From the Euler equation (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Euler"
nolink "false"

\end_inset

)
\begin_inset Formula 
\begin{eqnarray*}
c & = & \beta c\left(1+\alpha k^{\alpha-1}n^{1-\alpha}\right)\\
n^{1-\alpha} & = & k^{1-\alpha}\left[\frac{1}{\alpha}\left(\frac{1}{\beta}-1+\delta\right)\right]\\
n & = & k\left[\frac{1}{\alpha}\left(\frac{1}{\beta}-1+\delta\right)\right]^{\frac{\text{1}}{1-\alpha}}
\end{eqnarray*}

\end_inset

Define
\begin_inset Formula 
\begin{equation}
\varphi:=\left[\frac{1}{\alpha}\left(\frac{1}{\beta}-1+\delta\right)\right]^{\frac{\text{1}}{1-\alpha}}\label{eq:phi}
\end{equation}

\end_inset

so
\begin_inset Formula 
\begin{equation}
n=\varphi k\label{steadystate2}
\end{equation}

\end_inset

Now set equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:steadystate1"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "steadystate2"
nolink "false"

\end_inset

 equal,
 and solve for steady state level of consumption:
\begin_inset Formula 
\begin{eqnarray*}
\theta kc^{-\gamma/\alpha} & = & \varphi k\\
c^{-\gamma/\alpha} & = & \frac{\varphi}{\theta}\\
c & = & \left(\frac{\theta}{\varphi}\right)^{\frac{\alpha}{\gamma}}
\end{eqnarray*}

\end_inset

From equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:capital"
nolink "false"

\end_inset

,
 steady state investment satisfies 
\begin_inset Formula 
\[
i=\delta k
\]

\end_inset

and combining this with equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:output"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:investment"
nolink "false"

\end_inset

,
 we can obtain (using equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "steadystate2"
nolink "false"

\end_inset

)
\begin_inset Formula 
\[
c+\delta k=k^{\alpha}(\varphi k)^{1-\alpha}
\]

\end_inset

which solves as
\begin_inset Formula 
\[
k=\frac{c}{\varphi^{1-\alpha}-\delta}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\color blue
For lecture:
 
\color inherit
To summarize,
 the steady state values for the 9 endogenous variables are given in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Deterministic-steady-state"
nolink "false"

\end_inset

,
 and given the 9 parameters of the model,
 these can be solved for in the order 
\begin_inset Formula $c,\,k,\,n,\,y,\,i,\,w,r$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Deterministic-steady-state"

\end_inset

Deterministic steady state
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Variable
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Description
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Steady state
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
output
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k^{\alpha}n^{1-\alpha}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
consumption
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $c=\left(\frac{\theta}{\varphi}\right)^{\frac{\alpha}{\gamma}}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
capital
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k=\frac{c}{\varphi^{1-\alpha}-\delta}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
investment
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y-c$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
hours
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $n=\varphi k$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to labor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1-\alpha\right)k^{\alpha}n^{-\alpha}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $r$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to capital
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha k^{\alpha-1}n^{1-\alpha}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\eta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
preference shock
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $z$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
technology shock
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
True parameter values and priors 
\end_layout

\begin_layout Itemize
For this model,
 given the observable variables,
 we can recover 
\begin_inset Formula $\alpha=1-\frac{wn}{y}$
\end_inset

,
 by substituting the production function into the MPL or MPK.
 
\end_layout

\begin_layout Itemize
Once we have 
\begin_inset Formula $\alpha,$
\end_inset

 we can solve for 
\begin_inset Formula $k$
\end_inset

,
 by substituting the production function into the 
\begin_inset Formula $r=MPK$
\end_inset

 equation.
 
\end_layout

\begin_layout Itemize
with 
\begin_inset Formula $k,$
\end_inset

 we can recover 
\begin_inset Formula $\delta$
\end_inset

,
 from the law of motion of capital.
\end_layout

\begin_layout Itemize
So,
 we will take 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\delta$
\end_inset

 as known parameters,
 as they can be recovered exactly from the observable data (assuming there is no measurement error,
 which is the case we are in).
 Henceforth,
 we set 
\begin_inset Formula $\alpha=0.33$
\end_inset

 and 
\begin_inset Formula $\delta=0.025$
\end_inset

,
 which are commonly used in the literature.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Estimation by GMM does not require specifying a prior distribution for the parameters,
 but Bayesian methods do,
 so here,
 I discuss the true parameter values used for the simulations,
 as well as priors.
 Economic intuition can guide choice of priors for most parameters.
 However,
 we may be less confident specifying a prior for the marginal rate of substitution,
 
\begin_inset Formula $\psi.$
\end_inset

 
\end_layout

\begin_layout Itemize
Instead,
 following 
\begin_inset CommandInset citation
LatexCommand citet
key "RugeMurcia"
literal "false"

\end_inset

,
 we may place a prior on steady state hours,
 
\begin_inset Formula $n$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Given that hours 
\begin_inset Formula $n_{t}$
\end_inset

 must satisfy the constraint 
\begin_inset Formula $0\le n_{t}\le1$
\end_inset

,
 and we know that normally around 8 hours per day is dedicated to work,
 it is relatively straightforward to place a prior on 
\begin_inset Formula $n$
\end_inset

.
 
\end_layout

\begin_layout Itemize
If steady state hours,
 
\begin_inset Formula $n$
\end_inset

 is given,
 say as a draw from it's prior,
 then this,
 along with the parameters,
 excepting 
\begin_inset Formula $\psi$
\end_inset

,
 allows us to solve for the steady state values of the other variables and for 
\begin_inset Formula $\psi$
\end_inset

,
 as follows:
 
\end_layout

\begin_deeper
\begin_layout Itemize
Given 
\begin_inset Formula $n,$
\end_inset

 we can compute 
\begin_inset Formula $k=n/\varphi,$
\end_inset

 then 
\begin_inset Formula $i=\delta k,$
\end_inset

 then 
\begin_inset Formula $y=k^{\alpha}n^{1-\alpha}$
\end_inset

,
 then 
\begin_inset Formula $c=y-i$
\end_inset


\end_layout

\begin_layout Itemize
finally,
 using equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MRS-MPL"
nolink "false"

\end_inset

,
 we can compute the 
\begin_inset Formula $\psi$
\end_inset

 that is consistent with the given steady state 
\begin_inset Formula $n$
\end_inset

 and the other parameters of the model as 
\begin_inset Formula 
\begin{eqnarray*}
\psi & = & c^{-\gamma}\left(1-\alpha\right)k^{\alpha}n^{-\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

I use independent uniform priors for all parameters except 
\begin_inset Formula $\psi,$
\end_inset

 and a uniform prior for steady state hours,
 
\begin_inset Formula $n.$
\end_inset

 The true values of the parameters and the supports of the uniform priors are given in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:True-parameters-and"
nolink "false"

\end_inset

.
 I believe that most economists will find these priors to be quite loose,
 and the parameter values to be reasonable.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:True-parameters-and"

\end_inset

True parameters and support of uniform priors.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Lower bound 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Upper bound 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
True value 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.95 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.995 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.990 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\gamma$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.000 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{z}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.995 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.900 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{z}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.020 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{\eta}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.995
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.700 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{\eta}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\bar{n}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6/24 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9/24 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1/3 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Solution of the model and generation of data
\end_layout

\begin_layout Standard
Given a draw of the parameters other than 
\begin_inset Formula $\psi$
\end_inset

,
 
\begin_inset Formula $\psi$
\end_inset

 is computed as above,
 and we can form the full parameter vector,
 
\begin_inset Formula $\theta$
\end_inset

.
 The model is solved using SolveDSGE.jl,
 using a third order perturbation about the steady state.
 Once the model is solved,
 a simulation of length 660 is done,
 initialized at the steady state.
 We drop the first 500 observations as a burning period,
 to eliminate startup bias,
 and retain the last 160 observations,
 which mimic 40 years of quarterly data.
 The observable variables are 
\begin_inset Formula $y,\,c,\,n,\,w,$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

.
 The selection of observable variables is in line with much empirical work (e.g.,
 
\begin_inset CommandInset citation
LatexCommand cite
key "Smets2007"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand cite
key "Guerron2010"
literal "true"

\end_inset

).
\end_layout

\begin_layout Standard
The model file that SolveDSGE.jl uses is 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/GenData/CK.txt}{CK.txt} 
\end_layout

\end_inset

.
 It's remarkably close to the way the equations were presented above,
 I find.
 With this file,
 we can generate samples at the chosen parameter values.
 The script that generates the data file 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/GenData/dsgedata.csv}{dsgedata.csv} 
\end_layout

\end_inset

 is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/GenData/GenData.jl}{GenData.jl} 
\end_layout

\end_inset

.
 Here's a plot of the 160 observations of the five observed variables.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:The-DSGE-data"

\end_inset

The DSGE data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/GenData/dsgedata.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
recall that the true parameter values that generate this data were described above.
 
\end_layout

\begin_layout Itemize
Note that consumption is much more smooth than output
\end_layout

\begin_layout Itemize
the interest rate is pretty much constant,
 hours worked is a little more responsive
\end_layout

\begin_layout Itemize
wages move around quite a bit in response to shocks
\end_layout

\begin_layout Itemize
this data file will be treated as the true sample file in the examples that follow.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
In order to have a number of samples with which we can compare estimation methods via Monte Carlo,
 1000 samples were generated from the model at the above stated true parameters,
 using the script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/GenData/MakeMonteCarloData.jl}{MakeMonteCarloData.jl} 
\end_layout

\end_inset

.
 The 1000 samples are in the directory 
\family typewriter
./Examples/DSGE/GenData/MCdata
\family default
.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Maximum likelihood
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Ch.
 5
\end_layout

\begin_layout Standard
The maximum likelihood estimator is important because it uses all of the information in a fully specified statistical model.
 Its use of all of the information causes it to have a number of attractive properties,
 foremost of which is 
\emph on
asymptotic efficiency
\emph default
.
 For this reason,
 the ML estimator can serve as a benchmark against which other estimators may be measured.
 The ML estimator requires that the statistical model be fully specified,
 which essentially means that there is enough information to draw data from the DGP,
 given the parameter.
 This is a fairly strong requirement,
 and for this reason we need to be concerned about the possible misspecification of the statistical model.
 If this is the case,
 the ML estimator will not have the nice properties that it has under correct specification.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The likelihood function
\end_layout

\begin_layout Standard
Suppose we have a sample of size 
\begin_inset Formula $n$
\end_inset

 of the random vectors 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

.
 Suppose the joint density of 
\begin_inset Formula $Y=\left(\begin{array}{ccc}
y_{1} & \ldots & y_{n}\end{array}\right)$
\end_inset

 and 
\begin_inset Formula $Z=\left(\begin{array}{ccc}
z_{1} & \ldots & z_{n}\end{array}\right)$
\end_inset

 is characterized by a parameter vector 
\begin_inset Formula $\psi_{0}:$
\end_inset


\begin_inset Formula 
\[
f_{YZ}(Y,Z,\psi_{0}).
\]

\end_inset

This is the joint density of the sample (note:
 it's the same as what we see in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:parametric density of data"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

,
 except we're partitioning the data into two groups).
 The 
\emph on
likelihood
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
likelihood function
\end_layout

\end_inset

 function
\emph default
 is just this density evaluated at other values 
\begin_inset Formula $\psi$
\end_inset


\begin_inset Formula 
\[
L(Y,Z,\psi)=f(Y,Z,\psi),\psi\in\Psi,
\]

\end_inset

 where 
\begin_inset Formula $\Psi$
\end_inset

 is a 
\emph on
parameter
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
parameter space
\end_layout

\end_inset

 space.
\end_layout

\begin_layout Itemize
The 
\emph on
maximum likelihood estimator
\emph default
 of 
\begin_inset Formula $\psi_{0}$
\end_inset

 is the value of 
\begin_inset Formula $\psi$
\end_inset

 that maximizes the likelihood function.
\end_layout

\begin_layout Itemize
So,
 when we're doing ML,
 we assume we're in the Case 1 of the discussion around eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:parametric density of data"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Maybe we could call this estimator the maximum assumptions estimator,
 as we assume everything about the data generating process is known,
 except for the particular values of the parameter vector.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:Count-data.-Suppose"

\end_inset

Count data.
 Suppose we have a sample 
\begin_inset Formula $Y=\{y_{1},...,y_{n}\}$
\end_inset

 where the data are counts:
 the number of times some event occurs in a given interval of time,
 e.g.,
 number of visits to the doctor in a year.
 The simplest count data density is the Poisson:
\begin_inset Formula 
\[
f_{Y}(y;\lambda)=\frac{e^{-\lambda}\lambda^{y}}{y!}
\]

\end_inset

If the observations are i.i.d.
 distributed according to this density,
 then the joint density of the sample is
\begin_inset Formula 
\[
L(\lambda)=\prod_{i=1}^{n}\frac{e^{-\lambda}\lambda^{y_{i}}}{y_{i}!}=\frac{e^{-n\lambda}\lambda^{\sum y_{i}}}{\prod_{i}y_{i}!}
\]

\end_inset

A little calculus and algebra shows us that the value that maximizes this is 
\begin_inset Formula $\hat{\lambda}=\bar{y}$
\end_inset

.
\end_layout

\begin_layout Exercise
Prove this last statement
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Count-data.-Suppose"
nolink "false"

\end_inset

 we can compute the ML estimator without much trouble,
 and we have asymptotic theory that will allow us to test hypotheses about 
\begin_inset Formula $\lambda$
\end_inset

,
 because the estimator is the sample mean,
 and the LLN and CLT will apply.
 
\end_layout

\begin_layout Itemize
however,
 now suppose that each observation has its own 
\begin_inset Formula $\lambda_{i}=\exp(x_{i}^{\prime}\beta)$
\end_inset

,
 which depends on conditioning variables and a new parameter vector.
 We can now write the likelihood function in terms of 
\begin_inset Formula $\beta$
\end_inset

 (as was done in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"
nolink "false"

\end_inset

).
 
\color blue
For lectures,
 go look at that example now
\color inherit
,
 as a means of motivating studying ML.
\end_layout

\begin_deeper
\begin_layout Itemize
The problem is,
 we can't find an analytic solution for the ML estimator of 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Itemize
Even if we could,
 the 
\begin_inset Formula $\hat{\beta}$
\end_inset

 which solves the f.o.c.
 is a nonlinear function of the data,
 rather than a simple average.
 How could we test hypotheses?
 The t and F tests developed for the classical linear model do not apply.
\end_layout

\begin_layout Itemize
To solve these two problems,
 we need the methods from Ch.
 11 and the theory from Ch.
 12.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Exogenous variables
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "engle1983exogeneity"
literal "false"

\end_inset

 is a good reference for this part.
 The likelihood function can be factored as 
\begin_inset Formula 
\[
f_{YZ}(Y,Z,\psi)=f_{Y|Z}(Y|Z,\theta)f_{Z}(Z,\rho)
\]

\end_inset

where 
\begin_inset Formula $\theta$
\end_inset

 are whatever elements of 
\begin_inset Formula $\psi$
\end_inset

 that happen to enter in the conditional density,
 and 
\begin_inset Formula $\rho$
\end_inset

 are the elements that enter into the marginal density.
\end_layout

\begin_layout Standard

\color blue
Note that if 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\rho$
\end_inset

 share no elements,
 then the maximizer of the conditional likelihood function 
\begin_inset Formula $f_{Y|Z}(Y|Z,\theta)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 is the same as the maximizer of the overall likelihood function 
\begin_inset Formula $f_{YZ}(Y,Z,\psi)=f_{Y|Z}(Y|Z,\theta)f_{Z}(Z,\rho)$
\end_inset

,
 for the elements of 
\begin_inset Formula $\psi$
\end_inset

 that correspond to 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
In this case,
 the variables 
\begin_inset Formula $Z$
\end_inset

 are said to be 
\emph on
exogenous
\emph default
 for estimation of 
\begin_inset Formula $\theta$
\end_inset

,
 and we may more conveniently work with the conditional likelihood function 
\begin_inset Formula $f_{Y|Z}(Y|Z,\theta)$
\end_inset

 for the purposes of estimating 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
With exogeneity of 
\begin_inset Formula $Z$
\end_inset

,
 the maximum likelihood estimator of 
\begin_inset Formula $\theta_{0}$
\end_inset

 will be 
\begin_inset Formula 
\[
\hat{\theta}_{n}=\arg\max f_{Y|Z}(Y|Z,\theta).
\]

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
We'll suppose this framework holds in what follows.
 If it didn't,
 for some variables in 
\begin_inset Formula $Z,$
\end_inset

 then just move those variables from 
\begin_inset Formula $Z$
\end_inset

 to 
\begin_inset Formula $Y,$
\end_inset

 until it does hold.
\end_layout

\begin_layout Itemize
Where does information about exogeneity come from?
 From economic theory:
 exogeneity assumptions are part of the specification of the econometric model.
 These assumptions may be incorrect,
 of course.
 We can sometimes test exogeneity (e.g.,
 Hausman test,
 below),
 before simply accepting the results of an econometric model.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
A convenient factorization of the likelihood function
\end_layout

\begin_layout Itemize
If the 
\begin_inset Formula $n$
\end_inset

 observations are independent,
 the likelihood function can be written as 
\begin_inset Formula 
\[
L(Y|Z,\theta)=\prod_{t=1}^{n}f(y_{t}|z_{t},\theta)
\]

\end_inset

 
\end_layout

\begin_layout Itemize
If this is not possible,
 we can always factor the likelihood into 
\emph on
contributions of observations,

\emph default
 by using the fact that a joint density can be factored into the product of a marginal and conditional.
\end_layout

\begin_layout Itemize
Then
\begin_inset Formula 
\[
\begin{array}{c}
\underbrace{f(y_{1,}y_{2},\ldots y_{n-1},y_{n}|Z,\theta)}\\
\mathrm{joint}
\end{array}=\begin{array}{c}
\underbrace{f(y_{n}|y_{1,}y_{2},\ldots y_{n-1},Z,\theta)}\\
\mathrm{conditional}
\end{array}\begin{array}{c}
\underbrace{f(y_{1,}y_{2},\ldots y_{n-1}|Z,\theta)}\\
\mathrm{marginal}
\end{array}
\]

\end_inset


\end_layout

\begin_layout Itemize
do the same thing for 
\begin_inset Formula $y_{n-1}$
\end_inset

 in the last term,
 and keep iterating.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Then,
 in the end,
 we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
L(Y|Z,\theta) & =f(y_{n}|y_{1,}y_{2},\ldots y_{n-1},Z,\theta)\\
 & \cdot f(y_{n-1}|y_{1,}y_{2},\ldots y_{n-2},Z,\theta)\\
 & \cdots\\
 & \cdot f(y_{2}|y_{1},Z,\theta)\\
 & \cdot f(y_{1}|Z,\theta)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
To simplify notation,
 define 
\begin_inset Formula 
\begin{eqnarray*}
x_{t} & = & \{y_{1},y_{2},...,y_{t-1},Z\}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Usually,
 for time series data,
 conditional densities depend only on current period exogenous variables,
 as the effects of lagged exogenous variables are transmitted though the realizations of the lagged endogenous variables,
 and economic models normally don't involve leads of exogenous variables.
 If this is the case,
 
\begin_inset Formula $x_{1}=z_{1},$
\end_inset

 
\begin_inset Formula $x_{2}=\{y_{1},z_{2}\}$
\end_inset

,
 
\emph on
etc
\emph default
.
 - it contains exogenous and predetermined endogenous variables.
 
\end_layout

\begin_layout Itemize
it is also often the case that the data is Markovian,
 which means that only a limited number of lags of 
\begin_inset Formula $y$
\end_inset

 affect the current value.
 When this is the case,
 if 
\begin_inset Formula $m$
\end_inset

 is the maximum lag that still matters,
 then 
\begin_inset Formula 
\[
x_{t}=\{y_{t-m},y_{t-m+1},...,y_{t-1},Z\}
\]

\end_inset

when 
\begin_inset Formula $t>m$
\end_inset

.
 (Treatment of the observations where 
\begin_inset Formula $t\le m$
\end_inset

 is a bit complicated - these observations are often dropped,
 to keep things simple.
 Dropping information is not necessarily a good idea,
 but,
 here,
 we will not worry more about this problem.)
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
This factorization is natural and fairly obvious for time series data.
 Nevertheless,
 it can also be done for other sorts of data,
 too,
 though it might get more complicated.
 
\end_layout

\begin_layout Itemize
Regardless of the specific contents of 
\begin_inset Formula $x_{t},$
\end_inset

 the likelihood function can now be written as 
\begin_inset Formula 
\[
L(Y|Z;\theta)=\prod_{t=1}^{n}f(y_{t}|x_{t},\theta)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
The log likelihood function
\end_layout

\begin_layout Standard
The criterion function can be defined as the average log-likelihood function:
 
\begin_inset Formula 
\begin{equation}
s_{n}(\theta)=\frac{1}{n}\ln L(Y|Z;\theta)=\frac{1}{n}\sum_{t=1}^{n}\ln f(y_{t}|x_{t};\theta)\label{eq:average log likelihood}
\end{equation}

\end_inset

 The maximum likelihood estimator may thus be defined equivalently as 
\begin_inset Formula 
\[
\hat{\theta}=\arg\max s_{n}(\theta),
\]

\end_inset

where the set maximized over is defined below.
 Since 
\begin_inset Formula $\ln(\cdot)$
\end_inset

 is a monotonic increasing function,
 
\begin_inset Formula $\ln L$
\end_inset

 and 
\begin_inset Formula $L$
\end_inset

 maximize at the same value of 
\begin_inset Formula $\theta.$
\end_inset

 Dividing by 
\begin_inset Formula $n$
\end_inset

 has no effect on 
\begin_inset Formula $\hat{\theta}.$
\end_inset

 
\end_layout

\begin_layout Itemize

\emph on
Question:
 why do we do it,
 then?
 
\emph default
There are both theoretical and practical reasons:
\end_layout

\begin_deeper
\begin_layout Itemize
to get a LLN to apply:
 LNNs apply to averages of terms,
 not products
\end_layout

\begin_layout Itemize
to avoid loss of precision on a digital computer:
 the likelihood function in product form will tend rapidly to zero when each term is between 0 and 1.
 For a discrete r.v.,
 this will be the case,
 and it's usually the case for continuous R.V.s,
 too,
 unless they're highly concentrated.
 When this happens,
 then,
 as the sample size gets larger,
 the objective function gets smaller,
 and,
 before long,
 will dip below machine precision (which is about 16 decimal points in the usual case).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Example:
 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Example:-Bernoulli-trial"

\end_inset

Bernoulli trial
\begin_inset Newline newline
\end_inset

Suppose that we are flipping a coin that may be biased,
 so that the probability of a heads may not be 0.5.
 Maybe we're interested in estimating the probability of a heads.
 Let 
\begin_inset Formula $Y=1(heads)$
\end_inset

 be a binary variable that indicates whether or not a heads is observed.
 The outcome of a toss is a Bernoulli random variable:
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y,p_{0}) & = & p_{0}^{y}\left(1-p_{0}\right)^{1-y},y\in\{0,1\}\\
 & = & 0,y\notin\{0,1\}
\end{eqnarray*}

\end_inset

So a representative term that enters the likelihood function is
\begin_inset Formula 
\[
f_{Y}(y,p)=p^{y}\left(1-p\right)^{1-y}
\]

\end_inset

and
\begin_inset Formula 
\[
\ln f_{Y}(y,p)={\color{blue}y\ln p+\left(1-y\right)\ln\left(1-p\right)}
\]

\end_inset

For this example,
 the average log-likelihood function is
\begin_inset Formula 
\[
s_{n}(p)=\frac{1}{n}\sum_{t=1}^{n}y_{t}\ln p+\left(1-y_{t}\right)\ln\left(1-p\right).
\]

\end_inset


\end_layout

\begin_layout Example
* To explore the behavior of the likelihood,
 the log-likelihood,
 and the average log-likelihood,
 see the code 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/MLE/PlainLF.jl}{PlainLF.jl} 
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Newpage newpage
\end_inset

The derivative of a representative term is
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\ln f_{Y}(y,p)}{\partial p} & = & \frac{y}{p}-\frac{\left(1-y\right)}{\left(1-p\right)}\\
 & = & \frac{y-p}{p\left(1-p\right)}
\end{eqnarray*}

\end_inset

so,
 averaging this over a sample of size 
\begin_inset Formula $n$
\end_inset

,
 the gradient is
\begin_inset Formula 
\[
\frac{\partial s_{n}(p)}{\partial p}=\frac{1}{n}\sum_{t=1}^{n}\frac{y_{t}-p}{p\left(1-p\right)}.
\]

\end_inset

Setting to zero and solving gives
\begin_inset Formula 
\begin{equation}
\hat{p}=\bar{y}\label{eq:mle Bernoulli}
\end{equation}

\end_inset

So it's easy to calculate the MLE of 
\begin_inset Formula $p_{0}$
\end_inset

 in this case.
\begin_inset Newpage newpage
\end_inset

 
\end_layout

\begin_layout Itemize
We also know that the sample mean converges almost surely to the true mean,
 from basic statistics (LLN).
 
\end_layout

\begin_layout Itemize
The mean is 
\begin_inset Formula $E(Y)=\sum_{y=0}^{y=1}yp_{0}^{y}\left(1-p_{0}\right)^{1-y}=p_{0}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Thus,
 the MLE,
 which is the sample mean,
 is a consistent estimator of the parameter,
 because the population mean is the parameter..
 
\end_layout

\begin_layout Itemize
For future reference,
 note that 
\begin_inset Formula $Var(Y)=E(Y^{2})-\left[E(Y)\right]^{2}=p_{0}-p_{0}^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

We see that the ML estimator is consistent,
 for this model.
 However,
 let's verify that the consistency theorem for extremum estimators gives us the same result:
\end_layout

\begin_layout Itemize
A LLN tells us that,
 for a given 
\begin_inset Formula $p,$
\end_inset

 the objective function converges to the limit of its expectation
\end_layout

\begin_deeper
\begin_layout Itemize
because the data are i.i.d.,
 the expectation of the average log likelihood function is the expectation of a representative term:
 
\begin_inset Formula 
\begin{align*}
E(s_{n}(p)) & =E({\color{blue}y\ln p+\left(1-y\right)\ln\left(1-p\right)})\\
 & =p_{0}\ln p+(1-p_{0})\ln(1-p).
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
so,
 we get
\begin_inset Formula 
\[
s_{n}(p)\rightarrow^{a.s.}p_{0}\ln p+(1-p_{0})\ln(1-p).
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
The parameter space must be compact.
 We know that 
\begin_inset Formula $p_{0}$
\end_inset

 lies between 0 and 1,
 so this helps set the parameter space.
\end_layout

\begin_layout Itemize
the objective function is obviously continuous in the parameter
\end_layout

\begin_layout Itemize
we need the objective function to be bounded,
 for the simple sufficient conditions for the consistency theorem to hold.
 So,
 we need to assume that 
\begin_inset Formula $p$
\end_inset

 can't go to 0 or to 1.
 This means that the parameter space must be a compact subset of 
\begin_inset Formula $\left(0,1\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
With these conditions,
 the a.s.
 convergence is also uniform.
 
\end_layout

\begin_layout Itemize
So,
 we have assumptions 1 and 2 of the consistency theorem.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The consistency theorem for extremum estimators tells us that the ML estimator converges to the value that maximizes the limiting objective function.
 Because 
\begin_inset Formula 
\[
s_{\infty}(p)=p_{0}\ln p+(1-p_{0})\ln(1-p),
\]

\end_inset

we can easily check that the unique maximizer is 
\begin_inset Formula $p_{0}$
\end_inset

,
 by computing first and second derivatives.
 
\end_layout

\begin_layout Itemize
So,
 the three assumptions of the consistency theorem hold,
 and thus the ML estimator is consistent for the true probability.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example

\emph on
\begin_inset CommandInset label
LatexCommand label
name "exa:Likelihood-function-of"

\end_inset

Likelihood function and MLE of classical linear regression model
\emph default
.
 Let's suppose that a dependent variable is normally distributed:
 
\begin_inset Formula $y\sim N(\mu_{0},\sigma_{0}^{2})$
\end_inset

,
 so
\begin_inset Formula 
\[
f_{y}(y;{\color{blue}\mu_{0}},\sigma_{0}^{2})=\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}\exp\left(-\frac{(y-\mu_{0})^{2}}{2\sigma_{0}^{2}}\right)
\]

\end_inset

Suppose that the mean,
 
\begin_inset Formula $\mu_{0}$
\end_inset

,
 depends on some regressors,
 
\begin_inset Formula $x$
\end_inset

.
 The simplest way to do this is to assume that 
\begin_inset Formula $\mu_{0}=x^{\prime}\beta_{0}.$
\end_inset

 With this,
 the density,
 conditional on 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula 
\[
f_{y}(y|x;{\color{blue}\beta_{0}},\sigma_{0}^{2})=\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}\exp\left(-\frac{(y-x^{\prime}\beta_{0})^{2}}{2\sigma_{0}^{2}}\right)
\]

\end_inset

This is an example of 
\emph on
parameterization 
\emph default
of a density,
 making some parameters depend on additional variables and new parameters.
 With an i.i.d.
 sample of size 
\begin_inset Formula $n$
\end_inset

,
 the overall conditional density is the product of the conditional density of each observation:
 
\begin_inset Formula 
\[
f_{y}(y_{1},y_{2},...,y_{n}|x_{1},x_{2},...,x;\beta_{0},\sigma_{0}^{2})=\prod_{t=1}^{n}\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}\exp\left(-\frac{(y_{t}-x_{t}^{\prime}\beta_{0})^{2}}{2\sigma_{0}^{2}}\right)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Taking logarithms,
 and evaluating at some point in the parameter space,
 we get the log-likelihood function:
 
\begin_inset Formula 
\[
\ln L(Y|X;\beta,\sigma^{2})=-n\ln\sqrt{2\pi}-n\ln\sigma-\sum_{t=1}^{n}\frac{\left(y_{t}-x_{t}'\beta\right)^{2}}{2\sigma^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
Observe that the first order conditions for 
\begin_inset Formula $\beta$
\end_inset

 are the same as for the OLS estimator,
 because maximizing the log likelihood with respect to 
\begin_inset Formula $\beta$
\end_inset

 is the same as minimizing the sum of the squared residuals.
 
\color blue
For the 
\begin_inset Formula $\beta$
\end_inset

's,
 the OLS and ML estimators are identical,
 for the classical linear model with normal errors.
\end_layout

\begin_layout Itemize
An example that shows this is here:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/MLE/NerloveMLE.jl}{NerloveMLE.jl} 
\end_layout

\end_inset

,
 for the basic Nerlove model (eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "simple nerlove model"
nolink "false"

\end_inset

).
 (compare to GRETL)
\end_layout

\begin_layout Itemize
We know that the OLS estimator is consistent without making distributional assumptions regarding the errors.
 As long as the assumptions for consistency of OLS hold (fundamentally,
 weak exogeneity),
 then the 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimator will be consistent for 
\begin_inset Formula $\beta$
\end_inset

 as well,
 even if the normality assumption is not correct.
 This would be an example of 
\emph on
quasi-maximum likelihood
\emph default
 estimation:
 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimation of a misspecified model.
 Sometimes the QML estimator is consistent,
 sometimes it's not.
\end_layout

\begin_layout Itemize
A Julia example that shows how to compute the maximum likelihood estimator for data that follows the CLRM with normality is in 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/MLE/NormalExample.jl}{NormalExample.jl} 
\end_layout

\end_inset

.
 Examine the code,
 and figure out how the likelihood function is defined.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Consistency of MLE
\end_layout

\begin_layout Itemize
The MLE is an extremum estimator,
 given basic assumptions it is consistent for the value that maximizes the limiting objective function,
 following Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Itemize
The question is:
 what is the value that maximizes 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 when the criterion function is the average log-likelihood?
 
\end_layout

\begin_layout Itemize
For two cases (Bernoulli trial and ML of the linear model with normality) we have seen that the ML estimator converges to the true parameter of the d.g.p.
\end_layout

\begin_layout Itemize
Is this a general result?
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Remember that 
\begin_inset Formula $s_{n}(\theta)=\frac{1}{n}\ln L(Y|Z,\theta)$
\end_inset

,
 and 
\begin_inset Formula $L(Y|Z,\theta_{0})$
\end_inset

 is the true density of the sample data.
 For any 
\begin_inset Formula $\theta\neq\theta_{0}$
\end_inset


\begin_inset Formula 
\[
{\color{red}E\left(\ln\left(\frac{L(\theta)}{L(\theta_{0})}\right)\right)}\leq\ln\left({\color{green}{\color{blue}E\left(\frac{L(\theta)}{L(\theta_{0})}\right)}}\right)
\]

\end_inset

 by 
\begin_inset CommandInset href
LatexCommand href
name "Jensen's inequality"
target "http://en.wikipedia.org/wiki/Jensen's_inequality"
literal "false"

\end_inset

 ( 
\begin_inset Formula $\ln\left(\cdot\right)$
\end_inset

 is a concave function).
\end_layout

\begin_layout Standard
Now,
 the expectation on the RHS is 
\begin_inset Formula 
\[
{\color{green}{\color{blue}E\left(\frac{L(\theta)}{L(\theta_{0})}\right)}}=\int\frac{L(\theta)}{L(\theta_{0})}L(\theta_{0})dy=1,
\]

\end_inset

 since 
\begin_inset Formula $L(\theta_{0})$
\end_inset

 
\emph on
is
\emph default
 the density function of the observations,
 and since the integral of any density is 1
\begin_inset Formula $.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset

Therefore,
 since 
\begin_inset Formula $\ln(1)=0,$
\end_inset


\begin_inset Formula 
\[
{\color{red}E\left(\ln\left(\frac{L(\theta)}{L(\theta_{0})}\right)\right)}\leq0,
\]

\end_inset

 or (both sides have implicitly been multiplied by 
\begin_inset Formula $1/n)$
\end_inset


\begin_inset Formula 
\begin{align*}
E\left(s_{n}\left(\theta\right)\right)-E\left(s_{n}\left(\theta_{0}\right)\right) & \leq0
\end{align*}

\end_inset

or
\begin_inset Formula 
\[
E\left(s_{n}\left(\theta\right)\right)\leq E\left(s_{n}\left(\theta_{0}\right)\right)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Taking limits of each side:
\begin_inset Formula 
\[
s_{\infty}(\theta)\leq s_{\infty}(\theta_{0})
\]

\end_inset

except on a set of zero probability (by assumption b of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

).
 So the true parameter value is the maximizer of the limiting objective function (we are in Case 1 of the three cases discussed above - a fully correctly specified model).
\end_layout

\begin_layout Standard
If the identification assumption holds,
 then there is a unique maximizer,
 so the inequality is strict if 
\begin_inset Formula $\theta\neq\theta_{0}$
\end_inset

:
 
\begin_inset Formula 
\[
s_{\infty}(\theta)<s_{\infty}(\theta_{0}),\forall\theta\neq\theta_{0},\textnormal{a.s.}
\]

\end_inset

Therefore,
 
\begin_inset Formula $\theta_{0}$
\end_inset

 is the unique maximizer of 
\begin_inset Formula $s_{\infty}(\theta),$
\end_inset

 and thus,
 Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

 tells us that 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\hat{\theta}=\theta_{0},\textrm{ a}.\textrm{s}.
\]

\end_inset

 So,
 the ML estimator is consistent for the true parameter value.
\end_layout

\begin_layout Itemize
In practice,
 we will need to check identification for the specific model under consideration.
\end_layout

\begin_layout Itemize
Sometimes,
 we need to define the parameter space carefully to ensure identification.
 Examples:
 MA models,
 finite mixture models.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Verify by experiment the consistency of the ML estimator of the CLRM with normality by increasing 
\begin_inset Formula $n$
\end_inset

 in the example 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/MLE/NormalExample.jl}{NormalExample.jl} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The score function
\end_layout

\begin_layout Description
\noindent
Assumption:
 (Differentiability) Assume that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is twice continuously differentiable in a neighborhood 
\begin_inset Formula $N(\theta_{0})$
\end_inset

 of 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 at least when 
\begin_inset Formula $n$
\end_inset

 is large enough.
 
\end_layout

\begin_layout Itemize
with this,
 and with the result on consistency from above,
 assumptions (a) and (b) of theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

 hold,
 and 
\begin_inset Formula $\mathcal{J}_{n}(\hat{\theta})\stackrel{a.s.}{\rightarrow}\mathcal{J}_{\infty}(\theta_{0})$
\end_inset


\end_layout

\begin_layout Standard
To maximize the log-likelihood function,
 take derivatives:
 
\begin_inset Formula 
\begin{eqnarray}
g_{n}(Y|Z,\theta) & \equiv & D_{\theta}s_{n}(\theta)\nonumber \\
 & = & \frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f(y_{t}|x_{t},\theta)\label{eq:MLscore}\\
 & \equiv & \frac{1}{n}\sum_{t=1}^{n}g_{t}(\theta).\nonumber 
\end{eqnarray}

\end_inset

This is the 
\emph on
score vector
\emph default
 (with dim 
\begin_inset Formula $K\times1).$
\end_inset

 Note that the score function has 
\begin_inset Formula $Y\;$
\end_inset

as an argument,
 which implies that it is a random function.
 
\begin_inset Formula $Y$
\end_inset

 (and any exogeneous variables) will often be suppressed for clarity,
 but one should not forget that they are still there.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}\ln f(y_{t}|x_{t},\theta)
\]

\end_inset

If we define the contributions of each observation to the objective function as
\begin_inset Formula 
\[
s^{(t)}=\ln f(y_{t}|x_{t},\theta)
\]

\end_inset

then the objective function is the average of these terms.
 We can arrange the contributions to the objective function into the 
\begin_inset Formula $n$
\end_inset

-vector
\begin_inset Formula 
\[
s^{(1:n)}=\left[\begin{array}{c}
s^{(1)}\\
s^{(2)}\\
\vdots\\
s^{(n)}
\end{array}\right]
\]

\end_inset

Note that the Jacobian of this vector is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix with the score contributions 
\begin_inset Formula $g_{t}(\theta)=$
\end_inset

 
\begin_inset Formula $\frac{\partial}{\partial\theta}s^{(t)}$
\end_inset

 (transposed) in its rows:
\begin_inset Formula 
\[
\frac{\partial s^{(1:n)}}{\partial\theta^{\prime}}=\left[\begin{array}{c}
g_{1}^{\prime}(\theta)\\
g_{2}^{\prime}(\theta)\\
\vdots\\
g_{n}^{\prime}(\theta)
\end{array}\right]
\]

\end_inset

The averages of the columns of this matrix are the elements of the overall score vector.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

The ML estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 sets the derivatives to zero:
 
\begin_inset Formula 
\[
g_{n}(\hat{\theta})=\frac{1}{n}\sum_{t=1}^{n}g_{t}(\hat{\theta})\equiv0.
\]

\end_inset


\end_layout

\begin_layout Standard
We will show that 
\begin_inset Formula $E_{{\color{blue}\theta}}\left[g_{t}(\theta)|x_{t}\right]=0,$
\end_inset

 
\begin_inset Formula $\forall t.$
\end_inset

 
\emph on
This is the expectation taken with respect to the density
\emph default
 
\begin_inset Formula $f(y_{t}|x_{t},{\color{blue}\theta}),$
\end_inset

 not necessarily 
\begin_inset Formula $f\left(y_{t}|x_{t},\theta_{0}\right).$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
E_{\theta}\left[g_{t}(\theta)|x_{t}\right] & = & \int[D_{\theta}\ln f(y_{t}|x_{t},\theta)]f(y_{t}|x,\theta)dy_{t}\\
 & = & \int\frac{1}{{\color{green}{\color{red}f(y_{t}|x_{t},\theta)}}}\left[D_{\theta}f(y_{t}|x_{t},\theta)\right]{\color{red}f(y_{t}|x_{t},\theta)}dy_{t}\\
 & = & {\color{violet}\int D_{\theta}}f(y_{t}|x_{t},\theta)dy_{t}.
\end{eqnarray*}

\end_inset

 Given some regularity conditions on boundedness of 
\begin_inset Formula $D_{\theta}f,$
\end_inset

 we can switch the order of integration and differentiation,
 by the dominated convergence theorem (
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "false"

\end_inset

,
 Theorem 2.8).
 This gives 
\begin_inset Formula 
\begin{eqnarray}
E_{\theta}\left[g_{t}(\theta)|x_{t}\right] & = & {\color{violet}D_{\theta}\int}f(y_{t}|x_{t},\theta)dy_{t}\label{eq:ExpectationScore}\\
 & = & D_{\theta}1\nonumber \\
 & = & 0\nonumber 
\end{eqnarray}

\end_inset

where we use the fact that the integral of the density is 1.
\end_layout

\begin_layout Itemize
So 
\begin_inset Formula $E_{\theta}(g_{t}(\theta)|x_{t})=0:$
\end_inset

 
\emph on
the conditional expectation of the score vector is zero
\emph default
.
 Because this is true for all 
\begin_inset Formula $x_{t},$
\end_inset

 the unconditional expectation is also zero.
\end_layout

\begin_layout Itemize
This hold for all 
\begin_inset Formula $t,$
\end_inset

 so it implies that 
\begin_inset Formula $E_{\theta}g_{n}(Y|Z,\theta)=0.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:Law-of-iterated"

\end_inset

Law of iterated expectations.
 Suppose that a random variable 
\begin_inset Formula $Y$
\end_inset

 has zero expectation conditional on the random variable 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\[
E_{Y|X}Y={\color{red}\int Yf(Y|X)dY}=0
\]

\end_inset

 Then the unconditional expectation of the product of 
\begin_inset Formula $Y$
\end_inset

 and a function 
\begin_inset Formula $h(X)$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 is also zero.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To see this,
 the unconditional expectation is 
\begin_inset Formula 
\[
E\left(Yh(X)\right)=\int_{\mathcal{X}}\left(\int_{\mathcal{Y}}Yh(X)f(Y,X)dY\right)dX.
\]

\end_inset

 Factor the joint density in to conditional and marginal:
\begin_inset Formula 
\[
E\left(Yh(X)\right)=\int_{\mathcal{X}}\left(\int_{\mathcal{Y}}Y{\color{blue}h(X)}f(Y|X){\color{blue}f(X)}dY\right)dX.
\]

\end_inset

 Because 
\begin_inset Formula $f(X)$
\end_inset

 and 
\begin_inset Formula $h(X)$
\end_inset

 don't depend on 
\begin_inset Formula $Y$
\end_inset

,
 they can be pulled out of the integral 
\begin_inset Formula 
\[
E\left(Yh(X)\right)=\int_{\mathcal{X}}\left({\color{red}\int_{\mathcal{Y}}Yf(Y|X)dY}\right){\color{blue}h(X)f(X)}dX.
\]

\end_inset

 But the term in parentheses on the rhs is zero by assumption,
 so 
\begin_inset Formula 
\[
E\left(Yh(X)\right)=0
\]

\end_inset

 as claimed.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
This result implies that 
\begin_inset Formula $E_{\theta}(g_{t}(y_{t}|x_{t},\theta)g_{t-s}(y_{t-s}|x_{t-s},\theta)^{\prime})=0_{k\times k}$
\end_inset

:
 the score contributions are uncorrelated with one another.
\end_layout

\begin_layout Itemize
This comes from the fact that all random variables in 
\begin_inset Formula $g_{t-s}(y_{t-s}|x_{t-s},\theta)$
\end_inset

 are in the information set 
\begin_inset Formula $x_{t}$
\end_inset

 that 
\begin_inset Formula $g_{t}(y_{t}|x_{t},\theta)$
\end_inset

 conditions on,
 and applying iterated expectations.
\end_layout

\begin_layout Itemize
If one computes score contributions,
 and observes that they appear to be correlated,
 this casts doubt on the correct specification of the model.
 There are formal specification tests based on this result (
\begin_inset CommandInset citation
LatexCommand citet
key "WhiteMisspecifiedML1982"
literal "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Asymptotic normality of MLE
\end_layout

\begin_layout Standard
Recall that we assume that the log-likelihood function 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is twice continuously differentiable.
 Take a first order Taylor's series expansion of the score vector 
\begin_inset Formula $g_{n}(\hat{\theta})$
\end_inset

 about the true value 
\begin_inset Formula $\theta_{0}$
\end_inset

 (dropping the sub-n's,
 to avoid clutter):
\begin_inset Formula 
\begin{eqnarray*}
g(\hat{\theta}) & = & g(\theta_{0})+\left(D_{\theta^{\prime}}g(\theta^{*})\right)\left(\hat{\theta}-\theta_{0}\right)
\end{eqnarray*}

\end_inset

However,
 
\begin_inset Formula 
\[
g(\hat{\theta})\equiv0
\]

\end_inset

are the first order conditions that define the ML estimator.
 With this,
 and using the notation 
\begin_inset Formula $\mathcal{J}(\theta^{*})$
\end_inset

 to indicate the Hessian matrix 
\begin_inset Formula $D_{\theta^{\prime}}g(\theta^{*})$
\end_inset

,
 following what we did in the chapter on extremum estimators,
 we get
\begin_inset Formula 
\[
\mathcal{J}(\theta^{*})\left(\hat{\theta}-\theta_{0}\right)=-g(\theta_{0}),
\]

\end_inset

where 
\begin_inset Formula $\theta^{*}=\lambda\hat{\theta}+(1-\lambda)\theta_{0},0<\lambda<1.$
\end_inset

 Assume 
\begin_inset Formula $\mathcal{J}(\theta^{*})$
\end_inset

 is invertible (we'll justify this in a minute).
 So 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)=-\mathcal{J}(\theta^{*})^{-1}\sqrt{n}g(\theta_{0})\label{eq:TSexpansionMLgradient}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Now consider 
\begin_inset Formula $\mathcal{J}(\theta^{*}),$
\end_inset

 the matrix of second derivatives of the average log likelihood function (the Hessian matrix).
 This is 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{J}(\theta^{*}) & = & D_{\theta^{\prime}}g(\theta^{*})\\
 & = & D_{\theta}^{2}s_{n}(\theta^{*})\\
 & = & \frac{1}{n}\sum_{t=1}^{n}D_{\theta}^{2}\ln f_{t}(\theta^{*})
\end{eqnarray*}

\end_inset

 where the notation 
\begin_inset Formula 
\[
D_{\theta}^{2}s_{n}(\theta^{*})\equiv\left.\frac{\partial^{2}s_{n}(\theta)}{\partial\theta\partial\theta^{\prime}}\right|_{\theta=\theta^{*}}.
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Given that this is an average of terms,
 it should usually be the case that this satisfies a strong law of large numbers (SLLN).
 
\end_layout

\begin_layout Itemize

\emph on
Regularity conditions
\emph default
 are a set of assumptions that guarantee that this will happen.
 There are different sets of assumptions that can be used to justify appeal to different SLLN's.
 For example,
 the 
\begin_inset Formula $D_{\theta}^{2}\ln f_{t}(\theta^{*})$
\end_inset

 must not be too strongly dependent over time,
 and their variances must not become infinite.
 We don't assume any particular set here,
 since the appropriate assumptions will depend upon the particularities of a given model.
 However,
 we assume that a SLLN applies.
\end_layout

\begin_layout Standard
Also,
 since we know that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is consistent,
 and since 
\begin_inset Formula $\theta^{*}=\lambda\hat{\theta}+(1-\lambda)\theta_{0},$
\end_inset

 we have that 
\begin_inset Formula $\theta^{*}{\overset{a.s.}{\rightarrow}\theta_{0}}$
\end_inset

.
 Also,
 by the above differentiability assumption,
 
\begin_inset Formula $\mathcal{J}(\theta)$
\end_inset

 is continuous in 
\begin_inset Formula $\theta$
\end_inset

.
 Given this,
 
\begin_inset Formula $\mathcal{J}(\theta^{*})$
\end_inset

 converges to the limit of its expectation:
 
\begin_inset Formula 
\[
\mathcal{J}(\theta^{*})\overset{a.s.}{\rightarrow}\lim_{n\rightarrow\infty}\mathcal{E}\left(D_{\theta}^{2}s_{n}(\theta_{0})\right)=\mathcal{J}_{\infty}(\theta_{0})<\infty
\]

\end_inset

 
\emph on
This matrix converges to a finite limit.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Re-arranging orders of limits and differentiation,
 which is legitimate given certain regularity conditions related to the boundedness of the log-likelihood function,
 we get 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{J}_{\infty}(\theta_{0}) & = & D_{\theta}^{2}\lim_{n\rightarrow\infty}\mathcal{E}\left(s_{n}(\theta_{0})\right)\\
 & = & D_{\theta}^{2}s_{\infty}(\theta_{0})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From assumption (c) of the consistency proof,
\begin_inset Formula 
\[
s_{\infty}(\theta)<s_{\infty}(\theta_{0})
\]

\end_inset

 
\emph on
i.e.,

\emph default
 
\begin_inset Formula $\theta_{0}$
\end_inset

 maximizes the limiting objective function.
 Since there is a unique maximizer,
 and by the assumption that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is twice continuously differentiable (which holds in the limit),
 then 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 must be negative definite,
 and therefore of full rank.
 When 
\begin_inset Formula $n$
\end_inset

 is large enough so that we are close enough to 
\begin_inset Formula $\theta_{0},$
\end_inset

 the inversion of 
\begin_inset Formula $\mathcal{J}(\theta^{*})$
\end_inset

 is justified,
 so we get 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)=-\mathcal{J}(\theta^{*})^{-1}\sqrt{n}g(\theta_{0}).\label{anmle}
\end{equation}

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Now consider 
\begin_inset Formula $\sqrt{n}g(\theta_{0}).$
\end_inset

 For assumption (c) of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

 to apply,
 this quantity must follow a Central Limit Theorem.
 We have 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}g_{n}(\theta_{0}) & = & \sqrt{n}D_{\theta}s_{n}(\theta)\\
 & = & \sqrt{n}\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f_{t}(y_{t}|x_{t},\theta_{0})\\
 & = & \sqrt{n}{\color{blue}\frac{1}{n}\sum_{t=1}^{n}g_{t}(\theta_{0})}
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Itemize
We've already seen that 
\begin_inset Formula $\mathcal{E}_{\theta}\left[g_{t}(\theta)\right]=0,$
\end_inset

 for all 
\begin_inset Formula $\theta,$
\end_inset

 and,
 thus,
 for 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 too.
 Thus,
 the part in blue of the last line,
 without scaling by 
\begin_inset Formula $\sqrt{n}$
\end_inset

,
 would converge almost surely to zero,
 by a LLN (assuming the variances of the 
\begin_inset Formula $g_{t}$
\end_inset

 are bounded).
 To get a stable limiting distribution,
 we need to multiply by something that is tending to infinity,
 at the proper rate.
 It turns out that 
\begin_inset Formula $\sqrt{n}$
\end_inset

 is the quantity that fits the bill (see a proof of a CLT to understand why).
\end_layout

\begin_layout Itemize
Also,
 the elements of the sum are uncorrelated with one another (iterated expectations,
 above)
\end_layout

\begin_layout Itemize
As long as the 
\begin_inset Formula $g_{t}$
\end_inset

 have finite variances,
 a CLT  will apply.
 Checking this requires knowing what specific model we're working with,
 so for this general treatment,
 we will assume that the model satisfies this condition.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Assuming that a CLT applies:
 
\begin_inset Formula 
\begin{equation}
\sqrt{n}g_{n}(\theta_{0})\overset{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta_{0})\right]\label{eq:asymptoticnormalityofscores}
\end{equation}

\end_inset

 where 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta_{0}) & = & \lim_{n\rightarrow\infty}\mathcal{E}_{\theta_{0}}\left(n\left[g_{n}(\theta_{0})\right]\left[g_{n}(\theta_{0})\right]^{\prime}\right)\\
 & = & \lim_{n\rightarrow\infty}V_{\theta_{0}}\left(\sqrt{n}g_{n}(\theta_{0})\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

 is known as the 
\emph on
information matrix
\emph default
.
 It is the asymptotic variance of the score vector
\end_layout

\begin_layout Itemize
From the previous expression 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)=-\mathcal{J}(\theta^{*})^{-1}\sqrt{n}g(\theta_{0})$
\end_inset

,
 and noting that 
\begin_inset Formula $\mathcal{J}(\theta^{*})\overset{a.s.}{\rightarrow}\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

,
 we see that,
 following the 
\begin_inset CommandInset href
LatexCommand href
name "Slutsky theorem"
target "https://en.wikipedia.org/wiki/Slutsky%27s_theorem"
literal "false"

\end_inset


\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\rightarrow^{d}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right].
\]

\end_inset

 
\emph on
The MLE estimator is asymptotically normally distributed.
\end_layout

\begin_layout Itemize
The form of this expression is the same as what we saw for extremum estimators in general.
 This is no surprise,
 as the MLE is an extremum estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example,
 Coin flipping,
 again.
 Finding 
\begin_inset Formula $\mathcal{J}_{\infty}(p_{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{\infty}(p_{0})$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Coin-flipping,-again"

\end_inset


\end_layout

\begin_layout Standard
In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Example:-Bernoulli-trial"
nolink "false"

\end_inset

 we saw that the MLE for the parameter of a Bernoulli trial,
 with i.i.d.
 data,
 is the sample mean:
 
\begin_inset Formula $\hat{p}=\bar{y}$
\end_inset

 (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mle Bernoulli"
nolink "false"

\end_inset

).
 Now let's find the limiting variance of 
\begin_inset Formula $\sqrt{n}\left(\hat{p}-p_{0}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
We can do this in a simple way:
\begin_inset Formula 
\begin{eqnarray}
\lim Var\sqrt{n}\left(\hat{p}-p_{0}\right) & = & \lim nVar\left(\hat{p}-p_{0}\right)\nonumber \\
 & = & \lim nVar\left(\hat{p}\right)\nonumber \\
 & = & \lim nVar\left(\bar{y}\right)\nonumber \\
 & = & \lim nVar\left(\frac{\sum y_{t}}{n}\right)\nonumber \\
 & = & \lim\frac{1}{n}\sum Var(y_{t})\textnormal{ (by independence of obs.)}\nonumber \\
 & = & \lim\frac{1}{n}nVar(y)\textnormal{ (by identically distributed obs.)}\nonumber \\
 & = & Var(y)\nonumber \\
 & = & p_{0}\left(1-p_{0}\right)\label{eq:variance ML Bernoulli}
\end{eqnarray}

\end_inset

While that is simple,
 let's verify this using the methods we're studing in this chapter,
 because this simple method is not possible to use in general.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Paragraph

\series bold
Finding 
\series default

\begin_inset Formula $\mathcal{J}_{\infty}(p_{0})$
\end_inset


\end_layout

\begin_layout Standard
The log-likelihood function is
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(p) & = & \frac{1}{n}\sum_{t=1}^{n}\left\{ y_{t}\ln p+\left(1-y_{t}\right)\ln\left(1-p\right)\right\} 
\end{eqnarray*}

\end_inset

so
\begin_inset Formula 
\[
E_{p_{0}}s_{n}(p)=p_{0}\ln p+\left(1-p_{0}\right)\ln\left(1-p\right)
\]

\end_inset

by the fact that the observations are i.i.d.
 Note that this does not depend on the sample size,
 
\begin_inset Formula $n,$
\end_inset

 thus,
 
\begin_inset Formula 
\[
s_{\infty}(p)=p_{0}\ln p+\left(1-p_{0}\right)\ln\left(1-p\right).
\]

\end_inset

 A bit of calculation shows that
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(p_{0})=\left.D_{\theta}^{2}s_{\infty}(p)\right|_{p=p_{0}}=\frac{-1}{p_{0}\left(1-p_{0}\right)}.
\]

\end_inset

(note:
 we used dominated convergence).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Paragraph

\series bold
Finding 
\series default

\begin_inset Formula $\mathcal{I}_{\infty}(p_{0})$
\end_inset


\end_layout

\begin_layout Standard
As we have seen before,
 the score vector is 
\begin_inset Formula 
\[
g_{n}(p)=\frac{\partial s_{n}(p)}{\partial p}=\frac{1}{n}\sum_{t=1}^{n}\frac{y_{t}-p}{p\left(1-p\right)}.
\]

\end_inset

and we wish to compute the covariance of the score vector,
 after weighting by root-
\begin_inset Formula $n$
\end_inset

:
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathcal{I}_{\infty}(p_{0})=\lim_{n\rightarrow\infty}E_{p_{0}}\left(n\left[g_{n}(p_{0})\right]\left[g_{n}(p_{0})\right]^{\prime}\right)$
\end_inset

.
 Note that,
 playing with the 
\begin_inset Formula $n$
\end_inset

s 
\begin_inset Formula 
\begin{align*}
E_{p_{0}}\left(n\left[g_{n}(p_{0})\right]\left[g_{n}(p_{0})\right]^{\prime}\right) & ={\color{blue}n}E_{p_{0}}\left\{ \left({\color{blue}\frac{1}{n}}\sum_{t=1}^{n}\frac{y_{t}-p_{0}}{p_{0}\left(1-p_{0}\right)}\right)^{{\color{blue}{\color{blue}2}}}\right\} ,\\
 & ={\color{blue}\frac{1}{n}}{\color{purple}E_{p_{0}}\sum_{t=1}^{n}\left(\frac{y_{t}-p_{0}}{p_{0}\left(1-p_{0}\right)}\right)^{2}},
\end{align*}

\end_inset

where the last line follows due to the fact that the score contributions are uncorrelated,
 so that the expectations of the cross products are zero.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Next,
 the terms in the sum are i.i.d.,
 so the expection of the sum is 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color red

\begin_inset Formula $n$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 times the expectation of a representatiive term,
 so
\begin_inset Formula 
\begin{align*}
{\color{blue}\frac{1}{n}}{\color{purple}E_{p_{0}}\sum_{t=1}^{n}\left(\frac{y_{t}-p_{0}}{p_{0}\left(1-p_{0}\right)}\right)^{2}} & ={\color{green}\frac{{\color{blue}1}}{{\color{blue}n}}}{\color{red}n}E_{p_{0}}\left(\frac{y-p_{0}}{p_{0}\left(1-p_{0}\right)}\right)^{2}\\
 & =E_{p_{0}}\left(\frac{y-p_{0}}{p_{0}\left(1-p_{0}\right)}\right)^{2}\\
 & =\frac{1}{p_{0}^{2}\left(1-p_{0}\right)^{2}}E_{p_{0}}\left(y^{2}-2yp_{0}+p_{0}^{2}\right)\\
 & =\frac{1}{p_{0}^{2}\left(1-p_{0}\right)^{2}}(p_{0}-2p_{0}^{2}+p_{0}^{2})\qquad\mathrm{(because}\,E(y)=E(y^{2})=p_{0})\\
 & =\frac{1}{p_{0}(1-p_{0})}
\end{align*}

\end_inset

which does not depend on 
\begin_inset Formula $n,$
\end_inset

 so its limit is the same thing,
 and,
 therefore,
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(p_{0})=\frac{1}{p_{0}(1-p_{0})}.
\]

\end_inset

 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
So,
 combining things,
 we know that the theory says that 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\rightarrow^{d}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right].
\]

\end_inset

 In the present case,
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\begin{align*}
\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1} & =p^{0}\left(1-p^{0}\right)\frac{1}{p_{0}(1-p_{0})}p^{0}\left(1-p^{0}\right)\\
 & =p_{0}-p_{0}^{2},
\end{align*}

\end_inset

which is the same limiting variance that we computed directly,
 above,
 in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:variance ML Bernoulli"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The information matrix equality
\end_layout

\begin_layout Standard
We will show that 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta)=-\mathcal{I}_{\infty}(\theta).$
\end_inset

 
\end_layout

\begin_layout Itemize
The example we just looked at exhibits this property.
 
\end_layout

\begin_layout Itemize
Now,
 we will see that it's a general property of ML estimators.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f_{t}(\theta)$
\end_inset

 be short for 
\begin_inset Formula $f(y_{t}|x_{t},\theta)$
\end_inset

.
 This is a density function,
 which integrates to 1,
 so:
\begin_inset Formula 
\begin{eqnarray*}
1 & = & \int f_{t}(\theta)dy,\textrm{ so}\\
0 & = & \int D_{\theta}f_{t}(\theta)dy\\
 & = & \int\left(D_{\theta}\ln f_{t}(\theta)\right)f_{t}(\theta)dy
\end{eqnarray*}

\end_inset

 Now differentiate again,
 using the product rule,
 because 
\begin_inset Formula $\theta$
\end_inset

 appears in two terms that multiply one another:
\begin_inset Formula 
\begin{eqnarray}
0 & = & \int\left[D_{\theta}^{2}\ln f_{t}(\theta)\right]f_{t}(\theta)dy+\int\left[D_{\theta}\ln f_{t}(\theta)\right]{\color{green}{\color{blue}\left\{ D_{\theta^{\prime}}f_{t}(\theta)\right\} }}dy\nonumber \\
 & = & E_{\theta}\left[D_{\theta}^{2}\ln f_{t}(\theta)\right]+\int\left[D_{\theta}\ln f_{t}(\theta)\right]{\color{green}{\color{blue}\left\{ \left[D_{\theta^{\prime}}\ln f_{t}(\theta)\right]f_{t}(\theta)\right\} }}dy\nonumber \\
 & = & E_{\theta}\left[D_{\theta}^{2}\ln f_{t}(\theta)\right]+E_{\theta}\left[D_{\theta}\ln f_{t}(\theta)\right]\left[D_{\theta^{\prime}}\ln f_{t}(\theta)\right]\nonumber \\
 & = & E_{\theta}\left[\mathcal{J}_{t}(\theta)\right]+E_{\theta}\left[g_{t}(\theta)\right]\left[g_{t}(\theta)\right]^{\prime}\label{informationmatrixequality,singleobservation}
\end{eqnarray}

\end_inset


\begin_inset Newpage newpage
\end_inset

 Now sum over 
\begin_inset Formula $n$
\end_inset

 and multiply by 
\begin_inset Formula $\frac{1}{n}$
\end_inset


\begin_inset Formula 
\begin{equation}
E_{\theta}\frac{1}{n}\sum_{t=1}^{n}\left[\mathcal{J}_{t}(\theta)\right]={\color{violet}E_{\theta}\left[\mathcal{J}_{n}(\theta)\right]}={\color{blue}-E_{\theta}\left[\frac{1}{n}\sum_{t=1}^{n}\left[g_{t}(\theta)\right]\left[g_{t}(\theta)\right]^{\prime}\right]}\label{eq:outerproductsocrecontribs}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Itemize
The scores 
\begin_inset Formula $g_{t}$
\end_inset

 and 
\begin_inset Formula $g_{s}$
\end_inset

 are uncorrelated for 
\begin_inset Formula $t\neq s,$
\end_inset

 as we saw above (
\begin_inset CommandInset ref
LatexCommand ref
reference "prop:Law-of-iterated"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

).
 This allows us to write:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\color{violet}E_{\theta}\left[\mathcal{J}_{n}(\theta)\right]}={\color{red}-E_{\theta}\left(n\left[g_{n}(\theta)\right]\left[g_{n}(\theta)\right]^{\prime}\right)}.
\]

\end_inset

To see that this is equal to the expression in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:outerproductsocrecontribs"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

,
 recall that the score vector is the average of the score contributions:
 
\begin_inset Formula $g_{n}=\frac{1}{n}\sum_{t}g_{t}$
\end_inset

 (omitting the 
\begin_inset Formula $\theta$
\end_inset

).
 So,
 we have
\begin_inset Formula 
\begin{align*}
{\color{red}-E_{\theta}\left(n\left[g_{n}(\theta)\right]\left[g_{n}(\theta)\right]^{\prime}\right)} & =-E_{\theta}\frac{\left(\sum_{t}g_{t}\right)\left(\sum_{t}g_{t}^{\prime}\right)}{n}\\
 & =-E_{\theta}\frac{\left(g_{1}+g_{2}+...+g_{n}\right)\left(g_{1}^{\prime}+g_{2}^{\prime}+...+g_{n}^{\prime}\right)}{n}\\
 & ={\color{blue}-E_{\theta}\left[\frac{1}{n}\sum_{t=1}^{n}\left[g_{t}(\theta)\right]\left[g_{t}(\theta)\right]^{\prime}\right]},
\end{align*}

\end_inset

which is the original expression in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:outerproductsocrecontribs"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

,
 because all cross products between different periods expect to zero.
 Finally take limits,
 we get 
\begin_inset Formula 
\begin{equation}
\mathcal{J}_{\infty}(\theta)=-\mathcal{I}_{\infty}(\theta).\label{information matrix equality}
\end{equation}

\end_inset

 This holds for all 
\begin_inset Formula $\theta,$
\end_inset

 in particular,
 for 
\begin_inset Formula $\theta_{0}.$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset

Using this,
 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a.s.}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]
\]

\end_inset

 simplifies to 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a.s.}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta_{0})^{-1}\right]\label{Simple MLE asymp. cov.}
\end{equation}

\end_inset

or
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a.s.}{\rightarrow}N\left[0,-\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]\label{Simple MLE asymp. cov.-1}
\end{equation}

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\series bold
 To estimate the asymptotic variance
\series default
,
 we need estimators of 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

.
 We can use 
\begin_inset Formula 
\begin{eqnarray*}
\widehat{\mathcal{I}_{\infty}(\theta_{0})} & = & \frac{1}{n}\sum_{t=1}^{n}g_{t}(\hat{\theta})g_{t}(\hat{\theta})^{\prime}\\
\widehat{\mathcal{J}_{\infty}(\theta_{0})} & = & \mathcal{J}_{n}(\hat{\theta}).
\end{eqnarray*}

\end_inset

as is intuitive if one considers equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:outerproductsocrecontribs"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Note,
 one can't use
\begin_inset Formula 
\[
\widehat{I_{\infty}(\theta_{0})}=n\left[g_{n}(\hat{\theta})\right]\left[g_{n}(\hat{\theta})\right]^{\prime}
\]

\end_inset

to estimate the information matrix.
 Why not?
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
From this we see that there are alternative ways to estimate 
\begin_inset Formula $V_{\infty}(\theta_{0})$
\end_inset

 that are all valid.
 These include 
\begin_inset Formula 
\begin{eqnarray*}
\widehat{V_{\infty}(\theta_{0})} & = & -\widehat{\mathcal{J}_{\infty}(\theta_{0})}^{-1}\\
\widehat{V_{\infty}(\theta_{0})} & = & \widehat{\mathcal{I}_{\infty}(\theta_{0})}^{-1}\\
\widehat{V_{\infty}(\theta_{0})} & = & \widehat{\mathcal{J}_{\infty}(\theta_{0})}^{-1}\widehat{\mathcal{I}_{\infty}(\theta_{0})}\widehat{\mathcal{J}_{\infty}(\theta_{0})}^{-1}
\end{eqnarray*}

\end_inset

 These are known as the 
\emph on
inverse Hessian,
 outer product of the gradient
\emph default
 (OPG) and 
\emph on
sandwich
\emph default
 estimators,
 respectively.
 The sandwich form is the most robust,
 since it coincides with the covariance estimator of the 
\emph on
quasi-
\emph default
ML estimator.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
With a little more detail,
 the methods are:
\end_layout

\begin_layout Itemize
The sandwich version:
 
\begin_inset Formula 
\[
\widehat{V_{\infty}}=n\left\{ \begin{array}{c}
\left\{ \sum_{t=1}^{n}D_{\theta}^{2}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right\} \times\\
\left\{ \sum_{t=1}^{n}\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]^{\prime}\right\} ^{-1}\times\\
\left\{ \sum_{t=1}^{n}D_{\theta}^{2}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right\} 
\end{array}\right\} ^{-1}
\]

\end_inset


\end_layout

\begin_layout Itemize
or the inverse of the negative of the Hessian:
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widehat{V_{\infty}}=\left[-1/n\sum_{t=1}^{n}D_{\theta}^{2}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]^{-1},
\]

\end_inset


\end_layout

\begin_layout Itemize
or the inverse of the outer product of the gradient:
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widehat{V_{\infty}}=\left\{ 1/n\sum_{t=1}^{n}\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]^{\prime}\right\} ^{-1}.
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
This simplification is a special result for the MLE estimator - it 
\series bold
doesn't apply
\series default
 to extremum estimators in general.
\end_layout

\begin_layout Itemize
Asymptotically,
 if the model is correctly specified,
 all of these forms converge to the same limit.
 In small samples they will differ.
 In particular,
 there is evidence that the outer product of the gradient formula does not perform very well in small samples (
\emph on
e.g.,
 
\emph default
see Davidson and MacKinnon,
 pg.
 477).
 
\end_layout

\begin_layout Itemize
White's 
\emph on
Information matrix test
\emph default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "WhiteMisspecifiedML1982"
literal "false"

\end_inset

) is based upon comparing the two ways to estimate the information matrix:
 outer product of gradient or negative of the Hessian.
 If they differ by too much,
 this is evidence of misspecification of the model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Once we have the estimated asymptotic variance,
 
\begin_inset Formula $\widehat{V_{\infty}(\theta_{0})}$
\end_inset

,
 the 
\bar under
approximate
\bar default
 small-sample distribution of the estimator is
\begin_inset Formula 
\[
\hat{\theta}\approx N(\theta_{0},\frac{\widehat{V_{\infty}(\theta_{0})}}{n})
\]

\end_inset

 and this can be used to compute confidence intervals,
 test hypotheses,
 etc.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example:
 Maximum likelihood estimation using count data:
 The MEPS data and the Poisson model
\end_layout

\begin_layout Standard
To show optimization methods in practice,
 using real economic data,
 this section presents maximum likelihood estimation results for a particular model using real data.
 The focus at present is simply on numeric optimization.
 Later,
 after studying maximum likelihood estimation,
 this section can be read again.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Demand for health care is usually thought of a a derived demand:
 health care is an input to a home production function that produces health,
 and health is an argument of the utility function.
 Grossman (1972),
 for example,
 models health as a capital stock that is subject to depreciation (e.g.,
 the effects of ageing).
 Health care visits restore the stock.
 Under the home production framework,
 individuals decide when to make health care visits to maintain their health stock,
 or to deal with negative shocks to the stock in the form of accidents or illnesses.
 As such,
 individual demand will be a function of the parameters of the individuals' utility functions.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset CommandInset label
LatexCommand label
name "The-,-meps1996.data,"

\end_inset

The 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/meps1996.data}{MEPS health data file} 
\end_layout

\end_inset

,
 
\family typewriter
meps1996.data,

\family default
 contains 4564 observations on six measures of health care usage.
 The data is from the 1996 Medical Expenditure Panel Survey (MEPS).
 There are now more than 20 years of data!
 You can get more information at 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.meps.ahrq.gov/
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
The six measures of use are are
\end_layout

\begin_layout Itemize
office-based visits (OBDV)
\end_layout

\begin_layout Itemize
outpatient visits (OPV)
\end_layout

\begin_layout Itemize
inpatient visits (hospitalizations) (IPV)
\end_layout

\begin_layout Itemize
emergency room visits (ERV)
\end_layout

\begin_layout Itemize
dental visits (DV)
\end_layout

\begin_layout Itemize
and number of prescription drugs taken (PRESCR).
 These form columns 1 - 6 of 
\family typewriter
meps1996.data
\family default
.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The conditioning variables are 
\end_layout

\begin_layout Itemize
public insurance (PUBLIC)
\end_layout

\begin_layout Itemize
private insurance (PRIV)
\end_layout

\begin_layout Itemize
sex (SEX)
\end_layout

\begin_layout Itemize
age (AGE)
\end_layout

\begin_layout Itemize
years of education (EDUC)
\end_layout

\begin_layout Itemize
and income (INCOME).
\end_layout

\begin_layout Standard
These form columns 7 - 12 of the file
\family typewriter
,

\family default
 in the order given here.
 PRIV and PUBLIC are 0/1 binary variables,
 where a 1 indicates that the person has access to public or private insurance coverage.
 SEX is also 0/1,
 where 1 indicates that the person is female.
 This data will be used in several examples in what follows.
\end_layout

\begin_layout Itemize
Perhaps there may be common factors behind health care visits and the decision to buy private insurance.
 If there are common unobserved factors (perhaps,
 health,
 for example),
 then private insurace would be an endogenous variable.
 We will ignore that at present,
 but come back to it later.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The program 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/MEPS-I/ExploreMEPS.jl}{ExploreMEPS.jl} 
\end_layout

\end_inset

 shows how the data may be read in,
 and gives some descriptive information about variables,
 which follows:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status collapsed

\begin_layout Plain Layout

MEPS data,
 1996,
 complete data set statistics
\end_layout

\begin_layout Plain Layout

4564 observations
\end_layout

\begin_layout Plain Layout

            mean    st.
 dev.
         min         max  
\end_layout

\begin_layout Plain Layout

OBDV       3.279       6.171       0.000     133.000
\end_layout

\begin_layout Plain Layout

OPV        0.260       1.962       0.000      78.000
\end_layout

\begin_layout Plain Layout

IPV        0.194       0.637       0.000      17.000
\end_layout

\begin_layout Plain Layout

ERV        0.086       0.389       0.000       5.000
\end_layout

\begin_layout Plain Layout

DV         1.054       1.875       0.000      32.000
\end_layout

\begin_layout Plain Layout

RX         8.384      18.852       0.000     316.000
\end_layout

\begin_layout Plain Layout

PUB        0.141       0.334       0.000       1.000
\end_layout

\begin_layout Plain Layout

PRIV       0.674       0.449       0.000       1.000
\end_layout

\begin_layout Plain Layout

SEX        0.517       0.500       0.000       1.000
\end_layout

\begin_layout Plain Layout

AGE       39.354      12.198      18.000      65.000
\end_layout

\begin_layout Plain Layout

EDUC      12.652       2.896       0.000      17.000
\end_layout

\begin_layout Plain Layout

INC    42803.630   34108.362       0.000  250463.330
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
All of the measures of use are count data,
 which means that they take on the values 
\begin_inset Formula ${0,1,2,...}$
\end_inset

.
 It might be reasonable to try to use this information by specifying the density as a count data density.
 One of the simplest count data densities is the Poisson density,
 which is
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y) & = & \frac{\exp(-\lambda)\lambda^{y}}{y!},\,y=0,1,2,....
\end{eqnarray*}

\end_inset


\color blue
For this density,
 
\begin_inset Formula $E(Y)=V(Y)=\lambda$
\end_inset

.
 
\end_layout

\begin_layout Standard
The Poisson average log-likelihood function (assuming the cross sectional observations are independent of one another,
 so the joint density is the product of the marginals) is 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\left(-\lambda_{i}+y_{i}\ln\lambda_{i}-\ln y_{i}!\right)
\]

\end_inset

We will parameterize the model as
\begin_inset Formula 
\begin{eqnarray}
\lambda_{i} & = & \exp(\mathbf{x}_{i}^{\prime}\beta)\nonumber \\
\mathbf{x}_{i} & = & [1\,\,PUBLIC\,\,PRIV\,\,SEX\,\,AGE\,\,EDUC\,\,INC]^{\prime}\label{eq:Poisson model OBDV}
\end{eqnarray}

\end_inset

This ensures that the mean is positive,
 as is required for the Poisson model,
 and now the mean (and the variance) depend upon explanatory variables.
 Note that for this parameterization
\begin_inset Formula 
\[
\frac{\partial\lambda}{\partial x_{j}}=\lambda\beta_{j},
\]

\end_inset

so the elasticity of the conditional mean of 
\begin_inset Formula $y$
\end_inset

 with respect to the 
\begin_inset Formula $j^{th}$
\end_inset

 conditioning variable is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\lambda}{\partial x_{j}}\frac{x_{j}}{\lambda}=\beta_{j}x_{j}
\]

\end_inset

 Thus,
 the interpretation of the parameters is the same as for a semi-log linear regression model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/MEPS-I/EstimatePoisson.jl}{EstimatePoisson.jl}
\end_layout

\end_inset

 estimates a Poisson model using the full data set.
 The results of the estimation,
 using OBDV as the dependent variable are here:
 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MEPS-I/PoissonOBDV.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Examine the code in 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/MEPS-I/EstimatePoisson.jl}{EstimatePoisson.jl}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./src/ML/mle.jl}{mle.jl}
\end_layout

\end_inset

 to figure out how the variance-covariance of the parameters has been estimated.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:PoissonOBDV_results"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 show the results using the sandwich and OPG forms.
 Note the radical changes in the t-statistics.
 This probably indicates that the Poisson model is not well-specified,
 following the logic of the information matrix test.
 When the t-statistics change so much,
 it means that the estimates of 
\begin_inset Formula $I_{\infty}$
\end_inset

 and 
\begin_inset Formula $-J_{\infty}$
\end_inset

 are far from one another.
\end_layout

\begin_layout Exercise
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:PoissonOBDV_results"

\end_inset

Alternative variance computations for the OBDV Poisson model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
OPG
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/MLE/AlternativeVariances1.png
	scale 50

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Sandwich
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/MLE/AlternativeVariances2.png
	scale 50

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The Cramér-Rao lower bound
\end_layout

\begin_layout Definition
Consistent and asymptotically normal (CAN).
 
\begin_inset CommandInset label
LatexCommand label
name "def:CAN"

\end_inset

An estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 of a parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

 is 
\begin_inset Formula $\sqrt{n}$
\end_inset

-consistent and asymptotically normally distributed if 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{d}{\rightarrow}N\left(0,V_{\infty}\right)$
\end_inset

 where 
\begin_inset Formula $V_{\infty}$
\end_inset

 is a finite positive definite matrix.
\end_layout

\begin_layout Standard
There do exist,
 in special cases,
 estimators that are consistent such that 
\begin_inset Formula $\mbox{\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{p}{\rightarrow}0.}$
\end_inset

 These are known as 
\emph on
superconsistent
\emph default
 estimators,
 since in ordinary circumstances with stationary data,
 
\begin_inset Formula $\sqrt{n}$
\end_inset

 is the highest factor that we can multiply by and still get convergence to a stable limiting distribution.
 
\end_layout

\begin_layout Definition
Asymptotically unbiased.
 An estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 of a parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

 is asymptotically unbiased if 
\end_layout

\begin_layout Standard
\begin_inset Formula $\lim_{n\rightarrow\infty}\mathcal{E_{\theta}}(\hat{\theta})=\theta$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Estimators that are CAN are asymptotically unbiased
\emph default
,
 though not all consistent estimators are asymptotically unbiased.
 Such cases are unusual,
 though.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Theorem

\emph on
[Cramer-Rao Lower Bound]
\emph default
 
\begin_inset CommandInset label
LatexCommand label
name "[Cramer-Rao-Lower-Bound]"

\end_inset

The limiting variance of a CAN estimator of 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 say 
\begin_inset Formula $\tilde{\theta}$
\end_inset

,
 minus the inverse of the information matrix is a positive semidefinite matrix.
\end_layout

\begin_layout Theorem
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Proof:
 Since the estimator is CAN,
 it is asymptotically unbiased,
 so 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}_{\theta}(\tilde{\theta}-\theta)=0
\]

\end_inset

 Differentiate wrt 
\begin_inset Formula $\theta^{\prime}$
\end_inset

,
 and make liberal use of dominated convergence:
\begin_inset Formula 
\begin{eqnarray*}
D_{\theta^{\prime}}\lim_{n\rightarrow\infty}\mathcal{E}_{\theta}(\tilde{\theta}-\theta) & = & \lim_{n\rightarrow\infty}\int D_{\theta^{\prime}}\left[\left(\tilde{\theta}-\theta\right)f(Y,\theta)\right]dy\\
 & = & 0.\textrm{ }
\end{eqnarray*}

\end_inset

(The RHS is zero,
 because we're differentiating something that is zero to start with).
 
\end_layout

\begin_layout Standard
Now,
 take the RHS,
 and differentiate.
 Noting that 
\begin_inset Formula $D_{\theta^{\prime}}f(\theta)=f(\theta)D_{\theta^{\prime}}\ln f(\theta)$
\end_inset

 (a trick we have seen a few times already),
 and applying the product rule to differentiate the two parts that depend on 
\begin_inset Formula $\theta,$
\end_inset

 we can write 
\begin_inset Formula 
\[
{\color{green}{\color{red}\lim_{n\rightarrow\infty}\int\left(\tilde{\theta}-\theta\right)f(\theta)D_{\theta^{\prime}}\ln f(\theta)dy}}+{\color{blue}\lim_{n\rightarrow\infty}\int\left[D_{\theta^{\prime}}\left(\tilde{\theta}-\theta\right)\right]f(\theta)dy}=0.
\]

\end_inset

 Now note that 
\begin_inset Formula ${\color{blue}D_{\theta^{\prime}}\left(\tilde{\theta}-\theta\right)=-I_{K}},$
\end_inset

 and 
\begin_inset Formula ${\color{blue}\int f(\theta)(-I_{K})dy=-I_{K}}.$
\end_inset

 With this we have 
\begin_inset Formula 
\[
{\color{green}{\color{red}\lim_{n\rightarrow\infty}\int\left(\tilde{\theta}-\theta\right)f(\theta)D_{\theta^{\prime}}\ln f(\theta)dy}}={\color{blue}I_{K}}.
\]

\end_inset

 Playing with powers of 
\begin_inset Formula $n$
\end_inset

 on the LHS,
 we get 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}\int\sqrt{n}\left(\tilde{\theta}-\theta\right){\color{purple}\sqrt{n}\underbrace{\frac{1}{n}\left[D_{\theta^{\prime}}\ln f(\theta)\right]}}f(\theta)dy & = & I_{K}
\end{eqnarray*}

\end_inset

 Note that the bracketed part is just the transpose of the score vector,
 
\begin_inset Formula $g(\theta),$
\end_inset

 so we can write 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}_{\theta}\left[\sqrt{n}\left(\tilde{\theta}-\theta\right){\color{purple}\sqrt{n}g(\theta)^{\prime}}\right]=I_{K}
\]

\end_inset

 This means that the limiting covariance of the score function with 
\begin_inset Formula $\sqrt{n}\left(\tilde{\theta}-\theta\right),$
\end_inset

 for 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 
\series bold
any
\series default
 CAN estimator,
 is an identity matrix.
 Using this,
 suppose the variance of 
\begin_inset Formula $\sqrt{n}\left(\tilde{\theta}-\theta\right)$
\end_inset

 tends to 
\begin_inset Formula $V_{\infty}(\tilde{\theta}).$
\end_inset

 Therefore,
 
\begin_inset Formula 
\begin{equation}
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}g(\theta)
\end{array}\right]=\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right].\label{Cov. CAN and MLE score}
\end{equation}

\end_inset

 Since this is a covariance matrix,
 it is positive semi-definite.
 Therefore,
 for any 
\begin_inset Formula $K$
\end_inset

 -vector 
\begin_inset Formula $\alpha,$
\end_inset


\begin_inset Formula 
\[
\left[\begin{array}{cc}
\alpha^{\prime} & -\alpha^{\prime}\mathcal{I}_{\infty}^{-1}(\theta)\end{array}\right]\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right]\left[\begin{array}{c}
\alpha\\
-\mathcal{I}_{\infty}(\theta)^{-1}\alpha
\end{array}\right]\geq0.
\]

\end_inset

 This simplifies to 
\begin_inset Formula 
\[
\alpha^{\prime}\left[V_{\infty}(\tilde{\theta})-\mathcal{I}_{\infty}^{-1}(\theta)\right]\alpha\geq0.
\]

\end_inset

 Since 
\begin_inset Formula $\alpha$
\end_inset

 is arbitrary,
 
\begin_inset Formula $V_{\infty}(\tilde{\theta})-\mathcal{I}_{\infty}^{-1}(\theta)$
\end_inset

 must be positive semidefinite.
 This concludes the proof.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
Interpretation:
\end_layout

\begin_layout Itemize
The above is taking expectations w.r.t.
 
\begin_inset Formula $f(\theta).$
\end_inset

 When we do this for the true density of the data,
 it is w.r.t.
 
\begin_inset Formula $f(\theta_{0}),$
\end_inset

 and the result will be
\begin_inset Formula 
\[
V_{\infty}(\tilde{\theta})-\mathcal{I}_{\infty}^{-1}(\theta_{0})
\]

\end_inset

 is positive semidefinite.
 However,
 the asymptotic variance of the ML estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is 
\begin_inset Formula $\mathcal{I}_{\infty}^{-1}(\theta_{0})$
\end_inset

,
 so we can say that
\begin_inset Formula 
\[
V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})
\]

\end_inset

 is positive semidefinite.
\end_layout

\begin_layout Itemize
An informal way of indicating this is to write
\begin_inset Formula 
\[
V_{\infty}(\tilde{\theta})\ge V_{\infty}(\hat{\theta})
\]

\end_inset


\end_layout

\begin_layout Itemize
To interpret this,
 it means that any linear combination of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 will have an asymptotic variance greater than or equal to the same linear combination of the ML estimator.
\end_layout

\begin_layout Itemize

\emph on
e.g.,
 
\emph default
the individual variances of each 
\begin_inset Formula $\tilde{\theta_{j}}$
\end_inset

 will be no smaller than the corresponding element of the ML estimator,
 
\begin_inset Formula $j=1,2,...,k$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathcal{I}_{\infty}^{-1}(\theta)$
\end_inset

 is a 
\emph on
lower bound
\emph default
 for the asymptotic variance of a CAN estimator.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
(
\emph on
Asymptotic efficiency
\emph default
) Given two CAN estimators of a parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 say 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}$
\end_inset

,
 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is asymptotically efficient with respect to 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 if 
\begin_inset Formula $V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})$
\end_inset

 is a positive semidefinite matrix.
\end_layout

\begin_layout Itemize

\emph on
the MLE is asymptotically efficient with respect to any other CAN estimator.
\end_layout

\begin_layout Itemize
this is the reason that the ML estimator is so important:
 it provides a benchmark for efficiency.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The strong assumptions on which it depends may be questionable,
 though.
\end_layout

\begin_layout Itemize
If we can find another estimator that obtains an asymptotic variance similar to that of ML,
 but is consistent under weaker assumptions,
 we might choose to use it instead.
 But it's useful to know that this entails a potential loss of efficiency.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Section
Likelihood ratio-type tests
\end_layout

\begin_layout Standard
Suppose we would like to test a set of 
\begin_inset Formula $q$
\end_inset

 possibly nonlinear restrictions 
\begin_inset Formula $r(\theta)=0,$
\end_inset

 where the 
\begin_inset Formula $q\times k$
\end_inset

 matrix 
\begin_inset Formula $D_{\theta^{\prime}}r(\theta)$
\end_inset

 has rank 
\begin_inset Formula $q$
\end_inset

.
 The Wald test can be calculated using the unrestricted model.
 The score test can be calculated using only the restricted model.
 The likelihood ratio test,
 on the other hand,
 uses both the restricted and the unrestricted estimators.
 The test statistic is 
\begin_inset Formula 
\[
LR=2\left(\ln L(\hat{\theta})-\ln L(\tilde{\theta})\right)=2n\left[s_{n}(\hat{\theta})-s_{n}(\tilde{\theta})\right]
\]

\end_inset

where 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the unrestricted estimate and 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is the restricted estimate.
 We will show that,
 under the null hypothesis that 
\begin_inset Formula $r(\theta)=0$
\end_inset

,
 
\begin_inset Formula 
\[
LR\overset{d}{\rightarrow}\chi^{2}(q).
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\emph on
The following sketch of a proof should be read carefully by students with a special interest in econometric theory,
 but it can be scanned lightly by most students.
\end_layout

\begin_layout Standard
To show that it is asymptotically 
\begin_inset Formula $\chi^{2},$
\end_inset

 take a second order Taylor's series expansion of 
\begin_inset Formula $\ln L(\tilde{\theta})$
\end_inset

 about 
\begin_inset Formula $\hat{\theta}:$
\end_inset


\begin_inset Formula 
\[
\ln L(\tilde{\theta})\simeq\ln L(\hat{\theta})+\frac{n}{2}\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\mathcal{J}(\hat{\theta})\left(\tilde{\theta}-\hat{\theta}\right)
\]

\end_inset

 (note,
 the first order term drops out since 
\begin_inset Formula $D_{\theta}\ln L(\hat{\theta})\equiv0$
\end_inset

 by the first order necessary conditions,
 and we need to multiply the second-order term by 
\begin_inset Formula $n$
\end_inset

 since 
\begin_inset Formula $\mathcal{J}(\theta)$
\end_inset

 is defined in terms of 
\begin_inset Formula $\frac{1}{n}\ln L(\theta)$
\end_inset

) so 
\begin_inset Formula 
\[
LR\simeq-n\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\mathcal{J}(\hat{\theta})\left(\tilde{\theta}-\hat{\theta}\right)
\]

\end_inset

 As 
\begin_inset Formula $n\rightarrow\infty,\mathcal{J}(\hat{\theta})\rightarrow\mathcal{J}_{\infty}(\theta_{0})=-\mathcal{I}(\theta_{0}),$
\end_inset

 by the information matrix equality.
 So 
\begin_inset Formula 
\begin{equation}
LR\overset{a}{=}n\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\mathcal{I}_{\infty}(\theta_{0})\left(\tilde{\theta}-\hat{\theta}\right)\label{eq:LR2}
\end{equation}

\end_inset

 We also have that,
 from the theory on the asymptotic normality of the MLE and the information matrix equality 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a}{=}{\color{green}{\color{blue}\sqrt{n}\mathcal{I}_{\infty}(\theta_{0})^{-1}}}{\color{magenta}I_{n}}{\color{red}g(\theta_{0})}.
\]

\end_inset

An analogous result for the restricted estimator is (this is unproven here,
 to prove this set up the Lagrangean for MLE subject to 
\begin_inset Formula $r(\theta)=0,$
\end_inset

 and manipulate the first order conditions.
 Also note,
 
\begin_inset Formula $R$
\end_inset

 is notation for 
\begin_inset Formula $R=D_{\theta}r^{\prime}(\theta)$
\end_inset

):
 
\begin_inset Formula 
\[
\sqrt{n}\left(\tilde{\theta}-\theta_{0}\right)\overset{a}{=}{\color{green}{\color{blue}\sqrt{n}\mathcal{I}_{\infty}(\theta_{0})^{-1}}}{\color{magenta}\left(I_{n}-R^{\prime}\left(R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\right)^{-1}R\mathcal{I}_{\infty}(\theta_{0})^{-1}\right)}{\color{red}g(\theta_{0})}.
\]

\end_inset

Subtracting the penultimate equation from the last one,
 we get (the magenta colored term below is the difference between the two magenta terms in the last two lines,
 with the minus sign moved out to the front) 
\begin_inset Formula 
\[
\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right)\overset{a}{=}-{\color{green}{\color{blue}\sqrt{n}\mathcal{I}_{\infty}(\theta_{0})^{-1}}}{\color{magenta}R^{\prime}\left(R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\right)^{-1}R\mathcal{I}_{\infty}(\theta_{0})^{-1}}{\color{red}g(\theta_{0})}
\]

\end_inset

 so,
 substituting into [
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LR2"
nolink "false"

\end_inset

] ,
 we get some nice cancellations,
 which lead to (note:

\emph on
 renewing use of colors here
\emph default
)
\begin_inset Formula 
\[
LR\overset{a}{=}\left[n^{1/2}g(\theta_{0})^{\prime}\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\right]\left[{\color{brown}{\color{orange}{\color{blue}R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}}}}\right]^{-1}\left[{\color{blue}{\color{purple}R\mathcal{I}_{\infty}(\theta_{0})^{-1}n^{1/2}g(\theta_{0})}}\right]
\]

\end_inset

 But since 
\begin_inset Formula 
\[
n^{1/2}g(\theta_{0})\overset{d}{\rightarrow}N\left(0,\mathcal{I}_{\infty}(\theta_{0})\right)
\]

\end_inset

 the linear function 
\begin_inset Formula 
\[
{\color{blue}{\color{purple}R\mathcal{I}_{\infty}(\theta_{0})^{-1}n^{1/2}g(\theta_{0})}}\overset{d}{\rightarrow}N(0,{\color{brown}{\color{blue}R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}}}).
\]

\end_inset

 We can see that LR is a quadratic form of this random variable,
 with the inverse of its variance in the middle,
 so,
 by the 
\begin_inset CommandInset href
LatexCommand href
name "Continuous Mapping Theorem"
target "https://en.wikipedia.org/wiki/Continuous_mapping_theorem"
literal "false"

\end_inset

 (also known as the Mann-Wald Theorem) (see 
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "true"

\end_inset

 Theorem 4.7 for a statement):
\begin_inset Formula 
\[
LR\overset{d}{\rightarrow}\chi^{2}(q).
\]

\end_inset


\end_layout

\begin_layout Example

\emph on
Likelihood ratio test
\emph default
.
 Continuing with the same code as was used in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Likelihood-function-of"
nolink "false"

\end_inset

,
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/MLE/LikelihoodRatioTest.jl}{LikelihoodRatioTest.jl} 
\end_layout

\end_inset

 computes the LR statistic for a simple linear 
\begin_inset Formula $H_{0}$
\end_inset

.
 This uses the Julia function fmincon to perform the restricted estimation.
\end_layout

\begin_layout Itemize
run the test a number of times to explore size
\end_layout

\begin_layout Itemize
change the value of 
\begin_inset Formula $r$
\end_inset

 in the null hypothesis 
\begin_inset Formula $R\beta=r$
\end_inset

 to make it false,
 and run the test a number of times,
 to check power.
\end_layout

\begin_layout Itemize
explore how power depends on the sample size
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*

\series bold
Summary of MLE
\end_layout

\begin_layout Itemize
Consistent
\end_layout

\begin_layout Itemize
Asymptotically normal (CAN)
\end_layout

\begin_layout Itemize
Asymptotically efficient
\end_layout

\begin_layout Itemize
Asymptotically unbiased
\end_layout

\begin_layout Itemize
LR test is available for testing hypothesis
\end_layout

\begin_layout Itemize
The presentation is for general MLE:
 we haven't specified the distribution or the linearity/nonlinearity of the estimator
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Standard
This section is quite long winded,
 with several examples of ML estimation.
 Just focusing on a couple of examples should be sufficient for most students.
\end_layout

\begin_layout Subsection
ML of Nerlove model,
 assuming normality
\end_layout

\begin_layout Standard
(We have seen this before,
 but here it is again,
 in case you can't find it.) As we saw in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Asymptotic-efficiency"
nolink "false"

\end_inset

,
 the ML and OLS estimators of 
\begin_inset Formula $\beta$
\end_inset

 in the linear model 
\begin_inset Formula $y=X\beta+\epsilon$
\end_inset

 coincide when 
\begin_inset Formula $\epsilon$
\end_inset

 is assumed to be i.i.d.
 normally distributed.
 The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/MLE/NerloveMLE.jl}{NerloveMLE.jl} 
\end_layout

\end_inset

 verifies this result,
 for the basic Nerlove model (eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "simple nerlove model"
nolink "false"

\end_inset

).
 The output of the script follows:
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MLE/NerloveMLE.out"
literal "true"

\end_inset

Compare the output to that of 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/Nerlove.jl}{Nerlove.jl} 
\end_layout

\end_inset

,
 which does OLS.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example:
 Binary response models:
 theory
\end_layout

\begin_layout Standard
This section extends the Bernoulli trial model to binary response models with conditioning variables,
 as such models arise in a variety of contexts.
\end_layout

\begin_layout Standard
Assume that
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & x^{\prime}\theta-\epsilon\\
y & = & 1(y^{*}>0)
\end{eqnarray*}

\end_inset

Here,
 
\begin_inset Formula $y^{*}$
\end_inset

 is an unobserved (latent) continuous variable,
 and 
\begin_inset Formula $y$
\end_inset

 is a binary variable that indicates whether 
\begin_inset Formula $y^{*}$
\end_inset

is negative or positive.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The variable 
\begin_inset Formula $y$
\end_inset

 is a binary response is a variable that takes on only two values,
 customarily 0 and 1,
 which can be thought of as codes for whether or not a condition is satisfied.
 For example,
 0=drive to work,
 1=take the bus.
 
\end_layout

\begin_layout Standard
We have
\begin_inset Formula 
\begin{eqnarray*}
Pr(y=1) & = & F_{\epsilon}[x^{\prime}\theta]\\
 & \equiv & p(x,\theta).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The log-likelihood function is the Bernoulli:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}\ln p(x_{i},\theta)+(1-y_{i})\ln\left[1-p(x_{i},\theta)\right]\right)
\]

\end_inset

We need to parameterize the probability that 
\begin_inset Formula $y=1,$
\end_inset

which will depend on the distribution of 
\begin_inset Formula $\epsilon$
\end_inset

:
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\varepsilon\sim N(0,1)$
\end_inset

 Then the 
\emph on
probit 
\emph default
model results,
 where 
\begin_inset Formula $Pr(y=1|x)=Pr(\varepsilon<x^{\prime}\theta)=\Phi(x^{\prime}\theta)$
\end_inset

,
 where 
\begin_inset Formula 
\[
\Phi(\cdot)=\int_{-\infty}^{x\beta}(2\pi)^{-1/2}\exp(-\frac{\varepsilon^{2}}{2})d\varepsilon
\]

\end_inset

is the standard normal distribution function.
\end_layout

\begin_layout Itemize
The 
\emph on
logit 
\emph default
model results if the errors 
\begin_inset Formula $\epsilon$
\end_inset

 are not normal,
 but rather have a logistic distribution.
 This distribution is similar to the standard normal,
 but has fatter tails.
 The probability,
 in this case,
 has the following parameterization
\begin_inset Formula 
\[
Pr(y=1|x)=\Lambda(x^{\prime}\theta)=\left(1+\exp(-x^{\prime}\theta)\right)^{-1}.
\]

\end_inset


\end_layout

\begin_layout Itemize
In general,
 a binary response model will require that the choice probability be parameterized in some form which could be logit,
 probit,
 or something else.
 For a vector of explanatory variables 
\begin_inset Formula $x$
\end_inset

,
 the response probability will be parameterized in some manner
\begin_inset Formula 
\[
Pr(y=1|x)=p(x,\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
Again,
 if 
\begin_inset Formula $p(x,\theta)=\Lambda(x^{\prime}\theta),$
\end_inset

 we have a logit model.
 If 
\begin_inset Formula $p(x,\theta)=\Phi(x^{\prime}\theta),$
\end_inset

 where 
\begin_inset Formula $\Phi(\cdot)$
\end_inset

 is the standard normal distribution function,
 then we have a probit model.
 These two models give very similar coefficient estimates,
 apart from a scaling factor that is due to the fatter tails of the logistic distribution.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Estimation of the logit model
\begin_inset CommandInset label
LatexCommand label
name "subsec:Discrete-Choice:logit model"

\end_inset


\end_layout

\begin_layout Standard
In this section we will consider maximum likelihood estimation of the logit model for binary 0/1 dependent variables.
\end_layout

\begin_layout Standard
As above,
 for the logit model,
 the probability has the specific form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(x,\theta)=\frac{1}{1+\exp(-x^{\prime}\theta)}
\]

\end_inset


\end_layout

\begin_layout Standard
You should download and examine 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/MLE/LogitDGP.jl}{LogitDGP.jl} 
\end_layout

\end_inset

,
 which generates data according to the logit model,
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./src/ML/Likelihoods/logit.jl}{logit.jl} 
\end_layout

\end_inset

,
 which calculates the loglikelihood,
 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/MLE/EstimateLogit.jl}{EstimateLogit.jl} 
\end_layout

\end_inset

,
 which sets things up and calls the estimation routine.
 
\end_layout

\begin_layout Standard
\paragraph_spacing single
Here are some estimation results with 
\begin_inset Formula $n=30,$
\end_inset

 and the true 
\begin_inset Formula $\theta=(0,1)^{\prime}.$
\end_inset

 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MLE/Logit.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The estimation program is calling 
\family typewriter
mleresults
\family default
.jl,
 which in turn calls other routines.
 See 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./src/ML/mleresults.jl}{mleresults.jl}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./src/ML/mle.jl}{mle.jl}
\end_layout

\end_inset

 for the details.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example:
 the MEPS Data
\end_layout

\begin_layout Standard
We first saw the MEPS data in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"
nolink "false"

\end_inset

,
 where a Poisson model was estimated by maximum likelihood.
 To check the plausibility of the Poisson model for the MEPS data,
 we can compare the sample unconditional variance with the estimated unconditional variance that is implied by the Poisson model:
 
\begin_inset Formula $\widehat{V(y)}=\frac{\sum_{t=1}^{n}\hat{\lambda}_{t}}{n}$
\end_inset

.
 (remember that,
 for the Poisson model,
 the parameter 
\begin_inset Formula $\lambda$
\end_inset

 is both the mean and the variance).
 Using the program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/MEPS-I/PoissonVariance.jl}{PoissonVariance.jl}
\end_layout

\end_inset

,
 for OBDV and ERV,
 we get the results in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Marginal-Variances,-Sample"
nolink "false"

\end_inset

.
 
\begin_inset Float table
placement htbp
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Marginal-Variances,-Sample"

\end_inset

Marginal Variances,
 Sample and Estimated (Poisson)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sample
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
38.09
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.151
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Estimated
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.28
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.086
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

We see that even after conditioning,
 the overdispersion is not captured in either case.
 There is huge problem with OBDV,
 and a significant problem with ERV.
 In both cases the Poisson model does not appear to be plausible.
 You can check this for the other use measures if you like.
\end_layout

\begin_layout Subsubsection
Infinite mixture models:
 the negative binomial model
\begin_inset CommandInset label
LatexCommand label
name "subsec:Infinite-mixture-models:"

\end_inset


\end_layout

\begin_layout Standard
Reference:
 Cameron and Trivedi (1998) 
\emph on
Regression analysis of count data,

\emph default
 chapter 4.
\end_layout

\begin_layout Standard
The two measures seem to exhibit extra-Poisson variation.
 To capture unobserved heterogeneity,
 a possibility is the 
\emph on
random parameters
\emph default
 approach.
 Consider the possibility that the parameter in a Poisson model were random:
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y|\lambda,v) & = & \frac{\exp(-\theta)\theta^{y}}{y!}
\end{eqnarray*}

\end_inset

where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\theta=\lambda v$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
.
 
\end_layout

\begin_layout Itemize
Now 
\begin_inset Formula $\nu$
\end_inset

 is a multiplicative random term.
 The problem is that we don't observe 
\begin_inset Formula $\nu$
\end_inset

,
 so we will need to marginalize it to get a usable density.
 
\end_layout

\begin_layout Itemize
Suppose that 
\begin_inset Formula $f_{v}(z;\psi)$
\end_inset

 is the density of the latent term.
 We will need to assume that this is known - it is part of the specification of the econometric model.
 
\end_layout

\begin_layout Itemize
Then,
 the marginal density of 
\begin_inset Formula $Y$
\end_inset

 is
\begin_inset Formula 
\[
f_{Y}(y|\lambda,\psi)=\int_{-\infty}^{\infty}\frac{\exp[-\lambda z]\left[\lambda z\right]^{y}}{y!}f_{v}(z;\psi)dz
\]

\end_inset

This density 
\emph on
can
\emph default
 be used directly,
 perhaps using numerical integration to evaluate the likelihood function.
 Simulation-based approaches are another possibility.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In some cases,
 though,
 the integral will have an analytic solution.
 For example,
 if 
\begin_inset Formula $\nu$
\end_inset

 follows a certain one parameter gamma density,
 then 
\begin_inset Formula 
\begin{equation}
f_{Y}(y|\lambda,\psi)=\frac{\Gamma(y+\psi)}{\Gamma(y+1)\Gamma(\psi)}p^{\psi}(1-p)^{y}\label{eq:negbindensity}
\end{equation}

\end_inset

where 
\begin_inset Formula $p=\frac{\psi}{\psi+\lambda}$
\end_inset

.
 
\begin_inset Formula $\psi$
\end_inset

 appears since it is the parameter of the gamma density.
 The above density is the 
\emph on
negative binomial 
\emph default
density.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "gourieroux1984pseudo"
literal "false"

\end_inset

 for an influential paper on the topic.
\end_layout

\begin_layout Itemize
We usually parameterize 
\begin_inset Formula $\lambda=\exp(\mathbf{x}'\beta)$
\end_inset

,
 as before,
 for the Poisson model.
\end_layout

\begin_layout Itemize
The variance depends upon how 
\begin_inset Formula $\psi$
\end_inset

 is parameterized.
 
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $\psi=\lambda/\alpha$
\end_inset

,
 where 
\begin_inset Formula $\alpha>0$
\end_inset

,
 then 
\begin_inset Formula $V(y|\mathbf{x})=\lambda+\alpha\lambda$
\end_inset

.
 Note that 
\begin_inset Formula $\lambda$
\end_inset

 is a function of 
\begin_inset Formula $\mathbf{x}$
\end_inset

,
 so that the variance is,
 too.
 This is referred to as the NB-I model.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\psi=1/\alpha$
\end_inset

,
 where 
\begin_inset Formula $\alpha>0$
\end_inset

,
 then 
\begin_inset Formula $V(y|\mathbf{x})=\lambda+\alpha\lambda^{2}$
\end_inset

.
 This is referred to as the NB-II model.
\end_layout

\begin_layout Itemize
In both cases,
 the conditional mean is 
\begin_inset Formula $E(y|\boldsymbol{x})=\lambda=\exp(\mathbf{x}'\beta)$
\end_inset

,
 and the density is parameterized as 
\begin_inset Formula $f_{Y}(y|\boldsymbol{x},\theta)$
\end_inset

 where 
\begin_inset Formula $\theta=(\beta,\alpha)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
So both forms of the NB model allow for overdispersion,
 with the NB-II model allowing for a more radical form.
 There are other parameterizations,
 but these two are the most widely-used.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Testing reduction of a NB model to a Poisson model cannot be done by testing 
\begin_inset Formula $\alpha=0$
\end_inset

 using standard Wald or LR procedures.
 The critical values need to be adjusted to account for the fact that 
\begin_inset Formula $\alpha=0$
\end_inset

 is on the boundary of the parameter space.
\end_layout

\begin_layout Itemize
Without getting into details,
 suppose that the data were in fact Poisson,
 so there is equidispersion and the true 
\begin_inset Formula $\alpha=0$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Then about half the time the sample data will be underdispersed,
 and about half the time overdispersed.
\end_layout

\begin_layout Itemize
When the data is underdispersed,
 the MLE of 
\begin_inset Formula $\alpha$
\end_inset

 will be 
\begin_inset Formula $\hat{\alpha}=0$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Thus,
 under the null,
 there will be a probability spike in the asymptotic distribution of 
\begin_inset Formula $\sqrt{n(}\hat{\alpha}-\alpha)=\sqrt{n}\hat{\alpha}$
\end_inset

 at 0,
 so the asymptotic distribution will not be normal,
 and standard testing methods will not be valid.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/MEPS-II/EstimateNegBin.jl}{This program} 
\end_layout

\end_inset

 will do estimation using the NB model .
 Results for NB-I and NB-II models are
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/MEPS-II/NB1results.png
	width 20cm

\end_inset


\begin_inset Newpage newpage
\end_inset


\begin_inset Graphics
	filename Examples/MEPS-II/NB2results.png
	width 20cm

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
For the OBDV usage measure,
 the NB-II model does a slightly better job than the NB-I model,
 in terms of the average log-likelihood and the information criteria (more on this last in a moment).
 
\end_layout

\begin_layout Itemize
Note that both versions of the NB model fit much better than does the Poisson model (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:PoissonOBDV_results"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

).
\end_layout

\begin_layout Itemize
The estimated 
\begin_inset Formula $\alpha$
\end_inset

 is highly significant.
\end_layout

\begin_layout Standard
To check the plausibility of the NB-II model,
 we can compare the sample unconditional variance with the estimated unconditional variance according to the NB-II model:
 
\begin_inset Formula $\widehat{V(y)}=\frac{\sum_{t=1}^{n}\hat{\lambda}_{t}+\hat{\alpha}\left(\hat{\lambda}_{t}\right)^{2}}{n}$
\end_inset

.
 For OBDV and ERV (estimation results not reported),
 we get
\begin_inset Float table
placement htbp
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Marginal Variances,
 Sample and Estimated (NB-II)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sample
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
38.09
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.151
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Estimated
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30.58
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.182
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

For OBDV,
 the overdispersion problem is significantly better than in the Poisson case,
 but there is still some that is not captured.
 For ERV,
 the negative binomial model seems to capture the overdispersion adequately.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Finite mixture models:
 the mixed negative binomial model
\end_layout

\begin_layout Standard
Finite mixture models are quite widely used in statistical modeling.
 The finite mixture approach to fitting health care demand was introduced by Deb and Trivedi (1997).
 The mixture approach has the intuitive appeal of allowing for subgroups of the population with different health status.
 If individuals are classified as healthy or unhealthy then two subgroups are defined.
 A finer classification scheme would lead to more subgroups.
 Many studies have incorporated objective and/or subjective indicators of health status in an effort to capture this heterogeneity.
 The available objective measures,
 such as limitations on activity,
 are not necessarily very informative about a person's overall health status.
 Subjective,
 self-reported measures may suffer from the same problem,
 and may also not be exogenous
\end_layout

\begin_layout Standard
Finite mixture models are conceptually simple.
 The density is
\begin_inset Formula 
\[
f_{Y}(y,\phi_{1},...,\phi_{p},\pi_{1},...,\pi_{p-1})=\sum_{i=1}^{p-1}\pi_{i}f_{Y}^{(i)}(y,\phi_{i})+\pi_{p}f_{Y}^{p}(y,\phi_{p}),
\]

\end_inset

where 
\begin_inset Formula $\pi_{i}>0,i=1,2,...,p$
\end_inset

,
 
\begin_inset Formula $\pi_{p}=1-\sum_{i=1}^{p-1}\pi_{i}$
\end_inset

,
 and 
\begin_inset Formula $\sum_{i=1}^{p}\pi_{i}=1$
\end_inset

.
 Identification requires that the 
\begin_inset Formula $\pi_{i}$
\end_inset

 are ordered in some way,
 for example,
 
\begin_inset Formula $\pi_{1}\geq\pi_{2}\geq\cdots\geq\pi_{p}$
\end_inset

 and 
\begin_inset Formula $\phi_{i}\neq\phi_{j},i\neq j$
\end_inset

.
 This is simple to accomplish post-estimation by rearrangement and possible elimination of redundant component densities.
 
\end_layout

\begin_layout Itemize
The properties of the mixture density follow in a straightforward way from those of the components.
 In particular,
 the moment generating function is the same mixture of the moment generating functions of the component densities,
 so,
 for example,
 
\begin_inset Formula $E(Y|x)=\sum_{i=1}^{p}\pi_{i}\mu_{i}(x)$
\end_inset

,
 where 
\begin_inset Formula $\mu_{i}(x)$
\end_inset

 is the mean of the 
\begin_inset Formula $i^{th}$
\end_inset

 component density.
\end_layout

\begin_layout Itemize
Mixture densities may suffer from overparameterization,
 since the total number of parameters grows rapidly with the number of component densities.
 It is possible to constrain parameters across the mixtures.
\end_layout

\begin_layout Itemize
Testing for the number of component densities is a tricky issue.
 For example,
 testing for 
\begin_inset Formula $p=1$
\end_inset

 (a single component,
 which is to say,
 no mixture) versus 
\begin_inset Formula $p=2$
\end_inset

 (a mixture of two components) involves the restriction 
\begin_inset Formula $\pi_{1}=1$
\end_inset

,
 which is on the boundary of the parameter space.
 Not that when 
\begin_inset Formula $\pi_{1}=1$
\end_inset

,
 the parameters of the second component can take on any value without affecting the density.
 Usual methods such as the likelihood ratio test are not applicable when parameters are on the boundary under the null hypothesis.
 Information criteria means of choosing the model (see below) are valid.
 
\end_layout

\begin_layout Standard
The following results are for a mixture of 2 NB-II models,
 for the OBDV data,
 which you can replicate using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/MEPS-II/EstimateMixNegBin.m}{this program} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MEPS-II/MixNegBin.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
It is worth noting that the mixture parameter is not significantly different from zero,
 but also note that the coefficients of public insurance and age,
 for example,
 differ quite a bit between the two latent classes.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Information criteria
\begin_inset CommandInset label
LatexCommand label
name "subsec:Information-criteria"

\end_inset


\end_layout

\begin_layout Standard
As seen above,
 a Poisson model can't be tested (using standard methods) as a restriction of a negative binomial model.
 But it seems,
 based upon the values of the likelihood functions and the fact that the NB model fits the variance much better,
 that the NB model is more appropriate.
 How can we determine which of a set of competing models is the best?
\end_layout

\begin_layout Standard
The 
\emph on
information criteria
\emph default
 approach is one possibility.
 Information criteria are functions of the log-likelihood,
 with a penalty for the number of parameters used.
 The idea is to try to choose a model that fits well,
 but doesn't use an excessive number of parameters to obtain the good fit.
 Three popular information criteria are the Akaike (AIC),
 Bayes (BIC) and consistent Akaike (CAIC).
 The formulae are
\begin_inset Formula 
\begin{eqnarray*}
CAIC & = & -2\ln L(\hat{\theta})+k(\ln n+1)\\
BIC & = & -2\ln L(\hat{\theta})+k\ln n\\
AIC & = & -2\ln L(\hat{\theta})+2k
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
For a given criterion,
 the model that has the 
\emph on
lowest 
\emph default
value of the criterion is favored
\end_layout

\begin_layout Itemize
It can be shown that the CAIC and BIC will select the correctly specified model from a group of models,
 asymptotically.
\end_layout

\begin_layout Itemize
This doesn't mean,
 of course,
 that the correct model is necessarily in the group.
 
\end_layout

\begin_layout Itemize
The AIC is not consistent,
 and will asymptotically favor an over-parameterized model over the correctly specified model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Here are information criteria values,
 normalized by 
\begin_inset Formula $n$
\end_inset

,
 for the models we've seen,
 for OBDV.
\begin_inset Float table
placement htbp
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Information-Criteria,-OBDV"

\end_inset

Average Information Criteria,
 OBDV
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
AIC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BIC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CAIC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Poisson
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.345
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.355
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.357
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NB-I
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.375
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.386
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.388
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NB-II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.373
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.385
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.386
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MNB-II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.337
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.361
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.365
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

Pretty clearly,
 the NB models are better than the Poisson.
 The one additional parameter gives a very significant improvement in the likelihood function value.
 Between the NB-I and NB-II models,
 the NB-II is very slightly favored.
 But one should remember that information criteria values are statistics,
 with variances.
 With another sample,
 it may well be that the NB-I model would be favored,
 since the differences are so small.
 The MNB-II model is favored over the others,
 by all 3 information criteria,
 but it is less than 1% lower than the scores for the other NB models.
 Perhaps the additional complexity is not worth the small improvement in fit?
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To summarize:
\end_layout

\begin_layout Itemize
the Poisson model is clearly not appropriate,
 as it can't deal with the overdispersion that the data exhibits.
\end_layout

\begin_layout Itemize
the NB variants all perform in the same ballpark
\end_layout

\begin_layout Itemize
the issue of possible endogeneity of private insurance has been ignored.
 We will return to that once we have some more tools.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:DSGE-ML"

\end_inset

ML estimation of the DSGE model
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Application:-a-simple"
nolink "false"

\end_inset

 introduced a simple dynamic stochastic general equilibrium model.
 The file 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/ML/Estimate.jl}{Estimate.jl} 
\end_layout

\end_inset

 allows you to explore maximum likelihood estimation of the model using Kalman filtering,
 using the 
\begin_inset CommandInset href
LatexCommand href
name "http://www.dynare.org/"
target "http://www.dynare.org/"
literal "false"

\end_inset

 Julia package.
 
\series bold
PLEASE NOTE
\series default
:
 this file should be run following the instructions in the README in the same directory.
 
\end_layout

\begin_layout Standard
Note that Dynare is doing ML by first linearizing the model,
 then using Kalman filtering to compute the likelihood,
 and then is maximizing the Bayesian posterior likelihood (see Chapter 17 for more information).
 Because the priors are uniform,
 the posterior maximizes at the same place as does the likelihood,
 so these are (quasi) ML estimates.
 I say 
\begin_inset Quotes sld
\end_inset

quasi
\begin_inset Quotes srd
\end_inset

 because,
 presumably,
 some information is lost in the linearization,
 depending on how nonlinear is the original model,
 so this is not actually ML.
 Some output that can be obtained is:
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
using c and n:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/ML/cn.png
	width 15cm

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "ML of DSGE"

\end_inset


\end_layout

\begin_layout Standard

\series bold
using y and w:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/ML/yw.png

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
the parameter estimates are pretty good,
 
\begin_inset Formula $\rho_{2}$
\end_inset

 is the parameter that is farther from it's true value,
 the rest are quite close.
 
\end_layout

\begin_layout Itemize
note that the standard errors and t statistics change quite a bit depending on which variables are used,
 especially for 
\begin_inset Formula $\gamma$
\end_inset


\end_layout

\begin_layout Itemize
It's hard to say if confidence intervals are reliable,
 as these are 80% intervals,
 and we are using a single sample.
 They do appear to be questionable,
 though.
\end_layout

\begin_layout Itemize
When order=1,
 the estimation process involves forming a linear approximation to the true model,
 which means that the estimator is not actually the true maximum likelihood estimator,
 it is actually a 
\begin_inset Quotes sld
\end_inset

quasi-ML
\begin_inset Quotes srd
\end_inset

 estimator (refer to 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Linearization-of"
nolink "false"

\end_inset

).
 The quasi-likelihood is computed by putting the linearized model in state-space form,
 and then computing the likelihood iteratively using Kalman filtering,
 which relies on the assumption that shocks to the model are normally distributed (which is true,
 in this case).
 State space models and Kalman filtering are introduced in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:State-space-models"
nolink "false"

\end_inset

.
 Once the (quasi) likelihood function is available,
 the methods studied in this Chapter may be applied.
 
\end_layout

\begin_layout Itemize
The linearization of the model,
 combined with the fact that it has only two shocks,
 leads to a problem of 
\begin_inset Quotes sld
\end_inset

stochastic singularity
\begin_inset Quotes srd
\end_inset

,
 which means that at most two observed variables may be used to compute the likelihood function.
 The code lets you explore the choice.
 It seems to work best using c and n.
 It won't work using y and r.
 When you don't know the true parameters,
 how will you choose which results to believe?
\end_layout

\begin_layout Itemize
This is not a problem of the ML method,
 it is a problem due to the fact that we are not really estimating the true model,
 we're working with a linear approximation.
 Sometimes,
 people add measurement error to the variables (which may in fact be realistic),
 which gets around the stochastic singularity problem.
 I have not yet seen a careful study of the effect of estimating assuming measurement error when there really is no measurement error.
\end_layout

\begin_layout Itemize
There exist other estimation methods that do not require linearizing the model,
 and which may be more reliable,
 and some of these are included in the Dynare.jl package.
 Going into that is outside the scope of this course.
 The message here is simply that one should be careful when choosing estimation methods for expediency,
 as it can introduce inaccuracies or lead to missleading conclusions- 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Here are Monte Carlo results for ML estimation,
 using c and n as the observed variables
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Examples/DSGE/ML/montecarlo.png
	width 20cm

\end_inset


\end_layout

\begin_layout Itemize
the results are quite good:
 small bias and fairly precise.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical Summary
\end_layout

\begin_layout Standard
The practical summary for the Chapter is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./PracticalSummaries/15-MaximumLikelihood.jl}{here}
\end_layout

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Consider coin tossing with a single possibly biased coin.
 The random variable 
\begin_inset Formula $Y$
\end_inset

 is equal to 1 if a heads results,
 or 0 if a tails results.
 The probability of a heads 
\begin_inset Formula $P(Y=1)$
\end_inset

 is 
\begin_inset Formula $p_{0}.$
\end_inset

 Thus,
 the density function for the random variable is 
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y,p_{0}) & = & p_{0}^{y}\left(1-p_{0}\right)^{1-y},y\in\{0,1\}\\
 & = & 0,y\notin\{0,1\}
\end{eqnarray*}

\end_inset

Suppose that we have a sample of size 
\begin_inset Formula $n$
\end_inset

.
 We know,
 or can show,
 that the ML estimator is 
\begin_inset Formula $\widehat{p_{0}}=\bar{y}$
\end_inset

.
 We also know from the theory above that 
\begin_inset Formula 
\[
\sqrt{n}\left(\bar{y}-p_{0}\right)\overset{a}{\sim}N\left[0,\mathcal{J}_{\infty}(p_{0})^{-1}\mathcal{I}_{\infty}(p_{0})\mathcal{J}_{\infty}(p_{0})^{-1}\right]
\]

\end_inset


\series bold
a)
\series default
 find the analytic expression for the score contribution 
\begin_inset Formula $g_{t}(\theta)$
\end_inset

 and show that 
\begin_inset Formula $\mathcal{E}_{\theta}\left[g_{t}(\theta)\right]=0$
\end_inset


\begin_inset Newline newline
\end_inset


\series bold
b)
\series default
 find the analytical expressions for 
\begin_inset Formula $\mathcal{J}_{\infty}(p_{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{\infty}(p_{0})$
\end_inset

 for this problem 
\begin_inset Newline newline
\end_inset


\series bold
c)
\series default
 verify that the result for 
\begin_inset Formula $\lim Var\sqrt{n}\left(\hat{p}-p\right)$
\end_inset

 found in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Coin-flipping,-again"
nolink "false"

\end_inset

 is equal to 
\begin_inset Formula $\mathcal{J}_{\infty}(p_{0})^{-1}\mathcal{I}_{\infty}(p_{0})\mathcal{J}_{\infty}(p_{0})^{-1}$
\end_inset


\begin_inset Newline newline
\end_inset


\series bold
d)
\series default
 Write an Julia program that does a Monte Carlo study that shows that 
\begin_inset Formula $\sqrt{n}\left(\bar{y}-p_{0}\right)$
\end_inset

 is approximately normally distributed when 
\begin_inset Formula $n$
\end_inset

 is large.
 Please give me histograms that show the sampling frequency of 
\begin_inset Formula $\sqrt{n}\left(\bar{y}-p_{0}\right)$
\end_inset

 for several values of 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Enumerate
The exponential density is 
\begin_inset Formula 
\[
f_{Y}(y)=\left\{ \begin{array}{c}
\frac{e^{-\frac{y}{\lambda_{0}}}}{\lambda_{0}},\,y\geqslant0\\
0,\,y<0
\end{array}\right.
\]

\end_inset

Suppose we have an independently and identically distributed sample of size 
\begin_inset Formula $n$
\end_inset

,
 
\begin_inset Formula $\left\{ y_{i}\right\} ,i=1,2,...,n$
\end_inset

,
 where each 
\begin_inset Formula $y_{i}$
\end_inset

 follows this exponential distribution.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
write the log likelihood function
\end_layout

\begin_layout Enumerate
find an analytic expression for the maximum likelihood estimator of the parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Enumerate
explain how to estimate the asymptotic variance of the ML estimator.
 That is,
 if 
\begin_inset Formula $\sqrt{n}\left(\hat{\lambda}-\lambda_{0}\right)\rightarrow^{d}N(0,V_{\infty})$
\end_inset

,
 give a consistent estimator of 
\begin_inset Formula $V_{\infty}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
explain how to compute an estimator of the standard error of 
\begin_inset Formula $\hat{\lambda}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Generate a sample of 100 observations from the exponential model of the previous question,
 using the Julia commands 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\family typewriter
using Distributions
\begin_inset Newline newline
\end_inset

n=100
\begin_inset Newline newline
\end_inset

lambda0 = 4.
\begin_inset Newline newline
\end_inset

y = rand(Exponential(lambda0),
 n)
\family default

\begin_inset Newline newline
\end_inset

and use this data to:
\end_layout

\begin_deeper
\begin_layout Enumerate
estimate the parameter by ML,
 using both an analytic formula,
 and by numerically minimizing the negative log likelihood function.
\end_layout

\begin_layout Enumerate
compute the estimated standard error of the estimated parameter,
 and give a 95% confidence interval for 
\begin_inset Formula $\lambda_{0}.$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Now,
 assume the parameter of the exponential distribution depends on a regressor:
 
\begin_inset Formula $\lambda_{0}=\exp(\beta_{0}+\beta_{1}x).$
\end_inset

 Generate 100 observations from the exponential model of problem 3,
 using the commands
\begin_inset Newline newline
\end_inset


\family typewriter
n = 100
\begin_inset Newline newline
\end_inset

x = [ones(n) randn(n) rand(n)]
\begin_inset Newline newline
\end_inset

beta = [-0.5,
 1.,
 1.]
\begin_inset Newline newline
\end_inset

lambda0 = exp.(x*beta)
\begin_inset Newline newline
\end_inset

y = rand.(Exponential.(lambda0))
\begin_inset Newline newline
\end_inset


\family default
and use this data to estimate the parameter by ML by numerically minimizing the negative log likelihood function.
 You do not need to compute the variance or standard errors,
 only the estimates.
 To do this,
 modify your code for the previous problem.
 Note that element-by-element multiplication or division of vectors uses the .* and ./ operators,
 respectively.
\end_layout

\begin_layout Enumerate
Suppose we have an i.i.d.
 sample of size 
\begin_inset Formula $n$
\end_inset

 from the Poisson density.
 The Poisson density is 
\begin_inset Formula $f_{y}(y;\lambda)=\frac{e^{-\lambda}\lambda^{y}}{y!}$
\end_inset

.
 Verify that the ML estimator is asymptotically distributed as 
\begin_inset Formula $\sqrt{n}\left(\hat{\lambda}-\lambda_{0}\right)\stackrel{d}{\rightarrow}N(0,\lambda_{0})$
\end_inset

,
 where 
\begin_inset Formula $\lambda_{0}$
\end_inset

 is the true parameter value.
 Hint:
 compute the asymptotic variance using 
\begin_inset Formula $-\mathcal{J}_{\infty}(\lambda_{0})^{-1}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate

\series bold
ML.
 
\series default
Estimate a Poisson model by ML using the 10 independent data points
\begin_inset Newline newline
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="11">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

.
 
\begin_inset Newline newline
\end_inset

For the Poisson model,
 the density 
\begin_inset Formula $f_{Y}(y|\lambda)=\frac{\exp(-\lambda)\lambda^{y}}{y!},$
\end_inset

 
\begin_inset Formula $y=0,1,2,...$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
create a data file that contains these observations
\end_layout

\begin_layout Enumerate
find the log-likelihood function
\end_layout

\begin_layout Enumerate
find the analytic expression for the ML estimator,
 and find an analytic expression for the asymptotic variance of 
\begin_inset Formula $\sqrt{n}(\hat{\lambda}-\lambda^{0}).$
\end_inset


\end_layout

\begin_layout Enumerate
write a function that computes the log-likelihood function,
 using the form
\family typewriter
 loglikelihood(theta,
 data)
\family default
 and maximize it to find the ML estimator.
 You will probably want to use an anonymous function for this.
\end_layout

\begin_layout Enumerate
compute the ML estimator using your analytic expression.
 It should be very close to what you got using fminunc.
 Is it?
 If not,
 revise your code to make it work better.
\end_layout

\begin_layout Enumerate
compute the estimated standard deviation of 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 and report an asymptotic 95% confidence interval for 
\begin_inset Formula $\lambda^{0}.$
\end_inset


\end_layout

\begin_layout Enumerate
now,
 reparameterize the model as 
\begin_inset Formula $f_{Y}(y|\alpha)=\frac{\exp(-\lambda)\lambda^{y}}{y!}$
\end_inset

 where 
\begin_inset Formula $\lambda=\exp\alpha$
\end_inset

.
 The advantage of this is that 
\begin_inset Formula $\alpha$
\end_inset

 is unrestricted in sign,
 while the original 
\begin_inset Formula $\lambda$
\end_inset

 must be positive.
 This doesn't matter much at present,
 but it will when you allow the conditional mean to depend on other variables.
 Verify the invariance property of ML by estimating 
\begin_inset Formula $\alpha$
\end_inset

,
 and then showing that 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 from part (d) is equal to 
\begin_inset Formula $\exp\hat{\alpha}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Consider the model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\sigma\epsilon_{t}$
\end_inset

.
 Find the score function 
\begin_inset Formula $g_{n}(\theta)$
\end_inset

 where 
\begin_inset Formula $\theta=\left(\begin{array}{cc}
\beta^{\prime} & \text{\ensuremath{\sigma}}\end{array}\right)^{\prime}$
\end_inset

 and 
\end_layout

\begin_deeper
\begin_layout Enumerate
the errors follow the Cauchy (Student-t with 1 degree of freedom) density:
\begin_inset Formula 
\[
f(\epsilon_{t})=\frac{1}{\pi\left(1+\epsilon_{t}^{2}\right)},-\infty<\epsilon_{t}<\infty
\]

\end_inset

The Cauchy density has a shape similar to a normal density,
 but with much thicker tails.
 Thus,
 extremely small and large errors occur much more frequently with this density than would happen if the errors were normally distributed.
 
\end_layout

\begin_layout Enumerate
where the errors are independent standard normal random variables:
 
\begin_inset Formula $\epsilon_{t}\sim N(0,1)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Compare the first order conditions that define the ML estimators for these two cases,
 and interpret the differences.
 
\emph on
Why
\emph default
 are the first order conditions that define an efficient estimator different in the two cases?
 How do the weights on observations differ?
\end_layout

\end_deeper
\begin_layout Enumerate
Assume a d.g.p.
 follows the logit model:
 
\begin_inset Formula $\Pr(y=1|x)=\left(1+exp(-\beta_{0}x)\right)^{-1}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Assume that 
\begin_inset Formula $x\sim$
\end_inset

 uniform(-a,a).
 Find the asymptotic distribution of the ML estimator of 
\begin_inset Formula $\beta_{0}$
\end_inset

 (this is a scalar parameter).
\end_layout

\begin_layout Enumerate
Now assume that 
\begin_inset Formula $x\sim$
\end_inset

 uniform(-2a,2a).
 Again find the asymptotic distribution of the ML estimator of 
\begin_inset Formula $\beta_{0}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Comment on the results
\end_layout

\end_deeper
\begin_layout Enumerate
Estimate the simple Nerlove model discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"
nolink "false"

\end_inset

 by ML,
 assuming that the errors are i.i.d.
 
\begin_inset Formula $N(0,\sigma^{2})$
\end_inset

 and compare to the results you get from running 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/OLS/Nerlove.jl}{Nerlove.jl} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Enumerate
Using the fmincon routine in Econometrics
\end_layout

\begin_deeper
\begin_layout Enumerate
estimate the Nerlove model with the restriction that 
\begin_inset Formula $\beta_{L}+\beta_{F}+\beta_{K}=1$
\end_inset

 (the cost function satisfies homogeneity of degree one in factor prices).
 Test this restriction using the likelihood ratio test.
\end_layout

\begin_layout Enumerate
test the restriction that 
\begin_inset Formula $\beta_{Q}=\text{1 (the model exhibits constant returns to scale) using the LR test}$
\end_inset

.
\end_layout

\begin_layout Enumerate
test homogeneity of degree 1 and constant returns to scale jointly,
 using the LR test.
\end_layout

\end_deeper
\begin_layout Enumerate
Using 
\begin_inset CommandInset href
LatexCommand href
name "logit.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/ML/Likelihoods/logit.jl"
literal "false"

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/NonlinearOptimization/EstimateLogit.jl}{EstimateLogit.jl} 
\end_layout

\end_inset

 as templates,
 write a function to calculate the probit log likelihood,
 and a script to estimate a probit model.
 Run it using data that actually follows a logit model (you can generate it in the same way that is done in the logit example).
\end_layout

\begin_layout Enumerate
Study 
\family typewriter

\begin_inset CommandInset href
LatexCommand href
name "mleresults.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/ML/mleresults.jl"
literal "false"

\end_inset


\family default
 to see what it does.
 Examine the functions that
\family typewriter
 it 
\family default
calls.
 Write a complete description of how thechain works.
\end_layout

\begin_layout Enumerate
In Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"
nolink "false"

\end_inset

 a model is presented for data on health care usage,
 along with some Julia scripts.
 Look at the Poisson estimation results for the OBDV measure of health care use and give an economic interpretation.
 Estimate Poisson models for the other 5 measures of health care usage,
 using the provided scripts.
\end_layout

\begin_layout Enumerate
For practice using fminunc,
 estimate a Poisson model by ML using the 10 independent data points
\begin_inset Newline newline
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="11">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

.
 
\begin_inset Newline newline
\end_inset

For the Poisson model,
 the density 
\begin_inset Formula $f_{Y}(y|x)=\frac{\exp(-\lambda)\lambda^{y}}{y!},$
\end_inset

 
\begin_inset Formula $y=0,1,2,...$
\end_inset

.
 To make the model depend on conditioning variables,
 use the parameterization 
\begin_inset Formula $\lambda(x)=\exp(\theta_{1}+\theta_{2}x)$
\end_inset

.
 The example EstimatePoisson.jl,
 in the notes,
 should be helpful
\end_layout

\begin_deeper
\begin_layout Enumerate
create a data file that contains these observations
\end_layout

\begin_layout Enumerate
find the log-likelihood function
\end_layout

\begin_layout Enumerate
write a Julia function that computes the log-likelihood function.
\end_layout

\begin_layout Enumerate
use fminunc to find the ML estimator.
 You need to use an anonymous function for this.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Generalized-method-of"

\end_inset

Generalized method of moments
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
:
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Ch.
 6;
 Hamilton Ch.
 14
\begin_inset Formula $^{*}$
\end_inset

;
 Davidson and MacKinnon,
 Ch.
 17 (see pg.
 587 for refs.
 to applications),
 
\begin_inset CommandInset citation
LatexCommand citet
key "hansen1982"
literal "true"

\end_inset

,
  
\begin_inset CommandInset citation
LatexCommand citet
key "HansenSingleton1982"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand citet
key "NeweyMcfadden"
literal "true"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The GMM estimator is a consistent and asymptotically normal estimator (for at least some of the parameters) that does not require such strong assumptions as does the ML estimator.
\end_layout

\begin_layout Itemize
Many widely used estimators,
 such as instrumental variables estimators,
 least squares estimators,
 and maximum likelihood,
 can be put into the form of GMM estimators
\end_layout

\begin_layout Itemize
thus,
 the study of the theory for GMM can be applied in many cases
\end_layout

\begin_layout Itemize
GMM theory can be a simpler way to formulate theory than some of the original presentations.
 
\end_layout

\begin_layout Itemize
It is a convenient and relatively simple way to think of things,
 and it has wide application.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Moment conditions
\end_layout

\begin_layout Definition
Moment condition:
 a moment condition 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\bar{m}_{n}(\theta)=\bar{m}_{n}(Z_{n},\theta)$
\end_inset

 is a vector-valued function of the data 
\begin_inset Formula $Z_{n}$
\end_inset

 and the parameter 
\begin_inset Formula $\theta$
\end_inset

 that has mean zero,
 under the model,
 when evaluated at the true parameter value 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 and expectation different from zero when evaluated at other parameter values:

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\begin_inset Formula 
\begin{align*}
E\bar{m}_{n}(Z_{n},\theta_{0}) & =0\\
E\bar{m}_{n}(Z_{n},\theta) & \ne0,\,\theta\ne\theta_{0}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
The expectation operator 
\begin_inset Formula $E$
\end_inset

 supposes that expectations are taken with respect to the true density of the data.
 This may depend on more parameters than appear in 
\begin_inset Formula $\theta$
\end_inset

,
 if the model is semi-parametric.
\end_layout

\begin_layout Itemize
The moment condition may be vector-valued,
 with dimension 
\begin_inset Formula $G,$
\end_inset

 say.
\end_layout

\begin_layout Itemize
There are a couple of other details in the definition,
 which we'll get to.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
Moment contribution:
 we usually will be dealing with moment conditions that are defined as averages:
 
\begin_inset Formula $\bar{m}_{n}(\theta)=\frac{1}{n}\sum_{t}m(Z_{t},\theta)=\frac{1}{n}\sum_{t}m_{t}(\theta)$
\end_inset

.
 The functions 
\begin_inset Formula $m_{t}(\theta)$
\end_inset

 are the 
\emph on
moment contributions.

\emph default
 The 
\begin_inset Formula $t$
\end_inset

-th moment contribution,
 
\begin_inset Formula $m_{t}$
\end_inset

,
 is a function of the same observation's data.
 I'm casually using 
\begin_inset Formula $m(Z_{t},\theta),$
\end_inset

 
\begin_inset Formula $m_{t}(\theta)$
\end_inset

 and 
\begin_inset Formula $m_{t}$
\end_inset

 to all refer to the same thing.
 This first of these is the full expression,
 but I will suppress arguments when the context makes things clear enough,
 to reduce the notational burden.
 The main thing is that 
\begin_inset Formula $\bar{m}_{n}$
\end_inset

 most often refers to the average over the 
\begin_inset Formula $n$
\end_inset

 observations,
 and 
\begin_inset Formula $m_{t}$
\end_inset

 refers to the terms that are averaged.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
OLS.
 The classical linear model.
 Let 
\begin_inset Formula $\bar{m}_{n}(\beta)=\frac{1}{n}\sum_{t}x_{t}(y_{t}-x_{t}^{\prime}\beta).$
\end_inset

 So the moment contributions are 
\begin_inset Formula $m_{t}(\beta)=x_{t}(y_{t}-x_{t}^{\prime}\beta)$
\end_inset

.
 When 
\begin_inset Formula $\beta=\beta_{0},$
\end_inset

 
\begin_inset Formula $y_{t}-x_{t}^{\prime}\beta_{0}=\epsilon_{t},$
\end_inset

 and 
\begin_inset Formula $m_{t}=x_{t}\epsilon_{t}$
\end_inset

.
 We know that 
\begin_inset Formula $E(x_{t}\epsilon_{t})=0,$
\end_inset

 by the weak exogeneity assumption.
 Thus,
 the moment contributions,
 and the moment condition,
 which is their average,
 have expectation zero when evaluated at the true parameter value.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
ML.
 We have seen (see eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ExpectationScore"
nolink "false"

\end_inset

) that the score contributions of the ML estimator have mean zero:
 
\begin_inset Formula $E\left(D_{\theta}\ln f(y_{t}|x_{x},\theta_{0})\right)=0$
\end_inset

.
 So,
 we could set 
\begin_inset Formula $m_{t}(\theta)=D_{\theta}\ln f(y_{t}|x_{x},\theta)$
\end_inset

.
\end_layout

\begin_layout Standard
Of course,
 you would need to know the density 
\begin_inset Formula $f$
\end_inset

 to compute this,
 so could only use these moment conditions if you had enough information to do ML estimation,
 so there wouldn't be much point in doing GMM.
 However,
 we gain the insight that the optimal moments would be the score vector.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
Sampling from 
\begin_inset Formula $\chi^{2}.$
\end_inset

 Suppose we draw a random sample of 
\begin_inset Formula $y_{t}$
\end_inset

 from the 
\begin_inset Formula $\chi^{2}(\theta_{0})$
\end_inset

 distribution.
 Here,
 
\begin_inset Formula $\theta_{0}$
\end_inset

 is the parameter of interest.
 If 
\begin_inset Formula $Y\sim\chi^{2}(\theta_{0})$
\end_inset

,
 then the mean 
\begin_inset Formula $E(Y)=\theta_{0}$
\end_inset

.
 Let the moment contribution be 
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
m_{t}(\theta)=y_{t}-\theta
\]

\end_inset

Then 
\begin_inset Formula 
\begin{align*}
\bar{m}_{n}(\theta) & =\frac{1}{n}\sum_{t=1}^{n}m_{t}(\theta)=\bar{y}-\theta
\end{align*}

\end_inset

We know that 
\begin_inset Formula $E(\bar{y})=\theta_{0}.$
\end_inset


\end_layout

\begin_layout Itemize
Thus,
 
\begin_inset Formula $E\bar{m}_{n}(\theta_{0})=0.$
\end_inset

 
\end_layout

\begin_layout Itemize
However,
 
\begin_inset Formula $E\bar{m}_{n}(\theta)=\theta_{0}-\theta\ne$
\end_inset

0 if 
\begin_inset Formula $\theta\ne\theta_{0}.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
When the dimension of the moment conditions is the same as the dimension of the parameter vector 
\begin_inset Formula $\theta$
\end_inset

,
 the 
\emph on
method of moments principle
\emph default
 is to choose the estimator of the parameter to 
\emph on
set the moment condition equal to zero
\emph default
:
 
\begin_inset Formula $\bar{m}_{n}(\hat{\theta})\equiv0$
\end_inset

.
 Then the equation is solved for the estimator.
 
\end_layout

\begin_layout Itemize
In the case of OLS,
 this gives 
\begin_inset Formula $\sum_{t}x_{t}(y_{t}-x_{t}^{\prime}\hat{\beta})=0,$
\end_inset

 which gives a solution that you should already know.
 
\end_layout

\begin_layout Itemize
For the chi-squared example,
 
\begin_inset Formula 
\[
\bar{m}(\hat{\theta})=\bar{y}-\hat{\theta}=0
\]

\end_inset

is solved by 
\begin_inset Formula $\hat{\theta}=\bar{y}$
\end_inset

.
 Since 
\begin_inset Formula $\bar{y}=\sum_{t=1}^{n}y_{t}/n\stackrel{p}{\rightarrow}\theta_{0}$
\end_inset

 by the LLN,
 the estimator is consistent.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula $\chi^{2},$
\end_inset

 version 2.
 The variance of a 
\begin_inset Formula $\chi^{2}(\theta_{0})$
\end_inset

 r.v.
 is 
\begin_inset Formula 
\[
V\left(Y\right)=E\left(Y-\theta_{0}\right)^{2}=2\theta_{0}.
\]

\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula 
\begin{align*}
m_{t}(\theta) & =\frac{n}{n-1}\left(y_{t}-\bar{y}\right)^{2}-2\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Then
\begin_inset Formula 
\[
\bar{m}_{n}(\theta)=\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n-1}-2\theta.
\]

\end_inset

The first term is the unbiased formula for the sample variance,
 and thus has expectation equal to 
\begin_inset Formula $2\theta_{0}.$
\end_inset

 So if we evaluate 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 at 
\begin_inset Formula $\theta_{0},$
\end_inset

 the expectation is zero.
\end_layout

\begin_layout Example
The MM estimator using the variance would set 
\begin_inset Formula 
\[
\bar{m}_{n}(\hat{\theta})=\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n-1}-2\hat{\theta}\equiv0.
\]

\end_inset

Solving for the estimator,
 it is half the sample variance:
 
\begin_inset Formula 
\[
\hat{\theta}=\frac{1}{2}\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n-\text{1}}.
\]

\end_inset

Again,
 by the LLN,
 the sample variance is consistent for the true variance,
 that is,
 
\begin_inset Formula 
\[
\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n}\stackrel{p}{\rightarrow}2\theta_{0}.
\]

\end_inset

So,
 this MM is also consistent for 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:chi2 mm"

\end_inset

Try some MM estimation yourself:
 here's a Julia script that implements the two MM estimators discussed above:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/chi2mm.jl}{GMM/chi2mm.jl}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that when you run the script,
 the two estimators give different results.
 Each of the two estimators is consistent.
 
\end_layout

\begin_layout Itemize
For the 
\begin_inset Formula $\chi^{2}$
\end_inset

 example,
 we have two alternative moment conditions and only one parameter:
 we have 
\emph on
overidentification,

\emph default
 which means that we have more information than is strictly necessary for consistent estimation of the parameter.
\end_layout

\begin_layout Itemize
The idea behind GMM is to combine information from the two moment conditions to form a new estimator which will be
\emph on
 more efficient,

\emph default
 in general (proof of this below).
\end_layout

\begin_layout Itemize
Note that the fact that the data has a chi-squared distribution is not used in estimation,
 it just as easily could have been normally distributed,
 sampling from a 
\begin_inset Formula $N(\theta_{0},2\theta_{0})$
\end_inset

 distribution.
 As long as the assumptions regarding the mean or variance are correct,
 the MM estimators are consistent.
 So,
 we don't make use of distributional assumptions when doing method of moment estimation,
 we only rely on certain moments being correctly specified.
 
\end_layout

\begin_deeper
\begin_layout Itemize
In this way,
 method of moments estimation is 
\emph on
more robust
\emph default
 than is maximum likelihood estimation:
 we obtain a consistent estimator with fewer assumptions.
 
\end_layout

\begin_layout Itemize
There being no free lunch,
 we should expect to pay something for this,
 of course.
 The cost will be a loss of efficiency,
 in general.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
To summarize
\series default
,
 a moment condition is a vector valued function which has expectation zero at the true parameter value.
 We have seen some examples of where we might get such functions,
 and more will follow.
 For now,
 let's take moment conditions as given,
 and work out the properties of the estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Definition of GMM estimator
\end_layout

\begin_layout Standard
For the purposes of this course,
 the following definition of the GMM estimator is sufficiently general:
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "GMM estimator (defn)"

\end_inset

The GMM estimator of the 
\begin_inset Formula $k$
\end_inset

 -dimensional parameter vector 
\begin_inset Formula $\theta_{0},$
\end_inset

 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta),
\]

\end_inset

 
\end_layout

\begin_layout Itemize
where 
\begin_inset Formula $\bar{m}_{n}(Z_{n},\theta)$
\end_inset

 is a 
\begin_inset Formula $g$
\end_inset

-vector valued function,
 
\begin_inset Formula $g\geq k,$
\end_inset

 with 
\begin_inset Formula $E\bar{m}(Z_{n},\theta_{0})=0,$
\end_inset

 
\begin_inset Formula $E\bar{m}(Z_{n},\theta)\ne0$
\end_inset

,
 
\begin_inset Formula $\forall\theta\ne\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
and 
\begin_inset Formula $W_{n}$
\end_inset

 converges almost surely to a finite 
\begin_inset Formula $g\times g$
\end_inset

 symmetric positive definite matrix 
\begin_inset Formula $W_{\infty}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Usually,
 the moment conditions will be averages of terms:
\begin_inset Formula 
\begin{align*}
\bar{m}_{n}(\theta) & =\frac{1}{n}\sum_{t=1}^{n}m(Z_{t},\theta)\\
 & \equiv\frac{1}{n}\sum_{t=1}^{n}m_{t}(\theta)
\end{align*}

\end_inset

In this case the moment contributions 
\begin_inset Formula $m_{t}(\theta)\equiv m(Z_{t},\theta)$
\end_inset

 are a 
\begin_inset Formula $g$
\end_inset

-vector valued functions,
 
\begin_inset Formula $g\geq k,$
\end_inset

 with 
\begin_inset Formula 
\[
Em_{t}(\theta_{0})=0,
\]

\end_inset

and 
\begin_inset Formula 
\[
Em_{t}(\theta)\ne0,\forall\theta\ne\theta_{0}.
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
What's the reason for using GMM if MLE is asymptotically efficient?

\emph default
 
\end_layout

\begin_layout Itemize
Robustness:
 GMM is based upon a limited set of moment conditions.
 For consistency,
 only these moment conditions need to be correctly specified,
 whereas MLE in effect requires correct specification of 
\emph on
every conceivable
\emph default
 moment condition.
 GMM is 
\emph on
robust with respect to distributional misspecification.

\emph default
 The price for robustness is usually a loss of efficiency with respect to the MLE estimator.
 Keep in mind that the true distribution is 
\bar under
not known,

\bar default
 so if we erroneously specify a distribution and estimate by 
\begin_inset Quotes sld
\end_inset

MLE
\begin_inset Quotes srd
\end_inset

,
 the estimator will in fact be what is known as a 
\emph on
quasi-ML
\emph default
 or 
\emph on
pseudo-ML
\emph default
 estimator (
\begin_inset CommandInset citation
LatexCommand citet
key "gourieroux1984pseudo"
literal "false"

\end_inset

),
 and it will be inconsistent in general (but not in some special cases).
 
\end_layout

\begin_layout Itemize
Feasibility:
 in some cases the MLE estimator is not available,
 because we are not able to deduce or compute the likelihood function.
 More on this in the section on simulation-based estimation.
 The GMM estimator may still be feasible even though MLE is not available.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/chi2gmm.jl}{GMM/chi2gmm.jl}
\end_layout

\end_inset

 implements GMM using the same 
\begin_inset Formula $\chi^{2}$
\end_inset

 data as was using in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:chi2 mm"
nolink "false"

\end_inset

,
 above.
 The two moment conditions,
 based on the sample mean and sample variance are combined.
 The weight matrix is an identity matrix,
 
\begin_inset Formula $I_{2}.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Consistency
\end_layout

\begin_layout Standard
We simply assume that the assumptions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

 hold,
 so the GMM estimator is strongly consistent.
 The main requirement is that the moment conditions have mean zero at the true parameter value,
 
\begin_inset Formula $\theta_{0}.$
\end_inset

 This will be the case if our moment conditions are correctly specified.
 With this,
 it is clear that the minimum of the limiting objective function occurs at the true parameter value.
 
\end_layout

\begin_layout Itemize
The only assumption that warrants additional comment is that of identification.
 In Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

,
 the third assumption reads:
 (c) 
\shape italic
Identification:

\shape default
 
\begin_inset Formula $s_{\infty}(\cdot)$
\end_inset

 has a unique global maximum at 
\begin_inset Formula $\theta_{0},$
\end_inset

 
\shape italic
i.e.,

\shape default
 
\begin_inset Formula $s_{\infty}(\theta_{0})>s_{\infty}(\theta),$
\end_inset

 
\begin_inset Formula $\forall\theta\neq\theta_{0}.$
\end_inset

 
\end_layout

\begin_layout Itemize
Taking the case of a quadratic objective function 
\begin_inset Formula $s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta),$
\end_inset

 first consider 
\begin_inset Formula $\bar{m}_{n}(\theta).$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Applying a uniform law of large numbers,
 we get 
\begin_inset Formula $\bar{m}_{n}(\theta)\stackrel{a.s.}{\rightarrow}m_{\infty}(\theta).$
\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $E\bar{m}_{n}(\theta_{0})=0$
\end_inset

 by assumption,
 
\begin_inset Formula $m_{\infty}(\theta_{0})=0.$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Since 
\begin_inset Formula $s_{\infty}(\theta_{0})=m_{\infty}(\theta_{0})^{\prime}W_{\infty}m_{\infty}(\theta_{0})=0,$
\end_inset

 in order for asymptotic identification,
 we need that 
\begin_inset Formula $s_{\infty}(\theta)>0$
\end_inset

 for 
\begin_inset Formula $\theta\ne\theta_{0}$
\end_inset

.
 This requires that 
\begin_inset Formula $m_{\infty}(\theta)\neq0$
\end_inset

 for 
\begin_inset Formula $\theta\neq\theta_{0},$
\end_inset

 for at least some element of the vector.
 There can be no other parameter value that sets the moment conditions to zero (at least,
 in the limit).
 
\emph on
Draw picture here.
 
\emph default
This and the assumption that 
\begin_inset Formula $W_{n}\stackrel{a.s.}{\rightarrow}$
\end_inset

 
\begin_inset Formula $W_{\infty},$
\end_inset

 a finite positive 
\begin_inset Formula $g\times g$
\end_inset

 definite 
\begin_inset Formula $g\times g$
\end_inset

 matrix guarantee that 
\begin_inset Formula $\theta_{0}$
\end_inset

 is asymptotically identified.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
an example of lack of identification is a MA(1) model 
\begin_inset Formula $y_{t}=\epsilon_{t}+\theta\epsilon_{t-1}$
\end_inset

 where the 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 are i.i.d.
 
\begin_inset Formula $N(0,1).$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
consider the alternative MA(1) model 
\begin_inset Formula $y_{t}=\epsilon_{t}+(1/\theta)\epsilon_{t-1}$
\end_inset

 where the 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 are i.i.d.
 
\begin_inset Formula $N(0,\theta^{2}).$
\end_inset

 
\end_layout

\begin_layout Itemize
All of the moments of 
\begin_inset Formula $y_{t}$
\end_inset

 are the same in the two cases.
 For example,
 for the first model,
 
\begin_inset Formula 
\[
V(y_{t})=V(\epsilon_{t})+\theta^{2}V(\epsilon_{t-1})=1+\theta^{2}.
\]

\end_inset

 For the alternative model,
 you can work out that the variance is the same.
 
\end_layout

\begin_layout Itemize
So,
 a method of moments estimator can't distinguish between the two parameter values 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $1/\theta$
\end_inset

.
 If 
\begin_inset Formula $m_{\infty}(\theta)$
\end_inset

 is equal to zero,
 then so is 
\begin_inset Formula $m_{\infty}(1/\theta).$
\end_inset


\end_layout

\begin_layout Itemize
If we restrict the parameter of the model to be in the 
\begin_inset Formula $(-1,1)$
\end_inset

 interval,
 we eliminate the problem.
 This restriction makes the MA(1) model 
\emph on
invertible
\emph default
,
 which is useful for theoretical reasons (more on this later).
\end_layout

\end_deeper
\begin_layout Itemize
Note that asymptotic identification does not rule out the possibility of lack of identification for a given data set - there may be multiple minimizing solutions in finite samples.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Increase 
\begin_inset Formula $n$
\end_inset

 in the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/chi2gmm.jl}{GMM/chi2gmm.jl}
\end_layout

\end_inset

 to see evidence of the consistency of the GMM estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Asymptotic normality
\end_layout

\begin_layout Standard
We also simply assume that the conditions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

 hold,
 so we will have asymptotic normality.
 (Note that this implies that we are assuming that the moment conditions are continuous and differentiable.) However,
 we do need to find the structure of the asymptotic variance-covariance matrix of the estimator.
 From Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

,
 we have 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]
\]

\end_inset

 where 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 is the almost sure limit of 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{n}(\theta)$
\end_inset

 when evaluated at 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta_{0})=\lim_{n\rightarrow\infty}Var\sqrt{n}\frac{\partial}{\partial\theta}s_{n}(\theta_{0}).
\]

\end_inset

 We need to determine the form of these matrices given the objective function 
\begin_inset Formula $s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta).$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Now,
 using the product rule from section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Notation-for-differentiation"
nolink "false"

\end_inset

,
 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}s_{n}(\theta)=2\left[\frac{\partial}{\partial\theta}\bar{m}_{n}^{\prime}\left(\theta\right)\right]W_{n}\bar{m}_{n}\left(\theta\right)
\]

\end_inset

 (this is analogous to 
\begin_inset Formula $\frac{\partial}{\partial\beta}\left(\beta^{\prime}X^{\prime}X\beta\right)=2X^{\prime}X\beta$
\end_inset

,
 which appears when computing the first order conditions for the OLS estimator,
 because 
\begin_inset Formula $\frac{\partial}{\partial\beta}\beta^{\prime}=I_{K}$
\end_inset

).
\end_layout

\begin_layout Standard
Define the 
\begin_inset Formula $k\times g$
\end_inset

 matrix 
\begin_inset Formula 
\begin{equation}
D_{n}(\theta)\equiv\frac{\partial}{\partial\theta}\bar{m}_{n}^{\prime}\left(\theta\right),\label{eq:derivative of moments}
\end{equation}

\end_inset

 so:
\begin_inset Formula 
\begin{equation}
\frac{\partial}{\partial\theta}s(\theta)=2D(\theta)W\bar{m}\left(\theta\right).\label{gmmscores}
\end{equation}

\end_inset

 (Note that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

,
 
\begin_inset Formula $D_{n}(\theta),$
\end_inset

 
\begin_inset Formula $W_{n}$
\end_inset

 and 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 all depend on the sample size 
\begin_inset Formula $n,$
\end_inset

 but it is omitted to unclutter the notation).
\end_layout

\begin_layout Standard

\color blue
This is the gradient vector.
 To get the GMM estimator,
 set this equal to zero,
 and solve:
\color inherit

\begin_inset Formula 
\[
D(\hat{\theta})W\bar{m}\left(\hat{\theta}\right)=0.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To take second derivatives,
 let 
\begin_inset Formula $D_{i}$
\end_inset

 be the 
\begin_inset Formula $i-$
\end_inset

 th row of 
\begin_inset Formula $D(\theta).$
\end_inset

 This is a 
\begin_inset Formula $1\times g$
\end_inset

 row vector,
 and 
\begin_inset Formula 
\[
{\color{blue}\frac{\partial}{\partial\theta_{i}}s(\theta)}=2D_{i}(\theta)W\bar{m}\left(\theta\right)
\]

\end_inset


\series bold
is a scalar
\series default
.
 It is the element in the 
\begin_inset Formula $i$
\end_inset

th row of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
the column vector 
\begin_inset Formula $\frac{\partial}{\partial\theta}s(\theta)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
.
 The 
\begin_inset Formula $i$
\end_inset

th row of the matrix of second derivatives is (using the product rule in definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "def: Product-rule.-Let"
nolink "false"

\end_inset

),
 is the derivative of this real-valued function,
 with respect to 
\begin_inset Formula $\theta^{\prime}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\theta^{\prime}}{\color{blue}\frac{\partial}{\partial\theta_{i}}s(\theta)} & = & \frac{\partial}{\partial\theta^{\prime}}\left[2D_{i}(\theta)W\bar{m}\left(\theta\right)\right]\\
 & = & 2D_{i}WD^{\prime}+2\bar{m}^{\prime}W\left[\frac{\partial}{\partial\theta^{\prime}}D_{i}^{\prime}\right]
\end{eqnarray*}

\end_inset


\series bold
\emph on
Note to self for lectures:
 
\series default
\emph default

\begin_inset Formula $D_{i}^{\prime}$
\end_inset

 is 
\begin_inset Formula $g\times1,$
\end_inset

 so 
\begin_inset Formula $\left[\frac{\partial}{\partial\theta^{\prime}}D_{i}^{\prime}\right]$
\end_inset

 is 
\begin_inset Formula $g\times k$
\end_inset

.
\begin_inset Newline newline
\end_inset

Note that the first term contains a 
\begin_inset Formula $D^{\prime},$
\end_inset

 which appears due to 
\begin_inset Formula $\frac{\partial}{\partial\theta^{\prime}}\bar{m}_{n}\left(\theta\right)$
\end_inset

,
 which is the transpose of what we defined in eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:derivative of moments"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
 When evaluating the second term:
 
\begin_inset Formula 
\[
2\bar{m}(\theta)^{\prime}W\left[\frac{\partial}{\partial\theta^{\prime}}D(\theta)_{i}^{\prime}\right]
\]

\end_inset

(where the dependence of 
\begin_inset Formula $D$
\end_inset

 upon 
\begin_inset Formula $\theta$
\end_inset

 is emphasized) at 
\begin_inset Formula $\theta_{0},$
\end_inset

 assume that 
\begin_inset Formula $\frac{\partial}{\partial\theta^{\prime}}D(\theta)_{i}^{\prime}$
\end_inset

 satisfies a LLN (it is an average),
 so that it converges almost surely to a finite limit.
 In this case,
 we have 
\begin_inset Formula 
\[
2\bar{m}(\theta_{0})^{\prime}W\left[\frac{\partial}{\partial\theta^{\prime}}D(\theta_{0})_{i}^{\prime}\right]\stackrel{a.s.}{\rightarrow}0,
\]

\end_inset

because 
\begin_inset Formula $\bar{m}(\theta_{0})\stackrel{a.s.}{\rightarrow}0$
\end_inset

 and 
\begin_inset Formula $W\stackrel{a.s.}{\rightarrow}$
\end_inset

 
\begin_inset Formula $W_{\infty}$
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Stacking these results over the 
\begin_inset Formula $k$
\end_inset

 rows of 
\begin_inset Formula $D,$
\end_inset

 we get 
\begin_inset Formula 
\[
\lim\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{n}(\theta_{0})=\mathcal{J}_{\infty}(\theta_{0})=2D_{\infty}W_{\infty}D_{\infty}^{\prime},a.s.,
\]

\end_inset

 where we define 
\begin_inset Formula $\lim D=D_{\infty},$
\end_inset

 
\begin_inset Formula $a.s.,$
\end_inset

 and 
\begin_inset Formula $\lim W=W_{\infty},$
\end_inset

 a.s.
 (we assume a LLN holds).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
With regard to 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

,
 following equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "gmmscores"
nolink "false"

\end_inset

,
 and noting that the scores (in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "gmmscores"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

) have mean zero at 
\begin_inset Formula $\theta_{0}$
\end_inset

 (since 
\begin_inset Formula $\mathcal{E}\bar{m}(\theta_{0})=0$
\end_inset

 by assumption),
 we have
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta_{0}) & = & \lim_{n\rightarrow\infty}Var\sqrt{n}\frac{\partial}{\partial\theta}s_{n}(\theta_{0})\\
 & = & \lim_{n\rightarrow\infty}\mathcal{E}4nDW\bar{m}(\theta_{0})\bar{m}(\theta_{0})^{\prime}WD^{\prime}\\
 & = & \lim_{n\rightarrow\infty}\mathcal{E}4DW\left\{ \sqrt{n}\bar{m}(\theta_{0})\right\} \left\{ \sqrt{n}\bar{m}(\theta_{0})^{\prime}\right\} WD^{\prime}
\end{eqnarray*}

\end_inset

 Now,
 given that 
\begin_inset Formula $\bar{m}(\theta_{0})$
\end_inset

 is an average of centered (mean-zero) quantities,
 it is reasonable to expect a CLT to apply,
 after multiplication by 
\begin_inset Formula $\sqrt{n}$
\end_inset

.
 Assuming this,
 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\bar{m}(\theta_{0})\stackrel{d}{\rightarrow}N(0,\Omega_{\infty}),\label{eq:CLT applied to moment conditions}
\end{equation}

\end_inset

 where 
\begin_inset Formula 
\[
\Omega_{\infty}=\lim_{n\rightarrow\infty}\mathcal{E}\left[n\bar{m}(\theta_{0})\bar{m}(\theta_{0})^{\prime}\right].
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Using this,
 and the last equation,
 we get 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta_{0})=4D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}
\]

\end_inset

 Using these results,
 the asymptotic normality theorem (
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

) gives us 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}\right],
\]

\end_inset

 the asymptotic distribution of the GMM estimator for arbitrary weighting matrix 
\begin_inset Formula $W_{n}.$
\end_inset

 
\end_layout

\begin_layout Itemize
Note that for 
\begin_inset Formula $J_{\infty}$
\end_inset

 to be positive definite,
 
\begin_inset Formula $D_{\infty}$
\end_inset

 must have full row rank,
 
\begin_inset Formula $\rho(D_{\infty})=k$
\end_inset

.
 
\end_layout

\begin_layout Itemize
This is related to identification:
 we need that the parameters cause the moments to change,
 and each parameter must cause a change that is separate from the changes caused by the other parameters.
\end_layout

\begin_layout Itemize
Identification (
\begin_inset Formula $D_{\infty}$
\end_inset

 being full row rank),
 plus 
\begin_inset Formula $W_{\infty}$
\end_inset

 being positive definite,
 imply that 
\begin_inset Formula $J_{\infty}$
\end_inset

 is positive definite.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
There are two things that affect the asymptotic variance:
\end_layout

\begin_layout Itemize
the choice of the moment conditions,
 
\begin_inset Formula $\bar{m}_{n}(\theta$
\end_inset

),
 which determines both 
\begin_inset Formula $D_{\infty}$
\end_inset

 and 
\begin_inset Formula $\Omega_{\infty}$
\end_inset


\end_layout

\begin_layout Itemize
the choice of the weight matrix 
\begin_inset Formula $W_{n}$
\end_inset

,
 which determines 
\begin_inset Formula $W_{\infty}$
\end_inset


\end_layout

\begin_layout Standard
We would probably like to know how to choose both 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 and 
\begin_inset Formula $W_{n}$
\end_inset

 so that the asymptotic variance is a small as possible.
 That will be the topic of the next section.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example

\emph on
Asymptotic normality of a GMM estimator.
 
\emph default
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/AsymptoticNormalityGMM.jl}{GMM/AsymptoticNormalityGMM.jl}
\end_layout

\end_inset

 does a Monte Carlo of the GMM estimator for the 
\begin_inset Formula $\chi^{2}$
\end_inset

 data.
 Histograms for 1000 replications of 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)$
\end_inset

 are given in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Asymptotic-Normality-of"
nolink "false"

\end_inset

.
 On the left are results for 
\begin_inset Formula $n=10,$
\end_inset

 on the right are results for 
\begin_inset Formula $n=1000.$
\end_inset

 Note that the two distributions are more or less centered at 0.
 The distribution for the small sample size is somewhat asymmetric,
 which shows that the small sample distribution may be poorly approximated by the asymptotic distribution.
 This has mostly disappeared for the larger sample size.
\begin_inset Newpage newpage
\end_inset

 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Asymptotic-Normality-of"

\end_inset

Asymptotic Normality of GMM estimator,
 
\begin_inset Formula $\chi^{2}$
\end_inset

 example
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $n=10$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Asnorm_n30.png
	lyxscale 50
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $n=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Asnorm_n1000.png
	lyxscale 50
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Choosing the weighting matrix
\end_layout

\begin_layout Standard
The following figure shows how the GMM criterion function,
 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 combines information from two moment conditions.
 The place where the green line minimizes is between the places where the red and blue lines have their roots.
 Play around with 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/GMMcriterion.jl}{GMM/GMMcriterion.jl}
\end_layout

\end_inset

 to see how changing the weight matrix changes the GMM criterion.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/GMM/gmmcriterion.png
	width 15cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $W$
\end_inset

 is a 
\shape italic
weighting matrix,

\shape default
 which determines the relative importance of violations of the individual moment conditions in the overall objective function.
 For example,
 if we are much more sure of the first moment condition,
 which is based upon the variance,
 than of the second,
 which is based upon the fourth moment,
 we could set 
\begin_inset Formula 
\[
W=\left[\begin{array}{cc}
a & 0\\
0 & b
\end{array}\right]
\]

\end_inset

 with 
\begin_inset Formula $a$
\end_inset

 much larger than 
\begin_inset Formula $b.$
\end_inset

 In this case,
 errors in the second moment condition have less weight in the objective function.
\end_layout

\begin_layout Itemize
Since moments are not independent,
 in general,
 we should expect that there be a correlation between the moment conditions,
 so it may not be desirable to set the off-diagonal elements to 0.
 
\begin_inset Formula $W$
\end_inset

 may be a random,
 data dependent matrix.
\end_layout

\begin_layout Itemize
We have already seen that the choice of 
\begin_inset Formula $W$
\end_inset

 will influence the asymptotic distribution of the GMM estimator.
 Since the GMM estimator is already inefficient w.r.t.
 MLE,
 we might like to choose the 
\begin_inset Formula $W$
\end_inset

 matrix to make the GMM estimator efficient 
\emph on
within the class of GMM estimators
\emph default
 defined by 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
To provide a little intuition,
 consider the linear model 
\begin_inset Formula $y=\mathbf{x}^{\prime}\beta+\varepsilon,$
\end_inset

 where 
\begin_inset Formula $\varepsilon\sim N(0,\Omega).$
\end_inset

 That is,
 he have heteroscedasticity and autocorrelation.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The generalized least square estimator minimizes the objective function 
\begin_inset Formula $(y-\mathbf{X}\beta)^{\prime}\Omega^{-1}(y-\mathbf{X}\beta).$
\end_inset

 We have seen that the GLS estimator is efficient with respect to OLS,
 when there is het.
 and or aut.
 
\end_layout

\begin_layout Itemize
The GLS optimal weighting matrix is seen to be the inverse of the covariance matrix of the errors.
 A similar result holds for GMM estimation.
 
\end_layout

\begin_layout Itemize
Note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

this presentation of GLS is not a GMM estimator as defined above,
 because if we take the errors as 
\begin_inset Quotes sld
\end_inset

moment conditions
\begin_inset Quotes srd
\end_inset

,
 the dimension is the sample size,
 
\begin_inset Formula $n.$
\end_inset

 Thus,
 the dimension is not fixed.
 Also,
 they are not averages,
 as we require - see definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "GMM estimator (defn)"
nolink "false"

\end_inset

.
 Later we'll see that GLS can be expressed in the GMM framework.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "efficient weighting matrix"

\end_inset

If 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a GMM estimator that minimizes 
\begin_inset Formula $\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta),$
\end_inset

 the asymptotic variance of 
\begin_inset Formula $\hat{\theta}$
\end_inset

 will be minimized by choosing 
\begin_inset Formula $W_{n}$
\end_inset

 so that 
\begin_inset Formula $W_{n}\stackrel{a.s}{\rightarrow}W_{\infty}=\Omega_{\infty}^{-1},$
\end_inset

 where 
\begin_inset Formula $\Omega_{\infty}=\lim_{n\rightarrow\infty}\mathcal{E}\left[nm(\theta_{0})m(\theta_{0})^{\prime}\right].$
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Proof:

\series default
 For 
\begin_inset Formula $W_{\infty}=\Omega_{\infty}^{-1},$
\end_inset

 the asymptotic variance 
\begin_inset Formula 
\[
\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}
\]

\end_inset

 simplifies to 
\begin_inset Formula $\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}.$
\end_inset

 Now,
 let 
\begin_inset Formula $A$
\end_inset

 be the difference between the general form and the simplified form:
 
\begin_inset Formula 
\begin{eqnarray*}
A=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1} & - & \left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}
\end{eqnarray*}

\end_inset

Set 
\begin_inset Formula $B=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}-\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1}$
\end_inset

.
 One can show that 
\begin_inset Formula $A=B\Omega_{\infty}B^{'}$
\end_inset

.
 This is a quadratic form in a p.d.
 matrix,
 so it is p.s.d.,
 which concludes the proof.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
The result
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}\right]\label{gmm distribution with optimal weighting matrix}
\end{equation}

\end_inset

 allows us to treat 
\begin_inset Formula 
\[
\hat{\theta}\approx N\left(\theta_{0},\frac{\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}}{n}\right),
\]

\end_inset

 where the 
\begin_inset Formula $\approx$
\end_inset

 means 
\begin_inset Quotes erd
\end_inset

approximately distributed as.
\begin_inset Quotes erd
\end_inset

 To operationalize this we need estimators of 
\begin_inset Formula $D_{\infty}$
\end_inset

 and 
\begin_inset Formula $\Omega_{\infty}.$
\end_inset


\end_layout

\begin_layout Itemize
The obvious estimator of 
\begin_inset Formula $\widehat{D_{\infty}}$
\end_inset

 is simply 
\begin_inset Formula $\frac{\partial}{\partial\theta}\bar{m}_{n}\left(\hat{\theta}\right),$
\end_inset

 which is consistent by the consistency of 
\begin_inset Formula $\hat{\theta},$
\end_inset

 assuming that 
\begin_inset Formula $\frac{\partial}{\partial\theta}\bar{m}_{n}$
\end_inset

 is continuous in 
\begin_inset Formula $\theta.$
\end_inset

 Stochastic equicontinuity results can give us this result even if 
\begin_inset Formula $\frac{\partial}{\partial\theta}\bar{m}_{n}$
\end_inset

 is not continuous.
\end_layout

\begin_layout Itemize
Estimating 
\begin_inset Formula $\Omega_{\infty}$
\end_inset

 is taken up below.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
To see the effect of using an efficient weight matrix,
 consider the Julia script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/EfficientGMM.jl}{GMM/EfficientGMM.jl}
\end_layout

\end_inset

.
 This modifies the previous Monte Carlo for the 
\begin_inset Formula $\chi^{2}$
\end_inset

 data.
 This new Monte Carlo computes the GMM estimator in two ways:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Itemize
based on an identity weight matrix
\end_layout

\begin_layout Itemize
using an estimated optimal weight matrix.
 The estimated efficient weight matrix is computed as the inverse of the estimated covariance of the moment conditions,
 using the inefficient estimator of the first step.
 See the next section for more on how to do this.
\end_layout

\begin_layout Itemize
The following figure shows the results,
 plotting densities for 1000 replications of 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)$
\end_inset

.
 Note that the use of the estimated efficient weight matrix leads to much better results in this case.
 This is a simple case where it is possible to get a good estimate of the efficient weight matrix.
 This is not always so.
 See the next section.
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Examples/GMM/Efficient.png
	lyxscale 50
	width 10cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
What's the intuition behind 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}\right]$
\end_inset

?
\end_layout

\begin_layout Itemize
More variability of the moment conditions (
\begin_inset Formula $\Omega_{\infty}$
\end_inset

 
\begin_inset Quotes sld
\end_inset

large
\begin_inset Quotes srd
\end_inset

) leads to a more variable estimator,
 other things equal.
\end_layout

\begin_layout Itemize
The larger the magnitude of the elements of the Jacobian of the moment conditions,
 
\begin_inset Formula $D_{\infty}$
\end_inset

,
 the smaller the variance of the estimator.
 Moment conditions which change more in response to changes in parameters lead to more precise estimates.
\end_layout

\begin_layout Itemize

\emph on
in 
\emph default
class
\emph on
:
 
\emph default
make a figure,
 much like Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Effects-of-"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 .
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Estimation of the variance-covariance matrix
\end_layout

\begin_layout Standard

\series bold
(See Hamilton Ch.
 10,
 pp.
 261-2 and 280-84)
\series default

\begin_inset Formula $^{*}$
\end_inset


\series bold
.
\end_layout

\begin_layout Standard
In the case that we wish to use the optimal weighting matrix,
 we need an estimate of 
\begin_inset Formula $\Omega_{\infty},$
\end_inset

 the limiting variance-covariance matrix of 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\theta_{0})$
\end_inset

.
 Remember that 
\begin_inset Formula $\bar{m}_{n}$
\end_inset

 is the average of the moment contributions,
 
\begin_inset Formula $m_{t}$
\end_inset

,
 and,
 by assumption,
 
\begin_inset Formula $E(m_{t}(\theta_{0})=0.$
\end_inset

 In general,
 we expect that:
\end_layout

\begin_layout Itemize
\begin_inset Formula $m_{t}$
\end_inset

 will be autocorrelated (
\begin_inset Formula $\Gamma_{ts}=\mathcal{E}(m_{t}m_{t-s}^{\prime})\neq0$
\end_inset

).
 Note that this autocovariance matrix,
 which is of dimension 
\begin_inset Formula $g\times g,$
\end_inset

 will not depend on 
\begin_inset Formula $t$
\end_inset

 if the moment conditions are covariance stationary.
\end_layout

\begin_layout Itemize
contemporaneously correlated,
 since the individual moment contributions will not in general be independent of one another (
\begin_inset Formula $\mathcal{E}(m_{it}m_{jt})\neq0$
\end_inset

),
 where 
\begin_inset Formula $i,j\in\{1,2,...,k\}$
\end_inset

.
\end_layout

\begin_layout Itemize
and have different variances (
\begin_inset Formula $\mathcal{E}(m_{it}^{2})=\sigma_{it}^{2}$
\end_inset

 ).
 
\end_layout

\begin_layout Standard
Since we need to estimate so many components,
 it is unlikely that we would arrive at a correct parametric specification.
 For this reason,
 research has focused on consistent nonparametric estimators of 
\begin_inset Formula $\Omega_{\infty}.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Henceforth we assume that 
\begin_inset Formula $m_{t}$
\end_inset

 is 
\emph on
covariance stationary
\emph default
,
 so the covariance between 
\begin_inset Formula $m_{t}$
\end_inset

 and 
\begin_inset Formula $m_{t-s}$
\end_inset

 does not depend on 
\begin_inset Formula $t.$
\end_inset

 (See the first part of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Models-for-time"
nolink "false"

\end_inset

 for the definition).
 Thus,
 
\end_layout

\begin_layout Definition
(Autocovariance).
 Define the 
\begin_inset Formula $s-th$
\end_inset

 autocovariance of covariance stationary moment contributions as 
\begin_inset Formula $\Gamma_{s}=\mathcal{E}(m_{t}m_{t-s}^{\prime}).$
\end_inset

 
\end_layout

\begin_layout Standard
Because of stationarity,
 
\begin_inset Formula $\Gamma_{s}$
\end_inset

 does not depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Exercise
Show that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+s}^{\prime})=\Gamma_{s}^{\prime}.$
\end_inset

 Hint:
 note that 
\begin_inset Formula $E(A^{\prime})=\left[E(A)\right]^{\prime}.$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Recall that 
\begin_inset Formula $m_{t}$
\end_inset

 and 
\begin_inset Formula $\bar{m}_{n}$
\end_inset

 are functions of 
\begin_inset Formula $\theta,$
\end_inset

 so,
 for now,
 assume that we have some consistent estimator of 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 With this,
 a consistent estimator of 
\begin_inset Formula $m_{t}(\theta_{0})$
\end_inset

 is 
\begin_inset Formula $\hat{m}_{t}=m_{t}(\hat{\theta}).$
\end_inset

 Now 
\begin_inset Formula 
\begin{eqnarray*}
\Omega_{n} & = & \mathcal{E}\left[n\bar{m}_{n}(\theta_{0})\bar{m}_{n}(\theta_{0})^{\prime}\right]=\mathcal{E}\left[n\left(1/n\sum_{t=1}^{n}m_{t}\right)\left(1/n\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]\\
 & = & \mathcal{E}\left[1/n\left(\sum_{t=1}^{n}m_{t}\right)\left(\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]\\
 & = & \mathcal{E}\left[1/n\left(m_{1}+m_{2}+...+m_{n}\right)\left(m_{1}^{\prime}+m_{2}^{\prime}+...+m_{n}^{\prime}\right)\right]\\
 & = & \Gamma_{0}+\frac{n-1}{n}\left(\Gamma_{1}+\Gamma_{1}^{\prime}\right)+\frac{n-2}{n}\left(\Gamma_{2}+\Gamma_{2}^{\prime}\right)\cdots+\frac{1}{n}\left(\Gamma_{n-1}+\Gamma_{n-1}^{\prime}\right)
\end{eqnarray*}

\end_inset

 A natural,
 consistent estimator of 
\begin_inset Formula $\Gamma_{s}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\Gamma_{s}}=1/n\sum_{t=s+1}^{n}\hat{m}_{t}\hat{m}_{t-s}^{\prime}.
\]

\end_inset

 (you might use 
\begin_inset Formula $n-s$
\end_inset

 in the denominator instead).
 This is consistent,
 holding 
\begin_inset Formula $s$
\end_inset

 fixed and letting 
\begin_inset Formula $n$
\end_inset

 grow,
 because of the LLN,
 and the fact that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is consistent for 
\begin_inset Formula $\theta_{0}$
\end_inset

 (Slutsky theorem).
 
\begin_inset Newpage newpage
\end_inset

 So,
 a natural,
 but inconsistent,
 estimator of 
\begin_inset Formula $\Omega_{\infty}$
\end_inset

 would be 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Omega} & = & \widehat{\Gamma_{0}}+\frac{n-1}{n}\left(\widehat{\Gamma_{1}}+\widehat{\Gamma_{1}^{\prime}}\right)+\frac{n-2}{n}\left(\widehat{\Gamma_{2}}+\widehat{\Gamma_{2}^{\prime}}\right)+\cdots+\left(\widehat{\Gamma_{n-1}}+\widehat{\Gamma_{n-1}^{\prime}}\right)\\
 & = & \widehat{\Gamma_{0}}+\sum_{s=1}^{n-1}\frac{n-s}{n}\left(\widehat{\Gamma_{s}}+\widehat{\Gamma_{s}^{\prime}}\right).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
This estimator is inconsistent in general,
 because the number of parameters to estimate is more than the number of observations,
 and increases more rapidly than 
\begin_inset Formula $n$
\end_inset

,
 so information does not build up as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Itemize
There is always only one observation to estimate the highest order autocovariance.
\end_layout

\begin_layout Standard
On the other hand,
 supposing that 
\begin_inset Formula $\Gamma_{s}$
\end_inset

 tends to zero 
\emph on
sufficiently rapidly
\emph default
 as 
\begin_inset Formula $s$
\end_inset

 tends to 
\begin_inset Formula $\infty,$
\end_inset

 a modified estimator is 
\begin_inset Formula 
\[
\hat{\Omega}=\widehat{\Gamma_{0}}+\sum_{s=1}^{q(n)}\left(\widehat{\Gamma_{s}}+\widehat{\Gamma_{s}^{\prime}}\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
this will be consistent,
 provided that
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $q(n)\stackrel{p}{\rightarrow}\infty$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

,
 so that the higher order terms are eventually estimated,
 as the sample size becomes large enough (control the bias)
\end_layout

\begin_layout Itemize
\begin_inset Formula $q(n)$
\end_inset

 grows sufficiently slowly,
 so that we don't try to estimate terms unless we have enough information (control the variance)
\end_layout

\begin_layout Itemize
The term 
\begin_inset Formula $\frac{n-s}{n}$
\end_inset

 can be dropped because it converges to 1.
\end_layout

\begin_layout Itemize
as long as there is a balance between the rate at which 
\begin_inset Formula $q(n)$
\end_inset

 grows and the rate at which autocovariances decline over time,
 this will be consistent.
\end_layout

\begin_layout Itemize
A disadvantage of this estimator is that it may not be positive definite.
 This could cause one to calculate a negative 
\begin_inset Formula $\chi^{2}$
\end_inset

 statistic,
 for example!
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Heteroscedasticity and autocorrelation consistent covariance estimators
\end_layout

\begin_layout Standard
The Newey-West estimator (
\begin_inset CommandInset citation
LatexCommand citealp
key "NeweyWest1987"
literal "true"

\end_inset

) solves the problem of possible non-positive definiteness of the above estimator.
 Their estimator is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\Omega}=\widehat{\Gamma_{0}}+\sum_{s=1}^{q(n)}\left[1-\frac{s}{q+1}\right]\left(\widehat{\Gamma_{s}}+\widehat{\Gamma_{s}^{\prime}}\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
This estimator is p.d.
 by construction.
 
\end_layout

\begin_layout Itemize
The condition for consistency is that 
\begin_inset Formula $n^{-1/4}q\rightarrow0.$
\end_inset

 Note that this is a very slow rate of growth for 
\begin_inset Formula $q.$
\end_inset

 
\end_layout

\begin_layout Itemize
This estimator is nonparametric - we've placed no parametric restrictions on the form of 
\begin_inset Formula $\Omega.$
\end_inset

 It is an example of a 
\shape italic
kernel
\shape default
 estimator.
 Kernel estimators are discussed in more detail in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Nonparametric-inference"
nolink "false"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Around the same time as the paper by Newey and West,
 a number of other similar covariance matrix estimators were proposed,
 but the NW estimator seems to be the most widely used in empirical work.
\end_layout

\begin_layout Itemize
If there is no autocorrelation of the moments,
 then all 
\begin_inset Formula $\Gamma_{s},\,s>0$
\end_inset

 are in fact zero,
 and don't need to be estimated.
 Then,
 
\begin_inset Formula $\hat{\Omega}=\widehat{\Gamma_{0}}$
\end_inset

 is White's heteroscedastic consistent variance covariance estimator,
 
\begin_inset CommandInset citation
LatexCommand citet
key "white1980heteroskedasticity"
literal "true"

\end_inset

.
 This earlier paper no doubt inspired the papers that came later.
 
\end_layout

\begin_layout Itemize
A Julia implementation is at 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./src/NP/NeweyWest.jl}{NeweyWest.jl}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
in class:
 
\series default
Use Gretl for examples of both:
\end_layout

\begin_deeper
\begin_layout Itemize
het:
 Nerlove model:
 look at t stats with ordinary and White standard errors.
\end_layout

\begin_layout Itemize
aut:
 SP500 data (in Julia directory,
 load the csv file),
 rets and rets squared.
 Estimate an AR(4),
 with plain and HAC standard errors.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Two step and continuously updated GMM estimators
\end_layout

\begin_layout Subsection

\series bold
Two step GMM estimator:
\end_layout

\begin_layout Standard
The most common way to do efficient GMM estimation is the two step GMM estimator:
\end_layout

\begin_layout Enumerate
Set the weight matrix to some positive definite matrix.
 Most commonly,
 one uses an identity matrix of order 
\begin_inset Formula $g.$
\end_inset

 Obtain the GMM estimator that minimizes 
\begin_inset Formula $s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}W\bar{m}_{n}(\theta)$
\end_inset


\end_layout

\begin_layout Enumerate
Based on this initial estimate,
 
\begin_inset Formula $\hat{\theta}$
\end_inset

,
 compute the moment contributions 
\begin_inset Formula $m_{t}(\hat{\theta}),\,t=1,2,...,n$
\end_inset

.
 Compute an estimate of 
\begin_inset Formula $\Omega_{\infty}$
\end_inset

 based on the moment contributions,
 say 
\begin_inset Formula $\hat{\Omega}$
\end_inset

.
 The exact way to do this will depend upon the assumptions of the model.
 For example,
 if moment conditions are suspected to be autocorrelated,
 one might use the Newey-West estimator.
 Given the estimate,
 compute the efficient GMM estimator which minimizes
\begin_inset Formula 
\[
s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}\hat{\Omega}^{-1}\bar{m}_{n}(\theta).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\hat{\Omega}^{-1}$
\end_inset

 is fixed while numeric minimization finds the second step estimator.
 The result is the two step estimator.
 
\end_layout

\begin_layout Itemize
An example of this is given by running 
\family typewriter
gmmresults()
\family default
,
 using 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./src/GMM/gmmresults.jl}{gmmresults.jl}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Matlab/Octave code for GMM for Nerlove model.
 Examine and run 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/TwoStepGMM.m}{TwoStepGMM.m} 
\end_layout

\end_inset

,
 which illustrates how to do two step GMM for the Nerlove data.
 Note that the GMM results are the same as what you get estimating by OLS.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection

\series bold
Continuously updated GMM estimator:
\end_layout

\begin_layout Standard
The continuously updated estimator (
\begin_inset CommandInset citation
LatexCommand citet
key "HansenHeatonYaron1996"
literal "true"

\end_inset

) solves a minimization problem where the efficient weight matrix is estimated at each iteration of the numeric optimization process,
 using the current value of 
\begin_inset Formula $\theta$
\end_inset

.
 The CUE estimator solves the minimization problem 
\begin_inset Formula 
\[
s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}\hat{\Omega}(\theta)^{-1}\bar{m}_{n}(\theta).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that the covariance of the moment conditions will be updated at each trial value of the objective function during the course of minimization.
 
\end_layout

\begin_layout Itemize
This estimator is equivalent to an iterated version of the two step estimator.
 
\end_layout

\begin_layout Itemize
The CUE estimator can be shown to have a smaller bias than does the two step estimator,
 which may have a large small sample bias (
\begin_inset CommandInset citation
LatexCommand citet
key "NeweySmith2003"
literal "true"

\end_inset

).
 
\end_layout

\begin_layout Itemize
An example of CUE estimation is given by running 
\family typewriter
gmmresults()
\family default
,
 using 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./src/GMM/gmmresults.jl}{gmmresults.jl}
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
The two step estimator may be iterated,
 by recomputing the weight after the second step,
 and estimating again with the new weight.
 If this is done until neither the parameters nor the weight matrix change,
 it is equivalent to doing CUE (assuming we find the global minimizer in both cases).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Estimation-using-conditional"

\end_inset

Estimation using conditional moments
\end_layout

\begin_layout Standard
So far,
 the moment conditions have been presented as unconditional expectations.
 One common way of defining unconditional moment conditions is based upon conditional moment conditions.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
This is important from the point of view of constructing an econometric model,
 since economic models often imply restrictions on 
\emph on
conditional
\emph default
 moments.
\end_layout

\begin_layout Itemize
Suppose a model tells us that the function 
\begin_inset Formula $\epsilon(d_{t},\theta_{0})$
\end_inset

 has expectation,
 conditional on the information set 
\begin_inset Formula $I_{t},$
\end_inset

 equal to zero
\begin_inset Formula 
\[
E\left(\epsilon(d_{t},\theta_{0})|I_{t}\right)=0
\]

\end_inset

 where 
\begin_inset Formula $\epsilon$
\end_inset

 is meant to suggest 
\begin_inset Quotes sld
\end_inset

error
\begin_inset Quotes srd
\end_inset

 and 
\begin_inset Formula $d_{t}$
\end_inset

 is just a vector of endogenous and exogenous variables (
\begin_inset Formula $d$
\end_inset

 is for 
\begin_inset Quotes sld
\end_inset

data
\begin_inset Quotes srd
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
For example,
 consider the supply equation of eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:demand function"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

:
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\[
q_{t}=\beta_{1}^{0}+\beta{}_{2}^{0}p_{t}+\varepsilon_{2t}
\]

\end_inset

 where income,
 
\begin_inset Formula $m_{t},$
\end_inset

 is exogenous,
 and 
\begin_inset Formula $E(\epsilon_{2t}|m_{t})=0$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Set 
\begin_inset Formula $\epsilon(q_{t},p_{t},\theta_{0})=$
\end_inset


\begin_inset Formula $q_{t}-\beta_{1}^{0}-\beta_{2}^{0}p_{t}=\varepsilon_{2t}$
\end_inset

.
 Note:
 
\begin_inset Formula $d_{t}=(q_{t},p_{t}),$
\end_inset

 and the parameter vector 
\begin_inset Formula $\theta$
\end_inset

 holds all parameters,
 so,
 in this case 
\begin_inset Formula $\theta=(\beta_{1},\beta_{2})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
So 
\begin_inset Formula $\epsilon(q_{t},p_{t},\theta_{0})=\epsilon_{2t}$
\end_inset

,
 which has conditional expectation,
 given 
\begin_inset Formula $m_{t},$
\end_inset

 equal to zero,
 by assumption.
 
\end_layout

\begin_layout Itemize
At other parameter values,
 
\begin_inset Formula $\epsilon(q_{t},p_{t},\theta)=q_{t}-\beta_{1}+\beta_{2}p_{t}\ne\epsilon_{2t},$
\end_inset

 and this will not have conditional expectation equal to zero.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Returning to the general case,
 if identification holds,
 then we will have
\begin_inset Formula 
\[
E\epsilon_{t}(d_{t},\theta)|I_{t}\ne0,\,\theta\ne\theta_{0}.
\]

\end_inset

This is a 
\series bold
scalar
\series default
 moment condition,
 which isn't sufficient to identify a 
\begin_inset Formula $K$
\end_inset

 -dimensional parameter 
\begin_inset Formula $\theta$
\end_inset

 
\begin_inset Formula $(K>1)$
\end_inset

.
 However,
 we can take any variables in the information set,
 say,
 
\begin_inset Formula $w_{t}$
\end_inset

 (which may be vector-valued),
 and we may use tranformations of these variables,
 say,
 
\begin_inset Formula $Z(w_{t}),$
\end_inset

to obtain the moment contributions 
\begin_inset Formula 
\[
m_{t}(\theta)=Z(w_{t})\epsilon_{t}(d_{t},\theta).
\]

\end_inset

Now,
 let 
\begin_inset Formula $Z(w_{t})$
\end_inset

 be a 
\begin_inset Formula $g\times1$
\end_inset

-vector valued function of 
\begin_inset Formula $w_{t}$
\end_inset

 and 
\begin_inset Formula $w_{t}$
\end_inset

 is a set of variables drawn from the information set 
\begin_inset Formula $I_{t}.$
\end_inset

 The 
\begin_inset Formula $Z(w_{t})\;$
\end_inset

are 
\emph on
instrumental variables.

\emph default
 We now have 
\begin_inset Formula $g$
\end_inset

 moment conditions,
 so as long as 
\begin_inset Formula $g>K$
\end_inset

 the necessary condition for identification holds.
 
\end_layout

\begin_layout Itemize
By the law of iterated expectation (
\begin_inset CommandInset ref
LatexCommand ref
reference "prop:Law-of-iterated"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

),
 
\begin_inset Formula $E(m_{t}(\theta_{0}))=0.$
\end_inset


\end_layout

\begin_layout Itemize
The other thing we need,
 for identification,
 is that 
\begin_inset Formula $E(m_{t}(\theta))\ne0,\,\forall\theta\ne\theta⁰$
\end_inset

.
\end_layout

\begin_layout Itemize
In the above supply function example,
 
\begin_inset Formula $m_{t}$
\end_inset

 (income) is a variable in the information set.
 We can create instruments by using functions of 
\begin_inset Formula $m_{t}.$
\end_inset

 Using only 
\begin_inset Formula $m_{t}$
\end_inset

 itself,
 by the law of iterated expectations,
 
\begin_inset Formula $E\left(m_{t}\epsilon_{t}(\theta_{0})\right)=0.$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
In this example,
 we have only one moment condition,
 so far.
 We would need additional instruments to identify the parameters.
 We can use the constant as an instrument:
 
\begin_inset Formula $Z(m_{t}^{0})=1,\forall t$
\end_inset

.
 Combining this with the previous instrument gives us two instruments,
 which is enough to estimate the two 
\begin_inset Formula $\beta$
\end_inset

 parameters.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Following the idea,
 one can form the 
\begin_inset Formula $n\times g$
\end_inset

 matrix 
\begin_inset Formula 
\begin{eqnarray*}
Z_{n} & = & \left[\begin{array}{llll}
Z_{1}(w_{1}) & Z_{2}(w_{1}) & \cdots & Z_{g}(w_{1})\\
Z_{1}(w_{2}) & Z_{2}(w_{2}) &  & Z_{g}(w_{2})\\
\vdots &  &  & \vdots\\
Z_{1}(w_{n}) & Z_{2}(w_{n}) & \cdots & Z_{g}(w_{n})
\end{array}\right]\\
 & = & \left[\begin{array}{c}
Z_{1}^{\prime}\\
Z_{2}^{\prime}\\
\\Z_{n}^{\prime}
\end{array}\right]
\end{eqnarray*}

\end_inset

 With this we can form the 
\begin_inset Formula $g$
\end_inset

 moment conditions 
\begin_inset Formula 
\begin{eqnarray*}
\bar{m}_{n}(\theta) & = & \frac{1}{n}Z_{n}^{\prime}\left[\begin{array}{l}
\epsilon_{1}(\theta)\\
\epsilon_{2}(\theta)\\
\vdots\\
\epsilon_{n}(\theta)
\end{array}\right]
\end{eqnarray*}

\end_inset

With this,
 we can write
\begin_inset Formula 
\begin{align*}
\bar{m}_{n}(\theta) & =\frac{1}{n}\sum_{t=1}^{n}Z_{t}\epsilon_{t}(\theta)\\
 & =\frac{1}{n}\sum_{t=1}^{n}m_{t}(\theta)
\end{align*}

\end_inset

where 
\begin_inset Formula $Z_{(t,\cdot)}$
\end_inset

 is the 
\begin_inset Formula $t^{th}$
\end_inset

 row of 
\begin_inset Formula $Z_{n}.$
\end_inset

 
\end_layout

\begin_layout Itemize
This fits the previous treatment.
 It is an example of how moment conditions may be defined from conditional expectations and instrumental variables created using the conditioning information.
\end_layout

\begin_layout Itemize
once we have this,
 we just go on to do GMM,
 as usual.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example:-Generalized-instrumental"

\end_inset

Generalized instrumental variables estimator for linear models
\end_layout

\begin_layout Standard
The IV estimator may appear a bit unusual at first,
 but it will grow on you over time.
 
\end_layout

\begin_layout Standard
Let's look at the previous section's results in more detail,
 for the commonly encountered 
\color blue
special case of a linear model
\color inherit
 with iid errors (the iid part is easy to generalize),
 but with correlation between regressors and errors (failure of weak exogeneity):
 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\theta_{0}+\varepsilon_{t}\\
\mathcal{E}(\varepsilon_{t}|x_{t}) & \neq & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Let's assume,
 just to keep things simple,
 that the errors are iid (homoscedastic and non-autocorrelated).
 The generalization is very straightforward,
 but we want to focus on the endogeneity problem,
 for now.
\end_layout

\begin_layout Itemize
The model in matrix form is 
\begin_inset Formula $y=X\theta_{0}+\epsilon$
\end_inset

,
 
\color blue
which is linear in the parameters
\end_layout

\begin_layout Itemize
when weak exogeneity fails,
 we know that the OLS estimator will be inconsistent,
 in general
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
We have seen some cases where this problem arises:
\end_layout

\begin_layout Enumerate
measurement error of regressors:
 Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"
nolink "false"

\end_inset


\end_layout

\begin_layout Enumerate
lagged dependent variable and autocorrelated errors:
 Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:-Dynamic-model"
nolink "false"

\end_inset


\end_layout

\begin_layout Enumerate
simultaneous equations:
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Simultaneous-equations"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Let 
\begin_inset Formula $K=dim(x_{t}).$
\end_inset

 Consider some vector 
\begin_inset Formula $z_{t}$
\end_inset

 of dimension 
\begin_inset Formula $G\times1$
\end_inset

,
 where 
\begin_inset Formula $G\ge K.$
\end_inset

 Assume that 
\begin_inset Formula $E(\epsilon_{t}|z_{t})=0.$
\end_inset

 The variables 
\begin_inset Formula $z_{t}$
\end_inset

 are 
\emph on
instrumental variables.

\emph default
 
\end_layout

\begin_layout Standard
Consider the moment conditions
\begin_inset Formula 
\begin{align*}
m_{t}(\theta) & =z_{t}\epsilon_{t}\\
 & =z_{t}\left(y_{t}-x_{t}^{\prime}\theta\right)
\end{align*}

\end_inset

We can arrange the instruments in the 
\begin_inset Formula $n\times G$
\end_inset

 matrix
\begin_inset Formula 
\begin{eqnarray*}
Z & = & \left[\begin{array}{c}
z_{1}^{\prime}\\
z_{2}^{\prime}\\
\vdots\\
z_{n}^{\prime}
\end{array}\right]
\end{eqnarray*}

\end_inset

The average moment conditions are
\begin_inset Formula 
\begin{align*}
\bar{m}_{n}(\theta) & =\frac{1}{n}Z^{\prime}\epsilon\\
 & =\frac{1}{n}(Z^{\prime}y-Z^{\prime}X\theta)
\end{align*}

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The 
\emph on
generalized instrumental variables
\emph default
 estimator is just the GMM estimator based upon these moment conditions.
 
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $G=K$
\end_inset

,
 we have exact identification,
 and it is referred to as the instrumental variables estimator.
 
\end_layout

\begin_layout Itemize
Given the form of the moment conditions,
 the general formulae for GMM lead to particular forms for the GIV estimator:
\end_layout

\begin_layout Standard
The first order conditions for GMM are 
\begin_inset Formula $D_{n}W_{n}\bar{m}_{n}(\hat{\theta})=0$
\end_inset

,
 which imply that 
\begin_inset Formula 
\[
D_{n}W_{n}Z^{\prime}X\hat{\theta}_{IV}=D_{n}W_{n}Z^{\prime}y
\]

\end_inset


\end_layout

\begin_layout Exercise
Verify that,
 in our special case,
 
\begin_inset Formula $D_{n}=-\frac{X^{\prime}Z}{n}$
\end_inset

.
 Remember that (assuming differentiability) identification of the GMM estimator requires that this matrix must converge to a matrix with full row rank.
 Can just any variable that is uncorrelated with the error be used as an instrument,
 or is there some other condition?
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Exercise
Verify that the efficient weight matrix is 
\begin_inset Formula $W_{n}=\left(\frac{Z^{\prime}Z}{n}\right)^{-1}$
\end_inset

 (up to a constant).
 Note,
 this would not hold if the errors were not i.i.d.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

If we accept what is stated in these two exercises,
 then the f.o.c.
\begin_inset Formula 
\[
D_{n}W_{n}Z^{\prime}X\hat{\theta}_{IV}=D_{n}W_{n}Z^{\prime}y
\]

\end_inset

become
\begin_inset Formula 
\[
\frac{X^{\prime}Z}{n}\left(\frac{Z^{\prime}Z}{n}\right)^{-1}Z^{\prime}X\hat{\theta}_{IV}=\frac{X^{\prime}Z}{n}\left(\frac{Z^{\prime}Z}{n}\right)^{-1}Z^{\prime}y
\]

\end_inset

Noting that the powers of 
\begin_inset Formula $n$
\end_inset

 cancel,
 we get
\begin_inset Formula 
\[
X^{\prime}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}X\hat{\theta}_{IV}=X^{\prime}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}y
\]

\end_inset

or
\begin_inset Formula 
\begin{equation}
\hat{\theta}_{IV}=\left(X^{\prime}{\color{red}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}}X\right)^{-1}X^{\prime}{\color{red}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}}y\label{eq:GIV estimator}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Another way of arriving to the same point is to define the projection matrix 
\begin_inset Formula $P_{Z}$
\end_inset

 
\begin_inset Formula 
\[
P_{Z}=Z(Z^{\prime}Z)^{-1}Z^{\prime}
\]

\end_inset


\end_layout

\begin_layout Itemize
The projection matrix finds the part of an object that is in the space spanned by 
\begin_inset Formula $Z.$
\end_inset

 
\end_layout

\begin_layout Itemize
Anything in this space is uncorrelated with the 
\begin_inset Formula $\epsilon$
\end_inset

,
 by the fact that the elements of 
\begin_inset Formula $Z$
\end_inset

 are drawn from the information set.
\end_layout

\begin_layout Standard
Transforming the model with this projection matrix we get 
\begin_inset Formula 
\[
P_{Z}y=P_{Z}X\beta+P_{Z}\varepsilon
\]

\end_inset

 or 
\begin_inset Formula 
\[
y^{*}=X^{*}\theta+\varepsilon^{*}
\]

\end_inset

 Now we have that 
\begin_inset Formula $\varepsilon^{*}$
\end_inset

 and 
\begin_inset Formula $X^{*}$
\end_inset

 are uncorrelated,
 since this is simply 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(X^{*\prime}\varepsilon^{*}) & = & E(X^{\prime}P_{Z}^{\prime}P_{Z}\varepsilon)\\
 & = & E({\color{red}X^{\prime}P_{Z}}{\color{blue}{\color{blue}\varepsilon}})\\
 & = & 0_{K\times1}
\end{eqnarray*}

\end_inset

 because 
\begin_inset Formula 
\[
P_{Z}X={\color{red}Z(Z^{\prime}Z)^{-1}Z^{\prime}X{\color{blue}\equiv X^{*}}}
\]

\end_inset

 is the fitted value from a regression of 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $Z.$
\end_inset

 This is a linear combination of the columns of 
\begin_inset Formula $Z,$
\end_inset

 so it must be uncorrelated with 
\color blue

\begin_inset Formula $\varepsilon$
\end_inset


\color inherit
,
 because we initially assumed that 
\begin_inset Formula $E(Z_{t}\epsilon_{t})=0$
\end_inset

.
 
\end_layout

\begin_layout Itemize
So,
 This implies that applying OLS to the model 
\begin_inset Formula 
\[
y^{*}=X^{*}\theta+\varepsilon^{*}
\]

\end_inset

 will lead to a consistent estimator,
 given a few more assumptions (identification).
\end_layout

\begin_layout Exercise
Verify algebraically that applying OLS to the above model gives the IV estimator of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:GIV estimator"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
With the definition of 
\begin_inset Formula $P_{Z}$
\end_inset

,
 we can write
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{\theta}_{IV}=(X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}{\color{red}y}\label{eq:GIVestimator2}
\end{equation}

\end_inset

 from which we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\theta}_{IV} & = & (X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}({\color{red}X\theta_{0}+\varepsilon})\\
 & = & \theta_{0}+(X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}\varepsilon
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\theta}_{IV}-\theta_{0} & = & (X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}\varepsilon\\
 & = & \left(X^{\prime}Z(Z^{\prime}Z)^{-1}Z^{\prime}X\right)^{-1}X^{\prime}Z(Z^{\prime}Z)^{-1}Z^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Now we can introduce factors of 
\begin_inset Formula $n$
\end_inset

 to get 
\begin_inset Formula 
\[
\hat{\theta}_{IV}-\theta_{0}=\left(\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}X}{n}\right)\right)^{-1}\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}\varepsilon}{n}\right)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Assuming that each of the terms with a 
\begin_inset Formula $n$
\end_inset

 in the denominator satisfies a LLN,
 so that
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{Z^{\prime}Z}{n}\overset{p}{\rightarrow}Q_{ZZ}$
\end_inset

,
 a finite pd matrix
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{X^{\prime}Z}{n}\overset{p}{\rightarrow}Q_{XZ},$
\end_inset

 a finite matrix with rank 
\begin_inset Formula $K$
\end_inset

 (= cols
\begin_inset Formula $(X)$
\end_inset

 ).
 That is to say,
 the instruments must be correlated with the regressors.
 More precisely,
 each regressor must be correlated with at least one instrument.
 Otherwise,
 the row of 
\begin_inset Formula $Q_{XZ}$
\end_inset

 corresponding to that regressor would be all zeros,
 and thus the rank of the matrix would be less than 
\begin_inset Formula $K.$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{Z^{\prime}\varepsilon}{n}\overset{p}{\rightarrow}0$
\end_inset


\end_layout

\begin_layout Standard
then the plim of the rhs is zero.
 This last term has plim 0 because we started with the assumption that 
\begin_inset Formula $Z$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are uncorrelated,
 e.g.,
 
\begin_inset Formula 
\[
\mathcal{E}(z_{t}^{\prime}\varepsilon_{t})=0,
\]

\end_inset

 Given these assumptions,
 the IV estimator is consistent 
\begin_inset Formula 
\[
\hat{\theta}_{IV}\overset{p}{\rightarrow}\theta_{0}.
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Furthermore,
 scaling by 
\begin_inset Formula $\sqrt{n,}$
\end_inset

 we have 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}_{IV}-\theta_{0}\right)=\left(\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}X}{n}\right)\right)^{-1}\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}\varepsilon}{\sqrt{n}}\right)\label{eq:asvarGIV}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Assuming that the far right term satisfies a CLT,
 so that
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{Z^{\prime}\varepsilon}{\sqrt{n}}\overset{d}{\rightarrow}N(0,Q_{ZZ}\sigma^{2})$
\end_inset

 
\end_layout

\begin_layout Standard
then we get (using some pleasing cancelations)
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{IV}-\theta_{0}\right)\overset{d}{\rightarrow}N\left(0,(Q_{XZ}Q_{ZZ}^{-1}Q_{XZ}^{\prime})^{-1}\sigma^{2}\right)
\]

\end_inset

The adjustment for heteroscedastic or autocorrelated errors should be apparent:
 we just assume that 
\begin_inset Formula $\frac{Z^{\prime}\varepsilon}{\sqrt{n}}\overset{d}{\rightarrow}N(0,\Omega)$
\end_inset

 and work out the algebra (also,
 see below,
 in the 2SLS section).
 And,
 we estimate 
\begin_inset Formula $\Omega$
\end_inset

 appropriately,
 according to the cases of heteroscedastic and/or autocorrelated errors.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

The estimators for 
\begin_inset Formula $Q_{XZ}$
\end_inset

 and 
\begin_inset Formula $Q_{ZZ}$
\end_inset

 are the obvious ones.
 An estimator for 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\sigma_{IV}^{2}}=\frac{1}{n}\left(y-X\hat{\theta}_{IV}\right)^{\prime}\left(y-X\hat{\theta}_{IV}\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that his is computed using the real regressors,
 
\begin_inset Formula $X,$
\end_inset

 not the projected regressors,
 
\begin_inset Formula $X^{*}$
\end_inset

.
\end_layout

\begin_layout Itemize
This estimator is consistent following the proof of consistency of the OLS estimator of 
\begin_inset Formula $\sigma^{2},$
\end_inset

 when the classical assumptions hold.
\end_layout

\begin_layout Standard
The formula used to estimate the variance of 
\begin_inset Formula $\hat{\theta}_{IV}$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{V}(\hat{\theta}_{IV})=\left(\left(X^{\prime}Z\right)\left(Z^{\prime}Z\right)^{-1}\left(Z^{\prime}X\right)\right)^{-1}\widehat{\sigma_{IV}^{2}}
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
The GIV estimator is
\end_layout

\begin_layout Enumerate
Consistent
\end_layout

\begin_layout Enumerate
Asymptotically normally distributed
\end_layout

\begin_layout Enumerate
Biased in general,
 because even though 
\begin_inset Formula $\mathcal{E}(X^{\prime}P_{Z}\varepsilon)=0,$
\end_inset

 
\begin_inset Formula $\mathcal{E}(X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}\varepsilon$
\end_inset

 may not be zero,
 because 
\begin_inset Formula $(X^{\prime}P_{Z}X)^{-1}$
\end_inset

 and 
\begin_inset Formula $X^{\prime}P_{Z}\varepsilon$
\end_inset

 are not independent.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

An important point is that the asymptotic distribution of 
\begin_inset Formula $\hat{\beta}_{IV}$
\end_inset

 depends upon 
\begin_inset Formula $Q_{XZ}$
\end_inset

 and 
\begin_inset Formula $Q_{ZZ},$
\end_inset

 and these depend upon the choice of 
\begin_inset Formula $Z.$
\end_inset

 
\emph on
The choice of instruments influences the efficiency of the estimator
\emph default
.
 
\end_layout

\begin_layout Itemize
When we have two sets of instruments,
 
\begin_inset Formula $Z_{1}$
\end_inset

 and 
\begin_inset Formula $Z_{2}$
\end_inset

 such that 
\begin_inset Formula $Z_{1}\subset Z_{2},$
\end_inset

 then the IV estimator using 
\begin_inset Formula $Z_{2}$
\end_inset

 is at least as efficiently asymptotically as the estimator that uses 
\begin_inset Formula $Z_{1}.$
\end_inset

 More instruments leads to more asymptotically efficient estimation,
 in general.
 
\end_layout

\begin_layout Itemize
The same holds for GMM in general:
 adding moment conditions cannot cause the asymptotic variance to become larger.
\end_layout

\begin_layout Itemize
The penalty for indiscriminate use of instruments is that the small sample bias of the IV estimator rises as the number of instruments increases.
 The reason for this is that 
\begin_inset Formula $P_{Z}X$
\end_inset

 becomes closer and closer to 
\begin_inset Formula $X$
\end_inset

 itself as the number of instruments increases.
 Remember,
 OLS gets a perfect fit when 
\begin_inset Formula $k=n$
\end_inset

,
 so the purged regressors after fitting them using the instruments will be identical to the original unpurged regressors,
 when the number of instruments is equal to the sample size.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:GIV-example.-Recall"

\end_inset

GIV example.
 Recall Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"
nolink "false"

\end_inset

 which deals with a dynamic model with measurement error.
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}^{*} & = & \alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}\\
y_{t} & = & y_{t}^{*}+\upsilon_{t}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 are independent Gaussian white noise errors.
 Suppose that 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is not observed,
 and instead we observe 
\begin_inset Formula $y_{t}$
\end_inset

.
 If we estimate the equation 
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\nu_{t}
\]

\end_inset

by OLS,
 we have seen in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"
nolink "false"

\end_inset

 that the estimator is biased and inconsistent.
 
\end_layout

\begin_layout Itemize
What about using the GIV estimator?
 Consider using as instruments 
\begin_inset Formula $Z=\left[1\,x_{t}\,x_{t-1}\,x_{t-2}\right]$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The lags of 
\begin_inset Formula $x_{t}$
\end_inset

 are correlated with 
\begin_inset Formula $y_{t-1}$
\end_inset

 as long as 
\begin_inset Formula $\beta$
\end_inset

 is different from zero,
 
\end_layout

\begin_layout Itemize
By assumption 
\begin_inset Formula $x_{t}$
\end_inset

 and its lags are uncorrelated with 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 (and thus they're also uncorrelated with 
\begin_inset Formula $\nu_{t})$
\end_inset

.
 Thus,
 these are legitimate instruments.
 As we have 4 instruments and 3 parameters,
 this is an overidentified situation.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/MeasurementErrorIV.jl}{GMM/MeasurementErrorIV.jl} 
\end_layout

\end_inset

 does a Monte Carlo study using 1000 replications,
 with a sample size of 100.
 The results are comparable with those in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"
nolink "false"

\end_inset

.
 A histogram for 
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

 is in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:GIV-estimation-results"
nolink "false"

\end_inset

.
 You can compare with the similar figure for the OLS estimator,
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:measurement error"
nolink "false"

\end_inset

.
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:GIV-estimation-results"

\end_inset

GIV estimation results for 
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

,
 dynamic model with measurement error
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/givrho.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
note that the IV estimator has little bias,
 but its variance is considerably larger that that of the inconsistent OLS estimator.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
2SLS
\end_layout

\begin_layout Standard
We can give an alternative formulation of the GIV estimator.
 Let 
\begin_inset Formula $\hat{X}=Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}X=P_{Z}X$
\end_inset

.
 These are the fitted values from a regression of the regressors upon the instruments.
 Substitute this into eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:GIVestimator2"
nolink "false"

\end_inset

,
 to get
\begin_inset Formula 
\[
\hat{\theta}_{IV}=(X^{\prime}\hat{X})^{-1}\hat{X}^{\prime}y
\]

\end_inset

or
\begin_inset Formula 
\[
\hat{\theta}_{IV}=(\hat{X}^{\prime}\hat{X})^{-1}\hat{X}^{\prime}y.
\]

\end_inset

These are numerically equivalent.
 So,
 the GIV estimator can be obtained by 
\end_layout

\begin_layout Enumerate
first regressing the regressors on the instruments,
 and obtaining the predicted values.
 These are linear combinations of the instruments,
 so they are uncorrelated with the error.
\end_layout

\begin_layout Enumerate
then regressing the dependent variable on the predicted regressors.
\end_layout

\begin_layout Standard
It's clear why it's called 2SLS,
 no?
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:asvarGIV"
nolink "false"

\end_inset

 simplifies to
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}_{IV}-\theta_{0}\right)=\left(\frac{\hat{X}^{\prime}\hat{X}}{n}\right)^{-1}\left(\frac{\hat{X}^{\prime}\varepsilon}{\sqrt{n}}\right)\label{eq:asvarGIV-1}
\end{equation}

\end_inset

From this,
 we can write (now allowing for possible HET/AUT)
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{IV}-\theta_{0}\right)\overset{d}{\rightarrow}N\left(0,(Q_{\hat{X}}^{-1}\Omega Q_{\hat{X}}^{-1}\right)
\]

\end_inset

where 
\begin_inset Formula $\Omega=limV\left(\frac{\hat{X}^{\prime}\varepsilon}{\sqrt{n}}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
this can be estimated using White's or Newey-West estimators,
 as appropriate,
 or simplified further (as above) if the classical assumptions regarding homoscedasticity and no autocorrelation hold.
\end_layout

\begin_layout Itemize
In either case,
 it is important to use the residuals 
\begin_inset Formula $y-X\hat{\theta}_{IV},$
\end_inset

 not 
\begin_inset Formula $y-\hat{X}\hat{\theta}_{IV}$
\end_inset

,
 to estimate 
\begin_inset Formula $\Omega$
\end_inset

 properly.
\end_layout

\begin_layout Itemize
Go to Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Example:-Klein's-Model"
nolink "false"

\end_inset

 for an example.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
An example
\end_layout

\begin_layout Standard
Here's an example in the format of an exam question.
 Suppose we have the model
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \beta_{1}^{0}+\beta_{2}^{0}x_{t}+\epsilon_{t}
\end{eqnarray*}

\end_inset

where the shocks 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 are independent and identically distributed (i.i.d.) with mean zero and variance 
\begin_inset Formula $\sigma_{\epsilon}^{2}$
\end_inset

.
 Suppose that there is a varable 
\begin_inset Formula $z_{t}$
\end_inset

 that is i.i.d.
 with mean zero and variance 
\begin_inset Formula $\sigma_{z}^{2}$
\end_inset

.
 Suppose that 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $z_{s}$
\end_inset

 are independent for all 
\begin_inset Formula $t=1,2,...,n$
\end_inset

 and 
\begin_inset Formula $s=1,2,...,n$
\end_inset

,
 where 
\begin_inset Formula $n$
\end_inset

 is the number of observations.
 Finally,
 suppose that 
\begin_inset Formula $x_{t}=z_{t}+\epsilon_{t}.$
\end_inset


\end_layout

\begin_layout Enumerate
What is the covariance between 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}$
\end_inset

?
 Is the OLS estimator of the model consistent or no?
 Explain.
 What is the covariance between 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $z_{t}$
\end_inset

?
\begin_inset Newline newline
\end_inset


\begin_inset Box Shaded
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "default"
backgroundcolor "none"
status open

\begin_layout Plain Layout
First,
 
\begin_inset Formula $Cov(x_{t},\epsilon_{t})=E(x_{t}\epsilon_{t}),$
\end_inset

 because both have mean zero.
 So,
 
\begin_inset Formula $Cov(x_{t},\epsilon_{t})=E\left((z_{t}+\epsilon_{t})\epsilon_{t}\right)=\sigma_{\epsilon}^{2}$
\end_inset

.
 Thus,
 OLS will not be consistent,
 as the weak exogeneity assumption does not hold.
\end_layout

\begin_layout Plain Layout
Second,
 
\begin_inset Formula $Cov(x_{t},z_{t})=E(x_{t}z_{t}),$
\end_inset

 because both have mean zero.
 So,
 
\begin_inset Formula $Cov(x_{t},z_{t})=E\left((z_{t}+\epsilon_{t})z_{t}\right)=\sigma_{z}^{2}.$
\end_inset

 We can use 
\begin_inset Formula $z_{t}$
\end_inset

 as an instrument,
 as it is correlated with the regressor,
 but not with the error.
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Explain in detail how to estimate the parameters 
\begin_inset Formula $\beta_{1}$
\end_inset

 and 
\begin_inset Formula $\beta_{2}$
\end_inset

 consistently by GMM,
 detailing the specific moment conditions you propose.
\begin_inset Newline newline
\end_inset


\begin_inset Box Shaded
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "default"
backgroundcolor "none"
status open

\begin_layout Plain Layout
Let's use as instruments a constant and 
\begin_inset Formula $z_{t},$
\end_inset

 so
\begin_inset Formula 
\begin{align*}
m_{t}(\beta) & =\left[\begin{array}{c}
y_{t}-\beta_{1}-\beta_{2}x_{t}\\
z_{t}(y_{t}-\beta_{1}-\beta_{2}x_{t})
\end{array}\right]\\
 & =\left[\begin{array}{c}
\beta_{1}^{0}+\beta_{2}^{0}x_{t}+\epsilon_{t}-\beta_{1}-\beta_{2}x_{t}\\
z_{t}(\beta_{1}^{0}+\beta_{2}^{0}x_{t}+\epsilon_{t}-\beta_{1}-\beta_{2}x_{t})
\end{array}\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
It's easy to verify that the expectation of this is a vector of zeros,
 when 
\begin_inset Formula $\beta=\beta^{0},$
\end_inset

 the vector of true parameter values.
 So,
 GMM will be consistent,
 if the identification requirement is satistfied.
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Verify that the proposed moment conditions actually identify the parameters by showing that the limit of the expectation of the Jacobian matrix of the moment conditions,
 
\begin_inset Formula $\lim E\frac{\partial\bar{m_{n}}^{\prime}(\theta)}{\partial\theta}$
\end_inset

,
 has rank 2.
\begin_inset Newline newline
\end_inset


\begin_inset Box Shaded
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "default"
backgroundcolor "none"
status open

\begin_layout Plain Layout
For the chosen moment conditions,
 and given the i.i.d.
 nature of the data,
 
\begin_inset Formula $\lim ED_{n}(\beta)=\lim E\frac{\partial m_{t}^{\prime}(\beta)}{\partial\beta}$
\end_inset

.
 
\end_layout

\begin_layout Plain Layout
We have
\begin_inset Formula 
\begin{align*}
\frac{\partial m_{t}^{\prime}(\beta)}{\partial\beta} & =\left[\begin{array}{cc}
-1 & -z_{t}\\
-x_{t} & -x_{t}z_{t}
\end{array}\right]
\end{align*}

\end_inset

 and 
\begin_inset Formula 
\[
\lim E\frac{\partial m_{t}^{\prime}(\beta)}{\partial\beta}=\left[\begin{array}{cc}
-1 & 0\\
0 & -\sigma_{z}^{2}
\end{array}\right]
\]

\end_inset

which is rank 2,
 so the moment conditions identify the parameters.
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
What if you use as instruments 
\begin_inset Formula $z_{t-1}$
\end_inset

 and 
\begin_inset Formula $z_{t}?$
\end_inset

 Will the estimator be identified?
\begin_inset Newline newline
\end_inset


\begin_inset Box Shaded
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "default"
backgroundcolor "none"
status open

\begin_layout Plain Layout
In this case 
\begin_inset Formula 
\[
m_{t}(\beta)=\left[\begin{array}{c}
z_{t-1}(y_{t}-\beta_{1}-\beta_{2}x_{t})\\
z_{t}(y_{t}-\beta_{1}-\beta_{2}x_{t})
\end{array}\right]=\left[\begin{array}{c}
z_{t-1}(\beta_{1}^{0}+\beta_{2}^{0}x_{t}+\epsilon_{t}-\beta_{1}-\beta_{2}x_{t})\\
z_{t}(\beta_{1}^{0}+\beta_{2}^{0}x_{t}+\epsilon_{t}-\beta_{1}-\beta_{2}x_{t})
\end{array}\right]
\]

\end_inset

 which does have expectation zero at the true parameter values.
 However
\begin_inset Formula 
\[
\frac{\partial m_{t}^{\prime}(\beta)}{\partial\beta}=\left[\begin{array}{cc}
-z_{t-1} & -z_{t}\\
-z_{t-1}x_{t} & -z_{t}x_{t}
\end{array}\right]
\]

\end_inset

and
\begin_inset Formula 
\[
\lim E\frac{\partial m_{t}^{\prime}(\beta)}{\partial\beta}=\left[\begin{array}{cc}
0 & 0\\
0 & -\sigma_{z}^{2}
\end{array}\right]
\]

\end_inset

which has rank 1.
 So,
 these moment conditions do not identify the parameters.
 It makes sense that if the data is i.i.d,
 lags of variables will not work as instruments,
 because the lags are not correlated with the current period regressors.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
For code examples,
 see the following:
 The linear simultaneous equation model 
\series bold
Klein's model 1
\series default
 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Example:-Klein's-Model"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

) is estimated by GMM in the following code examples:
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Simeq/KleinGMM.jl}{Simeq/KleinGMM.jl}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Simeq/KleinCUE.jl}{Simeq/KleinCUE.jl}
\end_layout

\end_inset

.
 A 2SLS version is at 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Simeq/Klein2SLS.jl}{Simeq/Klein2SLS.jl}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:A-specification-test"

\end_inset

The Hansen-Sargan (or J) test
\end_layout

\begin_layout Standard
The first order conditions for minimization,
 using the an estimate of the optimal weighting matrix,
 are 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}s(\hat{\theta})=2\left[\frac{\partial}{\partial\theta}\bar{m}_{n}\left(\hat{\theta}\right)\right]\hat{\Omega}^{-1}\bar{m}_{n}\left(\hat{\theta}\right)\equiv0
\]

\end_inset

 or
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\hat{\theta})\equiv0
\]

\end_inset


\end_layout

\begin_layout Standard
Consider a Taylor expansion of 
\begin_inset Formula $\bar{m}(\hat{\theta})$
\end_inset

 about the true parameter value:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\bar{m}(\hat{\theta})=\bar{m}_{n}(\theta_{0})+D_{n}^{\prime}(\theta^{*}){\color{blue}\left(\hat{\theta}-\theta_{0}\right)}\label{TS expansion of moments}
\end{equation}

\end_inset

where 
\begin_inset Formula $\theta^{*}$
\end_inset

 is between 
\begin_inset Formula $\hat{\theta}$
\end_inset

 and 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Multiplying by 
\begin_inset Formula $D(\hat{\theta})\hat{\Omega}^{-1}$
\end_inset

 we obtain 
\begin_inset Formula 
\[
D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}(\hat{\theta})=D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta_{0})+D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\left(\hat{\theta}-\theta_{0}\right)
\]

\end_inset

 The lhs is zero,
 by the first order conditions for the GMM estimator,
 so 
\begin_inset Formula 
\[
D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta_{0})=-\left[D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right]\left(\hat{\theta}-\theta_{0}\right)
\]

\end_inset

 or
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\color{blue}\left(\hat{\theta}-\theta_{0}\right)}=-{\color{purple}\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta_{0})}
\]

\end_inset


\end_layout

\begin_layout Standard
Substitute the RHS into the last part of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "TS expansion of moments"
nolink "false"

\end_inset

),
 and multiply by 
\begin_inset Formula $\sqrt{n}$
\end_inset

,
 to get 
\begin_inset Formula 
\[
\sqrt{n}\bar{m}_{n}(\hat{\theta})=\sqrt{n}\bar{m}_{n}(\theta_{0})-\sqrt{n}D_{n}^{\prime}(\theta^{*}){\color{purple}\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta_{0}).}
\]

\end_inset

 With some factoring,
 this last can be written as
\begin_inset Formula 
\begin{align*}
\sqrt{n}\bar{m}_{n}(\hat{\theta}) & =\left(\hat{\Omega}^{1/2}-D_{n}^{\prime}(\theta^{*})\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1/2}\right)\left(\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\theta_{0})\right)
\end{align*}

\end_inset

(verify it by multiplying out the last expression).
 Also,
 a note:
 the matrix square root of a matrix 
\begin_inset Formula $A$
\end_inset

 is any matrix 
\begin_inset Formula $A^{1/2}$
\end_inset

 such that 
\begin_inset Formula $A=A^{1/2}A^{1/2}$
\end_inset

.
 Any positive definite matrix has an invertible matrix square root.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Aside on matrix square root
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

using LinearAlgebra
\end_layout

\begin_layout Plain Layout

a = randn(3,3)
\end_layout

\begin_layout Plain Layout

a = transpose(a)*a # make it PD,
 so it will have a matrix square root
\end_layout

\begin_layout Plain Layout

s = sqrt(a)
\end_layout

\begin_layout Plain Layout

eigvals(s) # check that it's invertible
\end_layout

\begin_layout Plain Layout

s*s - a # verify that it's the square root
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Next,
 multiply by 
\begin_inset Formula $\hat{\Omega}^{-1/2}$
\end_inset

 to get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
{\color{purple}\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\hat{\theta})} & =\left({\color{red}I_{g}-\hat{\Omega}^{-1/2}D_{n}^{\prime}(\theta^{*})\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1/2}}\right)\left({\color{blue}\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\theta_{0})}\right)\label{eq:JtestIntermediate}\\
 & \equiv{\color{red}P}{\color{blue}X}
\end{align}

\end_inset


\end_layout

\begin_layout Itemize
the big matrix 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula ${\color{red}P=I_{g}-\hat{\Omega}^{-1/2}D_{n}^{\prime}(\theta^{*})\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1/2}}$
\end_inset

 converges in probability to 
\begin_inset Formula $P_{\infty}=I_{g}-\Omega_{\infty}^{-1/2}D_{\infty}^{\prime}\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1/2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
One can easily verify that 
\begin_inset Formula $P_{\infty}$
\end_inset

 is idempotent and has rank 
\begin_inset Formula $g-K,$
\end_inset

 (recall that the rank of an idempotent matrix is equal to its trace)
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 
\end_layout

\begin_layout Itemize
The far right term,
 
\begin_inset Formula $X={\color{blue}\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\theta_{0})}$
\end_inset

 converges to a 
\begin_inset Formula $g$
\end_inset

 vector of i.i.d.
 standard normal random variables,
 by the LLN and the Slutsky theorem.
 This is because 
\begin_inset Formula $\sqrt{n}\bar{m}(\theta_{0})\stackrel{d}{\rightarrow}N(0,\Omega_{\infty})$
\end_inset

 (from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:CLT applied to moment conditions"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

) and we're multiplying by something that converges to 
\begin_inset Formula $\Omega_{\infty}^{-1/2}$
\end_inset

.
 Note that 
\begin_inset Formula $\Omega_{\infty}^{-1/2}\Omega_{\infty}\Omega_{\infty}^{-1/2}=I_{g}$
\end_inset

.
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Thus,
 
\begin_inset Formula $X^{\prime}PX\stackrel{d}{\rightarrow}\chi^{2}(d)$
\end_inset

,
 where 
\begin_inset Formula $d$
\end_inset

 is the rank of 
\begin_inset Formula $P,$
\end_inset

 by the Continuous Mapping Theorem
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 (
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "true"

\end_inset

,
 Theorem 4.7) 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 This is because,
 asymptotically,
 it is a quadratic form of standard normal variables,
 weighted by an idempotent matrix.
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
S
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
o,
 the inner product of the r.h.s.
 of eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:JtestIntermediate"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 has an asymptotic chi-square distribution.
 The inner product using the l.h.s.
 must also have the same distribution,
 so we finally get
\begin_inset Formula 
\[
\left({\color{purple}\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\hat{\theta})}\right)^{\prime}\left({\color{purple}\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\hat{\theta})}\right)=n{\color{blue}\bar{m}_{n}(\hat{\theta})^{\prime}\hat{\Omega}^{-1}\bar{m}_{n}(\hat{\theta})}\stackrel{d}{\rightarrow}\chi^{2}(g-K)
\]

\end_inset

because our particular idempotent matrix has rank 
\begin_inset Formula $g-K.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\series bold
\emph on
Hansen-Sargan
\series default
 
\emph default
test
\emph on
:
 
\emph default
Supposing that the moment conditions actually have expectation zero at the true parameter value,
 and that we are using an estimate of the efficient weight matrix,
 then
\begin_inset Formula 
\[
n\cdot{\color{blue}s_{n}(\hat{\theta})}\stackrel{d}{\rightarrow}\chi^{2}(g-K).
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a convenient test since we just multiply the optimized value of the objective function by 
\begin_inset Formula $n,$
\end_inset

 and compare with a 
\begin_inset Formula $\chi^{2}(g-K)$
\end_inset

 critical value.
 The test is a general test of whether or not the moments used to estimate are correctly specified.
\end_layout

\begin_layout Itemize
This won't work when the estimator is just identified.
 The f.o.c.
 are 
\begin_inset Formula 
\[
D_{\theta}s_{n}(\theta)=D_{n}\hat{\Omega}^{-1}\bar{m}_{n}(\hat{\theta})\equiv0.
\]

\end_inset

But with exact identification,
 both 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $\hat{\Omega}$
\end_inset

 are square and invertible (at least asymptotically,
 assuming that asymptotic normality hold),
 so
\begin_inset Formula 
\[
\bar{m}_{n}(\hat{\theta})\equiv0.
\]

\end_inset

So the moment conditions are zero 
\emph on
regardless
\emph default
 of the weighting matrix used.
 As such,
 we might as well use an identity matrix and save trouble.
 Also 
\begin_inset Formula $s_{n}(\hat{\theta})=0$
\end_inset

,
 so the test breaks down.
\end_layout

\begin_layout Itemize
This sort of test often over-rejects in finite samples.
 One should be cautious in rejecting a model when this test rejects.
\end_layout

\begin_layout Itemize
This test goes by several names:
 Hansen test,
 Sargan test,
 Hansen-Sargan test,
 J test.
 I call it the GMM criterion test.
 An old name for GMM estimation is 
\begin_inset Quotes sld
\end_inset

minimum chi-square
\begin_inset Quotes srd
\end_inset

 estimation.
 This makes sense:
 the criterion function at the estimate (which makes the criterion as small as possible),
 scaled by 
\begin_inset Formula $n$
\end_inset

,
 has a 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution.
 GMM minimizes it.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/SpecTest.jl}{GMM/SpecTest.jl} 
\end_layout

\end_inset

 does a Monte Carlo study of the Hansen-Sargan test,
 for same the dynamic model with measurement error as was discussed in Examples 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:GIV-example.-Recall"
nolink "false"

\end_inset

,
 which did GIV estimation,
 and shows that it over-rejects a correctly specified model,
 in this case.
 For example,
 if the significance level is set to 10%,
 the test rejects about 16% of the time.
 This is a common result for this test.
 Results from a run are:
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GMM/SpecTest.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Other estimators interpreted as GMM estimators
\end_layout

\begin_layout Subsection
Maximum likelihood
\end_layout

\begin_layout Standard
In the introduction we argued that ML will in general be more efficient than GMM since ML implicitly uses all of the moments of the distribution while GMM uses a limited number of moments.
 Actually,
 a distribution with 
\begin_inset Formula $P$
\end_inset

 parameters can be uniquely characterized by 
\begin_inset Formula $P$
\end_inset

 moment conditions.
 However,
 some sets of 
\begin_inset Formula $P$
\end_inset

 moment conditions may contain more information than others,
 since the moment conditions could be highly correlated.
 A GMM estimator that chose an optimal set of 
\begin_inset Formula $P$
\end_inset

 moment conditions would be fully efficient.
 The optimal moment conditions are simply the scores of the ML estimator.
\end_layout

\begin_layout Standard
In the chapter on maximum likelihood,
 we saw in eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLscore"
nolink "false"

\end_inset

 that the first derivative of the average log likelihood function is 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f(y_{t}|x_{x},\theta)
\]

\end_inset

and that the ML estimator is obtained by setting this to zero,
 and solving.
 We also saw in eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ExpectationScore"
nolink "false"

\end_inset

 that the expectation of the score vector is zero,
 when evaluated at the true parameter values.
 Thus,
 the score vector satisfies the requirement to serve as moment conditions.
 Set 
\begin_inset Formula 
\[
m_{t}(\theta)\equiv D_{\theta}\ln f(y_{t}|x_{t},\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
Recall that the score contributions are both conditionally and unconditionally uncorrelated.
 Conditional uncorrelation follows from the fact that 
\begin_inset Formula $m_{t-s}$
\end_inset

 if is a function of lagged endogenous variables,
 then they are included in 
\begin_inset Formula $x_{t}$
\end_inset

,
 which is what we are conditioning on at time 
\begin_inset Formula $t$
\end_inset

.
 Unconditional uncorrelation follows from the fact that conditional uncorrelation hold regardless of the realization of 
\begin_inset Formula $y_{t-1},$
\end_inset

 so marginalizing with respect to 
\begin_inset Formula $Y_{t-1}$
\end_inset

 preserves uncorrelation (see the section on ML estimation,
 above).
 
\end_layout

\begin_layout Itemize
The fact that the scores are serially uncorrelated implies that 
\begin_inset Formula $\Omega$
\end_inset

 can be estimated by the estimator of the 0
\begin_inset Formula $^{th}$
\end_inset

 autocovariance of the moment conditions:
 
\begin_inset Formula 
\[
\widehat{\Omega}=1/n\sum_{t=1}^{n}m_{t}(\hat{\theta})m_{t}(\hat{\theta})^{\prime}=1/n\sum_{t=1}^{n}\left[D_{\theta}\ln f(y_{t}|x_{t},\hat{\theta})\right]\left[D_{\theta}\ln f(y_{t}|x_{t},\hat{\theta})\right]^{\prime}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
note that this is the estimator of the information matrix,
 from ML
\end_layout

\begin_layout Itemize
There is no need for a Newey-West style estimator,
 the heteroscedastic-consistent estimator of White is sufficient.
\end_layout

\begin_layout Itemize
Also,
 the fact that the scores of ML are uncorrelated suggests a means of testing the correct specification of the model:
 see if the fitted scores (
\begin_inset Formula $m_{t}(\hat{\theta})$
\end_inset

 show evidence of serial correlation.
 If they do,
 the correctness of the specification of the model is subject to doubt.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
OLS as a GMM estimator - the Nerlove model again
\end_layout

\begin_layout Example
Matlab/Octave code for GMM for Nerlove model.
 Examine and run 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/TwoStepGMM.m}{TwoStepGMM.m} 
\end_layout

\end_inset

,
 which illustrates how to do two step GMM for the Nerlove data.
 Note that the GMM results are the same as what you get estimating by OLS.
\end_layout

\begin_layout Standard
The simple Nerlove model can be estimated using GMM,
 as we've seen.
 So,
 OLS is a special case of GMM.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The Hausman Test
\end_layout

\begin_layout Standard
This section discusses the Hausman test (
\begin_inset CommandInset citation
LatexCommand cite
key "Hausman1978"
literal "true"

\end_inset

).
\end_layout

\begin_layout Standard
Consider the simple linear regression model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\epsilon_{t}.$
\end_inset

 We assume that the functional form and the choice of regressors is correct,
 but that the some of the regressors may be correlated with the error term,
 which as you know will produce inconsistency of 
\begin_inset Formula $\hat{\beta}.$
\end_inset

 For example,
 this will be a problem if
\end_layout

\begin_layout Itemize
if some regressors are endogenous
\end_layout

\begin_layout Itemize
some regressors are measured with error 
\end_layout

\begin_layout Itemize
some relevant regressors are omitted (equivalent to imposing false restrictions)
\end_layout

\begin_layout Itemize
lagged values of the dependent variable are used as regressors and 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is autocorrelated.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To illustrate,
 the Julia program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/Hausman/OLSvsIV.jl}{OLSvsIV.jl} 
\end_layout

\end_inset

 performs a Monte Carlo experiment where errors are correlated with regressors,
 and estimation is by OLS and IV.
 
\end_layout

\begin_layout Itemize
The true value of the slope coefficient used to generate the data is 
\begin_inset Formula $\beta=2.$
\end_inset

 
\end_layout

\begin_layout Itemize
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:OLS-and-IV"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 shows that the OLS estimator is quite biased and that the IV estimator is on average much closer to the true value.
 
\end_layout

\begin_layout Itemize
If you play with the program,
 increasing the sample size,
 you can see evidence that the OLS estimator is asymptotically biased,
 while the IV estimator is consistent.
 
\end_layout

\begin_layout Itemize
You can also play with the covariances of the instrument and regressor,
 and the covariance of the regressor and the error.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:OLS-and-IV"

\end_inset

OLS and IV (
\begin_inset Formula $n=1000,$
\end_inset

cov(X,
\begin_inset Formula $\epsilon)=0.5,$
\end_inset

cov(W,
\begin_inset Formula $\epsilon)=0.5$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Hausman/olsiv.png
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

We have seen that inconsistent and the consistent estimators converge to different probability limits.
 This is the idea behind the Hausman test - a pair of consistent estimators converge to the same probability limit,
 while if one is consistent and the other is not they converge to different limits.
 If we accept that one is consistent (
\emph on
e.g.
\emph default
,
 the IV estimator),
 but we are doubting if the other is consistent (
\emph on
e.g.,

\emph default
 the OLS estimator),
 we might try to check if the difference between the estimators is significantly different from zero.
\end_layout

\begin_layout Itemize
If we're doubting about the consistency of OLS (or QML,
 
\emph on
etc
\emph default
.),
 why should we be interested in testing - why not just use the IV estimator?
 Because the OLS estimator is 
\emph on
more efficient
\emph default
 when the regressors are exogenous and the other classical assumptions (including normality of the errors) hold.
 
\end_layout

\begin_layout Itemize
Play with the above script to convince yourself of this point:
 make exogeneity hold,
 and compare the variances of OLS and IV
\end_layout

\begin_layout Itemize
When we have a more efficient estimator that relies on stronger assumptions (such as exogeneity) than the IV estimator,
 we might prefer to use it,
 unless we have evidence that the assumptions are false.
\end_layout

\begin_deeper
\begin_layout Itemize
When exogeneity holds,
 OLS is better.
 
\end_layout

\begin_layout Itemize
When exogeneity is sufficiently false,
 IV is better.
 
\end_layout

\begin_layout Itemize
How do we choose,
 if we're not sure about whether or not exogeneity holds?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
So,
 let's consider the covariance between the MLE estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 (or any other fully efficient estimator) and some other CAN estimator,
 say 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 Now,
 let's recall some results from MLE.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "anmle"
nolink "false"

\end_inset

 implies:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{d}{\rightarrow}-\mathcal{J}_{\infty}(\theta_{0})^{-1}\sqrt{n}g(\theta_{0}).
\]

\end_inset

Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "information matrix equality"
nolink "false"

\end_inset

 is
\begin_inset Formula 
\[
\mathcal{J}{}_{\infty}(\theta)=-\mathcal{I}_{\infty}(\theta).
\]

\end_inset

 Combining these two equations,
 we get
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{d}{\rightarrow}\mathcal{I}_{\infty}(\theta_{0})^{-1}\sqrt{n}g(\theta_{0}).
\]

\end_inset


\end_layout

\begin_layout Standard
Also,
 equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "Cov. CAN and MLE score"
nolink "false"

\end_inset

 tells us that the asymptotic covariance between any CAN estimator and the MLE score vector is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}g(\theta)
\end{array}\right]={\color{blue}\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right]}.
\]

\end_inset

These results imply that 
\begin_inset Formula 
\[
{\color{purple}\begin{bmatrix}I_{K} & 0_{K}\\
0_{K} & I_{\infty}(\theta)^{-1}
\end{bmatrix}}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}g(\theta)
\end{array}\right]\rightarrow^{d}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}\left(\hat{\theta}-\theta\right)
\end{array}\right].
\]

\end_inset

 The asymptotic covariance of the RHS is the same as the asymptotic variance of the LHS,
 so 
\begin_inset Formula 
\begin{eqnarray*}
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}\left(\hat{\theta}-\theta\right)
\end{array}\right] & = & {\color{purple}\begin{bmatrix}I_{K} & 0_{K}\\
0_{K} & I_{\infty}(\theta)^{-1}
\end{bmatrix}}{\color{blue}\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right]}{\color{purple}\begin{bmatrix}I_{K} & 0_{K}\\
0_{K} & I_{\infty}(\theta)^{-1}
\end{bmatrix}}\\
 & = & \left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{\infty}(\theta)^{-1}\\
I_{\infty}(\theta)^{-1} & I_{\infty}(\theta)^{-1}
\end{array}\right],
\end{eqnarray*}

\end_inset

which,
 for clarity in what follows,
 we might write as (note to self for lectures:
 the 2,2 element has changed)
\begin_inset Formula 
\[
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}\left(\hat{\theta}-\theta\right)
\end{array}\right]=\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{\infty}(\theta)^{-1}\\
I_{\infty}(\theta)^{-1} & V_{\infty}(\hat{\theta})
\end{array}\right].
\]

\end_inset

The main point of interest here are the off diagonal entries.
 So,
 the asymptotic covariance between the MLE and any other CAN estimator is equal to the MLE asymptotic variance (the inverse of the information matrix).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Now,
 suppose we wish to test whether the the two estimators are in fact both converging to 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 versus the alternative hypothesis that the 
\begin_inset Quotes sld
\end_inset

MLE
\begin_inset Quotes srd
\end_inset

 estimator is not in fact consistent (the consistency of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is a maintained hypothesis).
 Under the null hypothesis that they are equal,
 we have
\begin_inset Formula 
\[
\begin{bmatrix}I_{K} & -I_{K}\end{bmatrix}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta_{0}\right)\\
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)
\end{array}\right]=\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right),
\]

\end_inset

will be asymptotically normally distributed as (work out on blackboard)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right)\overset{d}{\rightarrow}N\left(0,V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})\right).
\]

\end_inset

 So,
 
\begin_inset Formula 
\[
n\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\left(V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})\right)^{-1}\left(\tilde{\theta}-\hat{\theta}\right)\overset{d}{\rightarrow}\chi^{2}(\rho),
\]

\end_inset

where 
\begin_inset Formula $\rho$
\end_inset

 is the rank of the difference of the asymptotic variances.
 A statistic that has the same asymptotic distribution is
\begin_inset Formula 
\[
\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\left(\hat{V}(\tilde{\theta})-\hat{V}(\hat{\theta})\right)^{-1}\left(\tilde{\theta}-\hat{\theta}\right)\overset{d}{\rightarrow}\chi^{2}(\rho),
\]

\end_inset

where the 
\begin_inset Formula $n$
\end_inset

 has disappeared because we are using the small sample estimates of the covariances of the two estimators.
 
\color blue
This is the Hausman test statistic
\color inherit
,
 in its original form.
 The reason that this test has power under the alternative hypothesis is that in that case the 
\begin_inset Quotes sld
\end_inset

MLE
\begin_inset Quotes srd
\end_inset

 estimator will not be consistent,
 and will converge to 
\begin_inset Formula $\theta_{A}$
\end_inset

,
 say,
 where 
\begin_inset Formula $\theta_{A}\neq\theta_{0}$
\end_inset

.
 Then the mean of the asymptotic distribution of vector 
\begin_inset Formula $\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right)$
\end_inset

 will be 
\begin_inset Formula $\theta_{0}-\theta_{A}$
\end_inset

,
 a non-zero vector,
 so the test statistic will eventually reject,
 regardless of how small a significance level is used.
\end_layout

\begin_layout Itemize
The quantity 
\begin_inset Formula $V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})$
\end_inset

 may be a singular matrix,
 in which case the inverse in 
\begin_inset Formula $\left(V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})\right)^{-1}$
\end_inset

 must be replaced with a generalized inverse.
 This can occur when the two estimators are defined using some common moment conditions,
 which can introduce some linear dependencies between the estimators.
 
\end_layout

\begin_layout Itemize
When this is the case,
 the rank,
 
\begin_inset Formula $\rho$
\end_inset

,
 of the difference of the asymptotic variances will be less than the dimension of the matrices,
 and it may be difficult to determine what the true rank is.
 If the true rank is lower than what is taken to be true,
 the test will be biased against rejection of the null hypothesis.
 The null is that both estimators are consistent.
 Failure to reject when this hypothesis is false would cause us to use an inconsistent estimator:
 not a desirable outcome!
 The contrary holds if we underestimate the rank.
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Incorrect rank and the Hausman test
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Hausman/RankProblems.jpg
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
A solution to this problem is to use a rank 1 test,
 by comparing only a single coefficient.
 For example,
 if a variable is suspected of possibly being endogenous,
 that variable's coefficients may be compared.
\end_layout

\begin_layout Itemize
Note:
 if the test is based on a sub-vector of the entire parameter vector of the MLE,
 it is possible that the inconsistency of the MLE will not show up in the portion of the vector that has been used.
 If this is the case,
 the test may not have power to detect the inconsistency.
 This may occur,
 for example,
 when the consistent but inefficient estimator is not identified for all the parameters of the model,
 so that we estimate only some of the parameters using the inefficient estimator,
 and the test does not include the others.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The simple formula for the Hausman test only holds when the estimator that is being tested for consistency is 
\emph on
fully
\emph default
 efficient under the null hypothesis.
\end_layout

\begin_layout Itemize
This means that it must be a ML estimator or a fully efficient estimator that has the same asymptotic distribution as the ML estimator.
 
\end_layout

\begin_layout Itemize
This is quite restrictive since modern estimators such as GMM,
 QML,
 or even OLS with heteroscedastic consistent standard errors are not in general fully efficient.
\end_layout

\begin_layout Standard
Following up on this last point,
 let's think of two not necessarily efficient estimators,
 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

,
 where one is assumed to be consistent,
 but the other may not be.
 
\end_layout

\begin_layout Itemize
For example,
 one could be the GIV estimator,
 with het.
 and/or aut.
 of unknown form,
 and the other could be OLS with het and/or aut.
 of unknown form.
\end_layout

\begin_layout Itemize
With weak exogeneity,
 the OLS versions will very likely be more efficient,
 so we would like to use it,
 if we can
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
We assume for expositional simplicity that both 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 belong to the same parameter space,
 and that each estimator can be expressed as generalized method of moments (GMM) estimator.
 The estimators are defined (suppressing the dependence upon data) by
\begin_inset Formula 
\begin{eqnarray*}
\hat{\theta}_{i} & = & \arg\min_{\theta_{i}\in\Theta}\,\bar{m}_{_{i}}(\theta_{i})^{\prime}\,W_{i}\,\bar{m}_{i}(\theta_{i})
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\bar{m}_{i}(\theta_{i})$
\end_inset

 is a 
\begin_inset Formula $g_{i}\times1$
\end_inset

 vector of moment conditions,
 and 
\begin_inset Formula $W_{i}$
\end_inset

 is a 
\begin_inset Formula $g_{i}\times g_{i}$
\end_inset

 positive definite weighting matrix,
 
\begin_inset Formula $i=1,2.$
\end_inset

 Consider the omnibus GMM estimator
\begin_inset Formula 
\begin{equation}
\left(\hat{\theta}_{1},\hat{\theta}_{2}\right)=\arg\min_{\Theta\times\Theta}\,\left[\begin{array}{cc}
\bar{m}_{1}(\theta_{1})^{\prime} & \bar{m}_{2}(\theta_{2})^{\prime}\end{array}\right]\left[\begin{array}{cc}
W_{1} & \mathbf{0}_{\left(g_{1}\times g_{2}\right)}\\
\mathbf{0}_{\left(g_{2}\times g_{1}\right)} & W_{2}
\end{array}\right]\left[\begin{array}{c}
\bar{m}_{1}(\theta_{1})\\
\bar{m}_{2}(\theta_{2})
\end{array}\right].\label{Standard Omnibus}
\end{equation}

\end_inset

The minimizer just gives the two separate GMM estimators.
\end_layout

\begin_layout Standard
Suppose that the asymptotic covariance of the omnibus moment vector is
\begin_inset Formula 
\begin{eqnarray}
\Sigma & = & \lim_{n\rightarrow\infty}Var\left\{ \sqrt{n}\left[\begin{array}{c}
\bar{m}_{1}(\theta_{1})\\
\bar{m}_{2}(\theta_{2})
\end{array}\right]\right\} \label{omnibus variance}\\
 & \equiv & \left(\begin{array}{cc}
\Sigma_{1} & \Sigma_{12}\\
\cdot & \Sigma_{2}
\end{array}\right).\nonumber 
\end{eqnarray}

\end_inset

The standard Hausman test is equivalent to a Wald test of the equality of 
\begin_inset Formula $\theta_{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}$
\end_inset

 (or subvectors of the two) applied to the omnibus GMM estimator,
 but with the covariance of the moment conditions estimated as 
\begin_inset Formula 
\[
\widehat{\Sigma}=\left(\begin{array}{cc}
\widehat{\Sigma_{1}} & \mathbf{0}_{\left(g_{1}\times g_{2}\right)}\\
\mathbf{0}_{\left(g_{2}\times g_{1}\right)} & \widehat{\Sigma_{2}}
\end{array}\right).
\]

\end_inset

While this is clearly an inconsistent estimator in general,
 the omitted 
\begin_inset Formula $\Sigma_{12}$
\end_inset

 term cancels out of the test statistic when one of the estimators is asymptotically efficient,
 as we have seen above,
 and thus it need not be estimated.
\end_layout

\begin_layout Standard
The general solution when neither of the estimators is efficient is clear:
 the entire 
\begin_inset Formula $\Sigma$
\end_inset

 matrix must be estimated consistently,
 since the 
\begin_inset Formula $\Sigma_{12}$
\end_inset

 term will not cancel out.
 Methods for consistently estimating the asymptotic covariance of a vector of moment conditions are well-known
\emph on
,
 e.g.,

\emph default
 the Newey-West estimator discussed previously.
 The Hausman test using a proper estimator of the overall covariance matrix will now have an asymptotic 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution when neither estimator is efficient.
 
\end_layout

\begin_layout Standard
However,
 the test suffers from a loss of power due to the fact that the omnibus GMM estimator of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "Standard Omnibus"
nolink "false"

\end_inset

 is defined using an inefficient weight matrix.
 A new test can be defined by using an alternative omnibus GMM estimator
\begin_inset Formula 
\begin{equation}
\left(\hat{\theta}_{1},\hat{\theta}_{2}\right)=\arg\min_{\Theta\times\Theta}\left[\begin{array}{cc}
\bar{m}_{1}(\theta_{1})^{\prime} & \bar{m}_{2}(\theta_{2})^{\prime}\end{array}\right]\left(\widetilde{\Sigma}\right)^{-1}\left[\begin{array}{c}
\bar{m}_{1}(\theta_{1})\\
\bar{m}_{2}(\theta_{2})
\end{array}\right],\label{New Omnibus}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\widetilde{\Sigma}$
\end_inset

 is a consistent estimator of the overall covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

 of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "omnibus variance"
nolink "false"

\end_inset

.
 By standard arguments,
 this is a more efficient estimator than that defined by equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "Standard Omnibus"
nolink "false"

\end_inset

,
 so the Wald test using this alternative is more powerful.
 See my article in 
\emph on
Applied Economics
\emph default
,
 2004,
 for more details,
 including simulation results.
 The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/Hausman/hausman.m}{hausman.m} 
\end_layout

\end_inset

 calculates the Wald test corresponding to the efficient joint GMM estimator (the 
\begin_inset Quotes sld
\end_inset

H2
\begin_inset Quotes srd
\end_inset

 test in my paper),
 for a simple linear model,
 and compares to the standard Hausman test.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
More moments are better (?)
\end_layout

\begin_layout Standard
A similar framework allows us to show that an overidentified GMM estimator will be asymptotically efficient relative to a GMM estimator that only uses a subset of moment conditions that still maintain identification.
 Let 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 be partitioned as 
\begin_inset Formula 
\[
\bar{m}_{n}(\theta)=\left[\begin{array}{c}
\bar{m}_{n}^{1}(\theta)\\
\bar{m}_{n}^{2}(\theta)
\end{array}\right]
\]

\end_inset

where 
\begin_inset Formula $\bar{m}_{n}^{1}(\theta)$
\end_inset

 is a subset of moment conditions that still identifies the parameters.
 Suppose that the limiting covariance of 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\theta)$
\end_inset

 is 
\begin_inset Formula 
\[
\Omega_{\infty}=\left[\begin{array}{cc}
\Omega_{11} & \Omega_{12}\\
\Omega_{12}^{\prime} & \Omega_{22}
\end{array}\right],.
\]

\end_inset

which is just the ordinary asymptotic variance of the moment conditions,
 partitioned in the same way as we partition the moment conditions.
 The efficient weight matrix is something that converges to the inverse of this.
 The GMM estimator that uses only the subset of moment conditions,
 
\begin_inset Formula $\bar{m}_{n}^{1}(\theta)$
\end_inset

,
 is equivalent to a GMM estimator that uses all of the moment conditions,
 but with the weight matrix being something that converges to
\begin_inset Formula 
\[
\left[\begin{array}{cc}
\Omega_{11}^{-1} & 0\\
0 & 0
\end{array}\right],
\]

\end_inset

where the zeros are appropriately sized matrices of zeros.
 This is not the efficient weight matrix for the full set of moment conditions,
 so this GMM estimator will not be efficient with respect to the efficiently weighted GMM estimator that uses all of the moment conditions.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
so,
 comparing two GMM estimators,
 A and B,
 where B uses a subset of the moment conditions that A uses,
 and both are identified,
 A will be asymptotically efficient with respect to B.
\end_layout

\begin_layout Itemize
however,
 the efficiency gain may be zero,
 or very little (recall that the definition of 
\begin_inset Quotes sld
\end_inset

efficient with respect to
\begin_inset Quotes srd
\end_inset

 uses positive semi-definiteness).
\end_layout

\begin_layout Itemize
the small sample bias of the GMM estimator tends to increase as the number of moment conditions increase,
 and can be large (
\begin_inset CommandInset citation
LatexCommand citet
key "donald2001choosing,DonaldImbensNewey2009"
literal "false"

\end_inset

)
\end_layout

\begin_layout Itemize
so,
 estimator A may very easily have worse small sample performance than version B,
 in spite of being relatively asymptotically efficient
\end_layout

\begin_layout Itemize
experience has shown that using only reasonably informative moments,
 with a small degree of overidentification,
 is usually better than the 
\begin_inset Quotes sld
\end_inset

kitchen sink
\begin_inset Quotes srd
\end_inset

 approach.
 See the literature on weak instruments,
 etc.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
Linear IV:
 The Card returns to schooling data
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "card1993using"
literal "true"

\end_inset

 presents an analysis of returns to schooling using the data from the National Longitudinal Survey of Young Men,
 for those interviewed in 1976.
 Card presents OLS and instrumental variables estimates for a number of specifications,
 using college proximity as an instrument for years of education,
 and age as an instrument for experience.
 Here,
 we work with the simple model from column (1) of Card's Table 2.
 The model is 
\begin_inset Formula 
\begin{align*}
\ln W & =\beta_{0}+\beta_{EDUC}EDUC+\beta_{X}EXP+\beta_{EXP^{2}}\frac{EXP^{2}}{100}\\
 & +\beta_{BLACK}BLACK+\beta_{SMSA}SMSA+\beta_{SOUTH}SOUTH+\epsilon
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
the dependent variable 
\begin_inset Formula $\ln W$
\end_inset

 is log hourly earnings (in cents)
\end_layout

\begin_layout Itemize
the regressors are years of education (EDUC),
 experience (EXP),
 experience squared divided by 100,
 a black indicator (BLACK),
 a metropolitan area indicator (SMSA),
 and a South indicator (SOUTH).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
We explore estimation treating all variables as exogenous,
 or treating education and experience as endogenous,
 and the others as exogenous.
\end_layout

\begin_layout Itemize
If uncontrolled for factors that affect wages also affect education,
 then education will be endogenous.
\end_layout

\begin_layout Itemize
EXPER is defined as EXPER = AGE-EDUC-6.
 So,
 if EDUC is endogenous,
 so is EXPER.
\end_layout

\begin_layout Itemize
Instruments:
\end_layout

\begin_deeper
\begin_layout Itemize
we use proximity to an accredited four year college (NEARC4) as an instrumental variable that should be correlated with EDUC 
\end_layout

\begin_layout Itemize
We use AGE as an instrument for EXPER,
 and AGE squared for EXP squared.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The Card data set is provided with the Wooldridge data set for GRETL,
 see the GRETL web page.
 A version prepared for the model used here is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/card.gdt}{card.gdt} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Itemize
The data is also here:
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Julia/cooked.csv}{cooked.csv} 
\end_layout

\end_inset

,
 ready for use with Julia.
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/GMM/Card.jl}{Card.jl} 
\end_layout

\end_inset

 does OLS,
 GMM-CUE,
 and 2 step GMM.
\end_layout

\begin_layout Itemize
The effect of an additional year of education on wages is about 7%,
 according to OLS,
 and about 13%,
 according to IV.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sec:Application:-Nonlinear-rational"

\end_inset

Application:
 Hansen-Singleton,
 1982
\end_layout

\begin_layout Standard

\series bold
Readings:

\series default
 
\begin_inset CommandInset citation
LatexCommand citep
key "HansenSingleton1982"
literal "true"

\end_inset

;
 
\begin_inset CommandInset citation
LatexCommand citep
key "Tauchen1986"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Though GMM estimation has many applications,
 application to rational expectations models is elegant,
 since theory directly suggests the moment conditions.
 Hansen and Singleton's 1982 paper is also a classic worth studying in itself.
 Though I strongly recommend reading the paper,
 I'll use a simplified model with notation similar to Hamilton's.
 The literature on estimation of these models has grown a lot since these early papers.
 After work like the cited papers,
 people moved to ML estimation of linearized models,
 using Kalman filtering.
 Current methods are usually Bayesian,
 and involve sophisticated filtering methods to compute the likelihood function for nonlinear models with non-normal shocks.
 The methods explained in this section are intended to provide an example of GMM estimation.
 They are not the state of the art for estimation of such models.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
We assume a representative consumer maximizes expected discounted utility over an infinite horizon.
 Expectations are rational,
 and the agent has full information (is fully aware of the history of the world up to the current time period - how's that for an assumption!).
 Utility is temporally additive,
 and the expected utility hypothesis holds.
 The future consumption stream is the stochastic sequence 
\begin_inset Formula $\left\{ c_{t}\right\} _{t=0}^{\infty}.$
\end_inset

 The objective function at time 
\begin_inset Formula $t$
\end_inset

 is the discounted expected utility 
\begin_inset Formula 
\begin{equation}
\sum_{s=0}^{\infty}\beta^{s}\mathcal{E}\left(u(c_{t+s})|I_{t}\right).\label{umax}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
The parameter 
\begin_inset Formula $\beta$
\end_inset

 is between 0 and 1,
 and reflects discounting.
\end_layout

\begin_layout Itemize
\begin_inset Formula $I_{t}$
\end_inset

 is the 
\shape italic
information set
\shape default
 at time 
\begin_inset Formula $t,$
\end_inset

 and includes all realizations of all random variables index by 
\begin_inset Formula $t$
\end_inset

 or earlier.
 NOTE:
 you may need to be careful when preparing a data set to respect this convention.
\end_layout

\begin_layout Itemize
The choice variable is 
\begin_inset Formula $c_{t}$
\end_inset

 - current consumption,
 which is constrained to be less than or equal to current wealth 
\begin_inset Formula $w_{t}.$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Suppose the consumer can invest in a risky asset.
 A dollar invested in the asset yields a gross return 
\begin_inset Formula 
\[
r_{t+1}=\frac{p_{t+1}-p_{t}+d_{t+1}}{p_{t}}
\]

\end_inset

 where 
\begin_inset Formula $p_{t}$
\end_inset

 is the price and 
\begin_inset Formula $d_{t}$
\end_inset

 is the dividend in period 
\begin_inset Formula $t.$
\end_inset

 Thus,
 
\begin_inset Formula $r_{t+1}$
\end_inset

 is the net return on a dollar invested in period 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Itemize
The price of 
\begin_inset Formula $c_{t}$
\end_inset

 is normalized to 
\begin_inset Formula $1.$
\end_inset


\end_layout

\begin_layout Itemize
Current wealth 
\begin_inset Formula $w_{t}=(1+r_{t})i_{t-1}$
\end_inset

,
 where 
\begin_inset Formula $i_{t-1}$
\end_inset

 is investment in period 
\begin_inset Formula $t-1$
\end_inset

.
 At time 
\begin_inset Formula $t,$
\end_inset

 the consumer observes 
\begin_inset Formula $r_{t},$
\end_inset

 which determines wealth.
 So the problem is to allocate current wealth between current consumption and investment to finance future consumption:
 
\begin_inset Formula $w_{t}=c_{t}+i_{t}$
\end_inset

.
 The choice variable can be taken as 
\begin_inset Formula $c_{t},$
\end_inset

which determines 
\begin_inset Formula $i_{t}$
\end_inset

,
 given 
\begin_inset Formula $w_{t}.$
\end_inset


\end_layout

\begin_layout Itemize
Future net rates of return 
\begin_inset Formula $r_{t+s},s>0$
\end_inset

 are 
\shape italic
not known
\shape default
 in period 
\begin_inset Formula $t$
\end_inset

:
 the asset is risky.
\end_layout

\begin_layout Itemize
The information set 
\begin_inset Formula $I_{t}$
\end_inset

 includes 
\begin_inset Formula $r_{t},$
\end_inset


\begin_inset Formula $w_{t}$
\end_inset

,
 
\begin_inset Formula $p_{r},$
\end_inset


\begin_inset Formula $d_{t}$
\end_inset

 and 
\begin_inset Formula $c_{t-1}$
\end_inset

 and 
\begin_inset Formula $i_{t-1}$
\end_inset

,
 as well as all lags of these variables.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
A partial set of necessary conditions for utility maximization have the form:
 
\begin_inset Formula 
\begin{equation}
u^{\prime}(c_{t})=\beta\mathcal{E}\left\{ \left(1+r_{t+1}\right)u^{\prime}(c_{t+1})|I_{t}\right\} .\label{foc}
\end{equation}

\end_inset

To use this in an econometric model,
 we need to choose the functional form of utility.
 A constant relative risk aversion (CRRA) form is 
\begin_inset Formula 
\[
u(c_{t})=\frac{c_{t}^{1-\gamma}-1}{1-\gamma}
\]

\end_inset

where 
\begin_inset Formula $\gamma$
\end_inset

 is the coefficient of relative risk aversion.
 With this form,
 
\begin_inset Formula 
\[
u^{\prime}(c_{t})=c_{t}^{-\gamma}
\]

\end_inset

 so the foc are 
\begin_inset Formula 
\[
c_{t}^{-\gamma}=\beta\mathcal{E}\left\{ \left(1+r_{t+1}\right)c_{t+1}^{-\gamma}|I_{t}\right\} 
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 While it is true that 
\begin_inset Formula 
\[
\mathcal{E}\left(c_{t}^{-\gamma}-\beta\left\{ \left(1+r_{t+1}\right)c_{t+1}^{-\gamma}\right\} \right)|I_{t}=0
\]

\end_inset

 so that we could use this to define moment conditions,
 it is unlikely that 
\begin_inset Formula $c_{t}$
\end_inset

 is stationary,
 even though it is in real terms,
 and our theory requires stationarity.
 To solve this,
 divide though by 
\begin_inset Formula $c_{t}^{-\gamma}$
\end_inset


\begin_inset Formula 
\[
\mathcal{E}\left(\textrm{1-}\beta\left\{ \left(1+r_{t+1}\right)\left(\frac{c_{t+1}}{c_{t}}\right)^{-\gamma}\right\} \right)|I_{t}=0
\]

\end_inset

 (note that 
\begin_inset Formula $c_{t}$
\end_inset

 can be passed though the conditional expectation since 
\begin_inset Formula $c_{t}$
\end_inset

 is chosen based only upon information available in time 
\begin_inset Formula $t).$
\end_inset

 That is to say,
 
\begin_inset Formula $c_{t}$
\end_inset

 is in the information set 
\begin_inset Formula $I_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
Now
\begin_inset Formula 
\[
\textrm{1-}\beta\left\{ \left(1+r_{t+1}\right)\left(\frac{c_{t+1}}{c_{t}}\right)^{-\gamma}\right\} 
\]

\end_inset

is analogous to 
\begin_inset Formula $h_{t}(\theta)$
\end_inset

 defined above:
 it's a scalar moment condition that has conditional expectation equal to zero.
 To get a vector of moment conditions we need some instruments.
 Suppose that 
\begin_inset Formula $\mathbf{z}_{t}$
\end_inset

 is a vector of variables drawn from the information set 
\begin_inset Formula $I_{t}.$
\end_inset

 We can use the necessary conditions to form the expressions 
\begin_inset Formula 
\[
\begin{array}{c}
\left[1-\beta\left(1+r_{t+1}\right)\left(\frac{c_{t+1}}{c_{t}}\right)^{-\gamma}\right]\mathbf{z}_{t}\end{array}\equiv m_{t}(\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta$
\end_inset

 represents 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\gamma.$
\end_inset


\end_layout

\begin_layout Itemize
Therefore,
 the above expression may be interpreted as a moment condition which can be used for GMM estimation of the parameters 
\begin_inset Formula $\theta_{0}.$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Note that at time 
\begin_inset Formula $t,$
\end_inset

 
\begin_inset Formula $m_{t-s}$
\end_inset

 has been observed,
 and is therefore an element of the information set.
 By rational expectations,
 the autocovariances of the moment conditions other than 
\begin_inset Formula $\Gamma_{0}$
\end_inset

 should be zero.
 The optimal weighting matrix is therefore the inverse of the variance of the moment conditions:
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Omega_{_{\infty}}=\lim E\left[n\bar{m}(\theta_{0})\bar{m}(\theta_{0})^{\prime}\right]
\]

\end_inset

 which can be consistently estimated by White's heteroscedastic consistent estimator:
\begin_inset Formula 
\[
\hat{\Omega}=1/n\sum_{t=1}^{n}m_{t}(\hat{\theta})m_{t}(\hat{\theta})^{\prime}
\]

\end_inset

 As before,
 this estimate depends on an initial consistent estimate of 
\begin_inset Formula $\theta,$
\end_inset

 which can be obtained by setting the weighting matrix 
\begin_inset Formula $W$
\end_inset

 arbitrarily (to an identity matrix,
 for example).
 After obtaining 
\begin_inset Formula $\hat{\theta},$
\end_inset

 we then minimize 
\begin_inset Formula 
\[
s(\theta)=\bar{m}(\theta)^{\prime}\hat{\Omega}^{-1}\bar{m}(\theta).
\]

\end_inset

 As usual,
 this process can be iterated,
 e.g.,
 use the new estimate to re-estimate 
\begin_inset Formula $\Omega,$
\end_inset

 use this to estimate 
\begin_inset Formula $\theta_{0},$
\end_inset

 and repeat until the estimates don't change.
 Or,
 use the CUE version of GMM.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In principle,
 we could use a very large number of moment conditions in estimation,
 because we can use current period exogenous variables,
 and lags of endogenous and exogenous variables.
 Since use of more moment conditions will lead to a more (asymptotically) efficient estimator,
 one might be tempted to use many instrumental variables.
 We will do a computer lab that will show that this may not be a good idea with finite samples.
 This issue has been studied using Monte Carlos (Tauchen,
 
\emph on
JBES,

\emph default
 1986).
 The reason for poor performance when using many instruments is that the estimate of 
\begin_inset Formula $\Omega$
\end_inset

 becomes very imprecise.
 
\end_layout

\begin_layout Itemize
Empirical papers that use this approach often have serious problems in obtaining precise estimates of the parameters,
 and identification can be problematic.
 Note that we are basing everything on a single partial first order condition.
 Probably this f.o.c.
 is simply not informative enough.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The Octave/Matlab program 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/GMM/HallGMM.m}{HallGMM.m}
\end_layout

\end_inset

 estimates a model of this sort,
 following Chapter 23 of the 
\begin_inset CommandInset href
LatexCommand href
name "Gretl Users Guide"
target "https://gretl.sourceforge.net/gretl-help/gretl-guide.pdf"
literal "false"

\end_inset

.
 I encourage you to verify that you can obtain the same results using Gretl and Octave.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sec:GMM-estimation-of"

\end_inset

GMM estimation of the DSGE example
\end_layout

\begin_layout Standard
Here we return to the DSGE model of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Application:-a-simple"
nolink "false"

\end_inset

,
 and derive some moment conditions that can be used for estimation.
 
\end_layout

\begin_layout Itemize
this example shows how moment conditions can be derived from the structure of a model
\end_layout

\begin_layout Itemize
it will also illustrate the care that is sometimes needed when doing numeric optimization
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
MRS and wage
\end_layout

\begin_layout Standard
From the first order conditions of the model (see eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Euler"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 and following),
 we have 
\begin_inset Formula 
\begin{eqnarray}
w_{t} & = & \psi\eta_{t}c_{t}^{\gamma}\nonumber \\
\eta_{t} & = & \frac{w_{t}}{\psi c_{t}^{\gamma}}\nonumber \\
\ln\eta_{t} & = & \ln w_{t}-\ln\psi-\gamma\ln c_{t}\label{eq:-2}
\end{eqnarray}

\end_inset

The real values of this shock 
\begin_inset Formula $\eta_{t}$
\end_inset

 are not observed,
 but,
 given a guess for the parameters 
\begin_inset Formula $\psi$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

,
 and the data,
 the right hand side of the above equation can be calculated.
 Also,
 we have
\begin_inset Formula 
\[
\ln\eta_{t}=\rho\ln\eta_{t-1}+\sigma_{\eta}\epsilon_{t}.
\]

\end_inset

So,
 we can regress the calculated 
\begin_inset Formula $\ln\eta_{t}$
\end_inset

 on their lags.
 The FOC for the OLS estimator set the mean of 
\begin_inset Formula 
\[
u_{t}=\ln\eta_{t-1}[\ln\eta_{t}-\rho_{\eta}\ln\eta_{t-1}]
\]

\end_inset

 to zero.
 At the true parameter values,
 this expression has mean zero,
 so it can be used to define a moment condition.
 We also have that 
\begin_inset Formula 
\[
E\left(u_{t}^{2}-\sigma_{\eta}^{2}\right)=0
\]

\end_inset

 at the true parameters,
 so this gives us a second moment condition.
 These two moment conditions are informative for all of the parameters that enter into their definitions:
 
\begin_inset Formula $\gamma,\rho_{\eta},\sigma_{\eta}$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

,
\begin_inset Formula $\beta,$
\end_inset


\begin_inset Formula $\delta$
\end_inset

 and 
\begin_inset Formula $\bar{n}$
\end_inset

 (because 
\begin_inset Formula $\psi$
\end_inset

 depends on them,
 see above).
 We're only missing 
\begin_inset Formula $\rho_{z}$
\end_inset

 and 
\begin_inset Formula $\sigma_{z}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Euler equation
\end_layout

\begin_layout Standard
The Euler equation is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
c_{t}^{-\gamma}=E\left(\beta\cdot c_{t+1}^{-\gamma}\left[1+MPK_{t+1}-\delta\right]\right),
\]

\end_inset

where the expectation is taken conditional on the information available in period 
\begin_inset Formula $t$
\end_inset

 (which include variables indexed 
\begin_inset Formula $t$
\end_inset

 and before).
 But 
\begin_inset Formula $r=MPK,$
\end_inset

 so
\begin_inset Formula 
\[
E\left(\beta\cdot c_{t+1}^{-\gamma}\left[1+r_{t+1}-\delta\right]\right)-c_{t}^{-\gamma}=0
\]

\end_inset

Thus,
\begin_inset Formula 
\begin{equation}
v_{t}=\beta\cdot c_{t+1}^{-\gamma}\left[1+r_{t+1}-\delta\right]-c_{t}^{-\gamma}\label{eq:EulerError}
\end{equation}

\end_inset

has mean zero,
 conditional on information available in period 
\begin_inset Formula $t.$
\end_inset

 Moment conditions that use this error should be informative for 
\begin_inset Formula $\gamma,$
\end_inset


\begin_inset Formula $\delta$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Estimation by GMM
\end_layout

\begin_layout Standard
A sample of size 160,
 generated from the model at the true parameter values,
 above,
 is at 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/DSGE/GenData/dsgedata.txt}{dsgedata.txt}
\end_layout

\end_inset

.
 The columns are y,
 c,
 n,
 r,
 w.
 Have another look at the data,
 if you like:
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-DSGE-data"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard
A Julia function to compute the moment conditions discussed above,
 and others,
 is at 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/DSGE/GMM/DSGEmoments.jl}{DSGEmoments.jl}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
The script 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/DSGE/GMM/DoGMM.jl}{DoGMM.jl}
\end_layout

\end_inset

 implements CUE-GMM estimation of the model,
 using the selected moment conditions,
 using simulated annealing to do the minimization.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The final estimates,
 standard errors,
 and 95% CI bounds,
 are
\begin_inset Newline newline
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "DSGE GMM results"

\end_inset


\begin_inset Graphics
	filename Examples/DSGE/GMM/gmm.png
	width 15cm

\end_inset

 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
the point estimates are very good,
 at least as good as ML.
\end_layout

\begin_layout Itemize
the standard errors are small,
 often smaller than those of the ML estimator
\end_layout

\begin_layout Itemize
unfortunately,
 the standard error estimates appear to be too small:
 the 95% confidence intervals do not contain the true parameters for 4 of the 7 cases.
 This is a common problem with overidentified GMM:
 the true probability of type 1 error is higher than it should be.
\end_layout

\begin_layout Itemize
care is needed to obtain a real global minimum.
 The attempt to use ordinary gradient-based minimization fails,
 as you will see if you run the code.
 A person who tried these methods might conclude that the moments don't identify the parameters well,
 but this is not the case:
 it is possible to obtain good results using GMM.
\end_layout

\begin_layout Itemize
Simulated annealing,
 on the other hand,
 converges to the same value in repeated runs.
 It is possible that on a given run,
 a different outcome might be obtained,
 if the cooling rate is too rapid,
 but I have yet to see this with the current setup.
 SA requires many function evaluations,
 about 30000 with the setting in the example code.
 However,
 it doesn't take too long,
 only about 11 seconds.
 This doesn't seem like too much time to get a reliable answer.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Here are Monte Carlo results for 1000 replications of the GMM estimator,
 using sample drawn at the true parameter values.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/GMM/montecarlo.png
	width 20cm

\end_inset


\end_layout

\begin_layout Itemize
the results,
 overall,
 are at least as good as the results using maximum likelihood.
 
\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $\gamma,$
\end_inset

 there is some bias,
 but RMSE is lower
\end_layout

\begin_layout Itemize
for 
\begin_inset Formula $\rho_{2},$
\end_inset

 there is less bias and lower RMSE
\end_layout

\begin_layout Itemize
for 
\begin_inset Formula $\beta$
\end_inset

,
 RMSE is lower by about half
\end_layout

\end_deeper
\begin_layout Itemize
note the 
\begin_inset Quotes sld
\end_inset

MLE
\begin_inset Quotes srd
\end_inset

 is based upon the linearized model,
 it is not truly MLE.
 Also,
 it can use only two observed variables,
 due to stochastic singularity.
 GMM is based on the actual nonlinear model,
 and uses all of the observed variables.
 Perhaps the linearization and limited use of observed variables provokes a loss of information for the ML estimator,
 with a consequent somewhat poorer performance.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Some take home conclusions here are:
\end_layout

\begin_deeper
\begin_layout Itemize
that GMM can give reliable estimates,
 but we need to be aware of confidence intervals potentially being too tight.
\end_layout

\begin_layout Itemize
multiple local minima and irregular objective functions really can be a problem,
 even with simple models like this one.
 Imagine what would happen with a large scale DSGE model!
 For similar problems with a model that is much more simple,
 see 
\begin_inset CommandInset citation
LatexCommand cite
key "HansenHeatonYaron1996"
literal "true"

\end_inset

.
\end_layout

\begin_layout Itemize
the difficulties with extremum estimation may motivate other computational methods,
 such as using a Bayesian approach to compute classical estimators as was proposed by 
\begin_inset CommandInset citation
LatexCommand cite
key "ChernozhukovHong2003"
literal "true"

\end_inset

.
 We will return to this idea in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Bayesian-methods"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical Summary
\end_layout

\begin_layout Standard
The practical summary for the Chapter is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./PracticalSummaries/16-GMM.jl}{here}
\end_layout

\end_inset

,
 but the previous examples,
 especially the Card example,
 are important,
 too,
 as those concepts are not repeated in this summary.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
\noindent
Suppose you have data on a dependent variable 
\begin_inset Formula $y_{i}$
\end_inset

 and a column vector of regressors 
\begin_inset Formula $x_{i}$
\end_inset

.
 Consider the model
\begin_inset Formula 
\[
y_{i}=x_{i}^{\prime}\beta_{0}+\epsilon_{i}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\noindent
Suppose that 
\begin_inset Formula $E[\epsilon_{i}|x_{i}]=0$
\end_inset

.
 Use this information to propose a GMM estimator that is equivalent to the OLS estimator.
 Your answer should include:
\end_layout

\begin_deeper
\begin_layout Enumerate
\noindent
state the moment conditions and the GMM objective function clearly
\end_layout

\begin_layout Enumerate
\noindent
compute the first order conditions for minimization of the GMM criterion function and solve them to find the expression for the estimator
\end_layout

\end_deeper
\begin_layout Enumerate
Now,
 suppose that 
\begin_inset Formula $E[\epsilon_{i}|x_{i}]\ne0$
\end_inset

 but that there is another vector 
\begin_inset Formula $z_{i}$
\end_inset

 with 
\begin_inset Formula $\dim z=\dim x$
\end_inset

 such that 
\begin_inset Formula $E[\epsilon_{i}|z_{i}]=0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
Show that the OLS estimator of 
\begin_inset Formula $\beta_{0}$
\end_inset

 is not consistent,
 given this information.
\end_layout

\begin_layout Enumerate
Propose a consistent GMM estimator of the parameter vector 
\begin_inset Formula $\beta_{0}$
\end_inset

 that uses this information.
 Your answer should include:
\end_layout

\begin_deeper
\begin_layout Enumerate
a clear statement of the moment conditions and the GMM objective function to be minimized which defines the estimator.
\end_layout

\begin_layout Enumerate
a closed-form expression (that is,
 an explicit formula) for the estimator.
\end_layout

\begin_layout Enumerate
a proof that the estimator is consistent.
 If you need to make additional assumptions to prove consistency,
 state them.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Enumerate
Consider the linear regression model 
\begin_inset Formula $y_{i}=x_{i}^{\prime}\beta_{0}+\epsilon_{i}$
\end_inset

.
 Suppose that the 
\begin_inset Formula $n$
\end_inset

 observations satisfy all assumptions of the classical linear regression model,
 except they are heteroscedastic,
 so that 
\begin_inset Formula $V(\epsilon_{i}|x_{i})=\sigma_{i}^{2}$
\end_inset

.
 Suppose that 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 is 
\series bold
known
\series default
,
 for all 
\begin_inset Formula $i$
\end_inset

.
 As such,
 we can compute the generalized least squares (GLS) estimator,
 which can be expressed as 
\begin_inset Formula $\hat{\beta}_{GLS}=\left(X^{\prime}\Sigma^{-1}X\right)^{-1}X^{\prime}\Sigma^{-1}y$
\end_inset

,
 where 
\begin_inset Formula $\Sigma$
\end_inset

 is a diagonal matrix with 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 in position 
\begin_inset Formula $(i,i),$
\end_inset

 
\begin_inset Formula $i=1,2,...,n$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Define moment conditions which define a GMM estimator that is equivalent to the GLS estimator.
 Prove that the estimators are equivalent.
\end_layout

\begin_layout Enumerate
Is your GMM estimator exactly identified,
 or overidentified?
 
\end_layout

\begin_layout Enumerate
For efficient estimation using the moment conditions which you have defined,
 is single step GMM estimation using an identity matrix as the weight matrix fully efficient,
 or will two step estimation be more efficient?
 Explain.
\end_layout

\end_deeper
\begin_layout Enumerate
Do the exercises in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Generalized-instrumental"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Enumerate
Show how the GIV estimator presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Generalized-instrumental"
nolink "false"

\end_inset

 can be adapted to account for an error term with HET and/or AUT.
\end_layout

\begin_layout Enumerate
For the GIV estimator presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Generalized-instrumental"
nolink "false"

\end_inset

,
 find the form of the expressions 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 that appear in the asymptotic distribution of the estimator,
 assuming that an efficient weight matrix is used.
\end_layout

\begin_layout Enumerate
Consider the linear regression 
\begin_inset Formula $y_{t}=\beta_{1}+\beta_{2}x_{t}+\epsilon_{t},$
\end_inset

 for which we have 
\begin_inset Formula $n$
\end_inset

 observations.
 Suppose that all variables are weakly stationary,
 with finite variances,
 and that the error is Gaussian white noise with variance equal to 1:
 
\begin_inset Formula $\epsilon_{t}\sim N(0,1)\,\forall t$
\end_inset

.
 However,
 weak exogeneity does not hold,
 so 
\begin_inset Formula $E(\epsilon_{t}|x_{t})\ne0$
\end_inset

,
 and the OLS estimator is not consistent.
 So,
 we will estimate using the method of instrumental variables.
 Suppose we have a single instrumental variable,
 
\begin_inset Formula $z_{t}$
\end_inset

,
 and we use it,
 along with the constant,
 to form the instrument matrix
\begin_inset Formula 
\[
Z=\left[\begin{array}{cc}
1 & z_{1}\\
1 & z_{2}\\
\vdots & \vdots\\
1 & z_{n}
\end{array}\right]
\]

\end_inset

We contemplate estimation of 
\begin_inset Formula $\theta=(\beta_{1}$
\end_inset

,
 
\begin_inset Formula $\beta_{2}$
\end_inset

) using GMM,
 using the moment conditions 
\begin_inset Formula $\bar{m}_{n}(\theta)=\frac{1}{n}Z^{\prime}(y-X\beta).$
\end_inset

 A requirement for identification of the GMM estimator is that the matrix 
\begin_inset Formula $D_{n}=\frac{\partial\bar{m}_{n}^{\prime}(\theta)}{\partial\theta}$
\end_inset

 must converge to a matrix with full row rank.
 Suppose that the chosen instrument is independent of the regressor,
 which implies that 
\begin_inset Formula $E(z_{t}x_{t})=E(z)E(x),$
\end_inset


\begin_inset Formula $\forall t$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
If we write the model in matrix form,
 
\begin_inset Formula $y=X\beta+\epsilon$
\end_inset

,
 carefully explain what is the form of 
\begin_inset Formula $X$
\end_inset

.
 (dimension,
 and an indication of the contents of each column).
\end_layout

\begin_layout Enumerate
What is the number of parameters to estimate,
 
\begin_inset Formula $k$
\end_inset

?
 What is the number of moment conditions,
 
\begin_inset Formula $g?$
\end_inset


\end_layout

\begin_layout Enumerate
Write out carefully an expression for each element of the matrix 
\begin_inset Formula $D_{n}.$
\end_inset


\end_layout

\begin_layout Enumerate
What is the form of the limiting 
\begin_inset Formula $D_{\infty},$
\end_inset

 the matrix to which 
\begin_inset Formula $D_{n}$
\end_inset

 converges,
 almost surely?
\end_layout

\begin_layout Enumerate
What is the rank of 
\begin_inset Formula $D_{\infty}?$
\end_inset


\end_layout

\begin_layout Enumerate
Is the proposed GMM estimator consistent or not?
 Explain.
\end_layout

\end_deeper
\begin_layout Enumerate
Using Julia,
 generate data from the logit dgp.
 The script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/NonlinearOptimization/EstimateLogit.jl}{EstimateLogit.jl} 
\end_layout

\end_inset

 should prove quite helpful.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Recall that 
\begin_inset Formula $E(y_{t}|\mathbf{x}_{t})=\mathbf{p}(\mathbf{x}_{t},\theta)=[1+\exp(-\mathbf{x}_{t}\prime\theta)]^{-1}$
\end_inset

.
 Consider the moment condtions (exactly identified) 
\begin_inset Formula $m_{t}(\theta)=[y_{t}-p(\mathbf{x}_{t},\theta)]\mathbf{x}_{t}$
\end_inset

.
 Estimate by GMM (using 
\family typewriter
gmmresults
\family default
),
 using these moments.
\end_layout

\begin_layout Enumerate
Estimate by ML (using 
\family typewriter
mleresults
\family default
).
\end_layout

\begin_layout Enumerate
The two estimators should coincide.
 Prove analytically that the estimators coincide.
\end_layout

\end_deeper
\begin_layout Enumerate
When working out the structure of 
\begin_inset Formula $\Omega_{n}$
\end_inset

,
 show that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+s}^{\prime})=\Gamma_{s}^{\prime}.$
\end_inset

 
\end_layout

\begin_layout Enumerate
Verify the missing steps needed to show that 
\begin_inset Formula $n\cdot\bar{m}(\hat{\theta})^{\prime}\hat{\Omega}^{-1}\bar{m}(\hat{\theta})$
\end_inset

 has a 
\begin_inset Formula $\chi^{2}(g-K)$
\end_inset

 distribution.
 That is,
 show that the monster matrix is idempotent and has trace equal to 
\begin_inset Formula $g-K.$
\end_inset


\end_layout

\begin_layout Enumerate
Run the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/chi2gmm.jl}{GMM/chi2gmm.jl}
\end_layout

\end_inset

 with several sample sizes.
 Do the results you obtain seem to agree with the consistency of the GMM estimator?
 Explain.
 
\end_layout

\begin_layout Enumerate
The GMM estimator with an arbitrary weight matrix has the asymptotic distribution
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}\right]
\]

\end_inset

Supposing that you compute a GMM estimator using an arbitrary weight matrix,
 so that this result applies.
 Carefully explain how you could test the hypothesis 
\begin_inset Formula $H_{0}:R\theta_{0}=r$
\end_inset

 versus 
\begin_inset Formula $H_{A}:R\theta_{0}\ne r$
\end_inset

,
 where 
\begin_inset Formula $R$
\end_inset

 is a given 
\begin_inset Formula $q\times k$
\end_inset

 matrix,
 and 
\begin_inset Formula $r$
\end_inset

 is a given 
\begin_inset Formula $q\times1$
\end_inset

 vector.
 I suggest that you use a Wald test.
 Explain exactly what is the test statistic,
 and how to compute every quantity that appears in the statistic.
 
\end_layout

\begin_layout Enumerate
(proof that the GMM optimal weight matrix is one such that 
\begin_inset Formula $W_{\infty}=\Omega_{\infty}^{-1})$
\end_inset

 Consider the difference of the asymptotic variance using an arbitrary weight matrix,
 minus the asymptotic variance using the optimal weight matrix:
 
\begin_inset Formula 
\begin{eqnarray*}
A=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1} & - & \left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}
\end{eqnarray*}

\end_inset

Set 
\begin_inset Formula $B=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}-\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1}$
\end_inset

.
 Verify that 
\begin_inset Formula $A=B\Omega_{\infty}B^{'}$
\end_inset

.
 What is the implication of this?
 Explain.
\end_layout

\begin_layout Enumerate
\align left
The asymptotic distribution of the GMM estimator,
 using a non-optimal weight matrix,
 is
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}\right]
\]

\end_inset

We know that in the case of exact identification,
 the GMM estimator does not depend on the weight matrix,
 
\begin_inset Formula $W.$
\end_inset

 If this is the case,
 the asymptotic covariance matrix must not depend on 
\begin_inset Formula $W_{\infty}$
\end_inset

,
 either.
 Prove that this is true,
 by showing that 
\begin_inset Formula $W$
\end_inset

 cancels out of the asymptotic variance.
 Hint:
 
\begin_inset Formula $\left(AB\right)^{-1}=B^{-1}A^{-1}$
\end_inset

 if both 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are invertible matrices.
\end_layout

\begin_layout Enumerate
In the context of the Hansen-Sargan test for correct specification of moments,
 discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:A-specification-test"
nolink "false"

\end_inset

,
 prove that the matrix 
\begin_inset Formula $P_{\infty}=I_{g}-\Omega_{\infty}^{-1/2}D_{\infty}^{\prime}\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1/2}$
\end_inset

 is idempotent and that its rank is 
\begin_inset Formula $g-K,$
\end_inset

 where 
\begin_inset Formula $g$
\end_inset

 is the number of moment conditions and 
\begin_inset Formula $K$
\end_inset

 is the number of parameters.
\end_layout

\begin_layout Enumerate
Consider the two equation model
\begin_inset Formula 
\begin{eqnarray*}
\text{Demand:\;\ }q_{t} & = & \alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}y_{t}+\varepsilon_{1t}\\
\text{Supply:\;\ }q_{t} & = & \beta_{1}+\beta_{2}p_{t}+\varepsilon_{2t}\\
\mathcal{E}\left(\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}
\end{array}\right]|y_{t}\right) & = & \left[\begin{array}{c}
0\\
0
\end{array}\right]\\
\mathcal{E}\left(\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}
\end{array}\right]\left[\begin{array}{ll}
\varepsilon_{1t} & \varepsilon_{2t}\end{array}\right]|y_{t}\right) & = & \left[\begin{array}{ll}
\sigma_{11} & \sigma_{12}\\
\sigma_{12} & \sigma_{22}
\end{array}\right],\forall t
\end{eqnarray*}

\end_inset

The variables 
\begin_inset Formula $q_{t}$
\end_inset

 and 
\begin_inset Formula $p_{t}$
\end_inset

 are endogenous,
 and the variable 
\begin_inset Formula $y_{t}$
\end_inset

 is weakly exogenous.
 Assume that the observations are independent over time.
 Consider GMM estimation of the parameters of the two equations implemented as two stage least squares (2SLS).
 Recall that the 2SLS estimator uses 
\begin_inset Formula $\widehat{p_{t}}$
\end_inset

 as an instrument for the endogenous regressor 
\begin_inset Formula $p_{t}$
\end_inset

,
 where 
\begin_inset Formula $\widehat{p_{t}}$
\end_inset

 is the fitted value from OLS applied to the equation 
\begin_inset Formula $p_{t}=\pi_{1}+\pi_{2}y_{t}+v_{t}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
Show that the regressor 
\begin_inset Formula $p_{t}$
\end_inset

 is correlated with each of the structural errors 
\begin_inset Formula $\varepsilon_{1t}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{2t}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Will OLS give a consistent estimator of the parameters of the supply equation?
 Explain your answer.
\end_layout

\begin_layout Enumerate
Give the exact expression for the 2SLS estimator of the parameters of the supply equation,
 and explain why the estimator is consistent.
\end_layout

\begin_layout Enumerate
Give the exact expression for the 2SLS estimator of the parameters of the demand equation,
 and carefully explain why 2SLS will 
\emph on
not
\emph default
 give a consistent estimator of these parameters.
 Note that the 2SLS estimator is a particular GMM estimator,
 and it is a particular instrumental variables (IV) estimator.
 Keeping this in mind may help you to answer the question.
\end_layout

\end_deeper
\begin_layout Enumerate
Prove that the GMM estimator based upon the 
\begin_inset Formula $g$
\end_inset

 moment conditions 
\begin_inset Formula $\bar{m}_{n}(\theta)=\left[\begin{array}{cc}
p_{n}^{\prime}(\theta) & q_{n}^{\prime}(\theta)\end{array}\right]^{\prime}$
\end_inset

 and the corresponding true optimal weight matrix is asymptotically efficient with respect to the GMM estimator based upon the 
\begin_inset Formula $h<g$
\end_inset

 moment conditions 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p_{n}(\theta)$
\end_inset

 and the corresponding true optimal weight matrix.
\end_layout

\begin_deeper
\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Interpret the result
\end_layout

\begin_layout Enumerate
Discuss the importance of the result from an empirical point of view.
 Are there any cautions one should observe when doing applied GMM work?
 Describe any problems you can imagine.
\end_layout

\end_deeper
\begin_layout Enumerate
Recall the dynamic model with measurement error that was discussed in class:
 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}^{*} & = & \alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}\\
y_{t} & = & y_{t}^{*}+\upsilon_{t}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 are independent Gaussian white noise errors.
 Suppose that 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is not observed,
 and instead we observe 
\begin_inset Formula $y_{t}$
\end_inset

.
 We can estimate the equation 
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\nu_{t}
\]

\end_inset

using GIV,
 as was done above.
 The Julia script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/GMM/SpecTest.jl}{GMM/SpecTest.jl} 
\end_layout

\end_inset

 performs a Monte Carlo study of the performance of the GMM criterion test,
 
\begin_inset Formula 
\[
n\cdot s_{n}(\hat{\theta})\stackrel{d}{\rightarrow}\chi^{2}(g-K)
\]

\end_inset

Examine the script and describe what it does.
 Run this script to verify that the test over-rejects.
 Increase the sample size,
 to determine if the over-rejection problem becomes less severe.
 Discuss your findings.
\end_layout

\begin_layout Enumerate
Suppose we have two equations
\begin_inset Formula 
\begin{eqnarray*}
y_{t1} & = & \alpha_{1}+\alpha_{2}y_{t2}+\epsilon_{t1}\\
y_{t2} & = & \beta_{1}+\beta_{2}x_{t}+\epsilon_{t2}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $V(\epsilon_{t1})=\sigma_{1}^{2}>0$
\end_inset

,
 
\begin_inset Formula $V(\epsilon_{t2})=\sigma_{2}^{2}>0$
\end_inset

,
 
\begin_inset Formula $E(\epsilon_{t1}\epsilon_{t2})=\sigma_{12}\ne0$
\end_inset

.
 The observations are independent over time,
 and the errors have zero mean.
 The variable 
\begin_inset Formula $x_{t}$
\end_inset

 is strictly exogenous:
 it is uncorrelated with the two epsilons at all time periods.
\end_layout

\begin_deeper
\begin_layout Enumerate
Is the OLS estimator of the parameters of the first equation consistent or not?
 Explain.
\end_layout

\begin_layout Enumerate
Is the OLS estimator of the parameters of the second equation consistent or not?
 Explain.
\end_layout

\begin_layout Enumerate
If the OLS estimator of the parameters of the first equation is not consistent,
 propose a consistent estimator of the parameters of the first equation and explain why the proposed estimator is consistent.
 
\end_layout

\begin_layout Enumerate
If the OLS estimator of the parameters of the second equation is not consistent,
 propose a consistent estimator of the parameters of the second equation and explain why the proposed estimator is consistent.
\end_layout

\end_deeper
\begin_layout Enumerate
Estimate a logit model by GMM using the 10 independent data points
\begin_inset Newline newline
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="11">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

.
 
\begin_inset Newline newline
\end_inset

For the logit model,
 the probability 
\begin_inset Formula $P(y_{t}=1|x_{t})=(1+\exp(-\theta_{1}-\theta_{2}x_{t}))^{-1}$
\end_inset

,
 and the probability that 
\begin_inset Formula $y_{t}=0$
\end_inset

 is the complement.
\end_layout

\begin_deeper
\begin_layout Enumerate
create a data file that contains these observations
\end_layout

\begin_layout Enumerate
find the conditional mean 
\begin_inset Formula $E(y|x)$
\end_inset

 and the conditional variance 
\begin_inset Formula $V(y|x$
\end_inset

)
\end_layout

\begin_layout Enumerate
propose at least 2 moment conditions,
 using the mean and the variance you found in (b)
\end_layout

\begin_layout Enumerate
write a Julia function that computes the GMM estimator using your two moment conditions
\end_layout

\begin_layout Enumerate
compute the two step efficient GMM estimator
\end_layout

\begin_layout Enumerate
comment on the results
\end_layout

\end_deeper
\begin_layout Enumerate
Given the 10 independent data points
\begin_inset Newline newline
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="11">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

.
 
\begin_inset Newline newline
\end_inset

For the Poisson model,
 the density 
\begin_inset Formula $f_{Y}(y|x)=\frac{\exp(-\lambda)\lambda^{y}}{y!},$
\end_inset

 
\begin_inset Formula $y=0,1,2,...$
\end_inset

.
 To make the model depend on conditioning variables,
 use the parameterization 
\begin_inset Formula $\lambda(x)=\exp(\theta_{1}+\theta_{2}x)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
The mean of a Poisson distribution with parameter 
\begin_inset Formula $\lambda$
\end_inset

 is equal to 
\begin_inset Formula $\lambda,$
\end_inset

 and so is the variance.
 Propose moment conditions to an overidentified 
\begin_inset Formula $(g>k)$
\end_inset

 GMM estimator of 
\begin_inset Formula $\theta_{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Discuss how your proposed moment conditions relate to the score function of the maximum likelihood estimator.
\end_layout

\begin_layout Enumerate
Estimate the parameters using two-step efficient GMM,
 using the moment conditions you have proposed.
\end_layout

\begin_layout Enumerate
Discuss the results,
 and compare them to your ML estimates for the similar problem in the chapter on ML estimation.
\end_layout

\end_deeper
\begin_layout Enumerate
Suppose that we have a rational expectations model.
 At time 
\begin_inset Formula $t,$
\end_inset

 the representative agent knows the values of all variables indexed 
\begin_inset Formula $t-1$
\end_inset

 and earlier.
 Suppose the production function is 
\begin_inset Formula $y_{t}=k_{t}^{\alpha}n_{t}^{1-\alpha}\exp(\epsilon_{t})$
\end_inset

,
 where 
\begin_inset Formula $y$
\end_inset

 is output,
 
\begin_inset Formula $k$
\end_inset

 is capital,
 
\begin_inset Formula $n$
\end_inset

 is labor,
 and 
\begin_inset Formula $\epsilon$
\end_inset

 is a white noise shock.
 One of the first order conditions tells us that the wage rate,
 
\begin_inset Formula $w_{t}$
\end_inset

,
 is equal to the marginal product of labor (recall:
 MPL=
\begin_inset Formula $\partial y/\partial n$
\end_inset

).
 The econometrician has a data set that includes only the variables 
\begin_inset Formula $k_{t},$
\end_inset


\begin_inset Formula $n_{t}$
\end_inset

 and 
\begin_inset Formula $w_{t}$
\end_inset

.
 The econometrician knows the correct specification of the production function,
 except for the value of 
\begin_inset Formula $\alpha$
\end_inset

,
 and also knows that 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is a white noise shock.
 At time 
\begin_inset Formula $t$
\end_inset

,
 the agent observes the shock 
\begin_inset Formula $\epsilon_{t},$
\end_inset

 and the capital level,
 
\begin_inset Formula $k_{t},$
\end_inset

 which is a non-stochastic function of variables that are pre-determined at time 
\begin_inset Formula $t$
\end_inset

,
 and then makes choices that determine the values of the variables 
\begin_inset Formula $y_{t},$
\end_inset


\begin_inset Formula $n_{t},$
\end_inset


\begin_inset Formula $w_{t},$
\end_inset


\begin_inset Formula $r_{t}$
\end_inset

,
 among others,
 using the information that has been given,
 plus some other unspecified equations.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
Consider the linear regression 
\begin_inset Formula $\log w_{t}=c+\alpha(\log k_{t}-\log n_{t})+\epsilon_{t}$
\end_inset

.
 Will the OLS estimate of 
\begin_inset Formula $\alpha$
\end_inset

 using this regression be consistent or not?
 Explain.
\end_layout

\begin_layout Enumerate
Provide at least two moment conditions that can be used to consistently estimate the parameter 
\begin_inset Formula $\alpha$
\end_inset

 by GMM.
\end_layout

\begin_layout Enumerate
Explain why the proposed moment conditions have mean zero at the true parameter value.
\end_layout

\begin_layout Enumerate
Explain how you could estimate 
\begin_inset Formula $\alpha$
\end_inset

 using a two step efficient GMM estimator.
 Give the explicit form of the optimal weight matrix,
 for your chosen moment conditions.
\end_layout

\end_deeper
\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\begin{align*}
y_{t} & =\alpha+\rho_{1}y_{t-1}+\rho_{2}y_{t-2}+\beta x_{t}+\epsilon_{t}
\end{align*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is a 
\begin_inset Formula $N(0,1)$
\end_inset

 white noise error.
 This is an autoregressive model of order 2 (AR2) model,
 with an additional exogenous regressor.
 Suppose that data is generated from the AR2 model,
 but the econometrician mistakenly decides to estimate an AR1 model,
 
\begin_inset Formula $y_{t}=\alpha+\rho_{1}y_{t-1}+\beta x_{t}+v_{t}$
\end_inset

.
 This is a case of omitted relevant variables.
\end_layout

\begin_deeper
\begin_layout Enumerate
show that weak exogeneity fails for the AR1 model.
\end_layout

\begin_layout Enumerate
Consider IV estimation of the AR1 model,
 using lags of 
\begin_inset Formula $x_{t}$
\end_inset

 as instruments.
 Is this a consistent estimator?
\end_layout

\begin_layout Enumerate
simulate data from the correct AR2 model,
 using 
\begin_inset Formula $\alpha=0$
\end_inset

,
 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset

,
 
\begin_inset Formula $\rho_{2}=0.4$
\end_inset

,
 
\begin_inset Formula $\beta=2$
\end_inset

,
 and 
\begin_inset Formula $x_{t}\sim IIN(0,1).$
\end_inset

 Use a sample size of 30 observations.
\end_layout

\begin_deeper
\begin_layout Enumerate
estimate the incorrectly specified AR1 model by OLS
\end_layout

\begin_layout Enumerate
estimate the correctly specified AR2 model by OLS
\end_layout

\begin_layout Enumerate
implement your proposed IV estimator of the AR1 model
\end_layout

\begin_layout Enumerate
embed the simulations and estimations in a loop,
 to do a Monte Carlo study using 1000 replications.
 Provide histograms for the distribtions of the estimators of the parameter 
\begin_inset Formula $\rho_{1}$
\end_inset

 for the 3 estimators.
\end_layout

\end_deeper
\begin_layout Enumerate
discuss all results thoroughly,
 focusing on bias and standard errors of the estimators of the autoregressive parameters
\end_layout

\end_deeper
\begin_layout Enumerate
Estimate the investment equation of the Klein Model 1 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Example:-Klein's-Model"
nolink "false"

\end_inset

) using GMM.
 See the example at the end of the discussion of 2SLS for a good hint.
\end_layout

\begin_layout Enumerate
Verify the missing steps needed to show that 
\begin_inset Formula $n\cdot m(\hat{\theta})^{\prime}\hat{\Omega}^{-1}m(\hat{\theta})$
\end_inset

 has a 
\begin_inset Formula $\chi^{2}(g-K)$
\end_inset

 distribution.
 That is,
 show that the big ugly matrix is idempotent and has trace equal to 
\begin_inset Formula $g-K.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Bayesian-methods"

\end_inset

Bayesian methods
\end_layout

\begin_layout Standard
This chapter provides a brief introduction to Bayesian methods,
 which form a large part of econometric research,
 especially in the last two decades.
 Advances in computational methods (e.g.,
 MCMC,
 particle filtering),
 combined with practical advantages of Bayesian methods (e.g.,
 no need for minimization and improved identification coming from the prior) have contributed to the popularity of this approach.
 References I have used to prepare these notes:
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Chapter 13;
 
\begin_inset CommandInset citation
LatexCommand cite
key "ChernozhukovHong2003"
literal "true"

\end_inset


\emph on
;
 
\emph default

\begin_inset CommandInset citation
LatexCommand citet
key "emm"
literal "false"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand citet
key "gallant2002emm"
literal "false"

\end_inset

;
 Hoogerheide,
 van Dijk and van Oest (2007) 
\begin_inset Quotes sld
\end_inset

Simulation Based Bayesian Econometric Inference:
 Principles and Some Recent Computational Advances
\begin_inset Quotes srd
\end_inset

.
 You might also like to read Mikusheva's MIT OpenCourseWare notes,
 lectures 23-26:
 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "Bayesian notes"
target "http://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2013/lecture-notes/"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Definitions
\end_layout

\begin_layout Standard
The Bayesian approach summarizes beliefs about parameters using a density function:
\end_layout

\begin_layout Itemize
There is a true unknown parameter vector,
 
\begin_inset Formula $\theta_{0}$
\end_inset

,
 and the density 
\begin_inset Formula $\pi(\theta)$
\end_inset

,
 which is known as the 
\emph on
prior
\emph default
,
 reflects current beliefs about the parameter,
 before observing the sample.
 It is assumed that the econometrician can provide this density.
\end_layout

\begin_layout Itemize
We also have sample information,
 
\begin_inset Formula $y$
\end_inset

=
\begin_inset Formula $\left\{ y_{1},y_{2},...y_{n}\right\} $
\end_inset

.
 We're already familiar with the 
\emph on
likelihood function,

\emph default
 
\begin_inset Formula $f(y|\theta)$
\end_inset

,
 which is the density of the sample given a parameter value.
 
\end_layout

\begin_layout Standard
Given these two pieces,
 we can write the joint density of the sample and the beliefs:
\begin_inset Formula 
\[
f(y,\theta)=f(y|\theta)\pi(\theta)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

We can get the 
\emph on
marginal likelihood 
\emph default
of the data by integrating out the parameter,
 integrating over its support 
\begin_inset Formula $\Theta$
\end_inset

:
\emph on

\begin_inset Formula 
\[
f(y)=\int_{\Theta}f(y,\theta)d\theta
\]

\end_inset


\emph default
The last step is to get the 
\emph on
posterior 
\emph default
of the parameter.
 This is simply the density of the parameter conditional on the sample,
 and we get it in the normal way we get a conditional density,
 using Bayes' theorem:
\begin_inset Formula 
\[
f(\theta|y)=\frac{f(y,\theta)}{f(y)}=\frac{f(y|\theta)\pi(\theta)}{f(y)}
\]

\end_inset


\end_layout

\begin_layout Itemize
The movement from the prior to the posterior reflects the learning that occurs about the parameter when one receives the sample information.
\end_layout

\begin_layout Itemize
The sources of information used to make the posterior are the prior and the likelihood function.
 
\end_layout

\begin_layout Itemize
Once we have the posterior,
 one can provide a complete probabilistic description about our updated beliefs about the parameter,
 using quantiles or moments of the posterior.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The posterior mean or median provide the Bayesian analogue of the frequentist point estimator,
 in the form of the ML estimator.
 
\end_layout

\begin_layout Itemize
One can show that these point estimators converge to the true 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
We can define regions analogous to confidence intervals by using quantiles of the posterior,
 or the marginal posterior.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
So far,
 this is pretty straightforward.
 The complications are mostly computational.
 To illustrate,
 the posterior mean is
\begin_inset Formula 
\begin{align*}
E(\theta|y) & =\int_{\Theta}\theta f(\theta|y)d\theta\\
 & =\frac{\int_{\Theta}\theta f(y|\theta)\pi(\theta)d\theta}{\int_{\Theta}f(y,\theta)d\theta}
\end{align*}

\end_inset

 
\end_layout

\begin_layout Itemize
One can see that a means of integrating will be needed.
\end_layout

\begin_layout Itemize
note that 
\begin_inset Formula $\theta$
\end_inset

 is a vector,
 so that the integrals are multiple
\end_layout

\begin_layout Itemize
Only in very special cases will the integrals have analytic solutions.
\end_layout

\begin_layout Itemize
Otherwise,
 computational methods will be needed.
 Advances in computational methods are what have lead to the increased use of Bayesian methods.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Philosophy,
 etc.
\end_layout

\begin_layout Standard
So,
 the classical paradigm views the data as generated by a data generating process,
 which is a perhaps unknown model characterized by a parameter vector,
 and the data is generated from the model at a particular value of the parameter vector,
 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Bayesians view data as given,
 and update beliefs about a parameter using the information about the parameter contained in the data.
 There's nothing obviously contradictory in these views.
 Nevertheless,
 it's not hard to find discussions where there are disagreements.
\end_layout

\begin_layout Standard
Here,
 I'm trying to address a model with a fixed non-random parameter about which we would like to learn.
 As long as the object of interest - the dgp and it's parameter - is agreed upon,
 then we can contemplate using any convenient tool.
\end_layout

\begin_layout Standard
Even if one is a strict frequentist,
 one shouldn't reinvent the wheel each time we get a new sample:
 previous samples have information about the parameter,
 and we should use all of the available information.
 A pure frequentist 
\begin_inset Quotes sld
\end_inset

full information
\begin_inset Quotes srd
\end_inset

 approach would require writing the joint likelihood of all samples,
 which would almost certainly constitute an impossible task.
 The Bayesian approach concentrates all of the information coming from previous work in the form of a prior.
 A fairly simple,
 easy to use prior may not 
\emph on
exactly
\emph default
 capture all previous information,
 but it could offer a handy and reasonably accurate summary,
 and it's almost certainly better than simply pretending that all of that previous information simply doesn't exist.
 So,
 the idea of a prior as a summary of what we have learned may simply be viewed as a practical solution to the problem of using all the available information.
 Given that it's a summary,
 one may as well use a convenient form,
 as long as it's plausible and the results don't depend too exaggeratedly upon particular form used.
\end_layout

\begin_layout Standard
As long as one takes the view that there is a fixed unknown parameter value 
\begin_inset Formula $\theta_{0}$
\end_inset

 which generates all samples,
 then frequentist and Bayesian methods are trying to inform us about the same object,
 and the choice between tools may become one of convenience.
 It turns out that one can analyze Bayesian estimators from a classical (frequentist) perspective.
 It also turns out that Bayesian estimators may be easier to compute reliably than analogous classical estimators.
 These computational advantages,
 combined with the ability to use information from previous work in an intelligent way,
 make the study of Bayesian methods attractive for frequentists.
 If a Bayesian takes the view that there is a fixed data generating process,
 and Bayesian learning leads in the limit to the same fixed true value that frequentists posit,
 then the study of frequentist theory will be useful to a Bayesian practitioner.
 For example,
 the GMM estimator is closely related to some versions of 
\begin_inset CommandInset href
LatexCommand href
name "Approximate Bayesian Computing"
target "http://en.wikipedia.org/wiki/Approximate_Bayesian_computation"
literal "false"

\end_inset

 (ABC).
 Thus,
 knowledge of theory and practical experience with GMM can be a useful guide to implementing ABC estimators.
\end_layout

\begin_layout Itemize
For the rest of this,
 I will adopt the classical,
 frequentist perspective,
 and study the behavior of Bayesian estimators in this context.
\end_layout

\begin_layout Itemize
One should note that the traditional Bayesian approach requires the likelihood function,
 just as is the case with ML.
 Thus,
 it uses 
\emph on
strong assumptions
\emph default
,
 for a given model.
 
\end_layout

\begin_layout Itemize
There are Bayesian methods for choosing between models,
 which we will not get into.
\end_layout

\begin_layout Itemize
There are also recent Bayesian-inspired methods that attempt to work without knowledge of the likelihood function.
 For instance,
 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHong2003"
literal "true"

\end_inset

 use Bayesian methods to compute a GMM estimator.
 Some such methods,
 e.g.
 
\begin_inset CommandInset href
LatexCommand href
name "Approximate Bayesian Computing"
target "http://en.wikipedia.org/wiki/Approximate_Bayesian_computation"
literal "false"

\end_inset

 require the model to be simulable,
 in which case,
 essentially the same strong assumptions as underlie ML are being used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example
\end_layout

\begin_layout Standard
Suppose data is generated by i.i.d.
 sampling from an exponential distribution with mean 
\begin_inset Formula $\theta$
\end_inset

.
 An exponential random variable takes values on the positive real numbers.
 Waiting times are often modeled using the exponential distribution.
\end_layout

\begin_layout Itemize
The density of a typical sample element is 
\begin_inset Formula $f(y|\theta)=\frac{1}{\theta}e^{-y/\theta}$
\end_inset

.
 The likelihood is simply the product of the sample contributions.
\end_layout

\begin_layout Itemize
Suppose the prior for 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula $\theta\sim\mathrm{lognormal}(1,1)$
\end_inset

.
 This means that the logarithm of 
\begin_inset Formula $\theta$
\end_inset

 is normally distributed with mean 1 and standard deviation 1.
 We use a lognormal prior because it enforces the requirement that the parameter of the exponential density be positive.
\end_layout

\begin_layout Itemize
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Bayesian/BayesExample1.jl}{BayesExample1.jl} 
\end_layout

\end_inset

 implements Bayesian estimation for this setup.
\end_layout

\begin_layout Standard
With a sample of 10 observations,
 we obtain the results in panel (a) of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Bayesian-estimation,-exponential"
nolink "false"

\end_inset

,
 while with a sample of size 50 we obtain the results in panel (b).
 Note how the posterior is more concentrated around the true parameter value in panel (b).
 Also note how the posterior mean is closer to the prior mean when the sample is small.
 When the sample is small,
 the likelihood function has less weight,
 and more of the information comes from the prior.
 When the sample is larger,
 the likelihood function will have more weight,
 and its effect will dominate the prior's.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Bayesian-estimation,-exponential"

\end_inset

Bayesian estimation,
 exponential likelihood,
 lognormal prior
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
N=10
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/BayesExampleN10.png
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
N=50
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/BayesExampleN50.png
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHong2003"
literal "false"

\end_inset

 
\begin_inset Quotes sld
\end_inset

An MCMC Approach to Classical Estimation
\begin_inset Quotes srd
\end_inset

 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.sciencedirect.com/science/article/pii/S0304407603001003
\end_layout

\end_inset

 is a very interesting article that shows how Bayesian methods may be used with criterion functions that are associated with classical estimation techniques.
 For example,
 it is possible to compute a posterior mean version of a GMM estimator.
 Chernozhukov and Hong provide their Theorem 2 (in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Chernozhukov-and-Hong,"
nolink "false"

\end_inset

),
 which proves consistency and asymptotic normality for a general class of such estimators.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Chernozhukov-and-Hong,"

\end_inset

Chernozhukov and Hong,
 Theorem 2
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/Theorem2.png
	lyxscale 50
	width 20cm

\end_inset


\end_layout

\end_inset


\emph on
When
\end_layout

\begin_layout Itemize
the criterion function 
\begin_inset Formula $L_{n}(\theta)$
\end_inset

 in their paper is set to the log-likelihood function
\end_layout

\begin_layout Itemize
the pseudo-prior 
\begin_inset Formula $\pi(\theta)$
\end_inset

 is a real Bayesian prior
\end_layout

\begin_layout Itemize
the penalty function 
\begin_inset Formula $\rho_{n}$
\end_inset

 is the squared loss function (which is symmetric,
 so that 
\begin_inset Formula $\xi$
\end_inset

 in the theorem is zero)
\end_layout

\begin_layout Standard

\emph on
then the class of estimators discussed by CH reduces to the ordinary Bayesian posterior mean.
\end_layout

\begin_layout Itemize
Their paper focuses on letting 
\begin_inset Formula $L_{n}(\theta)$
\end_inset

 be something else,
 for example the GMM criterion (two-step or CUE),
 which opens the door to using Bayesian methods to compute GMM (or other) estimators.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
As such,
 their Theorem 2,
 tells us that the ordinary Bayesian posterior mean is consistent and asymptotically normally distributed.
\end_layout

\begin_layout Itemize
In particular,
 the Bayesian posterior mean has the same asymptotic distribution as does the ordinary maximum likelihood estimator.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Itemize
the intuition is clear:
 The information content from the prior is fixed.
 As the amount of information coming from the sample increases,
 the likelihood function brings an increasing amount of information,
 relative to the prior.
 Eventually,
 the prior is no longer important for determining the shape of the posterior.
\end_layout

\begin_layout Itemize
so,
 when the sample is large,
 the shape of the posterior depends on the likelihood function.
 The likelihood function collapses around 
\begin_inset Formula $\theta_{0}$
\end_inset

 when the sample is generated at 
\begin_inset Formula $\theta_{0}.$
\end_inset

 The same is true of the posterior,
 it narrows around 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 This causes the posterior mean to converge to the true parameter value.
 In fact,
 all quantiles of the posterior converge to 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Chernozhukov and Hong discuss estimators defined using quantiles.
\end_layout

\begin_layout Itemize

\color blue
For an econometrician coming from the frequentist perspective,
 this is attractive.
 The Bayesian point estimator has the same asymptotic behavior as the MLE.
 There may be computational advantages to using the Bayesian approach,
 because there is no need for optimization.
 If the objective function that defines the classical estimator is irregular (multiple local optima,
 nondifferentiabilities,
 noncontinuities...),
 then optimization may be very difficult.
 However,
 Bayesian methods that use integration may be more tractable.
 This is the main motivation of CH's paper.

\color inherit
 
\end_layout

\begin_layout Itemize
Additional advantages include the benefits if an informative prior is available.
 When this is the case,
 the Bayesian estimator can have better small sample performance than the maximum likelihood estimator.
 The ML estimator uses only the information in the sample.
 Adding outside information can help.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Computational methods
\end_layout

\begin_layout Itemize
To compute the posterior mean,
 we need to evaluate
\begin_inset Formula 
\begin{align*}
E(\theta|y) & =\int_{\Theta}\theta f(\theta|y)d\theta\\
 & =\int_{\Theta}\theta\frac{f(y|\theta)\pi(\theta)}{f(y)}d\theta\\
= & \frac{1}{f(y)}{\color{blue}\int{}_{\Theta}\theta f(y|\theta)\pi(\theta)d\theta}
\end{align*}

\end_inset

where the marginal density of the data 
\begin_inset Formula $f(y)$
\end_inset

 is obtained from
\begin_inset Formula 
\[
f(y)={\color{blue}\int_{\Theta}f(y,\theta)d\theta}
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that both of the integrals in blue are multiple integrals,
 with the dimension given by that of the parameter,
 
\begin_inset Formula $\theta.$
\end_inset


\end_layout

\begin_layout Itemize
Under some special circumstances (
\emph on
e.g.,
 
\emph default
conjugate priors),
 the integrals may have analytic solutions:
 e.g.,
 Gaussian likelihood with a Gaussian prior leads to a Gaussian posterior.
 In principle,
 one shouldn't be adjusting priors for simple computational convenience....
\end_layout

\begin_layout Itemize
When the dimension of the parameter is low,
 quadrature methods may be used.
 What was done in 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Bayesian/BayesExample1.jl}{BayesExample1.jl} 
\end_layout

\end_inset

 is an unsophisticated example of this.
 More sophisticated methods use an intelligently chosen grid to reduce the number of function evaluations.
 Still,
 these methods only work for dimensions up to 3 or so.
\end_layout

\begin_layout Itemize
Otherwise,
 some form of simulation-based 
\begin_inset Quotes sld
\end_inset

Monte Carlo
\begin_inset Quotes srd
\end_inset

 integration must be used.
 The basic idea is that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E(\theta|y)$
\end_inset

 can be approximated by 
\begin_inset Formula $(1/S)\sum_{s=1}^{S}\theta^{s}$
\end_inset

,
 where 
\begin_inset Formula $\theta^{s}$
\end_inset

 is a random draw from the posterior distribution 
\begin_inset Formula $f(\theta|y)$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
The 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
trick is
\emph on
 how to make draws from the posterior
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 when in general we can't compute the posterior.
\end_layout

\begin_deeper
\begin_layout Itemize
the law of large numbers tells us that this average will converge to the desired expectation as 
\begin_inset Formula $S$
\end_inset

 gets large
\end_layout

\begin_layout Itemize
convergence will be more rapid if the random draws are independent of one another,
 but insisting on independence may have computational drawbacks.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Monte Carlo methods include:
\end_layout

\begin_layout Itemize
importance sampling
\end_layout

\begin_layout Itemize
Markov chain Monte Carlo (MCMC)
\end_layout

\begin_layout Itemize
sequential Monte Carlo (SMC,
 also known as particle filtering)
\end_layout

\begin_layout Itemize
Hamiltonian Monte Carlo (HMC).
 
\end_layout

\begin_layout Itemize
The great expansion of these methods over the years has caused Bayesian econometrics to become much more widely used than it was in the not so distant (for some of us) past.
 There is much literature - here we will only look at a basic example that captures the main ideas.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
MCMC
\end_layout

\begin_layout Standard
Variants of Markov chain Monte Carlo have become a very widely used means of computing Bayesian estimates.
 See Tierney (1994) 
\begin_inset Quotes sld
\end_inset

Markov Chains for Exploring Posterior Distributions
\begin_inset Quotes srd
\end_inset

 
\emph on
Annals of Statistics
\emph default
 and Chib and Greenberg (1995) 
\begin_inset Quotes sld
\end_inset

Understanding the Metropolis-Hastings algorithm
\begin_inset Quotes srd
\end_inset

 
\emph on
The American Statistician.
\end_layout

\begin_layout Standard
Let's consider the basic 
\series bold
Metropolis-Hastings MCMC
\series default
 algorithm.
 This is good to get the basic idea of Monte Carlo methods,
 but it is often not the best choice in practice.
\end_layout

\begin_layout Standard
We will generate a long realization of a Markov chain process for 
\begin_inset Formula $\theta$
\end_inset

,
 as follows:
\end_layout

\begin_layout Itemize
The prior density is 
\begin_inset Formula $\pi(\theta)$
\end_inset

,
 as above.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $g(\theta^{*};\theta^{s})$
\end_inset

 be a 
\emph on
proposal density
\emph default
,
 which describes the density of a trial value 
\begin_inset Formula $\theta^{*}$
\end_inset

 conditional on starting at 
\begin_inset Formula $\theta^{s}$
\end_inset

.
 It must be possible to sample from the proposal.
 This gives a new trial parameter value 
\begin_inset Formula $\theta^{*}$
\end_inset

,
 given the most recently accepted parameter value 
\begin_inset Formula $\theta^{s}$
\end_inset

.
 A proposal will be accepted if
\begin_inset Formula 
\[
\frac{f(\theta^{*}|y)}{f(\theta^{s}|y)}\frac{g(\theta^{s};\theta^{*})}{g(\theta^{*};\theta^{s})}>\alpha
\]

\end_inset

where 
\begin_inset Formula $\alpha$
\end_inset

 is a 
\begin_inset Formula $U(0,1)$
\end_inset

 random variate.
 
\end_layout

\begin_layout Standard
There are two parts to the numerator and denominator:
 the posterior,
 and the proposal density.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Focusing on the numerator,
 
\begin_inset Formula $f(\theta^{*}|y)g(\theta^{s};\theta^{*}),$
\end_inset

when the trial value of the proposal 
\begin_inset Formula $f(\theta^{*}|y)$
\end_inset

 has a higher posterior,
 acceptance is favored.
 
\end_layout

\begin_layout Itemize
The other factor is 
\begin_inset Formula $g(\theta^{s};\theta^{*})$
\end_inset

,
 the density associated with returning to 
\begin_inset Formula $\theta^{s}$
\end_inset

 (where we are) when starting at 
\begin_inset Formula $\theta^{*}$
\end_inset

 (where we will be if the proposal is accepted),
 which has to do with the reversibility of the Markov chain.
 If this is too low,
 acceptance is not favored.
 We don't want to jump to a new region if we will never get back,
 as we need to sample from the entire support of the posterior.
 Falling into a region we can never get out of will not let us explore the whole support.
\end_layout

\begin_layout Itemize
The two together mean that we will jump to a new area only if we are able to eventually jump back with a reasonably high probability.
 The probability of jumping is higher when the new area has a higher posterior density,
 but lower if it's hard to get back.
 
\end_layout

\begin_layout Itemize
The idea is to sample from all regions of the posterior,
 those with high and low density,
 sampling more heavily from regions of high density.
 We want to go occasionally to regions of low density,
 but it is important not to get stuck there.
\end_layout

\begin_layout Itemize
Consider a bimodal density:
 we want to explore the area around both modes.
 To be able to do that,
 it is important that the proposal density allows us to be able to jump between modes.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Note that the ratio of posteriors is equal to the ratio of likelihoods times the ratio of priors:
\begin_inset Formula 
\[
\frac{f(\theta^{*}|y)}{f(\theta^{s}|y)}=\frac{f(y|\theta^{*})}{f(y|\theta^{s})}\frac{\pi(\theta^{*})}{\pi(\theta^{s})}
\]

\end_inset

because the marginal likelihood 
\begin_inset Formula $f(y)$
\end_inset

 is the same in both cases,
 and cancels out.
 
\end_layout

\begin_deeper
\begin_layout Itemize
We don't need to compute that integral!
 
\end_layout

\begin_layout Itemize
We don't need to know the posterior,
 either.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
With this,
 the acceptance criterion can be written as:
 accept if
\begin_inset Formula 
\[
\frac{f(y|\theta^{*})}{f(y|\theta^{s})}\frac{\pi(\theta^{*})}{\pi(\theta^{s})}\frac{g(\theta^{s};\theta^{*})}{g(\theta^{*};\theta^{s})}>\alpha
\]

\end_inset

otherwise,
 reject
\end_layout

\begin_layout Itemize
From this,
 we see that the information needed to determine if a proposal is accepted or rejected is the prior,
 the proposal density,
 and the likelihood function 
\begin_inset Formula $f(y|\theta)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
in principle,
 the prior is non-negotiable.
 In practice,
 people often chose priors with convenience in mind
\end_layout

\begin_layout Itemize
the likelihood function is what it is,
 it comes from the model
\end_layout

\begin_layout Itemize
the place where artistry comes to bear is the choice of the proposal density
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
when the proposal density is 
\emph on
symmetric,
 
\emph default
so that 
\begin_inset Formula $g(\theta^{s};\theta^{*})=g(\theta^{*};\theta^{s})$
\end_inset

,
 the acceptance criterion simplifies to 
\begin_inset Formula 
\[
\frac{f(y|\theta^{*})}{f(y|\theta^{s})}\frac{\pi(\theta^{*})}{\pi(\theta^{s})}>\alpha
\]

\end_inset


\end_layout

\begin_layout Itemize
A random walk proposal,
 where the trial value is the current value plus a shock that doesn't depend on the current value,
 satisfies symmetry.
 This is often a reasonable choice.
\end_layout

\begin_layout Itemize
If the prior happens to be uniform over the support,
 then it also cancels out.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
the steps are:
\end_layout

\begin_layout Enumerate
the algorithm is initialized at some 
\begin_inset Formula $\theta^{1}$
\end_inset


\end_layout

\begin_layout Enumerate
for 
\begin_inset Formula $s=2,...,S,$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Enumerate
draw 
\begin_inset Formula $\theta^{*}$
\end_inset

 from 
\begin_inset Formula $g(\theta^{*};\theta^{s})$
\end_inset


\end_layout

\begin_layout Enumerate
according to the acceptance/rejection criterion,
 if the result is acceptance,
 set 
\begin_inset Formula $\theta^{s+1}=\theta^{*}$
\end_inset

,
 otherwise set 
\begin_inset Formula $\theta^{s+1}=\theta^{s}$
\end_inset


\end_layout

\begin_layout Enumerate
iterate
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Once the chain is considered to have stabilized,
 say at iteration 
\begin_inset Formula $r$
\end_inset

,
 the values of 
\begin_inset Formula $\theta^{s}$
\end_inset

 for 
\begin_inset Formula $s>r$
\end_inset

 are taken to be draws from the posterior.
 The posterior mean is computed as the simple average of the value.
 Quantiles,
 etc.,
 can be computed in the appropriate fashion.
\end_layout

\begin_layout Itemize
the art of applying these methods consists of providing a 
\color blue
good proposal density
\color inherit
 so that the acceptance rate is reasonably high,
 but not too high.
 
\color blue
There is a vast literature on this,
 and the vastness of the literature should serve as a warning that getting this to work in practice is not necessarily a simple matter.

\color inherit
 If it were,
 there would be fewer papers on the topic.
 A good proposal is one that looks like the posterior,
 because we are trying to sample from the posterior.
 However,
 we don't know the posterior,
 which is why we are doing MCMC in the first place....
\end_layout

\begin_deeper
\begin_layout Itemize
too high acceptance rate:
 this is usually due to a proposal density that gives proposals very close to the current value,
 e.g,
 a random walk with very low variance.
 This means that the posterior is being explored inefficiently,
 we travel around through the support at a very low rate,
 which means the chain will have to run for a (very,
 very...) long time to do a thorough exploration.
\end_layout

\begin_layout Itemize
too low acceptance rate:
 this means that the steps are too large,
 and we attempt to move to low posterior density regions too frequently.
 The chain will become highly autocorrelated,
 as it stays in the same place due to rejections,
 so long periods convey little additional information relative to a subset of the values in the interval 
\end_layout

\begin_layout Itemize
different ways to construct proposals forms a large part of the MCMC literature.
\end_layout

\begin_layout Itemize
the newer Hamiltonian Monte Carlo method has the advantage that one does not need to provide a proposal.
 However,
 the likelihood has to be differentiable to use HMC.
\end_layout

\end_deeper
\begin_layout Itemize
look at 
\begin_inset CommandInset href
LatexCommand href
name "Geoff Gordon's mh.m"
target "http://www.cs.cmu.edu/~ggordon/MCMC/"
literal "false"

\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
MCMC for the simple example
\end_layout

\begin_layout Standard
The simple exponential example with log-normal prior can be implemented using MH MCMC,
 and this is done in the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Bayesian/BayesExample2.jl}{BayesExample2.jl} 
\end_layout

\end_inset

.
 Play around with the sample size and the tuning parameter,
 and note the effects on the computed posterior mean and on the acceptance rate.
 An example of output is given in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Metropolis-Hastings-MCMC,-expone"
nolink "false"

\end_inset

,
 which shows the final draws of the chain,
 and the posterior density (computed using non-parametric density estimation,
 more on that later).
 In that Figure,
 the chain is probably too spiky:
 too many draws are being accepted (it's around 0.6,
 which you'll see if you run the code),
 meaning that the tuning parameter needs to be increased,
 to lower the acceptance rate.
 If you increase the sample size,
 you'll see how the posterior concentrates around the true value,
 3.
\end_layout

\begin_layout Standard
An example of estimating this same simple exponential model,
 but using a specialized package (
\begin_inset CommandInset href
LatexCommand href
name "Turing.jl"
target "https://turing.ml/stable/"
literal "false"

\end_inset

) and a more sophisticated sampling method (NUTS:
 no U-turn Hamiltonian Monte Carlo) that does not require that a proposal be specified is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Bayesian/BayesExample3.jl}{ BayesExample3.jl} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Metropolis-Hastings-MCMC,-expone"

\end_inset

Metropolis-Hastings MCMC,
 exponential likelihood,
 lognormal prior
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
chain,
 last 1000 draws
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/chain.png
	width 10cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
posterior
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/posterior.png
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Full sample Bayesian estimation of the DSGE model
\end_layout

\begin_layout Standard
In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:DSGE-ML"
nolink "false"

\end_inset

,
 a simple DSGE model was estimated by ML.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/Bayesian/Estimate.jl}{Estimate.jl} 
\end_layout

\end_inset

 allows estimation of the same model using Bayesian methods,
 with Metropolis-Hastings MCMC.
 
\series bold
PLEASE NOTE
\series default
:
 this file should be run following the instructions in the README in the same directory.
 
\end_layout

\begin_layout Standard
We can obtain results like
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MCMC-results-for"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Standard

\series bold
Some conclusions we can draw:
\end_layout

\begin_layout Itemize
Estimation of the parameters is quite good,
 in general.
 The first panel uses the variables c and n,
 and the second uses y and n.
 (using the other .mod file).
 
\end_layout

\begin_layout Itemize
The standard deviation of 
\begin_inset Formula $\gamma$
\end_inset

 changes quite a bit,
 depending on which observable variables are used.
 The other results are more similar across the two possibilities we're looking at.
\end_layout

\begin_layout Itemize
95% confidence intervals are not too reliable:
 the true parameters are not inside the intervals as often as they should be.
 This may due to the fact that the 
\begin_inset Quotes sld
\end_inset

likelihood
\begin_inset Quotes srd
\end_inset

 is misspecified,
 being based on the linearized model.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:MCMC-results-for"

\end_inset

MCMC results for simple DSGE example model (two different runs using different observed variables)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
MCMC,
 ver.
 1:
 C and N
\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/Bayesian/MCMC.png
	width 20cm

\end_inset


\end_layout

\begin_layout Plain Layout
MCMC,
 ver.
 2:
 Y and N
\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/Bayesian/MCMC2.png
	width 20cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:CGHK-model,-posteriors"
nolink "false"

\end_inset

 plots the priors and posteriors,
 when c and n are used.
 Note that the posterior is substantially different than the prior:
 we learn a lot from the sample.
 That's why MCMC and ML are substantially similar.
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:CGHK-model,-posteriors"

\end_inset

CGHK model,
 posteriors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/Bayesian/posterior.png
	width 20cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Following are Monte Carlo results for the posterior mean computed from MCMC with the linearized model,
 using c and n as the observered variables,
 using the 1000 data sets that were generated at the true parameters.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/Bayesian/montecarlo.png
	width 20cm

\end_inset


\end_layout

\begin_layout Itemize
the results are good.
 Estimation of 
\begin_inset Formula $\gamma$
\end_inset

 is most difficult,
 but is still quite good
\end_layout

\begin_layout Itemize
if you compare with the ML results from Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:DSGE-ML"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

,
 these are almost identical.
 ML is equivalent to the posterior mode,
 because the priors are uniform in this example.
 So,
 we see that using the posterior mode or the posterior mean doesn't make much difference in this example.
 That is not always the case,
 though,
 when the posterior is more asymmetric.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Bayesian GMM for the DSGE model
\end_layout

\begin_layout Standard
A script which shows how to do Bayesian GMM as proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHong2003"
literal "false"

\end_inset

 is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/BayesianGMM/DoMCMC.jl}{DoMCMC.jl} 
\end_layout

\end_inset

.
 The issue of how to come up with an effective proposal density is always important when doing MH MCMC.
 Examine the code to see what was done,
 which perhaps could be improved.
 Once we have the chain,
 it can be used to compute posterior densities for the parameters,
 for example,
 the estimated posterior for 
\begin_inset Formula $\gamma$
\end_inset

 follows.
 Recall that the true value that generated the sample is 
\begin_inset Formula $\gamma=2$
\end_inset

,
 so,
 for this sample,
 the method worked reasonably well for point estimation.
 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/BayesianGMM/gamma.svg
	width 15cm

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The results for all parameters follow.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/BayesianGMM/results.png
	width 20cm

\end_inset


\end_layout

\begin_layout Itemize
The point estimates (mean and 50% quantile) are very close to the true values
\end_layout

\begin_layout Itemize
The estimated standard deviations are low,
 comparable or better to what we saw with ML and GMM.
\end_layout

\begin_layout Itemize
and the 95% confidence intervals contain the true parameters in 6 of 7 cases (
\begin_inset Formula $\sigma_{2},$
\end_inset

 the standard deviation of the preference shock,
 is the exception),
 and the last case is a very marginal rejection.
 So,
 the evidence is that the CIs are working more or less properly.
\end_layout

\begin_layout Itemize
Overall,
 these results are the best so far.
 The use of MCMC seems to be improving on the unreliable asymptotic theory confidence intervals that come from ordinary extremum-estimation GMM.
\end_layout

\begin_layout Itemize
For reference,
 the results from ordinary CUE-GMM (extremum estimator) are in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GMM-estimation-of"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Here is a summary of the chain,
 using the very nice MCMCChains.jl package.
 The posteriors look fine.
 There is an odd bimodality in the case of 
\begin_inset Formula $\beta$
\end_inset

,
 the reason for which escapes me.
\end_layout

\begin_layout Standard
:
\begin_inset Graphics
	filename Examples/DSGE/BayesianGMM/allparams.svg
	lyxscale 75
	width 12cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical summary
\end_layout

\begin_layout Standard
I am not providing additional code,
 I recommend just working through the examples in the chapter.
 There are a lot of other resources available,
 with code.
 For examples using Julia,
 I recommend looking at 
\begin_inset CommandInset href
LatexCommand href
name "https://storopoli.io/Bayesian-Julia/"
target "https://storopoli.io/Bayesian-Julia/"
literal "false"

\end_inset

 by José Eduardo Storopoli,
 especially the chapter on MCMC,
 which is very nice.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
Experiment with the examples to learn about tuning,
 etc.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Models-for-time"

\end_inset

Simulation-based methods
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
:
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Ch.
 12;
 
\begin_inset CommandInset citation
LatexCommand citet
key "gourieroux1996simulation"
literal "true"

\end_inset

.
 There are many articles.
 Some of the seminal papers are 
\begin_inset CommandInset citation
LatexCommand cite
key "McFadden1989MSM"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand cite
key "PakesPollard"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand cite
key "GourierouxMonfortIndirect"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand citet
key "smith1993estimating"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand citet
key "duffie1993simulated"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand cite
key "emm"
literal "true"

\end_inset


\emph on
.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Human brain power is perhaps growing over time,
 but not as fast as thumb dexterity,
 at least in recent years,
 I would argue.
 
\end_layout

\begin_layout Itemize
On the other hand,
 computing power is growing more or less exponentially,
 according to 
\begin_inset CommandInset href
LatexCommand href
name "Moore's Law"
target "https://en.wikipedia.org/wiki/Moore%27s_law"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Any economist would argue that we need to use inputs in proportion to their relative prices,
 which means that we should increasingly be using computers to make advancements in econometrics (and,
 maybe,
 our thumbs,
 too).
 
\end_layout

\begin_layout Itemize
Simulation-based methods do just that.
 When intensive use of computer power is contemplated,
 it is possible to do things that are otherwise infeasible:
\end_layout

\begin_deeper
\begin_layout Itemize
obtaining more accurate results that what asymptotic theory gives us,
 using methods like bootstrapping,
\end_layout

\begin_layout Itemize
performing estimation of models that are complex enough so that analytic expressions for objective functions that define conventional estimators (e.g.,
 ML,
 GMM) are not available.
\end_layout

\begin_deeper
\begin_layout Itemize
Once you go down this rabbit hole,
 you can estimate 
\emph on
very 
\emph default
complex models.
 
\end_layout

\begin_layout Itemize
Keeping in mind that a model is used to extract the essential,
 interesting features of a problem,
 we don't necessarily want to go too far in adding complexity.
 Remember the old saying about loosing sight of the forest for the trees.
 It is possible to burn many CPU cycles to model well some uninteresting feature of the data.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Simulation based estimation,
 especially the method of simulated moments (MSM),
 has become quite standard in applied research,
 so it is important to understand how it works.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Motivation
\end_layout

\begin_layout Standard
Simulation methods are of interest when the DGP is fully characterized by a parameter vector,
 so that simulated data can be generated,
 but the likelihood function and/or analytic moments of the observable variables are not calculable,
 so that ordinary MLE or GMM estimation is not possible.
 
\end_layout

\begin_layout Itemize
Even only moderately complex models result in intractable likelihoods or moments.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset CommandInset citation
LatexCommand cite
key "McFadden1989MSM"
literal "true"

\end_inset

 is a seminal paper in simulation-based methods.
 He pointed out that the likelihood of the multinomial probit model can't be computed when the number of alternatives is at all large.
 
\end_layout

\begin_layout Itemize
another example are continuous time models with observations in discrete time.
 Computing the one step ahead transition density for the observables may be impossible,
 ruling out maximum likelihood.
\end_layout

\end_deeper
\begin_layout Itemize
Simulation-based estimation methods open up the possibility to estimate such models,
 and also truly complex models,
 such as agent-based models with potentially non-rational decision-making.
 
\end_layout

\begin_layout Itemize
The desirability of introducing a great deal of complexity may be an issue,
 but it least it becomes possibile.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Simulated maximum likelihood (SML)
\end_layout

\begin_layout Standard
As before,
 an ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\end_layout

\end_inset

estimator solves 
\begin_inset Formula 
\[
\hat{\theta}_{ML}=\arg\max s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}\ln f(y_{t}|X_{t},\theta)
\]

\end_inset

 where 
\begin_inset Formula $f(y_{t}|X_{t},\theta)$
\end_inset

 is the likelihood of the 
\begin_inset Formula $t^{th}$
\end_inset

 observation.
 
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $f(y_{t}|X_{t},\theta)$
\end_inset

 does not have a known closed form,
 
\begin_inset Formula $\hat{\theta}_{ML}$
\end_inset

 is an infeasible estimator.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
However,
 it may be possible to define a random function 
\begin_inset Formula $p(\nu,y_{t}|X_{t},\theta)$
\end_inset

 such that 
\begin_inset Formula 
\[
\mathcal{E}_{\nu}p(\nu,y_{t}|X_{t},\theta)=f(y_{t}|X_{t},\theta)
\]

\end_inset

 where the density of 
\begin_inset Formula $\nu$
\end_inset

 is known and from which we can simulate (make random draws).
\end_layout

\begin_layout Itemize
If this is the case,
 the simulator 
\begin_inset Formula 
\[
\tilde{f}\left(y_{t}|X_{t},\theta\right)=\frac{1}{H}\sum_{h=1}^{H}p(\nu_{th},y_{t}|X_{t},\theta)
\]

\end_inset

 is unbiased for 
\begin_inset Formula $f(y_{t}|X_{t},\theta).$
\end_inset


\end_layout

\begin_layout Itemize
The SML simply substitutes 
\begin_inset Formula $\tilde{f}\left(y_{t}|X_{t},\theta\right)$
\end_inset

 in place of 
\begin_inset Formula $f(y_{t}|X_{t},\theta)$
\end_inset

 in the log-likelihood function.
 That is,
 
\begin_inset Formula 
\[
\hat{\theta}_{SML}=\arg\max s_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\ln\tilde{f}\left(y_{t}|X_{t},\theta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
A simple example of computing 
\begin_inset Formula $f$
\end_inset

 by simulation:
\end_layout

\begin_layout Standard
To see how this might be done,
 suppose the data are a set of independent outcomes of Bernoulli trials,
 so that there is no regressor,
 and the true density is 
\begin_inset Formula 
\[
f(y_{t}|\theta)=\text{\ensuremath{\theta}}^{y_{t}}(1-\text{\ensuremath{\theta}})^{1-y_{t}}
\]

\end_inset

Suppose we define 
\begin_inset Formula $\nu=(\nu_{1},\nu_{2},...\nu_{H})$
\end_inset

 as 
\begin_inset Formula $H$
\end_inset

 i.i.d.
 
\begin_inset Formula $U(0,1)$
\end_inset

 draws,
 and
\begin_inset Formula 
\[
\tilde{\text{\ensuremath{\theta}}}(\nu,\theta)=\frac{1}{H}\sum_{h=1}^{H}1(\nu_{h}<\theta)
\]

\end_inset

where 
\begin_inset Formula $1()$
\end_inset

 is the indicator function that takes the value 1 if the argument is true,
 and zero otherwise.
 
\end_layout

\begin_layout Itemize
So,
 
\begin_inset Formula $\tilde{\text{\ensuremath{\theta}}}(\nu,\theta)$
\end_inset

 is the average of the outcomes of 
\begin_inset Formula $H$
\end_inset

 Bernoulli trials,
 sampled with probability of success being 
\begin_inset Formula $\theta$
\end_inset

,
 where the outcome of each trial depends on the latent 
\begin_inset Formula $\nu$
\end_inset

.
\end_layout

\begin_layout Itemize
The expectation of 
\begin_inset Formula $\tilde{\text{\ensuremath{\theta}}}(\nu,\theta)$
\end_inset

 is obviously equal to 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
Then,
 define
\begin_inset Formula 
\[
p(\nu,y_{t}|\theta)=\tilde{\text{\ensuremath{\theta}}}(\nu,\theta)^{y_{t}}\left(1-\tilde{\text{\ensuremath{\theta}}}(\nu,\theta)\right)^{1-y_{t}}
\]

\end_inset

We have
\begin_inset Formula 
\begin{align*}
\mathcal{E}_{\nu}p(\nu,y_{t} & =1|\theta)=\theta\\
 & =f_{y}(y_{t}=1|\theta)
\end{align*}

\end_inset

and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathcal{E}_{\nu}p(\nu,y_{t} & =0|\theta)=1-\theta\\
 & =f_{y}(y_{t}=0|\theta)
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
so,
 we have constructed an unbiased simulator of 
\begin_inset Formula $f(y_{t}|\theta)$
\end_inset


\end_layout

\begin_layout Itemize
this example is obviously very simple,
 and it is not needed in this case,
 where ordinary ML is perfectly feasible.
 It is just an example that shows that it may be possible to find a simulator with the required property.
\end_layout

\begin_layout Itemize
McFadden proposed to compute probabilities for the multinomial probit model in this way.
 That is a case not too different from the simple example above.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Properties
\end_layout

\begin_layout Standard
The properties of the SML estimator depend on how 
\begin_inset Formula $H$
\end_inset

 is set.
 The following is taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "lee1995asymptotic"
literal "true"

\end_inset

.
\end_layout

\begin_layout Theorem
[Lee] 1) if 
\begin_inset Formula $\lim_{n\rightarrow\infty}n^{1/2}/H=0,$
\end_inset

 then 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{SML}-\theta_{0}\right)\stackrel{d}{\rightarrow}N(0,\mathcal{I}^{-1}(\theta_{0}))
\]

\end_inset


\end_layout

\begin_layout Theorem
2) if 
\begin_inset Formula $\lim_{n\rightarrow\infty}n^{1/2}/H=\lambda,$
\end_inset

 
\begin_inset Formula $\lambda$
\end_inset

 a finite constant,
 then 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{SML}-\theta_{0}\right)\stackrel{d}{\rightarrow}N(B,\mathcal{I}^{-1}(\theta_{0}))
\]

\end_inset

 where 
\begin_inset Formula $B$
\end_inset

 is a finite vector of constants.
 
\end_layout

\begin_layout Itemize
This means that the SML estimator is asymptotically biased if 
\begin_inset Formula $H$
\end_inset

 doesn't grow faster than 
\begin_inset Formula $n^{1/2}.$
\end_inset


\end_layout

\begin_layout Itemize
The covariance matrix is the typical inverse of the information matrix,
 so that as long as 
\begin_inset Formula $H$
\end_inset

 grows fast enough,
 the estimator is consistent and fully asymptotically efficient.
 
\end_layout

\begin_layout Itemize
SML is actually not used nearly as often as is the method of simulated moments (MSM,
 below),
 in one of its variations,
 probably because one needs to use a large number of simulations to drive bias down to acceptable levels.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Method of simulated moments (MSM)
\end_layout

\begin_layout Standard
Suppose we have a data generating process DGP
\begin_inset Formula $(y|x,\theta)$
\end_inset

 which is simulable,
 given 
\begin_inset Formula $\theta$
\end_inset

 and exogenous variables 
\begin_inset Formula $x$
\end_inset

,
 but is such that the density of 
\begin_inset Formula $y$
\end_inset

 is not calculable.
\end_layout

\begin_layout Standard
A formulation of the GMM estimator which we have studied is based upon the moment conditions 
\begin_inset Formula $m_{t}=z_{t}\epsilon_{t}(\theta)$
\end_inset

,
 where 
\begin_inset Formula $\epsilon_{t}(\theta)$
\end_inset

 has conditional expectation equal to zero when evaluated at the true parameter value,
 and 
\begin_inset Formula $z_{t}$
\end_inset

 are instruments drawn from the information set.
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula 
\[
\epsilon_{t}(\theta)=K(y_{t},x_{t})-k(x_{t},\theta)
\]

\end_inset

where 
\begin_inset Formula $k(x_{t},\theta)=E_{\theta}K(y_{t},x_{t}|I_{t})$
\end_inset

,
 where 
\begin_inset Formula $I_{t}$
\end_inset

 is the information set at time 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Itemize
Then,
 at the true parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

 that generated the data,
 
\begin_inset Formula $E_{\theta_{0}}K(y_{t},x_{t}|I_{t})=k(x_{t},\theta_{0}).$
\end_inset

 
\end_layout

\begin_layout Itemize
From this,
 we could base GMM estimation on 
\begin_inset Formula $\text{\epsilon_{t}(\theta)},$
\end_inset

 crossed with instrumental variables drawn from 
\begin_inset Formula $I_{t}$
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
However,
 assume that we can't compute 
\begin_inset Formula $k(x_{t},\theta)=E_{\theta}K(y_{t},x_{t}|I_{t})$
\end_inset

,
 for some reason.
\end_layout

\begin_layout Itemize
Nevertheless,
 
\begin_inset Formula $k(x_{t},\theta)$
\end_inset

 is readily simulated (meaning that we can make random draws of 
\begin_inset Formula $k(x_{t},\theta)$
\end_inset

) using 
\begin_inset Formula 
\[
\widetilde{k}\left(x_{t},\theta\right)=\frac{1}{H}\sum_{h=1}^{H}K(\widetilde{y}_{t}^{h}(\theta),x_{t})
\]

\end_inset

where 
\begin_inset Formula $\widetilde{y}_{t}^{h}(\theta)$
\end_inset

 is drawn from DGP
\begin_inset Formula $(y|x,\theta)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $E_{\theta}K(\widetilde{y}_{t}^{h},x_{t}|I_{t})=k(x_{t},\theta)$
\end_inset

,
 and,
 by the law of large numbers,
 
\begin_inset Formula $\widetilde{k}\left(x_{t},\theta\right)\stackrel{a.s.}{\rightarrow}k\left(x_{t},\theta\right),$
\end_inset

 as 
\begin_inset Formula $H\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Itemize
This allows us to form the moment contributions 
\begin_inset Formula 
\begin{equation}
m_{t}(\theta)=\left[K(y_{t},x_{t})-\widetilde{k}\left(x_{t},\theta\right)\right]z_{t}
\end{equation}

\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 is drawn from the information set.
 As before,
 form 
\begin_inset Formula 
\begin{eqnarray}
\bar{m}_{n}(\theta) & = & \frac{1}{n}\sum_{i=1}^{n}m_{t}(\theta)\nonumber \\
 & = & \frac{1}{n}\sum_{i=1}^{n}\left[K(y_{t},x_{t})-\widetilde{k}\left(x_{t},\theta\right)\right]z_{t}\label{Linearity of MSM}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula 
\[
E\bar{m}_{n}(\theta)=k(x_{t},\theta_{0})-k(x_{t},\theta)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $EK(y_{t},x_{t})=k(x_{t},\theta_{0})$
\end_inset

,
 because the real data 
\begin_inset Formula $y_{t}$
\end_inset

 is assumed to have been generated at 
\begin_inset Formula $\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\widetilde{k}\left(x_{t},\theta\right)=k(x_{t},\theta),$
\end_inset

 because it is formed of 
\begin_inset Formula $H$
\end_inset

 i.i.d.
 terms,
 each of which has expectation equal to 
\begin_inset Formula $k(x_{t},\theta$
\end_inset

),
 because the simulated data 
\begin_inset Formula $\widetilde{y}_{t}^{h}(\theta)$
\end_inset

 is generated at 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Itemize
Thus,
 the moment conditions satisfy the essential condition of have expectation equal to zero when 
\begin_inset Formula $\theta=\theta_{0}.$
\end_inset


\end_layout

\begin_layout Itemize
As long as the moment conditions identify the parameters,
 GMM using these moments will be consistent and asymptotically normally distributed.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Properties
\end_layout

\begin_layout Standard
Suppose that the optimal weighting matrix is used.
 
\begin_inset CommandInset citation
LatexCommand citet
key "McFadden1989MSM"
literal "true"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "PakesPollard"
literal "true"

\end_inset

 show that the asymptotic distribution of the MSM estimator is very similar to that of the infeasible GMM estimator.
 In particular,
 assuming that the optimal weighting matrix is used,
 and for 
\begin_inset Formula $H$
\end_inset

 finite,
 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}_{MSM}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(1+\frac{1}{H}\right)\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}\right]
\end{equation}

\end_inset

where 
\begin_inset Formula $\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}$
\end_inset

 is the asymptotic variance of the infeasible GMM estimator.
\end_layout

\begin_layout Itemize
That is,
 the asymptotic variance is inflated by a factor 
\begin_inset Formula $1+1/H.$
\end_inset

 For this reason the MSM estimator is not fully asymptotically efficient relative to the infeasible GMM estimator,
 for 
\begin_inset Formula $H$
\end_inset

 finite,
 but the efficiency loss is small and controllable,
 by setting 
\begin_inset Formula $H$
\end_inset

 reasonably large.
\end_layout

\begin_layout Itemize
The estimator is asymptotically unbiased even for 
\begin_inset Formula $H=1.$
\end_inset

 This is an advantage relative to SML.
\end_layout

\begin_layout Itemize
If one doesn't use the optimal weighting matrix,
 the asymptotic varcov is just the ordinary GMM varcov,
 inflated by 
\begin_inset Formula $1+1/H.$
\end_inset

 Even for moderate 
\begin_inset Formula $H,$
\end_inset

 the variance inflation can be quite small.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The above presentation is in terms of a specific set of moment conditions based upon the conditional mean and instruments,
 and follows the most conventional GMM framework of averaging moment contributions.
 The MSM can be applied to moment conditions of other forms of moments,
 too.
 
\end_layout

\begin_layout Itemize
A leading example is 
\series bold
Indirect Inference
\series default
 (
\begin_inset CommandInset citation
LatexCommand citet
key "GourierouxMonfortIndirect"
literal "false"

\end_inset

) where we set 
\begin_inset Formula 
\[
\bar{m}_{n}(\theta)=\text{\ensuremath{\hat{\phi}} - \ensuremath{\frac{1}{H}\sum_{h=1}^{H}\tilde{\phi^{h}}(\theta)}}
\]

\end_inset

where,
 
\begin_inset Formula $\hat{\phi}$
\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
is an extremum estimator corresponding to some auxiliary model,
 in the formulation of 
\begin_inset CommandInset citation
LatexCommand citet
key "GourierouxMonfortIndirect"
literal "false"

\end_inset

.
\end_layout

\begin_layout Itemize
actually 
\begin_inset Formula $\hat{\phi}$
\end_inset

 can just be some vector of statistics computed from the sample data (
\begin_inset CommandInset citation
LatexCommand citet
key "jiang2004indirect"
literal "false"

\end_inset

).
 The main requirement is that 
\begin_inset Formula $\hat{\phi}$
\end_inset

 should converge in probability to a finite limit,
 and that is should satisfy a central limit theorem.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\tilde{\phi^{h}}(\theta)$
\end_inset

 are the same extremum estimator (statistics),
 computed using simulated data that is generated from the model,
 at 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The logic is that 
\begin_inset Formula $\hat{\phi}$
\end_inset

 will converge (as 
\begin_inset Formula $n$
\end_inset

 grows large) to a pseudo-true value,
 as it is an extremum estimator,
 and 
\begin_inset Formula $\tilde{\phi^{h}}(\theta)$
\end_inset

 will converge to another pseudo-true value,
 depending on the value of 
\begin_inset Formula $\theta$
\end_inset

 that generated the simulated data.
 When 
\begin_inset Formula $\theta=\theta_{0}$
\end_inset

,
 the two pseudo-true values will be the same,
 so 
\begin_inset Formula $\bar{m}_{n}(\theta_{0})$
\end_inset

 will converge to zero,
 and the GMM criterion will be minimized at 
\begin_inset Formula $\theta_{0},$
\end_inset

 asymptotically.
\end_layout

\begin_layout Itemize
Thus,
 trying to make the average of the simulated estimators as close as possible to the estimator generated by the real data will cause the MSM estimator to be consistent,
 given identification.
 
\end_layout

\begin_layout Itemize
Note:
 this form of moment conditions is actually more or less the same as the previous presentation,
 we are just using enough error functions so that instruments are not explicitly needed.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
For such an estimator to have good efficiency,
 we need the auxiliary model to 
\begin_inset Quotes sld
\end_inset

fit well
\begin_inset Quotes srd
\end_inset

:
 it should pick up the relevant features of the data that are informative about the parameters.
\end_layout

\begin_layout Itemize
one can combine moment conditions using indirect inference-type moments with the usual MSM moments.
\end_layout

\begin_layout Itemize
a potential drawback of the II estimator is that the auxiliary model may need to be computed many times,
 during the iterative process to find the MSM estimator.
 This is not a problem if it's a simple linear model or a vector of sample statistics,
 but it could be a problem if it's more complicated.
 For efficiency,
 we need a good fit,
 and a simple linear model,
 or a set of arbitrary sample statistics,
 may not provide this.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The EMM (
\begin_inset CommandInset citation
LatexCommand citet
key "emm"
literal "true"

\end_inset

) estimator was designed to get around this,
 by using the scores of the auxiliary model,
 evaluated at the simulated data,
 as the moment conditions
\end_layout

\begin_layout Itemize
The neural moments mentioned below are another solution.
 A pre-trained neural net is very quick to evaluate at simulated data.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Subsection
Numerical issues:
 
\begin_inset Quotes sld
\end_inset

Chatter
\begin_inset Quotes srd
\end_inset


\end_layout

\begin_layout Standard
When a function of interest is approximated with simulations,
 for example,
 approximating 
\begin_inset Formula $E_{\theta}K(y_{t},x_{t}|I_{t})$
\end_inset

 with the average of simulations 
\begin_inset Formula $\frac{1}{H}\sum_{h=1}^{H}k(\widetilde{y}_{t}^{h},x_{t})$
\end_inset

,
 where each 
\begin_inset Formula $\widetilde{y}_{t}^{h}$
\end_inset

 is drawn at 
\begin_inset Formula $\theta$
\end_inset

,
 we need to recognize that,
 if we repeat the simulation process,
 drawing 
\begin_inset Formula $H$
\end_inset

 new samples,

\color blue
 the value of 
\begin_inset Formula $\frac{1}{H}\sum_{h=1}^{H}k(\widetilde{y}_{t}^{h},x_{t})$
\end_inset

 will change
\color inherit
,
 even if 
\begin_inset Formula $\theta$
\end_inset

 is held constant,
 unless special care is taken.
 When the random elements of the simulation process are not held fixed for the simulations,
 
\color blue
this function is everywhere discontinuous in 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
From a theoretical point of view,
 this is not a problem,
 as the function will be stochastically equicontinuous (stochastically converging to a continuous function),
 under reasonable assumptions,
 and this still allows for consistency and asymptotic normality.
 
\end_layout

\begin_layout Itemize
However,
 from the point of view of computing an extremum estimator,
 using gradient-based optimization,
 or for computing a covariance matrix that depends on taking derivatives,
 it is a problem.
 
\end_layout

\begin_layout Itemize
In certain cases,
 when 
\begin_inset Formula $K(y_{t},x_{t})$
\end_inset

 is a continuous function in 
\begin_inset Formula $y$
\end_inset

,
 is is possible to control chatter by keeping the underlying random draws of latent variables fixed across simulations,
 as 
\begin_inset Formula $\theta$
\end_inset

 varies.
 The example in the next section does this.
\end_layout

\begin_layout Itemize
In other cases,
 this is harder.
 For example,
 if 
\begin_inset Formula $y$
\end_inset

 is a discrete random variable,
 then the function will be discontinuous in 
\begin_inset Formula $\theta$
\end_inset

,
 even if random draws of latent variables are held fixed.
 In these cases,
 extremum estimators have been devised that rely on forms of smoothing of the functions.
 
\end_layout

\begin_layout Itemize
In all of these cases,
 it is quite easy use Bayesian methods,
 where we don't need to optimize,
 and this will also be easier to program,
 as we won't need to keep draws of latent variable fixed.
 The practical example at the end of the chapter gives an example.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Example:
 stochastic volatility
\end_layout

\begin_layout Subsection
MSM and Bayesian MSM
\end_layout

\begin_layout Standard
The simple stochastic volatility model from Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Stochastic-volatility"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 is
\begin_inset Formula 
\begin{align*}
y_{t} & =\phi\exp(h_{t}/2)\epsilon_{t}\\
h_{t} & =\rho h_{t-1}+\sigma u_{t}
\end{align*}

\end_inset

Typical data generated at the true parameter values 
\begin_inset Formula $\phi=0.692,\,\rho=0.9,\,\sigma=0.363$
\end_inset

 (which are common values used in this literature to compare estimation methods),
 and a nonparametric density plot looks like what we see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SV-model,-typical-1"
nolink "false"

\end_inset

.
 Note the volatility clusters,
 leptokurtosis,
 and the fat tails of the density.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SV-model,-typical-1"

\end_inset

SV model,
 typical data and density
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/TimeSeries/svdata.png
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/SBEM/EstimateSV.jl}{EstimateSV.jl}
\end_layout

\end_inset

 estimates the stochastic volatility model by MSM,
 implemented as a two step GMM estimator,
 and then goes on to do the Chernozhukhov-Hong MCMC version of MSM.
\end_layout

\begin_layout Itemize
The minimization is by simulated annealing,
 to ensure robustness against numerical problems.
\end_layout

\begin_layout Itemize
Examine the script to see how the objective function is 
\begin_inset Quotes sld
\end_inset

bullet-proofed
\begin_inset Quotes srd
\end_inset


\end_layout

\begin_layout Itemize
Study what auxiliary statistics are used to define the moments,
 in the file 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/SBEM/SVlib.jl}{SVlib.jl}
\end_layout

\end_inset

 (this is the key to success or failure when doing moment-based estimation),
 and think about the problem to try to come up with some better ones.
\end_layout

\begin_layout Itemize
Also in that file,
 see how latent variables are held fixed across simulations,
 to eliminate chatter.
\end_layout

\begin_layout Itemize
The results use 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/TimeSeries/svdata.txt}{the sample}
\end_layout

\end_inset

 that is pictured in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SV-model,-typical-1"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

 (or,
 you have the option of generating a new sample).
\end_layout

\begin_layout Itemize
check the script to see how to compute standard errors,
 etc.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
the results for extremum MSM are in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MSM-for-SV"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Itemize
MCMC gives us a full posterior for the parameters,
 and we can use posterior quantiles to define alternative confidence intervals.
 For this sample,
 we get the results in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MCMC-estimation-using"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Itemize
If you compare the MSM and Bayesian MSM results,
 you'll see that the point estimates are very similar,
 but there are some differences in the CIs:
 the extremum estimator CIs are broader,
 in this case.
 
\end_layout

\begin_layout Itemize
So,
 this seems to be working well....
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:MSM-for-SV"

\end_inset

MSM for SV model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/msm.png
	width 20cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:MCMC-estimation-using"

\end_inset

MCMC estimation using simulated moments and limited information quasi-likelihood
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/mcmc.png
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\color blue
Unfortunately,
 inferences using overidentified GMM-type estimators are often not reliable in finite samples
\color inherit
 (
\begin_inset CommandInset citation
LatexCommand citet
key "DonaldImbensNewey2009"
literal "false"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand citet
key "HansenHeatonYaron1996"
literal "false"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand citet
key "Tauchen1986"
literal "false"

\end_inset

).
 
\end_layout

\begin_layout Itemize
The script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/SBEM/SV
\backslash
_MonteCarlo.jl}{SV
\backslash
_MonteCarlo.jl}
\end_layout

\end_inset

 runs 100 replications of estimation of the same model and computes confidence intervals using both asymptotic theory for MSM estimators,
 and using quantiles of the MCMC chain.
 
\end_layout

\begin_layout Itemize
Confidence interval coverage,
 which is the proportion of times the true parameters are inside the computed confidence intervals,
 is reported in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:CI-coverage,-SV"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Itemize
In all cases,
 the true parameters are over-rejected,
 which is to say,
 the confidence intervals are tighter than they should be,
 and Type-I error will occur more often than it should.
 
\end_layout

\begin_layout Itemize
This applies to both the extremum and Baysesian versions.
 The Bayesian version could perhaps be improved somewhat by more careful tuning of the MCMC algorithm (see below) but the extremum version does not use tuning,
 so the results are not dependent on this sort of qualification.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:CI-coverage,-SV"

\end_inset

CI coverage,
 SV model,
 MSM and Bayesian MSM
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/cicoverage.png
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Simulation-based estimation using neural nets
\end_layout

\begin_layout Standard
Neural nets can do amazing things when they are well trained.
 Training a net well requires access to a lot of training data.
 When the model is simulable,
 we can generate as much training data as we would like.
 Nets and simulation based estimation are tools that work very well together.
\end_layout

\begin_layout Subsection
Simulated Neural Moments (SNM)
\end_layout

\begin_layout Standard
A first example is what I call Simulated Neural Moments (SNM),
 which is based upon a previously defined vector of statistics.
\end_layout

\begin_layout Itemize
the input to the net is a vector of statistics,
 for example,
 of the sort used for MSM
\end_layout

\begin_layout Itemize
the output is the parameter vector that generated the statistics
\end_layout

\begin_layout Itemize
a net can be trained using many simulated (parameter,
 statistic) pairs,
 which is feasible when the model is simulable.
 The net will learn to recognize the parameters that are associated with given statistics.
 Then,
 when fed real data statistics,
 the output of the net is an estimator,
 which will fit well according to the loss function which was used in training.
 The fit is of the same dimension as the parameter vector.
\end_layout

\begin_deeper
\begin_layout Itemize
it can be used directly,
 as an estimator
\end_layout

\begin_layout Itemize
or,
 it can be taken as an informative,
 just-identifying vector of statistics,
 upon which to base subsequent (Bayesian) MSM.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset CommandInset citation
LatexCommand citet
key "creel2017neural"
literal "false"

\end_inset

 proposed using neural moments of this sort,
 and 
\begin_inset CommandInset citation
LatexCommand citet
key "creel2021inference"
literal "false"

\end_inset

 showed that inferences based on MSM using such moments are reliable.
\end_layout

\begin_layout Itemize
This can be done using the package 
\begin_inset CommandInset href
LatexCommand href
name "SimulatedNeuralMoments.jl"
target "https://github.com/mcreel/SimulatedNeuralMoments.jl"
literal "false"

\end_inset

.
 Using methods discussed there,
 one can obtain the CI coverage for the same SV model presented above that is seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:95%-CI-coverage,"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
 Note that when the neural statistics are used (columns labeled Z),
 the coverage is statistically correct.
 The results using the original statistics,
 without the neural net (the column labeled W) are far from correct.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:95%-CI-coverage,"

\end_inset

90,
 95 and 99% CI coverage,
 SV model,
 using simulated neural moments
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/90.png
	width 20cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/95.png
	width 20cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/99.png
	width 20cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Simulated Neural Moments estimation of the DSGE model
\end_layout

\begin_layout Standard
The script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/SimulatedNeuralMoments/Estimate.jl}{Estimate.jl}
\end_layout

\end_inset

 allows you to estimate the simple example DSGE model using Simulated Neural Moments.
 The statistics by the net as the input are the same statistics as were used for GMM estimation,
 in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GMM-estimation-of"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Here are the fits to several data sets which were drawn using parameters sampled from the prior:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/SimulatedNeuralMoments/fitpriordraws.png

\end_inset


\end_layout

\begin_layout Standard
The NN fit tracks the parameters pretty well.
 Thus,
 it seems to be an informative statistic upon which to base MSM or Bayesian MSM.
\begin_inset Newpage newpage
\end_inset

 The results of a Monte Carlo study of 1000 draws of the raw NN estimator applied to the Monte Carlo data sets that were drawn at the true parameters are:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/SimulatedNeuralMoments/nnfit.png
	scale 120

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
there is some bias,
 but the results are quite good,
 overall.
\end_layout

\begin_layout Itemize
RMSE is low,
 comparable or better than the ML estimator.
\end_layout

\begin_layout Itemize
these can be computed essentially instantly,
 but we don't get any estimated standard deviations for the individual raw NN estimates.
 
\end_layout

\begin_layout Itemize
We can use the SNM output as a statistic for Bayesian GMM.
 This will give standard deviations and confidence intervals,
 and it also has a bias correcting property.
\end_layout

\begin_layout Itemize
this shows that the neural net can give good parameter estimates,
 and that,
 if these estimates are used for MSM,
 they are informative.
\end_layout

\begin_layout Itemize
They are also exactly identifying,
 which is important for obtaining reliable inferences.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The Bayesian MSM results for the typical data set that we've been using,
 drawn at the true parameters,
 using the NN statistics as the moments,
 are
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/SimulatedNeuralMoments/msmfit.png
	width 15cm

\end_inset


\end_layout

\begin_layout Itemize
the posterior mean or median point estimates are good,
 not much different than the ML or GMM results seen above.
\end_layout

\begin_layout Itemize
the 95% confidence intervals always contain the true parameter values.
 This is different,
 compared to what we've seen for ML and GMM.
 Fairly low standard deviations and confidence intervals that don't over-reject true parameter values are something we have not previously seen.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand citet
key "creel2021inference"
literal "false"

\end_inset

,
 Bayesian MSM for the DSGE model,
 using the SNM estimator as the statistic,
 is done.
 
\end_layout

\begin_layout Itemize
Similarly as was seen for the SV model,
 this leads to reliable confidence intervals,
 because the moment conditions are now exactly identifying (results in the paper).
 
\end_layout

\begin_layout Itemize
The estimator is also precise;
 RMSE over 500 Monte Carlo replication is in the following table,
 in the lower block.
 These results are better than for any of the previous methods.
 For example,
 RMSE for 
\begin_inset Formula $\gamma$
\end_inset

 is about half what it was for the ML and Bayesian estimators.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/SBEM/dsge-rmse.png
	width 15cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Deep Simulated Moments (DSM)
\end_layout

\begin_layout Standard
This uses another architecture for the net,
 which is considerably more demanding to train.
 The advantage is that it does not require that one specify statistics.
 It can lead to even lower RMSE that the previous results.
 It is presently so costly computationally that it is probably mostly of interest for research,
 rather than practical use.
 Here are some results,
 in case anyone would like to investigate the idea further.
\end_layout

\begin_layout Itemize
The simulated neural moments approach requires specifying a vector of summary statistics.
 
\end_layout

\begin_layout Itemize
The efficiency of the final estimator will depend on the choice of statistics.
\end_layout

\begin_layout Itemize
An alternative approach,
 explored in 
\begin_inset CommandInset citation
LatexCommand citet
key "ChassotCreel2024"
literal "false"

\end_inset

,
 constructs the neural net using the full sample data directly as the input.
 
\end_layout

\begin_deeper
\begin_layout Itemize
To capture the information in the sequence of observations in the sample,
 a more sophisticated net is required.
 The paper proposes temporal convolutional nets (TCNs) for this purpose.
 
\end_layout

\begin_layout Itemize
This approach has been found to lead tp lower RMSE than the maximum likelihood estimator,
 for several small test models,
 and to have lower RMSE that a neural moment estimator that uses summary statistics.
 
\end_layout

\begin_layout Itemize
This paper is certainly not the last word on the topic,
 but it does show that neural nets have much promise for improving the efficiency of simulated method of moments type estimators,
 or to at least get around possible problems due to a poor choice of initial statistics.
\end_layout

\end_deeper
\begin_layout Itemize
software to train a net,
 and to do estimation is at 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/SNM-TCN/Estimate.jl}{Estimate.jl}
\end_layout

\end_inset

 and associated files.
 
\end_layout

\begin_layout Itemize
Because the net is much more complex,
 working with this code requires using a GPU,
 with CUDA computing.
 The programming is somewhat complex....
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
DSM results for the DSGE model
\end_layout

\begin_layout Standard
Monte Carlo results for 1000 replications of the TCN neural net estimator,
 for the simple DSGE model are below.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/SNM-TCN/montecarlo.png
	width 18cm

\end_inset


\end_layout

\begin_layout Itemize
These are quite similar to the neural net results that are based on an auxiliary statistic,
 above.
 They are not worse,
 but not much better,
 either.
\end_layout

\begin_layout Itemize
In principle,
 there may be information lost when using summary statistics.
 In the present case,
 the feed forward net that uses summary statistics seems not to have lost much information.
 This indicates that the chosen summary statistics seem to capture the sample information well.
\end_layout

\begin_layout Itemize
The performance is better than the quasi ML estimator,
 which may be further evidence that information has not been lost.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
There is some bias in the previous results,
 for the 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\rho_{2}$
\end_inset

 parameters,
 in particular.
 To illustrate the bias correcting property of MSM estimation,
 here is a small (100 replications) Monte Carlo study of the MSM point estimator,
 computed using simulated annealing to minimize the usual MSM criterion,
 where the moments are 
\begin_inset Formula $\bar{m}_{n}(\theta)=\text{\ensuremath{\hat{\phi}}-\ensuremath{\frac{1}{H}\sum_{h=1}^{H}\tilde{\phi^{h}}(\theta)}}$
\end_inset

 and the statistic 
\begin_inset Formula $\phi$
\end_inset

 is the output of the trained deep temporal convolutional net.
 To save time,
 all 
\begin_inset Formula $\theta$
\end_inset

 parameters were held at the TCN estimates using the real data,
 except for the two parameter that exhibit significant bias,
 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\rho_{2}$
\end_inset

 .
 Note how bias and RMSE decline.
 In particular,
 the estimator now has very low bias for all parameters,
 and RMSE for 
\begin_inset Formula $\gamma$
\end_inset

 is lower than what we have seen using any other estimation method.
 For the other parameters,
 RMSE is similar to that obtained using SNM.
 The SNM estimator can also be bias corrected in the same way we have done here,
 results for that are in 
\begin_inset CommandInset citation
LatexCommand citet
key "creel2021inference"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/SNM-TCN/ResultsSA.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Results for CUE-GMM using the typical data set we have used for all the methods,
 using the TCN fit as the simulated moments are
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/SNM-TCN/Results.png
	width 20cm

\end_inset


\end_layout

\begin_layout Itemize
The point estimates are very close to the true values,
 and the standard deviations are small
\end_layout

\begin_layout Itemize
the 95% CIs are tight,
 and contain the true parameters,
 in all cases.
\end_layout

\begin_layout Itemize
note that the Bayesian MSM version (last column) is closer to the true values than the raw net output (3rd) column,
 further evidence of bias correction working.
\end_layout

\begin_layout Itemize
The possible advantage of the 
\begin_inset Quotes sld
\end_inset

Deep Simulated Moments
\begin_inset Quotes srd
\end_inset

 TCN-based neural net version is that it doesn't require specifying the initial statistics.
 The disadvantage is that training such a net is more time consuming,
 and essentially requires using GPUs,
 which could be a barrier for many people.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
(advanced,
 but even if you don't do this you should be able to describe what needs to be done) Write code to do SML estimation of the probit model.
 Do an estimation using data generated by a probit model.
 Compare the SML estimates to ML estimates.
\end_layout

\begin_layout Enumerate
do the same,
 but computing a MSM estimator of the probit model.
\end_layout

\begin_layout Enumerate
(more advanced) Do a little Monte Carlo study to compare ML,
 SML and MSM estimation of the probit model.
 Investigate how the number of simulations affect the two simulation-based estimators.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical summary
\end_layout

\begin_layout Standard
The practical summary for the Chapter is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./PracticalSummaries/18-SimulationBased.jl}{here}
\end_layout

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Models for time series data
\end_layout

\begin_layout Standard
Hamilton,
 
\emph on
Time Series Analysis
\emph default
 is a good reference for this chapter.
 
\end_layout

\begin_layout Standard
Up to now we've considered the behavior of the dependent variable 
\begin_inset Formula $y_{t}$
\end_inset

 as a function of other variables 
\begin_inset Formula $x_{t}.$
\end_inset

 These variables can of course contain lagged dependent variables,
 e.g.,
 
\begin_inset Formula $x_{t}=(w_{t},y_{t-1},...,y_{t-j}).$
\end_inset

 Pure time series methods consider the behavior of 
\begin_inset Formula $y_{t}$
\end_inset

 as a function only of its own lagged values,
 unconditional on other observable variables.
 One can think of this as modeling the behavior of 
\begin_inset Formula $y_{t}$
\end_inset

 after marginalizing out all other variables.
 But,
 of course,
 general models will include lagged dependent variables and other explanatory variables.
 This Chapter gives a brief description of some of the widely used models.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Basic concepts
\end_layout

\begin_layout Definition
[Stochastic process]
\begin_inset CommandInset label
LatexCommand label
name "Stochastic process"

\end_inset

 A stochastic process is a sequence of random variables,
 indexed by time:
 
\begin_inset Formula $\{Y_{t}\}_{t=-\infty}^{\infty}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Definition
[Time series]
\begin_inset CommandInset label
LatexCommand label
name "Time series"

\end_inset

 A time series is 
\series bold
one
\series default
 observation of a stochastic process,
 over a specific interval:
 
\begin_inset Formula $\{y_{t}\}_{t=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
So a time series is a sample of size 1 of 
\begin_inset Formula $n$
\end_inset

 particular elements of a stochastic process.
 Each of the 
\begin_inset Formula $Y_{t}$
\end_inset

 is sampled only once,
 for 
\begin_inset Formula $t=1,2,...,n.$
\end_inset

 It's important to keep in mind that conceptually,
 one could draw another sample,
 and that the values would be different.
\end_layout

\begin_layout Definition
[Autocovariance] The 
\begin_inset Formula $j^{th}$
\end_inset

 autocovariance of a stochastic process is 
\begin_inset Formula $\gamma_{jt}=\mathcal{E}(Y_{t}-\mu_{t})(Y_{t-j}-\mu_{t-j})$
\end_inset

 where 
\begin_inset Formula $\mu_{t}=\mathcal{E}\left(Y_{t}\right).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
[Covariance (weak) stationarity] A stochastic process is covariance stationary if it has time constant mean and autocovariances of all orders:
 
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{eqnarray*}
\mu_{t} & =\mu, & \forall t\\
\gamma_{jt} & =\gamma_{j}, & \forall t
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As we've seen,
 this implies that 
\begin_inset Formula $\gamma_{j}=\gamma_{-j}:$
\end_inset

 the autocovariances depend only one the interval between observations,
 but not the time of the observations.
\end_layout

\begin_layout Definition
[Strong stationarity] A stochastic process is strongly stationary if the joint distribution of an arbitrary collection of the 
\begin_inset Formula $\left\{ Y_{t}\right\} $
\end_inset

,
 e.g.,
 
\begin_inset Formula 
\[
(Y_{t-j},Y_{t-k},...,Y_{t},...,Y_{t+l},Y_{t+m}\},
\]

\end_inset

 doesn't depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Standard
Since moments are determined by the distribution,
 strong stationarity
\begin_inset Formula $\Rightarrow$
\end_inset

weak stationarity.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

How can we estimate the mean of 
\begin_inset Formula $Y_{t}?$
\end_inset

 
\end_layout

\begin_layout Itemize
Here's where we need stationarity.
 Without it,
 each 
\begin_inset Formula $Y_{t}$
\end_inset

 will have a different mean,
 
\begin_inset Formula $\mu_{t},$
\end_inset

 and with a time series,
 we have only one observation to work with.
 
\end_layout

\begin_layout Itemize
The time series is one sample from the stochastic process,
 and each of the random variables over the sample interval is sampled only once.
 
\end_layout

\begin_layout Itemize
One could think of 
\begin_inset Formula $M$
\end_inset

 repeated samples from the stoch.
 proc.,
 e.g.,
 
\begin_inset Formula $\left\{ y_{tm}\right\} _{m=1}^{M}$
\end_inset

 By a LLN,
 we would expect that 
\begin_inset Formula 
\[
\frac{1}{M}\sum_{m=1}^{M}y_{tm}\overset{p}{\rightarrow}\mathcal{E}(Y_{t})
\]

\end_inset

as 
\begin_inset Formula $M$
\end_inset

 gets large.
 We would need 
\begin_inset Formula $M$
\end_inset

 universes,
 each observed at time 
\begin_inset Formula $t$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The problem is,
 we have only one sample to work with,
 since we can't go back in time,
 or to another universe,
 and collect another.
 How can 
\begin_inset Formula $\mathcal{E}(Y_{t})$
\end_inset

 be estimated then?
 It turns out that 
\emph on
ergodicity
\emph default
 is the needed property.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
[Ergodicity].
 A stationary stochastic process is ergodic (for the mean) if the time average converges to the mean
\begin_inset Formula 
\begin{equation}
\frac{1}{n}\sum_{t=1}^{n}y_{t}\overset{p}{\rightarrow}\mu
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
A sufficient condition for ergodicity is that the autocovariances be absolutely summable:
 
\begin_inset Formula 
\[
\sum_{j=0}^{\infty}|\gamma_{j}|<\infty
\]

\end_inset

This implies that the autocovariances die off,
 so that the 
\begin_inset Formula $Y_{t}$
\end_inset

 are not so strongly dependent that they don't satisfy a LLN.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
[Autocorrelation] The 
\begin_inset Formula $j^{th}$
\end_inset

 autocorrelation,
 
\begin_inset Formula $\rho_{j}$
\end_inset

 is just the 
\begin_inset Formula $j^{th}$
\end_inset

 autocovariance divided by the variance:
 
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{equation}
\rho_{j}=\frac{\gamma_{j}}{\gamma_{0}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Definition
[White noise] White noise is just the time series literature term for a classical error.
 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is white noise if i) 
\begin_inset Formula $\mathcal{E}(\epsilon_{t})=0,\forall t,$
\end_inset

 ii) 
\begin_inset Formula $V(\epsilon_{t})=\sigma^{2},\forall t$
\end_inset

 and iii) 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{s}$
\end_inset

 are independent,
 
\begin_inset Formula $t\neq s.$
\end_inset

 Gaussian white noise just adds a normality assumption.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
US quarterly macro data,
 used in 
\begin_inset CommandInset citation
LatexCommand citet
key "stock2011introduction"
literal "true"

\end_inset

,
 Chapter 14.
 The original materials are at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://wps.pearsoned.co.uk/ema_ge_stock_ieupdate_3/251/64413/16489878.cw/index.html
\end_layout

\end_inset

.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{./Examples/Data/us
\backslash
_macro
\backslash
_quarterly.gdt}{The data file} 
\end_layout

\end_inset

,
 in GRETL format.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{./Examples/Data/usmacro
\backslash
_quarterly
\backslash
_description.pdf}{The data description file.} 
\end_layout

\end_inset

 Use GRETL to:
\end_layout

\begin_layout Itemize
plot the GDP data,
 and notice that it's nonstationary.
\end_layout

\begin_layout Itemize
Plot the growth rate,
 and note that it's stationary.
\end_layout

\begin_layout Itemize
compute the autocorrelations for the annual growth rate of GDP,
 using the GRETL correlogram option:
 they die off fairly quickly,
 so ergodicity seems to hold
\end_layout

\begin_layout Itemize
compute the autocorrelations of GDP.
 HIGHLY PERSISTENT.
 Doubtful that the ergodicity condition will hold.
\end_layout

\begin_layout Itemize
we are going to want to work with stationary data,
 if we want to apply standard regression methods and inference.
\end_layout

\begin_layout Itemize
working with nonstationary data can give very misleading results,
 if we rely on standard theory for stationary data,
 as we will see.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
ARMA models
\end_layout

\begin_layout Standard
With these concepts,
 we can discuss ARMA models.
 These are closely related to the AR and MA error processes that we've already discussed.
 The main difference is that the lhs variable is observed directly now.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
MA(q) processes
\end_layout

\begin_layout Standard
A 
\begin_inset Formula $q^{th}$
\end_inset

 order moving average (MA) process is 
\begin_inset Formula 
\[
y_{t}=\mu+\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}+\theta_{2}\varepsilon_{t-2}+\cdots+\theta_{q}\varepsilon_{t-q}
\]

\end_inset

 where 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is white noise.
 The variance is 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{0} & = & \mathcal{E}\left(y_{t}-\mu\right)^{2}\\
 & = & \mathcal{E}\left(\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}+\theta_{2}\varepsilon_{t-2}+\cdots+\theta_{q}\varepsilon_{t-q}\right)^{2}\\
 & = & \sigma^{2}\left(1+\theta_{1}^{2}+\theta_{2}^{2}+\cdots+\theta_{q}^{2}\right)
\end{eqnarray*}

\end_inset

 Similarly,
 the autocovariances are 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{j} & = & \mathcal{E}\left[\left(y_{t}-\mu\right)\left(y_{t-j}-\mu\right)\right]\\
 & = & \sigma^{2}(\theta_{j}+\theta_{j+1}\theta_{1}+\theta_{j+2}\theta_{2}+\cdots+\theta_{q}\theta_{q-j}),j\leq q\\
 & = & 0,j>q
\end{eqnarray*}

\end_inset

 Therefore an MA(q) process is necessarily covariance stationary and ergodic,
 as long as 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and all of the 
\begin_inset Formula $\theta_{j}$
\end_inset

 are finite.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
For example,
 if we have an MA(1) model,
 then 
\begin_inset Formula $E(y_{t})=\mu$
\end_inset

,
 
\begin_inset Formula $V(y_{t})=\sigma^{2}(1+\theta_{1}^{2})$
\end_inset

,
 and 
\begin_inset Formula $\gamma_{1}=\sigma^{2}\theta_{1}$
\end_inset

.
 The higher order autocovariances are zero.
 
\end_layout

\begin_layout Itemize
Thus,
 if the model is MA(1) with normally distributed shocks,
 the density of the vector of 
\begin_inset Formula $n$
\end_inset

 observations,
 
\begin_inset Formula $y$
\end_inset

,
 is 
\begin_inset Formula 
\begin{align}
f_{Y}(y|\rho) & =\frac{\text{1}}{\sqrt{\left(2\pi\right)^{n}\left|\Sigma\right|}}\exp\left(-\frac{1}{2}\left(y-\mu\right)^{\prime}\Sigma^{-1}\left(y-\mu\right)\right)\label{eq:MA1likelihood}
\end{align}

\end_inset

where 
\begin_inset Formula 
\[
\Sigma=\sigma^{2}\left[\begin{array}{ccccc}
1+\theta_{1}^{2} & \theta_{1} & 0 & \cdots & 0\\
\theta_{1} & \ddots & \ddots & \ddots & \vdots\\
0 & \ddots & \ddots & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & \theta_{1}\\
0 & \cdots & 0 & \theta_{1} & 1+\theta_{1}^{2}
\end{array}\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
With this,
 it is very easy to program the log-likelihood function.
 For higher order MA models,
 the only difference is the structure of 
\begin_inset Formula $\Sigma$
\end_inset

 becomes more complicated.
 In this form,
 one needs a lot of computer memory.
 A more economical approach uses the Kalman filter,
 which we'll see in the discussion of state space models.
\end_layout

\end_deeper
\begin_layout Itemize
If we don't make assumptions on the distribution of the shocks,
 then method of moments estimation can be used.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Generate data that follows a simple MA(1) model:
 
\begin_inset Formula $y_{t}=\mu+\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}$
\end_inset

 for 
\begin_inset Formula $\mu=0$
\end_inset

 and 
\begin_inset Formula $\theta_{1}=0.5$
\end_inset

,
 and with 
\begin_inset Formula $\epsilon_{t}+1$
\end_inset

 distributed independently and identically 
\begin_inset Formula $\chi^{2}(1)$
\end_inset

.
 Do estimation by GMM,
 and verify experimentally (by increasing the sample size) that the estimator is consistent.
 Hint:
 generate 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 as the square of a standard normal,
 minus 1.
\end_layout

\begin_layout Exercise
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
An issue to be aware of is that MA models are not identified,
 in that there exist multiple parameter values that give the same value of the likelihood function.
\end_layout

\begin_layout Itemize
For example,
 the MA(1) model with 
\begin_inset Formula $\tilde{\sigma}^{2}=\theta^{2}\sigma^{2}$
\end_inset

 and 
\begin_inset Formula $\tilde{\theta}_{1}=\frac{1}{\theta_{1}}$
\end_inset

 has identical first and second moments to the original model,
 so the likelihood function has the same value.
\end_layout

\begin_layout Itemize
Normally,
 the parameterization that leads to an 
\emph on
invertible
\emph default
 MA model is the one that is selected.
 An invertible MA model is one that has a representation as a AR(
\begin_inset Formula $\infty)$
\end_inset

 model.
 For the MA(1) model,
 the invertible parameterization has 
\begin_inset Formula $\left|\theta_{1}\right|<1$
\end_inset

.
\end_layout

\begin_layout Itemize
This implies that parameter restrictions will need to be imposed when estimating the MA model,
 to enforce selection of the invertible model.
\end_layout

\begin_layout Itemize
Maximization of the conditional likelihood is also used for estimation,
 sometimes.
 Assuming that 
\begin_inset Formula $\epsilon_{0}$
\end_inset

 is known (for example,
 equal to zero),
 then one can compute 
\begin_inset Formula $\epsilon_{1}$
\end_inset

,
 given the parameters.
 Then one works forward recursively to get all of the 
\begin_inset Formula $\epsilon_{t}$
\end_inset

.
 With these,
 the likelihood function is very easy to compute.
 This is a convenient shortcut,
 but it's not recommended if the sample is not large,
 especially since it's not hard to compute the exact likelihood function.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
AR(p) processes
\end_layout

\begin_layout Standard
An AR(p) process can be represented as 
\begin_inset Formula 
\[
y_{t}=c+\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\cdots+\phi_{p}y_{t-p}+\varepsilon_{t}
\]

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is white noise.
 This is just a linear regression model,
 and assuming stationarity,
 we can estimate the parameters by OLS.
 What is needed for stationarity?
\end_layout

\begin_layout Standard
The dynamic behavior of an AR(p) process can be studied by writing this 
\begin_inset Formula $p^{th}$
\end_inset

 order difference equation as a vector first order difference equation (this is known as the companion form):
 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{t}\\
y_{t-1}\\
\vdots\\
y_{t-p+1}
\end{array}\right]=\left[\begin{array}{l}
c\\
0\\
\vdots\\
0
\end{array}\right]+\left[\begin{array}{lllll}
\phi_{1} & \phi_{2} & \cdots &  & \phi_{p}\\
1 & 0 & 0 &  & 0\\
0 & 1 & 0 & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & 0\cdots\\
0 & \cdots & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{l}
y_{t-1}\\
y_{t-2}\\
\vdots\\
y_{t-p}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{t}\\
0\\
\vdots\\
0
\end{array}\right]
\]

\end_inset

This is just the equation of the AR(p) process in the first row,
 and identities in the remaining rows.
 In matrix form:
 
\begin_inset Formula 
\[
Y_{t}={\color{blue}C+FY_{t-1}+E_{t}}
\]

\end_inset


\end_layout

\begin_layout Itemize
There are 
\begin_inset Formula $p$
\end_inset

 variables in this set of 
\begin_inset Formula $p$
\end_inset

 equations,
 so 
\begin_inset Formula $C$
\end_inset

 is 
\begin_inset Formula $p\times1,$
\end_inset


\begin_inset Formula $F$
\end_inset

 is 
\begin_inset Formula $p\times p$
\end_inset

,
 etc.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

 With this,
 we can recursively work forward in time:
 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t+1} & = & C+FY_{t}+E_{t+1}\\
 & = & C+F\left({\color{blue}C+FY_{t-1}+E_{t}}\right)+E_{t+1}\\
 & = & {\color{red}C+FC+F^{2}Y_{t-1}+FE_{t}+E_{t+1}}
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t+2} & = & C+FY_{t+1}+E_{t+2}\\
 & = & C+F\left({\color{red}C+FC+F^{2}Y_{t-1}+FE_{t}+E_{t+1}}\right)+E_{t+2}\\
 & = & C+FC+F^{2}C+F^{3}Y_{t-1}+F^{2}E_{t}+FE_{t+1}+E_{t+2}
\end{eqnarray*}

\end_inset

 or in general 
\begin_inset Formula 
\[
Y_{t+j}=C+FC+\cdots+F^{j}C+F^{j+1}Y_{t-1}+{\color{blue}\underbrace{F^{j}E_{t}}}+F^{j-1}E_{t+1}+\cdots+FE_{t+j-1}+E_{t+j}
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Consider the impact of a shock in period 
\begin_inset Formula $t$
\end_inset

 on 
\begin_inset Formula $y_{t+j}.$
\end_inset

 This is simply the partial of 
\begin_inset Formula $Y_{t+j}$
\end_inset

 with respect to the (1,1) element of the shocks in the bracketed term of the previous equation:
\begin_inset Formula 
\[
\frac{\partial Y_{t+j}}{\partial E_{t}^{\prime}}_{(1,1)}=F_{(1,1)}^{j}
\]

\end_inset

 If the system is to be stationary,
 then as we move forward in time this impact must die off.
 Otherwise a shock causes a permanent change in the mean of 
\begin_inset Formula $y_{t}.$
\end_inset

 Therefore,
 stationarity requires that 
\begin_inset Formula 
\[
\lim_{j\rightarrow\infty}F_{(1,1)}^{j}=0
\]

\end_inset


\end_layout

\begin_layout Itemize
Save this result,
 we'll need it in a minute.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Consider the eigenvalues of the matrix 
\begin_inset Formula $F.$
\end_inset

 These are the 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula 
\[
|F-\lambda I_{P}|=0
\]

\end_inset

 The determinant here can be expressed as a polynomial.
 For example,
 for 
\begin_inset Formula $p=1,$
\end_inset

 the matrix 
\begin_inset Formula $F$
\end_inset

 is simply 
\begin_inset Formula 
\[
F=\phi_{1}
\]

\end_inset

 so 
\begin_inset Formula 
\[
|\phi_{1}-\lambda|=0
\]

\end_inset

 can be written as 
\begin_inset Formula 
\[
\phi_{1}-\lambda=0
\]

\end_inset

 When 
\begin_inset Formula $p=2,$
\end_inset

 the matrix 
\begin_inset Formula $F$
\end_inset

 is 
\begin_inset Formula 
\[
F=\left[\begin{array}{ll}
\phi_{1} & \phi_{2}\\
1 & 0
\end{array}\right]
\]

\end_inset

 so 
\begin_inset Formula 
\[
F-\lambda I_{P}=\left[\begin{array}{ll}
\phi_{1}-\lambda & \phi_{2}\\
1 & -\lambda
\end{array}\right]
\]

\end_inset

 and 
\begin_inset Formula 
\[
|F-\lambda I_{P}|=\lambda^{2}-\lambda\phi_{1}-\phi_{2}
\]

\end_inset

 So the eigenvalues are the roots of the polynomial 
\begin_inset Formula 
\[
\lambda^{2}-\lambda\phi_{1}-\phi_{2}
\]

\end_inset

 which can be found using the quadratic equation.
 This generalizes.
 For a 
\begin_inset Formula $p^{th}$
\end_inset

 order AR process,
 the eigenvalues are the roots of 
\begin_inset Formula 
\[
\lambda^{p}-\lambda^{p-1}\phi_{1}-\lambda^{p-2}\phi_{2}-\cdots-\lambda\phi_{p-1}-\phi_{p}=0
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Supposing that all of the roots of this polynomial are distinct,
 then the matrix 
\begin_inset Formula $F$
\end_inset

 
\begin_inset CommandInset href
LatexCommand href
name "can be factored as"
target "http://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix"
literal "false"

\end_inset

 
\begin_inset Formula 
\[
F=T\Lambda T^{-1}
\]

\end_inset

 where 
\begin_inset Formula $T$
\end_inset

 is the matrix which has as its columns the eigenvectors of 
\begin_inset Formula $F,$
\end_inset

 and 
\begin_inset Formula $\Lambda$
\end_inset

 is a diagonal matrix with the eigenvalues on the main diagonal.
 Using this decomposition,
 we can write 
\begin_inset Formula 
\[
F^{j}=\left(T\Lambda T^{-1}\right)\left(T\Lambda T^{-1}\right)\cdots\left(T\Lambda T^{-1}\right)
\]

\end_inset

 where 
\begin_inset Formula $T\Lambda T^{-1}$
\end_inset

 is repeated 
\begin_inset Formula $j$
\end_inset

 times.
 This gives 
\begin_inset Formula 
\[
F^{j}=T\Lambda^{j}T^{-1}
\]

\end_inset

 and 
\begin_inset Formula 
\[
\Lambda^{j}=\left[\begin{array}{llll}
\lambda_{1}^{j} & 0 &  & 0\\
0 & \lambda_{2}^{j}\\
 &  & \ddots\\
0 &  &  & \lambda_{p}^{j}
\end{array}\right]
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Supposing that the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 
\begin_inset Formula $i=1,2,...,p$
\end_inset

 are all real valued,
 it is clear that 
\begin_inset Formula 
\[
\lim_{j\rightarrow\infty}F_{(1,1)}^{j}=0
\]

\end_inset

 requires that 
\begin_inset Formula 
\[
|\lambda_{i}|<1,i=1,2,...,p
\]

\end_inset

 e.g.,
 the eigenvalues must be less than one in absolute value.
\end_layout

\begin_layout Itemize
It may be the case that some eigenvalues are complex-valued.
 The previous result generalizes to the requirement that the eigenvalues be less than one in 
\emph on
modulus,

\emph default
 where the modulus of a complex number 
\begin_inset Formula $a+bi$
\end_inset

 is 
\begin_inset Formula 
\[
mod(a+bi)=\sqrt{a^{2}+b^{2}}
\]

\end_inset

 This leads to the famous statement that 
\begin_inset Quotes eld
\end_inset

stationarity requires the roots of the determinantal polynomial to lie inside the complex unit circle.
\begin_inset Quotes erd
\end_inset

 
\emph on
draw picture here
\emph default
.
\end_layout

\begin_layout Itemize
When there are roots on the unit circle (unit roots) or outside the unit circle,
 we leave the world of stationary processes.
\end_layout

\begin_layout Itemize
Dynamic multipliers:
 
\begin_inset Formula $\partial y_{t+j}/\partial\varepsilon_{t}=F_{(1,1)}^{j}$
\end_inset

 is a 
\emph on
dynamic multiplier
\emph default
 or an 
\emph on
impulse-response
\emph default
 function.
 Real eigenvalues lead to steady movements,
 whereas complex eigenvalues lead to oscillatory behavior.
 Of course,
 when there are multiple eigenvalues the overall effect can be a mixture.
 
\emph on
pictures
\emph default

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Moments of AR(p) process
\end_layout

\begin_layout Standard
The AR(p) process is 
\begin_inset Formula 
\[
y_{t}=c+\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\cdots+\phi_{p}y_{t-p}+\varepsilon_{t}
\]

\end_inset

 Assuming stationarity,
 
\begin_inset Formula $\mathcal{E}(y_{t})=\mu,\forall t,$
\end_inset

 so 
\begin_inset Formula 
\[
\mu=c+\phi_{1}\mu+\phi_{2}\mu+...+\phi_{p}\mu
\]

\end_inset

 so 
\begin_inset Formula 
\[
\mu=\frac{c}{1-\phi_{1}-\phi_{2}-...-\phi_{p}}
\]

\end_inset

 and 
\begin_inset Formula 
\[
c=\mu-\phi_{1}\mu-...-\phi_{p}\mu.
\]

\end_inset

 Subtracting the expression for 
\begin_inset Formula $\mu$
\end_inset

 from that for 
\begin_inset Formula $y_{t}$
\end_inset

,
 we get 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}-\mu & = & \phi_{1}(y_{t-1}-\mu)+\phi_{2}(y_{t-2}-\mu)+...+\phi_{p}(y_{t-p}-\mu)+\varepsilon_{t}
\end{eqnarray*}

\end_inset


\begin_inset Newpage newpage
\end_inset

 With this,
 the second moments are easy to find:
 The variance is 
\begin_inset Formula 
\[
\gamma_{0}=\phi_{1}\gamma_{1}+\phi_{2}\gamma_{2}+...+\phi_{p}\gamma_{p}+\sigma^{2}
\]

\end_inset

 The autocovariances of orders 
\begin_inset Formula $j\geq1$
\end_inset

 follow the rule 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{j} & = & \mathcal{E}\left[\left(y_{t}-\mu\right)\left(y_{t-j}-\mu)\right)\right]\\
 & = & \mathcal{E}\left[\left(\phi_{1}(y_{t-1}-\mu)+\phi_{2}(y_{t-2}-\mu)+...+\phi_{p}(y_{t-p}-\mu)+\varepsilon_{t}\right)\left(y_{t-j}-\mu\right)\right]\\
 & = & \phi_{1}\gamma_{j-1}+\phi_{2}\gamma_{j-2}+...+\phi_{p}\gamma_{j-p}
\end{eqnarray*}

\end_inset

 Using the fact that 
\begin_inset Formula $\gamma_{-j}=\gamma_{j},$
\end_inset

 one can take the 
\begin_inset Formula $p+1$
\end_inset

 equations for 
\begin_inset Formula $j=0,1,...,p$
\end_inset

,
 which have 
\begin_inset Formula $p+1$
\end_inset

 unknowns (
\begin_inset Formula $\sigma^{2},$
\end_inset

 
\begin_inset Formula $\gamma_{0},\gamma_{1},...,\gamma_{p})$
\end_inset

 and solve for the unknowns.
 With these,
 the 
\begin_inset Formula $\gamma_{j}$
\end_inset

 for 
\begin_inset Formula $j>p$
\end_inset

 can be solved for recursively.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
ARMA model
\end_layout

\begin_layout Standard
An ARMA(
\begin_inset Formula $p,q$
\end_inset

) model is 
\begin_inset Formula $(1+\phi_{1}L+\phi_{2}L^{2}+...+\phi_{p}L^{p})y_{t}=c+(1+\theta_{1}L+\theta_{2}L^{2}+...+\theta_{q}L^{q})\epsilon_{t}$
\end_inset

.
 These are popular in applied time series analysis.
\end_layout

\begin_layout Itemize
A high order AR process 
\emph on
may
\emph default
 be well approximated by a low order MA process,
 and a high order MA process 
\emph on
may
\emph default
 be well approximated by a low order AR process.
\end_layout

\begin_layout Itemize
By combining low order AR and MA processes in the same model,
 one can hope to fit a wide variety of time series using a parsimonious number of parameters.
 
\end_layout

\begin_layout Itemize
There is much literature on how to choose 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q,$
\end_inset

 which is outside the scope of this course.
 This involves comparing the ACF and PACFs.
\end_layout

\begin_layout Itemize
Estimation can be done using the Kalman filter,
 assuming that the errors are normally distributed.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Using GRETL,
 try out various models to explain the unemployment rate,
 using the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/us
\backslash
_macro
\backslash
_quarterly.gdt}{S
\backslash
&W US quarterly macro data.} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
first,
 set the sample to all observations except the last 8,
 to allow for out of sample forecasting.
\end_layout

\begin_layout Itemize
estimate a MA(4) model for the unemployment rate
\end_layout

\begin_layout Itemize
estimate a AR(4) model for the unemployment rate
\end_layout

\begin_layout Itemize
estimate an ARMAX(1,1) model using 4 lags of the GDP growth rate (don't use current value)
\end_layout

\begin_deeper
\begin_layout Itemize
interpret the estimated coefficients.
 What can we say about persistence and speed of transmission of effects in the economy?
\end_layout

\begin_layout Itemize
look at fit and residuals.
 Observe the 
\begin_inset Quotes eld
\end_inset

Great Moderation
\begin_inset Quotes erd
\end_inset

 of the 1990's,
 and the return to volatility after the 2007 Great Recession.
\end_layout

\end_deeper
\begin_layout Itemize
look at the BIC to help to decide which model to use
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:VAR-models"

\end_inset

VAR models
\end_layout

\begin_layout Standard
Consider the model
\begin_inset Formula 
\begin{align}
y_{t} & =C+A_{1}y_{t-1}+\epsilon_{t}\label{eq:VAR1}\\
E(\epsilon_{t}\epsilon_{t}^{\prime}) & =\Sigma\nonumber \\
E(\epsilon_{t}\epsilon_{s}^{\prime}) & =0,t\ne s\nonumber 
\end{align}

\end_inset

where 
\begin_inset Formula $y_{t}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 are 
\begin_inset Formula $G\times1$
\end_inset

 vectors,
 
\begin_inset Formula $C$
\end_inset

 is a 
\begin_inset Formula $G\times1$
\end_inset

 of constants,
 and 
\begin_inset Formula $A_{1}$
\end_inset

 is a 
\begin_inset Formula $G\times G$
\end_inset

 matrix of parameters.
 The matrix 
\begin_inset Formula $\Sigma$
\end_inset

 is a 
\begin_inset Formula $G\times G$
\end_inset

 covariance matrix.
 Assume that we have 
\begin_inset Formula $n$
\end_inset

 observations.
 This is a 
\emph on
vector autoregressive
\emph default
 model,
 of order 1 - commonly referred to as a VAR(1) model.
 It is a collection of 
\begin_inset Formula $G$
\end_inset

 AR(1) models,
 augmented to include lags of other endogenous variables,
 and the 
\begin_inset Formula $G$
\end_inset

 equations are contemporaneously correlated.
 The extension to a VAR(p) model is quite obvious.
\end_layout

\begin_layout Itemize
As shown in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:EstimationRF"
nolink "false"

\end_inset

,
 it is efficient to estimate a VAR model using OLS equation by equation,
 there is no need to use GLS,
 in spite of the cross equation correlations.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
A VAR model of this form can be thought of as the reduced form of a dynamic simultaneous equations system,
 with all of the variables treated as endogenous,
 and with lags of all of the endogenous variables present:
\end_layout

\begin_layout Itemize
The simultaneous equations model is (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SIMEQ structural form"
nolink "false"

\end_inset

)
\begin_inset Formula 
\[
Y_{t}^{\prime}\Gamma=X_{t}^{\prime}B+E_{t}^{\prime}
\]

\end_inset


\end_layout

\begin_layout Itemize
this can be written after transposing (and adapting notation to use small case,
 pulling the constant out of 
\begin_inset Formula $X_{t}$
\end_inset

 and using 
\begin_inset Formula $v_{t}$
\end_inset

 for the error) as 
\begin_inset Formula $\Gamma^{\prime}y_{t}=a+B^{\prime}x_{t}+v_{t}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $x_{t}=y_{t-1}.$
\end_inset

 Then we have 
\begin_inset Formula $\Gamma^{\prime}y_{t}=a+B^{\prime}y_{t-1}+v_{t}.$
\end_inset


\end_layout

\begin_layout Itemize
Premultiplying by the inverse of 
\begin_inset Formula $\Gamma^{\prime}$
\end_inset

 gives
\begin_inset Formula 
\[
y_{t}=\left(\Gamma^{\prime}\right)^{-1}a+\left(\Gamma^{\prime}\right)^{-1}B^{\prime}y_{t-1}+\left(\Gamma^{\prime}\right)^{-1}v_{t}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Finally define 
\begin_inset Formula $C=\left(\Gamma^{\prime}\right)^{-1}a$
\end_inset

,
 
\begin_inset Formula $A_{1}=\left(\Gamma^{\prime}\right)^{-1}B^{\prime}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}=\left(\Gamma^{\prime}\right)^{-1}v_{t}$
\end_inset

,
 and we have the VAR(1) model of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:VAR1"
nolink "false"

\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
C.
 Sims originally proposed reduced form VAR models as an alternative to structural simultaneous equations models,
 which were perceived to require too many unrealistic assumptions for their identification.
 He showed that VARs often forecast better than structural models,
 which cast doubt upon the identification restrictions of the structural model.
\end_layout

\begin_layout Itemize
However,
 the search for structural interpretations of VAR models slowly crept back into the literature,
 leading to 
\begin_inset Quotes sld
\end_inset

structural VARs
\begin_inset Quotes srd
\end_inset

.
 
\end_layout

\begin_layout Itemize
A structural VAR model is really just a certain form of dynamic linear simultaneous equations model,
 with other imaginative and hopefully more realistic methods used for identification.
 
\end_layout

\begin_layout Itemize
The issue of identifying the structural parameters 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is more or less the same problem that was studied in the context of simultaneous equations.
 
\end_layout

\begin_layout Itemize
There,
 identification was obtained through zero restrictions.
 In the structural VAR literature,
 zero restrictions are often used,
 but other information may also be used,
 such as covariance matrix restrictions or sign restrictions.
\end_layout

\begin_layout Itemize
Interest often focuses on the impulse-response functions.
 Identification of the impact of structural shocks (how to estimate the impact-response functions) is complicated,
 with many alternative methodologies,
 and is often a topic of much disagreement among practitioners.
 The estimated impulse response functions are often sensitive to the identification strategy that is used.
 There is a large literature.
 
\end_layout

\begin_layout Itemize
Papers by C.
 Sims are a good place to start,
 if one wants to learn more.
 He also offers a good deal of useful software on his web page.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An issue which arises when a VAR(p) model 
\begin_inset Formula $y_{t}=C+A_{1}y_{t-1}+\cdots+A_{p}y_{t-p}+\epsilon_{t}$
\end_inset

 is contemplated is that the number of parameters increases rapidly in p,
 which introduces severe collinearity problems.
 
\end_layout

\begin_layout Itemize
One can use Bayesian methods such as the 
\begin_inset Quotes sld
\end_inset

Minnesota prior
\begin_inset Quotes srd
\end_inset

 (search for papers by Litterman),
 which is a prior that each variable separately follows a random walk (an AR(1) model with 
\begin_inset Formula $\rho=1)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The prior on 
\begin_inset Formula $A_{1}$
\end_inset

 is that it is an identity matrix
\end_layout

\begin_layout Itemize
and the prior on the 
\begin_inset Formula $A_{j},\,j>1$
\end_inset

 is that they are zero matrices
\end_layout

\begin_layout Itemize
thus,
 each variable follows a random walk,
 according to the prior
\end_layout

\end_deeper
\begin_layout Itemize
This can be done using stochastic restrictions similar to what was in the discussion of collinearity and ridge regression.
 For example,
 a VAR(2) model in de-meaned variables,
 with 
\begin_inset Formula $G$
\end_inset

 variables,
 can be written as
\begin_inset Formula 
\[
Y=\left[\begin{array}{cc}
Y_{-1} & Y_{-2}\end{array}\right]\left[\begin{array}{c}
A_{1}\\
A_{2}
\end{array}\right]+\epsilon
\]

\end_inset

We can impose the stochastic restriction that 
\begin_inset Formula $A_{1}=I_{2}-v_{1}$
\end_inset

 and that 
\begin_inset Formula $A_{2}=0_{2}-v_{2}$
\end_inset

.
 Augmenting the data with these 4 
\begin_inset Quotes sld
\end_inset

artificial observations
\begin_inset Quotes srd
\end_inset

,
 we get
\begin_inset Formula 
\[
\left[\begin{array}{c}
Y\\
I_{G}\\
0_{G}
\end{array}\right]=\left[\begin{array}{cc}
Y_{-1} & Y_{-2}\\
I_{G} & 0_{G}\\
0_{G} & I_{G}
\end{array}\right]\left[\begin{array}{c}
A_{1}\\
A_{2}
\end{array}\right]+\left[\begin{array}{c}
\epsilon\\
v_{1}\\
v_{2}
\end{array}\right]
\]

\end_inset

Then we can impose how important the restrictions are by weighting the stochastic restrictions,
 along the lines of a GLS heteroscedasticity correction:
 
\begin_inset Formula 
\[
\left[\begin{array}{c}
Y\\
k_{1}I_{G}\\
0_{G}
\end{array}\right]=\left[\begin{array}{cc}
Y_{-1} & Y_{-2}\\
k_{1}I_{G} & 0_{G}\\
0_{G} & k_{2}I_{G}
\end{array}\right]\left[\begin{array}{c}
A_{1}\\
A_{2}
\end{array}\right]+\left[\begin{array}{c}
\epsilon\\
k_{1}v_{1}\\
k_{2}v_{2}
\end{array}\right]
\]

\end_inset

Then we fit by OLS.
 When 
\begin_inset Formula $k_{1}$
\end_inset

 is large,
 the estimated 
\begin_inset Formula $A_{1}$
\end_inset

 will be forced to be close to an identity matrix.
 When 
\begin_inset Formula $k_{2}$
\end_inset

 is large,
 the second lag coefficients are all forced to zero.
 Jointly,
 these restrictions push the model in the direction of separate random walks for each variable.
 The degree to which the model is pushed depends on the 
\begin_inset Formula $ks$
\end_inset

.
 When the 
\begin_inset Formula $ks$
\end_inset

 are small,
 the fit is close to the unrestricted OLS fit,
 when they are large,
 it is close to separate random walks.
 
\end_layout

\begin_layout Standard
\begin_inset Quotes sld
\end_inset

Bayesian VARs
\begin_inset Quotes srd
\end_inset

 is a now a substantial body of literature.
 An introduction to more formal Bayesian methods is given in a chapter that follows.
 For highly parameterized models,
 Bayesian methods can help to impose structure.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Using GRETL,
 using the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Data/us
\backslash
_macro
\backslash
_quarterly.gdt}{Stock and Watson US quarterly macro data.} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
compute the term spread using the US Macro data.
 (term spread is GS10 - TB3MS)
\end_layout

\begin_layout Itemize
estimate a VAR(1) model for unemployment rate,
 GDP growth rate,
 and the term spread
\end_layout

\begin_layout Itemize
examine the impulse-response functions
\end_layout

\begin_layout Itemize
using the BIC,
 is the equation for the unemployment rate preferred,
 compared to the models of the previous example?
\end_layout

\begin_layout Itemize
See Stock and Watson,
 Ch.
 14 for more discussion
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Get the simulation data from the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/DSGE/GenData/dsgedata.gdt}{example DSGE model} 
\end_layout

\end_inset

.
 Recall that this simulated data intends to be representative of 40 years of quarterly data.
 
\begin_inset Newline newline
\end_inset

1.
 Estimate a VAR(1) model.
 Do an analysis of collinearity.
 Compute impulse-response functions.
\begin_inset Newline newline
\end_inset

2.
 Estimate an AR(1) model for output (by using VAR,
 but only select output).
 Compare 
\begin_inset Formula $R^{2}$
\end_inset

 and impulse-responses to the VAR(1) model.
 Note that matching impulse response functions has sometimes been used for estimation of DSGE models.
 Perhaps we'll see this idea again.
\end_layout

\begin_layout Exercise
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
ARCH,
 GARCH and Stochastic volatility
\end_layout

\begin_layout Standard
ARCH (autoregressive conditionally heteroscedastic) models appeared in the literature in 1982,
 in Engle,
 Robert F.
 (1982).
 "Autoregressive Conditional Heteroscedasticity with Estimates of Variance of United Kingdom Inflation",
 
\emph on
Econometrica
\emph default
 50:987-1008.
 This paper stimulated a very large growth in the literature for a number of years afterward.
 The related GARCH (generalized ARCH) model is now one of the most widely used models for financial time series.
\end_layout

\begin_layout Standard
Financial time series often exhibit several type of behavior:
\end_layout

\begin_layout Itemize
volatility clustering:
 periods of low variation can be followed by periods of high variation
\end_layout

\begin_layout Itemize
fat tails,
 or 
\begin_inset CommandInset href
LatexCommand href
name "excess kurtosis"
target "http://en.wikipedia.org/wiki/Kurtosis"
literal "false"

\end_inset

:
 the marginal density of a series is more strongly peaked and has fatter tails than does a normal distribution with the same mean and variance.
\end_layout

\begin_layout Itemize
leverage (negative correlation between returns and volatility),
 which often shows up as negative 
\begin_inset CommandInset href
LatexCommand href
name " skewness"
target "http://en.wikipedia.org/wiki/Skewness"
literal "false"

\end_inset

 of returns
\end_layout

\begin_layout Itemize
perhaps slight autocorrelation within the bounds allowed by arbitrage
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The data set 
\begin_inset Quotes sld
\end_inset

nysewk.gdt
\begin_inset Quotes srd
\end_inset

,
 which is provided with Gretl,
 provides an example (or,
 use the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Data/sp500.csv}{SP500 data} 
\end_layout

\end_inset

) .
 If we compute 100 times the growth rate of the series,
 using log differences,
 we can obtain the plots in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dow-Jones-close,"
nolink "false"

\end_inset

 (Julia code for this is 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{./Examples/TimeSeries/MakePlots.jl}{here} 
\end_layout

\end_inset

).
 In the first we clearly see volatility clusters,
 and in the second,
 we see excess kurtosis,
 skew,
 and tails fatter than the normal distribution.
 The skewness suggests that leverage may be present.
 We'll see how the third plot was made in the chapter on nonparametric estimation.
\end_layout

\begin_layout Itemize
compute descriptive statistics:
 negative skew and positive excess kurtosis
\end_layout

\begin_layout Itemize
regress returns on its own lag and on squared returns and lags:
 low predictability
\end_layout

\begin_layout Itemize
regress squared returns on its own lags and on returns:
 more predictable,
 evidence of leverage
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dow-Jones-close,"

\end_inset

NYSE weekly close price,
 100 
\begin_inset Formula $\text{\times}$
\end_inset

log differences
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/TimeSeries/nyse.png
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The presence of volatility clusters indicates that the variance of the series is not constant over time,
 conditional on past events.
 Engle's ARCH paper was the first to model this feature.
 
\end_layout

\begin_layout Itemize
The frequency plot shows excess kurtosis and skew (leverage)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
ARCH
\end_layout

\begin_layout Standard
A basic ARCH specification is
\begin_inset Formula 
\begin{align*}
y_{t} & ={\color{red}\mu+\rho y_{t-1}}+\epsilon_{t}\\
 & ={\color{red}g}_{{\color{red}t}}+\epsilon_{t}\\
\epsilon_{t} & =\sigma_{t}u_{t}\\
\sigma_{t}^{2} & =\omega+\sum_{i=1}^{q}\alpha_{i}\epsilon_{t-i}^{2}
\end{align*}

\end_inset

where the 
\begin_inset Formula $u_{t}$
\end_inset

 are Gaussian white noise shocks.
 The ARCH variance is a moving average process.
 Previous large shocks to the series cause the conditional variance of the series to increase.
 There is no leverage:
 negative shocks have the same impact on the future variance as do positive shocks.
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 
\end_layout

\begin_layout Itemize
for 
\begin_inset Formula $\sigma_{t}^{2}$
\end_inset

 to be positive for all realizations of 
\begin_inset Formula $\left\{ \epsilon_{t}\right\} $
\end_inset

,
 we need 
\begin_inset Formula $\omega>0$
\end_inset

,
 
\begin_inset Formula $\alpha_{i}\ge0$
\end_inset

,
 
\begin_inset Formula $\forall i$
\end_inset

.
\end_layout

\begin_layout Itemize
to ensure that the model is covariance stationary,
 we need 
\begin_inset Formula $\sum_{i}\alpha_{i}<1$
\end_inset

.
 Otherwise,
 the variances will explode off to infinity.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Given that 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is normally distributed,
 to find the likelihood in terms of the observable 
\begin_inset Formula $y_{t}$
\end_inset

 instead of the unobservable 
\begin_inset Formula $\epsilon_{t}$
\end_inset

,
 first note that the series 
\begin_inset Formula $u_{t}=\left(y_{t}-g_{t}\right)/\sigma_{t}=\frac{\epsilon_{t}}{\sigma_{t}}$
\end_inset

 is iid Gaussian,
 so the likelihood is simply the product of standard normal densities.
 
\begin_inset Formula 
\begin{eqnarray*}
u & \sim & N(0,I),\textrm{ so}\\
f(u) & = & \prod_{t=1}^{n}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{u_{t}^{2}}{2}\right)
\end{eqnarray*}

\end_inset

 The joint density for 
\begin_inset Formula $y$
\end_inset

 can be constructed using a change of variables:
\end_layout

\begin_layout Itemize
We have 
\begin_inset Formula $u_{t}=\left(y_{t}-\mu-\rho y_{t-1}\right)/\sigma_{t}$
\end_inset

,
 so 
\begin_inset Formula $\frac{\partial u_{t}}{\partial y_{t}}=\frac{1}{\sigma_{t}}$
\end_inset

 and 
\begin_inset Formula $|\frac{\partial u}{\partial y^{\prime}}|=\prod_{t=1}^{n}\frac{1}{\sigma_{t}},$
\end_inset

 
\end_layout

\begin_layout Itemize
doing a change of variables,
 
\begin_inset Formula 
\[
f(y;\theta)=\prod_{t=1}^{n}\frac{1}{\sqrt{2\pi}}\frac{1}{\sigma_{t}}\exp\left(-\frac{1}{2}\left(\frac{y_{t}-\mu-\rho y_{t-1}}{\sigma_{t}}\right)^{2}\right)
\]

\end_inset

where 
\begin_inset Formula $\theta$
\end_inset

 is the vector of all parameters (the parameters in 
\begin_inset Formula $g_{t}$
\end_inset

,
 and the 
\begin_inset Formula $\omega$
\end_inset

 and alpha parameters of the ARCH specification.
 Taking logs,
 
\begin_inset Formula 
\[
\ln L(\theta)=-n\ln\sqrt{2\pi}-\sum_{t=1}^{n}\ln\sigma_{t}-\frac{1}{2}\sum_{t=1}^{n}\left(\frac{y_{t}-\mu-\rho y_{t-1}}{\sigma_{t}}\right)^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

In principle,
 this is easy to maximize.
 Some complications can arise when the restrictions for positivity and stationarity are imposed.
 Consider a fairly short data series with low volatility in the initial part,
 and high volatility at the end.
 This data appears to have a nonstationary variance sequence.
 If one attempts to estimate and ARCH model with stationarity imposed,
 the data and the restrictions are saying two different things,
 which can make maximization of the likelihood function difficult.
\end_layout

\begin_layout Itemize
use GRETL to estimate ARCH(1) and ARCH(4)
\end_layout

\begin_layout Itemize
if interested,
 adapt the Julia code for GARCH(1,1),
 below,
 to estimate an ARCH model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
GARCH
\end_layout

\begin_layout Standard
Note that an ARCH model specifies the variance process as a moving average.
 For the same reason that an ARMA model may be used to parsimoniously model a series instead of a high order AR or MA,
 one can do the same thing for the variance series.
 A basic GARCH(p,q) (Bollerslev,
 Tim (1986).
 "Generalized Autoregressive Conditional Heteroskedasticity",
 
\emph on
Journal of Econometrics
\emph default
,
 31:307-327) specification is
\begin_inset Formula 
\begin{align*}
y_{t} & =\mu+\rho y_{t-1}+\epsilon_{t}\\
\epsilon_{t} & =\sigma_{t}u_{t}\\
\sigma_{t}^{2} & =\omega+\sum_{i=1}^{q}\alpha_{i}\epsilon_{t-i}^{2}{\color{green}{\color{blue}+\sum_{i=1}^{p}\beta_{i}\sigma_{t-i}^{2}}}
\end{align*}

\end_inset

It's just an ARCH model,
 with an 
\color blue
autoregressive part
\color inherit
 added to the specification of the conditional variance.
 The idea is that a GARCH model with low values of p and q may fit the data as well or better than an ARCH model with large q.
\end_layout

\begin_layout Itemize
the model also requires restrictions for positive variance and stationarity,
 which are:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\omega>0$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha_{i}\ge0,\,i=1,...,q$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{i}\ge0,\,i=1,...,p$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\sum_{i=1}^{q}\alpha_{i}$
\end_inset

+
\begin_inset Formula $\sum_{i=1}^{p}\beta_{i}<1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
to estimate a GARCH model,
 you need to initialize 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 at some value.
 The sample unconditional variance is one possibility.
 Another choice could be the sample variance of the initial elements of the sequence.
 One can also 
\begin_inset Quotes sld
\end_inset

backcast
\begin_inset Quotes srd
\end_inset

 the conditional variance.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The GARCH model also requires restrictions on the parameters to ensure stationarity and positivity of the variance.
 
\end_layout

\begin_layout Itemize
A useful modification is the EGARCH model (exponential GARCH,
 Nelson,
 D.
 B.
 (1991).
 "Conditional heteroskedasticity in asset returns:
 A new approach",
 Econometrica 59:
 347-370).
 This model treats the logarithm of the variance as an ARMA process,
 so the variance will be positive without restrictions on the parameters.
\end_layout

\begin_layout Itemize
There are many variants that introduce asymmetry (leverage) and non-normality.
\end_layout

\begin_layout Itemize
GARCH(1,1) is a highly popular model in financial analysis.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/TimeSeries/Garch11Example.jl}{Garch11Example.jl} 
\end_layout

\end_inset

 illustrates estimation of a GARCH(1,1) model,
 using the NYSE closing price data.
 Results:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/TimeSeries/garch11.png
	width 15cm

\end_inset


\end_layout

\begin_layout Itemize
examine the code to see how start values were determined,
 and how the variance loop was initialized.
\end_layout

\begin_layout Itemize
The AR(1) in the mean is probably not needed.
\end_layout

\begin_layout Itemize
Compare BIC (see subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Information-criteria"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

) to ARCH(1) and ARCH(4),
 which you can obtain using GRETL.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

You can get the same results quickly and easily using Gretl:
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Model 1:
 GARCH,
 using observations 670078-672192 (T = 2115)
\end_layout

\begin_layout Plain Layout

Dependent variable:
 y
\end_layout

\begin_layout Plain Layout

Standard errors based on Hessian
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

             coefficient   std.
 error      z        p-value 
\end_layout

\begin_layout Plain Layout

  ----------------------------------------------------------
\end_layout

\begin_layout Plain Layout

  const      0.177119      0.0387575     4.570     4.88e-06  ***
\end_layout

\begin_layout Plain Layout

  y_1        0.00148067    0.0232384     0.06372   0.9492   
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  alpha(0)   0.155435      0.0451241     3.445     0.0006    ***
\end_layout

\begin_layout Plain Layout

  alpha(1)   0.111397      0.0171598     6.492     8.48e-11  ***
\end_layout

\begin_layout Plain Layout

  beta(1)    0.855317      0.0228815    37.38      8.18e-306 ***
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Mean dependent var   0.129001   S.D.
 dependent var   2.061158
\end_layout

\begin_layout Plain Layout

Log-likelihood      -4396.923   Akaike criterion     8805.846
\end_layout

\begin_layout Plain Layout

Schwarz criterion    8839.786   Hannan-Quinn         8818.273
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
There are some minor differences,
 because the Julia code initializes the variance in a different way,
 using only the first 10 observations.
 Also,
 the Julia code uses sandwich standard errors,
 while GRETL uses the Hessian,
 which tends to inflate t-statistics.
\end_layout

\begin_layout Itemize
Note that the 
\begin_inset Formula $\beta_{1}$
\end_inset

 parameter is highly significant.
 If you compare likelihood values or information criteria values with the ARCH results,
 you'll see that this model is favored - it fits better with fewer parameters.
\end_layout

\begin_layout Itemize
Gretl has a number of other ARCH/GARCH style models available.
\end_layout

\begin_layout Itemize
With Gretl,
 run the GARCH variants GJR(1,1) with skewed t shocks.
\end_layout

\begin_deeper
\begin_layout Itemize
Do a density plot
\end_layout

\begin_layout Itemize
note the BIC value
\end_layout

\begin_layout Itemize
there are a lot of options to explore
\end_layout

\end_deeper
\begin_layout Itemize
Note that the test of homoscedasticity against ARCH or GARCH involves parameters being on the boundary of the parameter space,
 which means that standard asymptotics do not apply.
 Also,
 the reduction of GARCH to ARCH has the same problem.
 Testing needs to be done taking this into account.
 See Demos and Sentana (1998) 
\emph on
Journal of Econometrics
\emph default
.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Stochastic-volatility"

\end_inset

Stochastic volatility
\end_layout

\begin_layout Standard
In ARCH and GARCH models,
 the same shocks that affect the level also affect the variance.
 The stochastic volatility model allows the variance to have its own random component.
 A simple example is
\begin_inset Formula 
\begin{align*}
y_{t} & =\phi\exp(h_{t}/2)\epsilon_{t}\\
h_{t} & =\rho h_{t-1}+\sigma u_{t}
\end{align*}

\end_inset

In this model,
 the log of the variance of the observed sequence follows an AR(1) model.
 Once can introduce leverage by allowing correlation between 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $u_{t}.$
\end_inset

 This model is used as an example in the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{https://github.com/mcreel/SimulatedNeuralMoments.jl}{SimulatedNeuralMoments}
\end_layout

\end_inset

 package,
 which can be used to generate data from the model.
 Typical data and a nonparametric density plot look like what we see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SV-model,-typical"
nolink "false"

\end_inset

.
 Note the volatility clusters,
 leptokurtosis,
 and the fat tails of the density.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SV-model,-typical"

\end_inset

SV model,
 typical data and density
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/TimeSeries/svdata.png
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
While the ARCH and GARCH models have a link between the shocks to 
\begin_inset Formula $y_{t}$
\end_inset

 and the dynamics of the variance of 
\begin_inset Formula $y_{t}$
\end_inset

,
 the stochastic volatility model has latent shocks to the variance which are not directly linked to the observed dependent variable.
 This may be perfectly reasonable:
 even when volatility is high,
 the mean of shocks to the observables may be zero.
 An ARCH model could not account for an increase in volatility without having a realized extreme shock to the level.
 The SV model can allow for this.
 
\end_layout

\begin_layout Itemize
The latent shocks complicate estimation.
 Many estimation methods have been proposed,
 and this sort of model helped to popularize Bayesian methods in econometrics:
 see Jacquier,
 E.,
 Polson,
 N.G.
 and Rossi,
 P.E.,
 2002.
 Bayesian analysis of stochastic volatility models.
 Journal of Business & Economic Statistics,
 20(1),
 pp.69-87.
 We will see an examples of estimation in the chapter on simulation-based estimation.
\end_layout

\begin_layout Itemize
Variants of this sort of model are widely used to model financial data,
 competing with the GARCH(1,1) model for being the most popular choice.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Diffusion-models"

\end_inset

Diffusion models
\end_layout

\begin_layout Standard
Financial data is often modeled using a continuous time specification.
 An example is the following model,
 taken from a paper of mine (JEF,
 2015,
 with D.
 Kristensen).
\end_layout

\begin_layout Standard
A basic model is a simple continuous time stochastic volatility model with leverage.
 Log price 
\begin_inset Formula $p_{t}=\log\left(P_{t}\right)$
\end_inset

,
 solves the following pure diffusion model,
\begin_inset Formula 
\[
dp_{t}=\left(\mu_{0}+\mu_{1}\text{\exp}\left(h_{t}-\alpha\right)\right)dt+\text{\exp}\left(\frac{h_{t}}{2}\right)dW_{1,t}
\]

\end_inset

where the spot volatility (the instantaneous variance of returns),
 
\begin_inset Formula $\exp(h_{t})$
\end_inset

 is modeled using its logarithm:
\begin_inset Formula 
\[
dh_{t}=\kappa(\alpha-h_{t})dt+\sigma dW_{2,t}.
\]

\end_inset

Here,
 
\begin_inset Formula $W_{1,t}$
\end_inset

 and 
\begin_inset Formula $W_{2,t}$
\end_inset

 are two standard Brownian motions with instantaneous correlation 
\begin_inset Formula $\rho=Cov\left(dW_{1,t},dW_{2,t}\right)$
\end_inset

.
 The parameters are interpreted as follows:
 
\begin_inset Formula $\mu_{0}$
\end_inset

 is the baseline drift of returns;
 
\begin_inset Formula $\mu_{1}$
\end_inset

 allows drift to depend upon spot volatility;
 
\begin_inset Formula $\alpha$
\end_inset

 is the mean of log volatility;
 
\begin_inset Formula $\kappa$
\end_inset

 is the speed of mean reversion of log volatility,
 such that low values of 
\begin_inset Formula $\kappa$
\end_inset

 imply high persistence of log volatility;
 
\begin_inset Formula $\sigma$
\end_inset

 is the so-called volatility of volatility;
 and 
\begin_inset Formula $\rho$
\end_inset

 is a leverage parameter that affects the correlation between returns and log volatility.
 We collect the parameters in 
\begin_inset Formula $\theta=\left(\mu_{0},\mu_{1},\alpha,\kappa,\sigma,\rho\right)$
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An extension is to add jumps to the above model.
 These occur with Poisson frequency,
 and are conditionally normally distributed.
 More specifically,
 log-price 
\begin_inset Formula $p_{t}$
\end_inset

 solves the following continuous-time jump-diffusion model,
\begin_inset Formula 
\[
dp_{t}=\left(\mu_{0}+\mu_{1}\text{\exp}\left(h_{t}/2\right)\right)dt+\text{\exp}\left(\frac{h_{t}}{2}\right)dW_{1,t}+J_{t}dN_{t}.
\]

\end_inset

The Poisson process 
\begin_inset Formula $N_{t}$
\end_inset

 counts the number of jumps up to time 
\begin_inset Formula $t,$
\end_inset

 and has jump intensity 
\begin_inset Formula $\lambda_{t}=\lambda_{0}+\lambda_{1}\text{\exp}\left(h_{t}-\alpha\right)$
\end_inset

 that varies with the volatility,
 while jump sizes,
 conditional on the occurrence of a jump,
 are independent and conditionally normally distributed:
 
\begin_inset Formula $J_{t}\sim N(\mu_{J},\sigma_{J}^{2})$
\end_inset

.
 The inclusion of the jump component adds four parameters to 
\begin_inset Formula $\theta$
\end_inset

 as defined above,
 
\begin_inset Formula $\mu_{J}$
\end_inset

,
 
\begin_inset Formula $\sigma_{J}^{2}$
\end_inset

 and 
\begin_inset Formula $\lambda_{0}$
\end_inset

,
 and 
\begin_inset Formula $\lambda_{1}$
\end_inset

.
 This jump model was considered in,
 for example,
 Andersen,
 Benzoni and Lund (2002).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An example of how returns,
 
\begin_inset Formula $r_{t}=100(p_{t}-p_{t-1})$
\end_inset

,
 generated by such a model might look is given in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Returns-from-jump-diffusion"
nolink "false"

\end_inset

.
 The spot volatility is plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Spot-volatility,-jump-diffusion"
nolink "false"

\end_inset

.
 Returns are observable,
 but spot volatility is not.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Returns-from-jump-diffusion"

\end_inset

Returns from jump-diffusion model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/rets.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Spot-volatility,-jump-diffusion"

\end_inset

Spot volatility,
 jump-diffusion model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/spotvol.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

One might want to try to infer the parameters of the model,
 and also the latent spot volatility,
 using the observable data.
 
\end_layout

\begin_layout Itemize
Estimation of the parameters of such models is complicated by the fact that data is available in discrete time:
 
\begin_inset Formula $p_{1},p_{2},...p_{n},$
\end_inset

 but the model is in continuous time.
 
\end_layout

\begin_layout Itemize
One can 
\begin_inset Quotes sld
\end_inset

discretize
\begin_inset Quotes srd
\end_inset

 the model,
 to obtain something like the discrete time SV model of the previous section,
 but the discrete time transition density implied by the approximating model is not the same as the true transition density 
\begin_inset Formula 
\[
p_{t}\sim f_{p}\left(p_{t}|p_{t-1},h_{t-1};\theta\right),
\]

\end_inset

induced by the continuous time model.
 
\end_layout

\begin_layout Itemize
This true density is unknown,
 however,
 so using it for ML estimation is not possible.
 If one estimates the discrete time version treating it as the actual density,
 there is an approximation misspecification that causes the estimates to be inconsistent:
 we're not doing ML,
 we're doing quasi-ML,
 which is in general an inconsistent estimator.
 
\end_layout

\begin_layout Itemize
Estimation of parameters can be done using simulation-based estimation,
 discussed in the previous chapter.
 A means of learning about spot volatility,
 
\begin_inset Formula $h_{t},$
\end_inset

 given estimated parameters and the history of observable variables,
 is discussed in the chapter on nonparametric inference,
 in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Limited-information-nonparametri"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:State-space-models"

\end_inset

State space models
\end_layout

\begin_layout Standard
For Kalman filtering,
 see Hamilton,
 
\emph on
Time Series Analysis,
 
\emph default
Chapter 13 and 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "Mikusheva's MIT Open Courseware notes"
target "http://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2013/lecture-notes/"
literal "false"

\end_inset


\bar default
,
 lectures 21 and 22.
 A tutorial with Julia code is here:
 
\begin_inset CommandInset href
LatexCommand href
name "Quantitative Economics Kalman filter"
target "https://julia.quantecon.org/tools_and_techniques/kalman.html"
literal "false"

\end_inset

.
 Another source is the summary in the introduction of 
\begin_inset CommandInset citation
LatexCommand citet
key "LopesTsayPArticleFilter2011"
literal "true"

\end_inset


\bar under
.

\bar default
 
\end_layout

\begin_layout Standard
For nonlinear state space models,
 or non-Gaussian state space models,
 the basic Kalman filter cannot be used,
 and the particle filter is becoming a widely-used means of computing the likelihood.
 This is a fairly new,
 computationally demanding technique,
 and is currently (this was written in 2013) an active area of research.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "LopesTsayPArticleFilter2011"
literal "true"

\end_inset

 for a review.
 Papers by Fernández-Villaverde and Rubio-Ramírez provide interesting and reasonably accessible applications in the context of estimating macroeconomic (DSGE) models.
\end_layout

\begin_layout Standard
Here's a document that gives a nice summary of Kalman filtering to compute the likelihood function of a state space model:
 
\begin_inset CommandInset href
LatexCommand href
name "R.D. Peng time series."
target "https://bookdown.org/rdpeng/timeseriesbook/"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Linear models with Gaussian shocks can often be put into what is known as state-space form.
 A simple state-space model,
 using Peng's notation,
 is 
\begin_inset Formula 
\begin{align*}
y_{t} & =A_{t}x_{t}+V_{t}\\
x_{t} & =B_{t}x_{t-1}+W_{t}\\
V_{t} & \sim N(0,S)\\
W_{t} & \sim N(0,R)
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
The first equation is called the measurement equation,
 and the second is the state equation.
\end_layout

\begin_layout Itemize
The variables 
\begin_inset Formula $y_{t}$
\end_inset

 are observed at time 
\begin_inset Formula $t.$
\end_inset

 
\end_layout

\begin_layout Itemize
The state variables may or may not be observed at time 
\begin_inset Formula $t$
\end_inset

 (that is,
 some or all of them may be latent).
 
\end_layout

\begin_layout Itemize
The shocks,
 
\begin_inset Formula $V_{t}$
\end_inset

 and 
\begin_inset Formula $W_{t}$
\end_inset

 are vector Gaussian white noise,
 and are uncorrelated with each other at all lags.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Example:
 MA(1)
\end_layout

\begin_layout Standard
As we saw above,
 for an MA model,
 we can compute the likelihood function using eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MA1likelihood"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
 That form of writing the likelihood uses a lot of computer memory,
 as the entire 
\begin_inset Formula $\Sigma$
\end_inset

 matrix must be stored.
 A more efficient method is to write the MA model as a linear Gaussian state-space model,
 and to use Kalman filtering to compute the likelihood.
 
\end_layout

\begin_layout Standard
The MA(1) model 
\begin_inset Formula 
\[
y_{t}=\epsilon_{t}+\theta_{1}\epsilon_{t-1}
\]

\end_inset

 can be put into state space form,
 using
\begin_inset Formula 
\[
x_{t}=\left[\begin{array}{c}
\epsilon_{t}\\
\epsilon_{t-1}
\end{array}\right],\,A_{t}=\left[\begin{array}{cc}
1 & \theta_{1}\end{array}\right],\,W_{t}=0,\,B_{t}=\left[\begin{array}{cc}
0 & 0\\
1 & 0
\end{array}\right],\,V_{t}=\left[\begin{array}{c}
\epsilon_{t}\\
0
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Itemize
many more complicated linear models,
 including ARMA(p,q) models,
 for example,
 can be put into this (or a slightly extended) form.
 
\end_layout

\begin_layout Itemize
Adding exogenous variables is straightforward.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Using Kalman filtering,
 one can compute the likelihood by computing it as the product of the conditional densities of the observed variables:
\begin_inset Formula 
\[
f(y_{t}|y_{t-1},y_{t-1},...,y_{1})
\]

\end_inset

 starting with 
\begin_inset Formula $f(y_{1}),$
\end_inset

 then 
\begin_inset Formula $f(y_{2}|y_{1}),$
\end_inset

 and working forward to the last measurement observation,
 
\begin_inset Formula $y_{t}$
\end_inset

.
\end_layout

\begin_layout Itemize
It turns out that marginal and conditional densities for joint normal random variables are also normal.
\end_layout

\begin_deeper
\begin_layout Itemize
We use marginalization to get rid of the unobserved state variables
\end_layout

\begin_layout Itemize
conditioning is used to update densities as information arrives in the form of measurement variables
\end_layout

\end_deeper
\begin_layout Itemize
So,
 if we can keep track of conditional means and variances,
 we can compute all of the conditional densities,
 starting with 
\begin_inset Formula $f(y_{1}),$
\end_inset

then getting 
\begin_inset Formula $f(y_{2}|y_{1}),$
\end_inset

 and working up to the last.
\end_layout

\begin_layout Itemize
There are convenient formulae for updating conditional means and variances,
 as new information (the measurement variables) arrives.
\end_layout

\begin_layout Itemize
All of the densities depend on the parameters,
 of course.
 We will estimate those by maximum likelihood.
\end_layout

\begin_layout Itemize
See the other references for more information on how the recursive updating works.
\end_layout

\begin_layout Itemize
This is easy to program,
 and is fast,
 as all equations are closed form.
 So,
 this is often the way that likelihoods are computed in practice,
 for models that can be put into state space form.
\end_layout

\begin_layout Itemize
For this to work,
 the model must be linear,
 and the shocks must be Gaussian.
 Without these conditions,
 other methods,
 more complicated and computationally demanding,
 must be used.
 So,
 there is a natural tendency to try to fit models into the state-space form,
 even if they don't actually fit exactly.
 An example would be linearizing a model,
 and the estimate the parameters by ML,
 computed using Kalman filtering.
 That can lead to inconsistency,
 as we saw....
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Nonstationarity and cointegration
\end_layout

\begin_layout Standard
I'm going to follow Karl Whelan's notes,
 which are available at
\bar under
 
\begin_inset CommandInset href
LatexCommand href
name "Whelan notes"
target "http://www.karlwhelan.com/Teaching/MA%20Econometrics/part4.pdf"
literal "false"

\end_inset


\bar default
.
 
\end_layout

\begin_layout Itemize
A Gretl script file which generates data following the random walk with drift example is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/TimeSeries/Drift.inp}{Drift.inp} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
A similar script generates random walks without drift.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/TimeSeries/RandomWalks.inp}{RandomWalks.inp} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
There is also one for a deterministic time trend.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/TimeSeries/Trend.inp}{Trend.inp} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
run the script to generate data.
 Then set the data set structure to time series.
\end_layout

\begin_layout Itemize
do a time series plot of the y and x series
\end_layout

\begin_layout Itemize
run an OLS of y on x
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Paragraph
CO
\begin_inset Formula $_{2}$
\end_inset

 concentration over time.
\end_layout

\begin_layout Itemize
search for 
\begin_inset Quotes sld
\end_inset

Keeling-Whorf
\begin_inset Quotes srd
\end_inset


\end_layout

\begin_layout Itemize
Here's 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Data/keeling_whorf_CO2.gdt}{Keeling and Whorf data} 
\end_layout

\end_inset

 on 
\begin_inset Formula $\mathrm{CO_{2}}$
\end_inset

 concentrations at Mona Loa.
\end_layout

\begin_deeper
\begin_layout Itemize
run model with constant and trend,
 observe residuals
\end_layout

\begin_layout Itemize
add monthly indicators for the seasonality:
 looks at fit and t-stats
\end_layout

\begin_layout Itemize
form of 
\begin_inset Formula $X^{\prime}X/T$
\end_inset


\end_layout

\begin_layout Itemize
do model in differences
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Use Matlab/Octave to estimate the same GARCH(1,1) model as in the GarchExample.jl script provided above.
 Also,
 estimate an ARCH(4) model for the same data.
 If unconstrained estimation does not satisfy stationarity restrictions,
 then do contrained estimation.
 Compare likelihood values.
 Which of the two models do you prefer?
 But do the models have the same number of parameters?
 Find out what is the 
\begin_inset Quotes sld
\end_inset

consistent Akaike information criterion
\begin_inset Quotes srd
\end_inset

 or the 
\begin_inset Quotes sld
\end_inset

Bayes information criterion
\begin_inset Quotes srd
\end_inset

 and what they are used for.
 Compute one or the other,
 or both,
 and discuss what they tell you about selecting between the two models.
\end_layout

\begin_layout Enumerate
Use Gretl to estimate (by ML) the same Garch(1,1) model as in the previous problem using the nysewk.gdt data set.
 Do you get the same parameter estimates?
\end_layout

\begin_layout Enumerate
Consider the data generating process 
\begin_inset Formula 
\begin{align*}
Y_{t} & =\alpha t+u_{t}\\
X_{t} & =\gamma t+v_{t}
\end{align*}

\end_inset

where 
\begin_inset Formula $t=1,2,...,T$
\end_inset

,
 and 
\begin_inset Formula $u_{t}$
\end_inset

 and 
\begin_inset Formula $v_{t}$
\end_inset

 are two independent 
\begin_inset Formula $N(0,1)$
\end_inset

 white noise processes.
 That is,
 both 
\begin_inset Formula $Y_{t}$
\end_inset

 and 
\begin_inset Formula $X_{t}$
\end_inset

 follow time trends,
 each with a different white noise shock.
 Consider the regression model 
\emph on
without constant
\emph default
 
\begin_inset Formula $Y_{t}=\beta X_{t}+\epsilon_{t}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
To what value does the OLS estimator of 
\begin_inset Formula $\beta$
\end_inset

 tend?
 Prove your result.
 Hints:
 recall that 
\begin_inset Formula $\frac{1}{T^{2}}\sum_{t=1}^{T}t\rightarrow\frac{1}{2}$
\end_inset

 and 
\begin_inset Formula $\frac{1}{T^{3}}\sum_{t=1}^{T}t^{2}\rightarrow\frac{1}{3}.$
\end_inset

 Also,
 the formula for the OLS estimator when there is no constant is 
\begin_inset Formula $\hat{\beta}=\frac{\sum_{t}Y_{t}X_{t}}{\sum_{t}X_{t}^{2}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Does this regression model exhibit the 
\emph on
spurious regression
\emph default
 problem?
 Explain why or why not.
\begin_inset Newline newline
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Write a Matlab/Julia/your favorite package script that generates two independent random walks,
 
\begin_inset Formula $x_{t}=x_{t-1}+u_{t}$
\end_inset

 and 
\begin_inset Formula $y_{t}=y_{t-1}+u_{t}$
\end_inset

,
 where the initial conditions are 
\begin_inset Formula $x_{0}=0$
\end_inset

 and 
\begin_inset Formula $y_{0}=0$
\end_inset

,
 and the two errors are both iid N(0,1).
 Use a sample size of 1000:
 
\begin_inset Formula $t=1,2,...,1000.$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
regress y upon x and a constant.
\end_layout

\begin_layout Enumerate
discuss your findings,
 especially the slope coefficient,
 the t statistic of the slope,
 and 
\begin_inset Formula $R^{2}$
\end_inset

.
 Are the findings sensible,
 given that we know that 
\begin_inset Formula $x$
\end_inset

 has nothing to do with 
\begin_inset Formula $y$
\end_inset

?
\end_layout

\begin_layout Enumerate
compute the variance of 
\begin_inset Formula $y_{t}$
\end_inset

 and 
\begin_inset Formula $x_{t}$
\end_inset

 conditional on the initial conditions 
\begin_inset Formula $y_{0}=0$
\end_inset

 and 
\begin_inset Formula $x_{0}=0.$
\end_inset

 Does the variance depend on 
\begin_inset Formula $t$
\end_inset

?
\end_layout

\begin_layout Enumerate
which of the assumptions of the classical linear regression model are not satisfied by this data generating process?
\end_layout

\begin_layout Enumerate
present estimation results using transformation(s) of y and/or x so that the regression using the transformed variables confirms that there is no relationship between the variables.
 Explain why the transformation(s) you use are successful in eliminating the problem of a spurious relationship.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Introduction-to-panel"

\end_inset

Introduction to panel data
\end_layout

\begin_layout Standard
References:
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Part V,
 Chapters 21 and 22 (plus 23 if you have special interest in the topic) and a basic discussion in Stock and Watson,
 
\emph on

\begin_inset CommandInset href
LatexCommand href
name "Introduction to Econometrics"
target "https://ebookcentral.proquest.com/lib/uab/detail.action?docID=5174962#"
literal "false"

\end_inset


\emph default
,
 Chapter 10.
 The GRETL manual also has two chapters,
 which are a nice reference.
 
\end_layout

\begin_layout Standard
In this chapter we'll look at panel data.
 Panel data is an important area in applied econometrics,
 simply because much of the available data has this structure.
 Also,
 it provides an example where things we've already studied (GLS,
 endogeneity,
 GMM,
 Hausman test) come into play.
 There has been much work in this area,
 and the intention is not to give a complete overview,
 but rather to highlight the issues and see how the tools we have studied can be applied.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Generalities
\end_layout

\begin_layout Standard
Panel data combines cross sectional and time series data:
 we have a time series for each of the agents observed in a cross section.
\end_layout

\begin_layout Itemize
The addition of temporal information to a cross sectional model can in principle allow us to investigate issues such as persistence,
 habit formation,
 and dynamics.
\end_layout

\begin_layout Itemize
Starting from the perspective of a single time series,
 the addition of cross-sectional information allows investigation of heterogeneity.
\end_layout

\begin_layout Itemize
In both cases,
 if parameters are common across units or over time,
 the additional data allows for more precise estimation.
 This is simply an example of estimation subject to restrictions,
 which improves efficiency 
\emph on
if the restrictions are correct
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Stock and Watson 
\begin_inset Quotes eld
\end_inset

Introduction to Econometrics
\begin_inset Quotes erd
\end_inset

,
 3rd Edition,
 discuss panel data in Chapter 10.
 The book is available from the UAB library 
\begin_inset CommandInset href
LatexCommand href
name "as an e-text"
target "https://ebookcentral.proquest.com/lib/uab/detail.action?docID=5174962#"
literal "false"

\end_inset

 (if you happen to be at the UAB).
 They use data which is available as a GRETL file 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/PanelData/fatality.gdt}{here}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Itemize
open up the data and examine the structure
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The basic idea is to allow variables to have two indices,
 
\begin_inset Formula $i=1,2,...,n$
\end_inset

 and 
\begin_inset Formula $t=1,2,...,T$
\end_inset

.
 The simple linear model
\begin_inset Formula 
\[
y_{i}=\alpha+x_{i}\beta+\epsilon_{i}
\]

\end_inset

 becomes
\begin_inset Formula 
\[
y_{it}=\alpha+x_{it}\beta+\epsilon_{it}
\]

\end_inset

We could think of allowing the parameters to change over time and over cross sectional units.
 This would give
\begin_inset Formula 
\[
y_{it}=\alpha_{it}+x_{it}\beta_{it}+\epsilon_{it}
\]

\end_inset

The problem here is that there are more parameters than observations,
 so the model is not identified.
 We need some restraint!
 The proper restrictions to use of course depend on the problem at hand,
 and a single model is unlikely to be appropriate for all situations.
 For example,
 one could have time and cross-sectional indicator variables,
 and slopes that are constant over time and across agents:
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+\gamma_{t}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

(in class,
 write out the form of the model in matrix notation).
 
\end_layout

\begin_layout Standard
There is a lot of room for playing around here.
 We also need to consider whether or not 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

 are fixed or growing.
 We'll need at least one of them to be growing in order to do asymptotics.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To provide some focus,
 we'll consider common slope parameters,
 but agent-specific intercepts,
 which:
\begin_inset Formula 
\begin{equation}
y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}\label{eq:simple linear panel model}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
I will refer to this as the 
\begin_inset Quotes sld
\end_inset

simple linear panel model
\begin_inset Quotes srd
\end_inset

.
 We assume that the regressors 
\begin_inset Formula $x_{it}$
\end_inset

 are exogenous,
 with no correlation with the error term.
\end_layout

\begin_layout Itemize
This is the model most often encountered in the applied literature.
 It is like the original cross-sectional model,
 in that the 
\begin_inset Formula $\beta's$
\end_inset

 are constant over time for all 
\begin_inset Formula $i.$
\end_inset

 However we're now allowing for the constant to vary across 
\begin_inset Formula $i$
\end_inset

 (some individual heterogeneity).
\end_layout

\begin_layout Itemize
We can consider what happens as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 but 
\begin_inset Formula $T$
\end_inset

 is fixed.
 This would be relevant for microeconometric panels,
 (e.g.,
 the PSID data) where a survey of a large number of individuals may be done for a limited number of time periods.
 
\end_layout

\begin_layout Itemize
Macroeconometric applications might look at longer time series for a small number of cross-sectional units (e.g.,
 40 years of quarterly data for 15 European countries).
 For that case,
 we could keep 
\begin_inset Formula $n$
\end_inset

 fixed (seems appropriate when dealing with the EU countries),
 and do asymptotics as 
\begin_inset Formula $T$
\end_inset

 increases,
 as is normal for time series.
\end_layout

\begin_layout Itemize
The asymptotic results depend on how we do this,
 of course.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Why bother using panel data,
 what are the benefits?

\series default
 
\end_layout

\begin_layout Standard
The model 
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

 is a restricted version of
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+x_{it}\beta_{i}+\epsilon_{it}
\]

\end_inset

which could be estimated for each 
\begin_inset Formula $i$
\end_inset

 in turn,
 using time series data.
 Why use the panel approach?
\end_layout

\begin_layout Itemize
Because the restrictions that 
\begin_inset Formula $\beta_{i}=\beta_{j}=...=\beta,$
\end_inset

 if true,
 lead to more efficient estimation.
 Estimation for each 
\begin_inset Formula $i$
\end_inset

 in turn will be very uninformative if 
\begin_inset Formula $T$
\end_inset

 is small.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Another reason is that panel data allows us to estimate parameters that are not identified by cross sectional (time series) data.
 
\end_layout

\begin_layout Itemize
For example,
 if the model is 
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+\gamma_{t}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

and we have only cross sectional data,
 we cannot estimate the 
\begin_inset Formula $\alpha_{i}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
If we have only time series data on a single cross sectional unit 
\begin_inset Formula $i=1$
\end_inset

,
 we cannot estimate the 
\begin_inset Formula $\gamma_{t}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Cross-sectional variation allows us to estimate parameters indexed by time,
 and time series variation allows us to estimate parameters indexed by cross-sectional unit.
 Parameters indexed by both 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 will require other forms of restrictions in order to be estimable.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
A 
\series bold
very important reason
\series default
 is that 
\begin_inset Formula $\alpha_{i}$
\end_inset

 can absorb any missing variables in the regression that don't change over time,
 and 
\begin_inset Formula $\gamma_{t}$
\end_inset

 can absorb missing variables that don't change across 
\begin_inset Formula $i$
\end_inset

.
 For example,
 suppose we have the model
\begin_inset Formula 
\begin{equation}
y_{it}=\alpha+x_{it}\beta+{\color{red}z_{i}}\gamma+\epsilon_{it}\label{eq:simple panel model}
\end{equation}

\end_inset

where the variables in 
\begin_inset Formula ${\color{red}z_{i}}$
\end_inset

 are unobserved,
 but are
\bar under
 constant over time
\bar default
.
 
\end_layout

\begin_layout Itemize
Assume that,
 as is 
\bar under
usually the case
\bar default
,
 there is some correlation between the variables in 
\begin_inset Formula $x_{it}$
\end_inset

 and 
\begin_inset Formula $z_{i}$
\end_inset

.
 That is to say,
 there is some ordinary collinearity of the regressors.
\end_layout

\begin_layout Itemize
If we have only one time period,
 then we have to estimate the model 
\begin_inset Formula 
\[
y_{i}=\alpha+x_{i}\beta+z_{i}\gamma+\epsilon_{i}
\]

\end_inset

using the observations 
\begin_inset Formula $i=1,2,...,n$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
Because 
\begin_inset Formula $z_{i}$
\end_inset

 is unobserved,
 we have to let it be absorbed in the error term.
 For convenience,
 and to keep the notation simple,
 assume that the mean of 
\begin_inset Formula $z_{i}\gamma$
\end_inset

 is zero (this does not affect the argument in any important way),
 so the model we can actually estimate is 
\begin_inset Formula 
\begin{align*}
y_{i} & =\text{\ensuremath{\alpha}}+x_{i}\beta+v_{i}
\end{align*}

\end_inset

where 
\begin_inset Formula $v_{i}=z_{i}\gamma+\epsilon_{i}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
This model has correlation between the regressors and the error,
 so the OLS estimates would be inconsistent.
 Furthermore,
 we don't have any natural instruments to estimate the model by IV.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
However,
 
\series bold
suppose we have at least two time periods
\series default
 of data,
 and 
\begin_inset Formula $n$
\end_inset

 cross-sectional observations.
 Then,
 we can let 
\begin_inset Formula $z_{i}\gamma$
\end_inset

 move into the constant,
 and we get the model
\begin_inset Formula 
\begin{align*}
y_{it} & =\alpha+x_{it}\beta+z_{i}\gamma+\epsilon_{it}\\
y_{it} & =\alpha_{i}+x_{it}\beta+\epsilon_{it}
\end{align*}

\end_inset

where 
\begin_inset Formula $\alpha_{i}=\alpha+z_{i}\gamma$
\end_inset

.
 This is the simple linear panel data model.
\end_layout

\begin_deeper
\begin_layout Itemize
Notice that the problematic 
\begin_inset Formula $z_{i}$
\end_inset

 have now disappeared!
\end_layout

\begin_layout Itemize
It turns out that OLS estimation of this model will give consistent estimates of the 
\begin_inset Formula $\beta$
\end_inset

 parameters,
 as the cross sectional size of the sample,
 
\begin_inset Formula $n$
\end_inset

 increases,
 as long as the regressors are exogenous.
 If it's not clear how this can be estimated by OLS,
 then consider estimating it using first differences:
 that model is pretty obviously consistently estimable using OLS.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Returing to panel data in general,

\series bold
 the main issues are:
\end_layout

\begin_layout Itemize
can 
\begin_inset Formula $\beta$
\end_inset

 be estimated consistently?
 This is almost always a goal.
\end_layout

\begin_layout Itemize
can the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 be estimated consistently?
 This is often of secondary interest.
\end_layout

\begin_layout Itemize
sometimes,
 we're interested in estimating the distribution of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 across 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
are the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 correlated with 
\begin_inset Formula $x_{it}$
\end_inset

?
 This is very likely the case.
\end_layout

\begin_layout Itemize
does the presence of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 complicate estimation of 
\begin_inset Formula $\beta$
\end_inset

?
\end_layout

\begin_layout Standard
what about the covariance structure?
\end_layout

\begin_layout Itemize
We're likely to have both HET and AUT,
 in the original model,
 so GLS issues will probably be relevant.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Potential for efficiency gains
\end_layout

\begin_layout Itemize
need to take care of it to obtain valid standard errors.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Static models and correlations between variables
\end_layout

\begin_layout Standard
To begin with,
 assume that:
\end_layout

\begin_layout Itemize
the 
\begin_inset Formula $x_{it}$
\end_inset

 are weakly exogenous variables (uncorrelated with 
\begin_inset Formula $\epsilon_{it})$
\end_inset


\end_layout

\begin_layout Itemize
the model is static:
 
\begin_inset Formula $x_{it}$
\end_inset

 does not contain lags of 
\begin_inset Formula $y_{it}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
then the basic problem we have in the panel data model 
\begin_inset Formula $y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}$
\end_inset

 is the presence of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

.
 These are individual-specific parameters.
 Or,
 possibly more accurately,
 they can be thought of as individual-specific variables that are not observed (latent variables).
 The reason for thinking of them as variables is because the agent may choose their values following some process,
 or may choose other variable taking these ones as given.
\end_layout

\begin_layout Standard
Define 
\begin_inset Formula $\alpha=E(\alpha_{i})$
\end_inset

,
 so 
\begin_inset Formula $E({\color{blue}\alpha_{i}-\alpha})=0,$
\end_inset

 where the expectation is with respect to the density that describes the distribution of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 in the population.
 Our model 
\begin_inset Formula $y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}$
\end_inset

 may be written 
\begin_inset Formula 
\begin{align*}
y_{it} & =\alpha_{i}+x_{it}\beta+\epsilon_{it}\\
 & =\alpha+x_{it}\beta+({\color{blue}\alpha_{i}-\alpha}+\epsilon_{it})\\
 & =\alpha+x_{it}\beta+\eta_{it}
\end{align*}

\end_inset

Note that 
\begin_inset Formula $E(\eta_{it})=0.$
\end_inset

 A way of thinking about the data generating process is this:
\end_layout

\begin_layout Itemize
First,
 
\begin_inset Formula $\alpha_{i}$
\end_inset

 is drawn,
 from the population density
\end_layout

\begin_layout Itemize
then 
\begin_inset Formula $T$
\end_inset

 values of 
\begin_inset Formula $x_{it}$
\end_inset

 are drawn from 
\begin_inset Formula $f_{X}(z|\alpha_{i}).$
\end_inset

 
\end_layout

\begin_layout Itemize
the important point is that the distribution of 
\begin_inset Formula $x$
\end_inset

 
\emph on
may vary depending on the realization of 
\begin_inset Formula $\alpha_{i}$
\end_inset

.

\emph default
 
\end_layout

\begin_layout Itemize
For example,
 if 
\begin_inset Formula $y$
\end_inset

 is the quantity demanded of a luxury good,
 then a high value of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 means that agent 
\begin_inset Formula $i$
\end_inset

 will buy a large quantity,
 on average.
 This may be possible only when the agent's income is also high.
 Thus,
 it may be possible to draw high values of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 only when income is also high,
 otherwise,
 the budget constraint would be violated.
 If income is one of the variables in 
\begin_inset Formula $x_{it},$
\end_inset

 then 
\begin_inset Formula $\alpha_{i}$
\end_inset

 and 
\begin_inset Formula $x_{it}$
\end_inset

 are not independent.
\end_layout

\begin_layout Itemize
Another example:
 consider returns to education,
 modeling wage as a function of education.
 
\begin_inset Formula $\alpha_{i}$
\end_inset

 could be an individual specific measure of ability.
 Ability could affect wages,
 but it could also affect the number of years of education.
 When education is a regressor and ability is a component of the error,
 we may expect an endogeneity problem.
\end_layout

\begin_layout Itemize
Thus,
 there may be correlation between 
\begin_inset Formula $\alpha_{i}$
\end_inset

 and 
\begin_inset Formula $x_{it}$
\end_inset

,
 in which case 
\begin_inset Formula $E(x_{it}\eta_{it})\ne$
\end_inset

0 in the above equation.
 
\end_layout

\begin_deeper
\begin_layout Itemize
This means that OLS estimation of the model would lead to biased and inconsistent estimates.
 
\end_layout

\begin_layout Itemize
However,
 it is possible (but unlikely for economic data) that 
\begin_inset Formula $x_{it}$
\end_inset

 and 
\begin_inset Formula $\eta_{it}$
\end_inset

 are independent or at least uncorrelated,
 if the distribution of 
\begin_inset Formula $x_{it}$
\end_inset

 is constant with respect to the realization of 
\begin_inset Formula $\alpha_{i}$
\end_inset

.
 In this case OLS estimation would be consistent.
\end_layout

\end_deeper
\begin_layout Standard

\series bold
\begin_inset Newpage newpage
\end_inset

Fixed effects
\series default
:
 when 
\begin_inset Formula $E($
\end_inset


\begin_inset Formula $\eta_{it}|x_{it})\ne$
\end_inset

0,
 the model is called the 
\begin_inset Quotes sld
\end_inset

fixed effects model
\begin_inset Quotes srd
\end_inset


\end_layout

\begin_layout Standard

\series bold
Random effects
\series default
:
 when 
\begin_inset Formula $E(\eta_{it}|x_{it})=0,$
\end_inset

 the model is called the 
\begin_inset Quotes sld
\end_inset

random effects model
\begin_inset Quotes srd
\end_inset

.
\end_layout

\begin_layout Standard
I find this to be pretty poor nomenclature,
 because the issue is not whether 
\begin_inset Quotes sld
\end_inset

effects
\begin_inset Quotes srd
\end_inset

 are fixed or random (they are always random,
 unconditional on 
\begin_inset Formula $i$
\end_inset

).
 The issue is whether or not the 
\begin_inset Quotes sld
\end_inset

effects
\begin_inset Quotes srd
\end_inset

 are correlated with the other regressors.
 
\end_layout

\begin_layout Itemize
In economics,
 it seems likely that the unobserved variable 
\begin_inset Formula $\alpha$
\end_inset

 is probably correlated with the observed regressors,
 
\begin_inset Formula $x$
\end_inset

 (this is simply the presence of collinearity between observed and unobserved variables,
 and collinearity is usually the rule rather than the exception).
\end_layout

\begin_layout Itemize
So,
 we expect that the 
\begin_inset Quotes sld
\end_inset

fixed effects
\begin_inset Quotes srd
\end_inset

 model is probably the relevant one unless special circumstances (e.g.,
 if one were working with experimental data) imply that the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are uncorrelated with the 
\begin_inset Formula $x_{it}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Estimation of the simple linear panel model
\end_layout

\begin_layout Subsection
\begin_inset Quotes sld
\end_inset

Fixed effects
\begin_inset Quotes srd
\end_inset

:
 The 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator
\end_layout

\begin_layout Standard
How can we estimate the parameters of the simple linear panel model (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:simple linear panel model"
nolink "false"

\end_inset

) and what properties do the estimators have?
 First,
 we assume that the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are correlated with the 
\begin_inset Formula $x_{it}$
\end_inset

 (
\begin_inset Quotes sld
\end_inset

fixed effects
\begin_inset Quotes srd
\end_inset

 model ).
 The model can be written as 
\begin_inset Formula $y_{it}=\alpha+x_{it}\beta+\eta_{it}$
\end_inset

,
 and we have that 
\begin_inset Formula $E(x_{it}\eta_{it})\ne$
\end_inset

0.
 As such,
 OLS estimation of this model will give biased an inconsistent estimated of the parameters 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator is a solution.
 First,
 go back to the original formulation of the model:
 
\begin_inset Formula $y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}$
\end_inset

 .
 The within estimator  involves subtracting the time series average from each cross sectional unit.
 
\begin_inset Formula 
\begin{align}
\overline{x}_{i} & =\frac{1}{T}\sum_{t=1}^{T}x_{it}\nonumber \\
\overline{\epsilon_{i}} & =\frac{1}{T}\sum_{t=1}^{T}\epsilon_{it}\nonumber \\
\overline{y}_{i} & =\frac{1}{T}\sum_{t=1}^{T}y_{it}=\alpha_{i}+\frac{1}{T}\sum_{t=1}^{T}x_{it}\beta+\frac{1}{T}\sum_{t=1}^{T}\epsilon_{it}\nonumber \\
\overline{y}_{i} & =\alpha_{i}+\overline{x}_{i}\beta+\overline{\epsilon_{i}}\label{eq:time averages}
\end{align}

\end_inset

The transformed model is
\begin_inset Formula 
\begin{align}
y_{it}-\overline{y}_{i} & =\alpha_{i}+x_{it}\beta+\epsilon_{it}-\alpha_{i}-\overline{x}_{i}\beta-\overline{\epsilon_{i}}\label{eq:within estimator}\\
y_{it}^{*} & =x_{it}^{*}\beta+\epsilon_{it}^{*}\nonumber 
\end{align}

\end_inset

where 
\begin_inset Formula $x_{it}^{*}=x_{it}-\overline{x}_{i}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{it}^{*}=\epsilon_{it}-\overline{\epsilon}_{i}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
In this model,
 it is clear that 
\begin_inset Formula $x_{it}^{*}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{it}^{*}$
\end_inset

 are uncorrelated,
 as long as the original regressors 
\begin_inset Formula $x_{it}$
\end_inset

 are 
\emph on
strongly
\emph default
 exogenous with respect to the original error 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 (
\begin_inset Formula $E(\epsilon_{is}|x_{it})=0,\,\forall t,s$
\end_inset

).
 In this case,
 OLS will give consistent estimates of the parameters of this model,
 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Exercise
Explain why we need strong exogeneity of the 
\begin_inset Formula $x_{it}$
\end_inset

 with respect to 
\begin_inset Formula $\epsilon_{it}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
What about the 
\begin_inset Formula $\alpha_{i}?$
\end_inset

 Can they be consistently estimated?
 An estimator is
\begin_inset Formula 
\[
\hat{\alpha}_{i}=\frac{1}{T}\sum_{t=1}^{T}\left(y_{it}-x_{it}\hat{\beta}\right)
\]

\end_inset

It's fairly obvious that this is a consistent estimator 
\emph on
if 
\emph default

\begin_inset Formula $T\rightarrow\infty$
\end_inset

.
 For a short panel with fixed 
\begin_inset Formula $T,$
\end_inset

 this estimator is not consistent.
 Nevertheless,
 the variation in the 
\begin_inset Formula $\hat{\alpha}_{i}$
\end_inset

 can be fairly informative about the heterogeneity.
 A couple of notes:
\end_layout

\begin_layout Itemize
an equivalent approach is to estimate the model
\begin_inset Formula 
\[
y_{it}=\sum_{j=1}^{n}d_{j,it}\alpha_{j}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

 by OLS.
 The 
\begin_inset Formula $d_{j}$
\end_inset

,
 
\begin_inset Formula $j=1,2,...,n$
\end_inset

 are 
\begin_inset Formula $n$
\end_inset

 indicator variables that take on the value 
\begin_inset Formula $1$
\end_inset

 if 
\begin_inset Formula $j=i,$
\end_inset

 zero otherwise.
 They are indicators of the cross sectional unit of the observation.
 For example,
 with 3 cross sectional units and 3 time periods,
 and a single 
\begin_inset Formula $x$
\end_inset

 regressor,
 the model in matrix form would look like 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y_{11}\\
y_{12}\\
y_{13}\\
y_{21}\\
y_{22}\\
y_{23}\\
y_{31}\\
y_{32}\\
y_{33}
\end{array}\right]=\left[\begin{array}{cccc}
1 & 0 & 0 & x_{11}\\
1 & 0 & 0 & x_{12}\\
1 & 0 & 0 & x_{13}\\
0 & 1 & 0 & x_{21}\\
0 & 1 & 0 & x_{22}\\
0 & 1 & 0 & x_{23}\\
0 & 0 & 1 & x_{31}\\
0 & 0 & 1 & x_{32}\\
0 & 0 & 1 & x_{33}
\end{array}\right]\left[\begin{array}{c}
\alpha_{1}\\
\alpha_{2}\\
\alpha_{3}\\
\beta
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{11}\\
\epsilon_{12}\\
\epsilon_{13}\\
\epsilon_{21}\\
\epsilon_{22}\\
\epsilon_{23}\\
\epsilon_{31}\\
\epsilon_{32}\\
\epsilon_{33}
\end{array}\right]
\]

\end_inset

Estimating this model directly by OLS gives numerically exactly the same results as the OLS version of the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator,
 and you get the 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\hat{\alpha}_{i}$
\end_inset

 automatically.
 See Cameron and Trivedi,
 section 21.6.4 for details.
 An interesting and important result known as the Frisch-Waugh-Lovell Theorem can be used to show that the two means of estimation give identical results.
\end_layout

\begin_layout Itemize
This last expression makes it clear why the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator cannot estimate slope coefficients corresponding to variables that have no time variation.
 Such variables are perfectly collinear with the cross sectional indicators 
\begin_inset Formula $d_{j}$
\end_inset

.
 The corresponding coefficients are not identified.
\end_layout

\begin_layout Itemize
OLS estimation of the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 model is consistent,
 but probably not efficient,
 because it is highly probable that the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not iid.
 There is very likely heteroscedasticity across the 
\begin_inset Formula $i$
\end_inset

 and autocorrelation between the 
\begin_inset Formula $T$
\end_inset

 observations corresponding to a given 
\begin_inset Formula $i$
\end_inset

.
 
\emph on
One needs to estimate the covariance matrix of the parameter estimates taking this into account
\emph default
.
 
\end_layout

\begin_deeper
\begin_layout Itemize
at a minimum,
 robust standard errors will be needed,
 to be able to get valid standard errors and to be able to test restrictions.
 We need panel robust standard errors,
 which are also referred to as cluster robust standard errors.
\end_layout

\begin_layout Itemize
It is possible to use GLS corrections if you make assumptions regarding the het.
 and autocor.
 Quasi-GLS,
 using a possibly misspecified model of the error covariance,
 can lead to more efficient estimates than simple OLS.
 One can then combine it with subsequent panel-robust covariance estimation to deal with the misspecification of the error covariance,
 which would invalidate inferences if ignored.
 The White heteroscedasticity consistent covariance estimator is easily extended to panel data with independence across 
\begin_inset Formula $i$
\end_inset

,
 but with heteroscedasticity and autocorrelation within 
\begin_inset Formula $i$
\end_inset

,
 and heteroscedasticity between 
\begin_inset Formula $i.$
\end_inset

 See Cameron and Trivedi,
 Section 21.2.3.
\series bold

\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Estimation with random effects
\end_layout

\begin_layout Standard
The original model is
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

This can be written as 
\begin_inset Formula 
\begin{eqnarray}
y_{it} & = & \alpha+x_{it}\beta+\left(\alpha_{i}-\alpha+\epsilon_{it}\right)\nonumber \\
y_{it} & = & \alpha+x_{it}\beta+\eta_{it}\label{eq:simple linear panel random effects}
\end{eqnarray}

\end_inset

Under random effects,
 the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are assumed not to be correlated with the 
\begin_inset Formula $x_{it},$
\end_inset

 so 
\begin_inset Formula $E(\eta_{it})=0,$
\end_inset

 and 
\begin_inset Formula $E(\eta_{it}|x_{it})=0$
\end_inset

.
 As such,
 the OLS estimator of this model is consistent.
 We can recover estimates of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 as discussed above.
 It is to be noted that the error 
\begin_inset Formula $\eta_{it}$
\end_inset

 is almost certainly heteroscedastic and autocorrelated,
 so OLS will not be efficient,
 and inferences based on OLS need to be done taking this into account.
 One could attempt to use GLS,
 or panel-robust covariance matrix estimation,
 or both,
 as above.
\end_layout

\begin_layout Standard
There are other estimators when we have random effects,
 a well-known example being the 
\begin_inset Quotes sld
\end_inset

between
\begin_inset Quotes srd
\end_inset

 estimator,
 which operates on the time averages of the cross sectional units.
 There is no advantage to doing this,
 as the overall estimator is already consistent,
 and averaging looses information (efficiency loss).
 One would still need to deal with cross sectional heteroscedasticity when using the between estimator,
 so there is no gain in simplicity,
 either.
\end_layout

\begin_layout Standard
It is to be emphasized that 
\begin_inset Quotes sld
\end_inset

random effects
\begin_inset Quotes srd
\end_inset

 
\emph on
is not a plausible assumption
\emph default
 with most economic data,
 so use of this estimator is discouraged,
 even if your statistical package offers it as an option.
 Think carefully about whether the assumption is warranted before trusting the results of this estimator.
\series bold

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Hausman test
\end_layout

\begin_layout Standard
Suppose you're doubting about whether fixed or random effects are present.
\end_layout

\begin_layout Itemize
If we have correlation between 
\begin_inset Formula $x_{it}$
\end_inset

 and 
\begin_inset Formula $\alpha_{i}$
\end_inset

 (fixed effects),
 then the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator will be consistent,
 but the random effects estimator of the previous section will not.
 
\end_layout

\begin_layout Itemize
Evidence that the two estimators are converging to different limits is evidence in favor of fixed effects,
 not random effects.
\end_layout

\begin_layout Itemize
A Hausman test statistic can be computed,
 using the difference between the two estimators.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The null hypothesis is that the effects are uncorrelated with the regressors in 
\begin_inset Formula $x_{it}$
\end_inset

 (
\begin_inset Quotes sld
\end_inset

random effects
\begin_inset Quotes srd
\end_inset

) so that both estimators are consistent under the null.
 
\end_layout

\begin_layout Itemize
When the test rejects,
 we conclude that fixed effects are present,
 so the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator should be used.
 
\end_layout

\begin_layout Itemize
Now,
 what happens if the test does not reject?
 One could optimistically turn to the random effects model,
 but it's probably more realistic to conclude that the test may have low power.
 Failure to reject does not mean that the null hypothesis is true.
 After all,
 estimation of the covariance matrices needed to compute the Hausman test is a non-trivial issue,
 and is a source of considerable noise in the test statistic (noise=low power).
\end_layout

\begin_layout Itemize
Finally,
 the simple version of the Hausman test requires that the estimator under the null be fully efficient.
 Achieving this goal is probably a utopian prospect.
 A conservative approach would acknowledge that neither estimator is likely to be efficient,
 and to operate accordingly.
 I have a little paper on this topic,
 Creel,
 
\emph on
Applied Economics,
 
\emph default
2004.
 See also Cameron and Trivedi,
 section 21.4.3.
\end_layout

\end_deeper
\begin_layout Standard

\series bold
In class,
 do the first part of the example at the end of the chapter at this time
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Dynamic panel data
\end_layout

\begin_layout Standard
When we have panel data,
 we have information on both 
\begin_inset Formula $y_{it}$
\end_inset

 as well as 
\begin_inset Formula $y_{i,t-1}$
\end_inset

.
 One may naturally think of including 
\begin_inset Formula $y_{i,t-1}$
\end_inset

 as a regressor,
 to capture dynamic effects that can't be analyed with only cross-sectional data.
 Excluding dynamic effects is often the reason for detection of spurious AUT of the errors.
 With dynamics,
 there is likely to be less of a problem of autocorrelation,
 but one should still be concerned that some might still be present.
 The model,
 using a single lag of the dependent variable,
 becomes
\begin_inset Formula 
\begin{eqnarray*}
y_{it} & = & \alpha_{i}+\gamma y_{i,t-1}+x_{it}\beta+\epsilon_{it}\\
y_{it} & = & \alpha+\gamma y_{i,t-1}+x_{it}\beta+\left(\alpha_{i}-\alpha+\epsilon_{it}\right)\\
y_{it} & = & \alpha+\gamma y_{i,t-1}+x_{it}\beta+\eta_{it}
\end{eqnarray*}

\end_inset

We assume that the 
\begin_inset Formula $x_{it}$
\end_inset

 are uncorrelated with 
\begin_inset Formula $\epsilon_{it}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\alpha_{i}$
\end_inset

 is a component that determines both 
\begin_inset Formula $y_{it}$
\end_inset

 and its lag,
 
\begin_inset Formula $y_{i,t-1}$
\end_inset

.
 Thus,
 
\begin_inset Formula $\alpha_{i}$
\end_inset

 and 
\begin_inset Formula $y_{i,t-1}$
\end_inset

 are correlated,
 even if the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are pure random effects (uncorrelated with 
\begin_inset Formula $x_{it}$
\end_inset

).
\end_layout

\begin_layout Itemize
So,
 
\begin_inset Formula $y_{i,t-1}$
\end_inset

 is correlated with 
\begin_inset Formula $\eta_{it}$
\end_inset

.
\end_layout

\begin_layout Itemize
For this reason,
 OLS estimation is inconsistent even for the random effects model,
 and it's also of course still inconsistent for the fixed effects model.
 
\end_layout

\begin_layout Itemize
When regressors are correlated with the errors,
 the natural thing to do is start thinking of instrumental variables estimation,
 or GMM.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Arellano-Bond estimator
\end_layout

\begin_layout Standard
The first thing is to realize that the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 that are a component of the error are correlated with all regressors in the general case of fixed effects.
 Getting rid of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 is a step in the direction of solving the problem.
 We could subtract the time averages,
 as above for the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator,
 but this would give us problems later when we need to define instruments.
 Instead,
 consider the model in first differences
\begin_inset Formula 
\begin{eqnarray*}
y_{it}-y_{i,t-1} & = & \left(\alpha_{i}+\gamma y_{i,t-1}+x_{it}\beta+\epsilon_{it}\right)-\left(\alpha_{i}+\gamma y_{i,t-2}+x_{i,t-1}\beta+\epsilon_{i,t-1}\right)\\
 & = & \gamma\left(y_{i,t-1}-y_{i,t-2}\right)+\left(x_{it}-x_{i,t-1}\right)\beta+\epsilon_{it}-\epsilon_{i,t-1}
\end{eqnarray*}

\end_inset

or
\begin_inset Formula 
\[
\Delta y_{it}=\gamma\Delta y_{i,t-1}+\Delta x_{it}\beta+\Delta\epsilon_{it}
\]

\end_inset


\end_layout

\begin_layout Itemize
Now the pesky 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are no longer in the picture.
 
\end_layout

\begin_layout Itemize
Note that we loose one observation when doing first differencing.
\end_layout

\begin_layout Itemize
OLS estimation of this model will still be inconsistent,
 because 
\begin_inset Formula $y_{i,t-1}$
\end_inset

 is clearly correlated with 
\begin_inset Formula $\epsilon_{i,t-1}.$
\end_inset

 
\end_layout

\begin_layout Itemize
Note also that the error 
\begin_inset Formula $\Delta\epsilon_{it}$
\end_inset

 is serially correlated even if the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not,
 as it follows an MA(1) model.
 
\end_layout

\begin_layout Itemize
There is no problem of correlation between 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\Delta x_{it}$
\end_inset

 and 
\begin_inset Formula $\Delta\epsilon_{it}$
\end_inset

.
 Thus,
 to do GMM,
 we need to find instruments for 
\begin_inset Formula $\Delta y_{i,t-1}$
\end_inset

,
 but the variables in 
\begin_inset Formula $\Delta x_{it}$
\end_inset

 can serve as their own instruments.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

How about using 
\color blue

\begin_inset Formula $y_{i.t-2}$
\end_inset


\color inherit
 as an instrument?
\end_layout

\begin_layout Itemize
It is clearly correlated with 
\begin_inset Formula $\Delta y_{i,t-1}=\left(y_{i,t-1}-{\color{blue}y_{i,t-2}}\right)$
\end_inset


\end_layout

\begin_layout Itemize

\emph on
as long as the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not serially correlated
\emph default
,
 then 
\begin_inset Formula $y_{i,t-2}$
\end_inset

 is not correlated with 
\begin_inset Formula $\Delta\epsilon_{it}=\epsilon_{it}-\epsilon_{i,t-1}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
We can also use additional lags 
\begin_inset Formula $y_{i,t-s}$
\end_inset

,
 
\begin_inset Formula $s\ge2$
\end_inset

 to increase efficiency,
 because GMM with additional instruments is asymptotically more efficient than with less instruments (but small sample bias may become a serious problem).
\end_layout

\begin_layout Standard
This sort of estimator is widely known in the literature as an Arellano-Bond estimator,
 due to the influential 1991 paper of Arellano and Bond (1991).
\end_layout

\begin_layout Itemize
Note that this sort of estimators requires 
\begin_inset Formula $T=3$
\end_inset

 at a minimum.
 
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $t=1$
\end_inset

 and 
\begin_inset Formula $t=2,$
\end_inset

 we cannot compute the moment conditions.
\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $t=1,$
\end_inset

 we do not have 
\begin_inset Formula $y_{i,t-1}=y_{i,0},$
\end_inset

 so we can't compute dependent variable.
\end_layout

\begin_layout Itemize
for 
\begin_inset Formula $t=2,$
\end_inset

 we can compute the dependent variable 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\Delta y_{i2}$
\end_inset

,
 but not the regressor 
\begin_inset Formula $\Delta y_{i,2-1}=y_{i,1}-y_{i,0}.$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
for 
\begin_inset Formula $t=3,$
\end_inset

 we can compute the dep.
 var.
 
\begin_inset Formula $\Delta y_{i,3}$
\end_inset

,
 the regressor 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\Delta y_{i,2}=y_{i,2}-y_{i,1},$
\end_inset

 and we have 
\begin_inset Formula $y_{i,1,}$
\end_inset

 to serve as an instrument for 
\begin_inset Formula $\Delta y_{i,2}$
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $T>3,$
\end_inset

 then when 
\begin_inset Formula $t=4,$
\end_inset

 we can use both 
\begin_inset Formula $y_{i,1}$
\end_inset

 and 
\begin_inset Formula $y_{i,2}$
\end_inset

 as instruments.
 This sort of unbalancedness in the instruments requires a bit of care when programming.
 Also,
 additional instruments increase asymptotic efficiency but can lead to increased small sample bias,
 so one should be a little careful with using too many instruments.
 Some robustness checks,
 looking at the stability of the estimates are a way to proceed.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
One should note that serial correlation of the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 will cause this estimator to be inconsistent,
 as the instruments will not be valid in this case.
\end_layout

\begin_layout Itemize
Serial correlation of the errors 
\emph on
may 
\emph default
be due to dynamic misspecification,
 and this can be solved by including additional lags of the dependent variable.
 However,
 too many lags leads to a reduction of the sample size,
 so there's a limit to what can be done without having variances explode.
 However,
 serial correlation may also be due to factors not captured in lags of the dependent variable.
 If this is a possibility,
 then the validity of the Arellano-Bond type instruments is in question.
\end_layout

\begin_layout Itemize
A final note is that the error 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\Delta\epsilon_{it}$
\end_inset

 is serially correlated even when the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not,
 and very likely heteroscedastic across 
\begin_inset Formula $i$
\end_inset

.
 One needs to take this into account when computing the covariance of the GMM estimator.
 One can also attempt to use GLS style weighting to improve efficiency.
 There are many possibilities.
\end_layout

\begin_layout Itemize
there is a 
\begin_inset Quotes sld
\end_inset

system
\begin_inset Quotes srd
\end_inset

 version of this sort of estimator that adds additional moment conditions,
 to improve efficiency
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
Arellano-Bond data
\end_layout

\begin_layout Standard
Use the GRETL data set abdata.gdt to illustrate fixed effects,
 random effects,
 and DPD estimation For FE and RE,
 use the model 
\begin_inset Formula 
\[
n_{it}=\alpha_{i}+\beta_{t}+\gamma w_{it}+\delta k_{it}+\phi ys_{it}+\epsilon_{it}
\]

\end_inset


\end_layout

\begin_layout Itemize
open abdata.gdt in GRETL
\end_layout

\begin_layout Itemize
read dataset info:
 9 years of data on 140 companies in manufacturing sector (different industries).
\end_layout

\begin_deeper
\begin_layout Itemize
examine the variables:
 note that the data set is not 
\begin_inset Quotes sld
\end_inset

balanced
\begin_inset Quotes srd
\end_inset

:
 some companies are not observed in some years
\end_layout

\begin_layout Itemize
taking care of this problem is annoying without using a well written panel data package.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
estimate fixed effects
\end_layout

\begin_deeper
\begin_layout Itemize
note the pattern of the coefficients of the yearly indicators:
 the 
\begin_inset Quotes sld
\end_inset

Margaret Thatcher effect
\begin_inset Quotes srd
\end_inset


\end_layout

\begin_layout Itemize
signs of coefficients seem ok.
 Exogeneity to be trusted?
\end_layout

\begin_layout Itemize
save fixed effects,
 save residuals
\end_layout

\begin_layout Itemize
do residuals appear to be normally distributed?
 Test,
 and nonparametric density.
 If not normal,
 then random effects is not fully efficient,
 even if exogeneity of effects is valid.
\end_layout

\begin_layout Itemize
is there evidence of serial correlation of residuals?
 Run AR(1) on residuals:
 significant autocorrelation.
 Suggests an omitted dynamic effect.
\end_layout

\begin_layout Itemize
do nonparametric density plot of fixed effects:
 mean is 1,
 but significant variation across companies (different industries have different labor intensity)
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
run random effects (tradition,
 but not logic,
 demands that we do it)
\end_layout

\begin_deeper
\begin_layout Itemize
Hausman test:
 rejects RE (unsurprisingly):
 we should favor FE.
 
\end_layout

\begin_deeper
\begin_layout Itemize
However,
 if errors are not normal (they are not,
 according to test)
\end_layout

\begin_layout Itemize
or if there is serial correlation (there is,
 according to AR1 fit)
\end_layout

\begin_layout Itemize
then the test is not valid.
 
\end_layout

\end_deeper
\begin_layout Itemize
Nevertheless,
 FE is probably favored on strictly theoretical grounds.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Given that the residuals seem to be serially correlated,
 we probably need to introduce dynamic structure.
 For DPD,
 use the model 
\begin_inset Formula 
\[
n_{it}=\alpha_{i}+\beta_{t}+\rho_{1}n_{i,t-1}+\gamma w_{it}+\delta k_{it}+\phi ys_{it}+\epsilon_{it}
\]

\end_inset


\end_layout

\begin_layout Itemize
the estimate of 
\begin_inset Formula $\rho_{1}$
\end_inset

 is economically and statistically significant
\end_layout

\begin_layout Itemize
note the important differences in the other coefficients compared to the FE model
\end_layout

\begin_layout Standard
check the serial correlation of the residuals:
 if it exists,
 the instruments are not valid.
 Possible solution is to augment the AR order,
 but the sample size gets smaller with every additional lag.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Stock and Watson Beer tax example
\end_layout

\begin_layout Standard
Stock and Watson 
\begin_inset Quotes eld
\end_inset

Introduction to Econometrics
\begin_inset Quotes erd
\end_inset

,
 3rd Edition,
 discuss panel data in Chapter 10.
 The book is available from the UAB library 
\begin_inset CommandInset href
LatexCommand href
name "as an e-text"
target "https://ebookcentral.proquest.com/lib/uab/detail.action?docID=5174962#"
literal "false"

\end_inset

 (if you happen to be at the UAB).
 They use data which is available as a GRETL file 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/PanelData/fatality.gdt}{here}
\end_layout

\end_inset

.
 Using this data,
 and the software of your choice (hint:
 GRETL is easy)
\end_layout

\begin_layout Enumerate
replicate the results in Stock and Watson,
 Ch.
 10,
 Eqn.
 10.2 and Figure 10.1
\end_layout

\begin_layout Enumerate
replicate the results in Eqn 10.8 and Figure 10.2.
\end_layout

\begin_layout Enumerate
replicate the fixed effects panel data results in column 2 of Table 10.1
\end_layout

\begin_layout Standard
A video which shows how to do this is 
\begin_inset CommandInset href
LatexCommand href
name "here."
target "https://youtu.be/1qdsiMg1ApU"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical summary
\end_layout

\begin_layout Standard
The practical summary for the Chapter is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./PracticalSummaries/20-PanelData.jl}{here}
\end_layout

\end_inset

.
 This does FE and DPD for the Arellano-Bond data,
 using Julia.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Stock and Watson,
 in their textbook 
\begin_inset CommandInset href
LatexCommand href
name "“Introduction to Econometrics”, 3rd Edition"
target "https://ebookcentral.proquest.com/lib/uab/detail.action?docID=5174962#"
literal "false"

\end_inset

,
 discuss panel data,
 in Chapter 10.
 They use the data set 
\begin_inset CommandInset href
LatexCommand href
name "fatality.xlsx"
target "https://wps.pearsoned.com/wps/media/objects/11422/11696965/datasets3e/datasets/fatality.xlsx"
literal "false"

\end_inset

,
 which is explained in 
\begin_inset CommandInset href
LatexCommand href
name "fatality.docx"
target "https://wps.pearsoned.com/wps/media/objects/11422/11696965/datasets3e/datasets/fatality.docx"
literal "false"

\end_inset

.
 Using this data,
 and the software of your choice (hint:
 Gretl is easy)
\end_layout

\begin_deeper
\begin_layout Enumerate
replicate the results in Eqn.
 10.2 and Figure 10.1.
\end_layout

\begin_layout Enumerate
replicate the results in Eqn 10.8 and Figure 10.2.
\end_layout

\begin_layout Enumerate
replicate the fixed effects panel data results in column 2 of Table 10.1.
\end_layout

\begin_layout Enumerate
give an economic interpretation of the results.
\end_layout

\end_deeper
\begin_layout Enumerate
In the context of a dynamic model with fixed effects,
 why is the differencing used in the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimation approach (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:within estimator"
nolink "false"

\end_inset

) problematic?
 That is,
 why does the Arellano-Bond estimator operate on the model in first differences instead of using the within approach?
\end_layout

\begin_layout Enumerate
Consider the simple linear panel data model with random effects (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:simple linear panel random effects"
nolink "false"

\end_inset

).
 Suppose that the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are independent across cross sectional units,
 so that 
\begin_inset Formula $E(\epsilon_{it}\epsilon_{js})=0,\,i\ne j,\,\forall t,s$
\end_inset

.
 With a cross sectional unit,
 the errors are independently and identically distributed,
 so 
\begin_inset Formula $E(\epsilon_{it}^{2})=\sigma_{i}^{2},$
\end_inset

 but 
\begin_inset Formula $E(\epsilon_{it}\epsilon_{is})=0,\,t\ne s.$
\end_inset

 More compactly,
 let 
\begin_inset Formula $\epsilon_{i}=\left[\begin{array}{cccc}
\epsilon_{i1} & \epsilon_{i2} & \cdots & \epsilon_{iT}\end{array}\right]^{\prime}$
\end_inset

 .
 Then the assumptions are that 
\begin_inset Formula $E(\epsilon_{i}\epsilon_{i}^{\prime})=\sigma_{i}^{2}I_{T},$
\end_inset

 and 
\begin_inset Formula $E(\epsilon_{i}\epsilon_{j}^{\prime})=0,\,i\ne j$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
write out the form of the entire covariance matrix (
\begin_inset Formula $nT\times nT$
\end_inset

) of all errors,
 
\begin_inset Formula $\Sigma=E(\epsilon\epsilon^{\prime})$
\end_inset

,
 where 
\begin_inset Formula $\epsilon=\left[\begin{array}{cccc}
\epsilon_{1}^{\prime} & \epsilon_{2}^{\prime} & \cdots & \epsilon_{T}^{\prime}\end{array}\right]^{\prime}$
\end_inset

 is the column vector of 
\begin_inset Formula $nT$
\end_inset

 errors.
\end_layout

\begin_layout Enumerate
suppose that 
\begin_inset Formula $n$
\end_inset

 is fixed,
 and consider asymptotics as 
\begin_inset Formula $T$
\end_inset

 grows.
 Is it possible to estimate the 
\begin_inset Formula $\Sigma_{i}$
\end_inset

 consistently?
 If so,
 how?
\end_layout

\begin_layout Enumerate
suppose that 
\begin_inset Formula $T$
\end_inset

 is fixed,
 and consider asymptotics an 
\begin_inset Formula $n$
\end_inset

 grows.
 Is it possible to estimate the 
\begin_inset Formula $\Sigma_{i}$
\end_inset

 consistently?
 If so,
 how?
\end_layout

\begin_layout Enumerate
For one of the two preceeding parts (b) and (c),
 consistent estimation is possible.
 For that case,
 outline how to do 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimation using a GLS correction.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Nonparametric-inference"

\end_inset

Nonparametric inference
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Ch.
 9;
 
\begin_inset CommandInset citation
LatexCommand cite
key "li2007nonparametric"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
What do we mean by the term 
\begin_inset Quotes eld
\end_inset

nonparametric inference
\begin_inset Quotes erd
\end_inset

?
 Remember that a parameter is a numerical feature of a population.
 We will still be concerned with learning about parameters,
 though of in this sense.
 Nonparametric inference means inferences that are possible without restricting the parameters of the populations to belong to a parametric family of functions.
\end_layout

\begin_layout Example
Demand function.
 Suppose that the parameter of interest is the conditional mean of quantity demanded,
 
\begin_inset Formula $x$
\end_inset

,
 given price 
\begin_inset Formula $p$
\end_inset

 and income 
\begin_inset Formula $m$
\end_inset

:
 
\begin_inset Formula $E(x|p,m)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
A parametric demand function:
 
\begin_inset Formula $x=\alpha+\beta p+\gamma m+\epsilon$
\end_inset

,
 where 
\begin_inset Formula $\epsilon|p,m\sim N(0,\sigma^{2})$
\end_inset

.
 Here,
 the functional form of the conditional mean is restricted to be linear in the parameter and the regressors,
 and the distribution of the error is restricted to the set of mean zero normal distributions.
 If the parameter of interest is the conditional mean,
 
\begin_inset Formula $E(x|p,m),$
\end_inset

the parametric model has imposed the parametric restriction
\begin_inset Formula 
\[
E(x|p,m)=\alpha+\beta p+\gamma m
\]

\end_inset


\end_layout

\begin_layout Itemize
Now,
 the parameter of interest is specified as belonging to a parametric family of functions,
 indexed by 
\begin_inset Formula $\alpha,\beta$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Learning about these statistical parameters will let us learn about the parameter of interest,
 
\begin_inset Formula $E(x|p,m).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
A nonparametric demand function:
 
\begin_inset Formula $x=x(p,m)+\epsilon$
\end_inset

,
 where 
\begin_inset Formula $E(\epsilon|p,m)=0.$
\end_inset

 The conditional mean is the function 
\begin_inset Formula $x(p,m),$
\end_inset

 but the form is not restricted.
 Also,
 the error has conditional mean zero,
 but may have any distribution that follows this restriction.
\end_layout

\begin_layout Itemize
Normally,
 it is good to use parametric restrictions 
\emph on
if we are confident that they are at least approximately true
\emph default
,
 as this will lead to low variance,
 low bias estimation.
 Information in any form usually helps to improve inference (more efficient,
 tighter confidence intervals,
 etc.) But it has to be information that actually informs.
\end_layout

\begin_layout Itemize
If we impose parametric restrictions for which we have little or no justification,
 we may provoke serious biases,
 which can lead to incorrect conclusions.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Motivation 
\series default
(see 
\begin_inset CommandInset citation
LatexCommand citet
key "white1980using"
literal "false"

\end_inset

).
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
The examples dir for this chapter has the file Elasticity.jl,
 which is a beginning of replicating this first section.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this section we return to an example which we've already seen:
 approximating a nonlinear in the variables regression line using a linear in the variables regression line.
 
\end_layout

\begin_layout Standard
We suppose that data is generated by random sampling of 
\begin_inset Formula $(y,x)$
\end_inset

,
 where 
\begin_inset Formula $y=f(x)$
\end_inset

 
\begin_inset Formula $+\varepsilon$
\end_inset

,
 
\begin_inset Formula $x$
\end_inset

 is uniformly distributed on 
\begin_inset Formula $(0,2\pi),$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 is a classical error with variance equal to 1.
 Suppose that the regression function is truly a quadratic function:
\begin_inset Formula 
\[
f(x)=1+\frac{3x}{2\pi}-\left(\frac{x}{2\pi}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
If we knew the functional form but not the coefficients,
 we could just estimate by least squares.
\begin_inset Newpage newpage
\end_inset

 
\end_layout

\begin_layout Itemize
But,
 let's assume that we do not know the functional form,
 to make things interesting
\end_layout

\begin_layout Itemize
Suppose that the problem of interest is to estimate the elasticity of 
\begin_inset Formula $f(x)$
\end_inset

 with respect to 
\begin_inset Formula $x,$
\end_inset

 throughout the range of 
\begin_inset Formula $x$
\end_inset

.
 Recall that the elasticity is an elasticity is the marginal function divided by the average function:
 
\begin_inset Formula 
\[
\varepsilon(x)=\frac{f^{\prime}(x)}{f(x)/x}
\]

\end_inset


\end_layout

\begin_layout Itemize
We would like to be able to estimate this quantity well for any arbitrary value 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

In general,
 the functional form of 
\begin_inset Formula $f(x)$
\end_inset

 is unknown.
 One idea is to take a Taylor's series approximation to 
\begin_inset Formula $f(x)$
\end_inset

 about some point 
\begin_inset Formula $x_{0}.$
\end_inset

 Flexible functional forms such as the transcendental logarithmic (usually known as the translog) can be interpreted as second order Taylor's series approximations.
 We'll work with a first order approximation,
 for simplicity.
 Approximating about 
\begin_inset Formula $x_{0}$
\end_inset

:
 
\begin_inset Formula 
\[
h(x)=f(x_{0})+D_{x}f(x_{0})\left(x-x_{0}\right)
\]

\end_inset

 If the approximation point is 
\begin_inset Formula $x_{0}=0,$
\end_inset

 we can write 
\begin_inset Formula 
\[
h(x)=a+bx
\]

\end_inset

 The coefficient 
\begin_inset Formula $a$
\end_inset

 is the value of the function at 
\begin_inset Formula $x=0,$
\end_inset

 and the slope is the value of the derivative at 
\begin_inset Formula $x=0.$
\end_inset

 These are of course not known.
 One might try estimation by ordinary least squares.
 The objective function is 
\begin_inset Formula 
\[
s(a,b)=1/n\sum_{t=1}^{n}\left(y_{t}-h(x_{t})\right)^{2}.
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 The limiting objective function,
 following the argument we used to get equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "olslim"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlslim"
nolink "false"

\end_inset

 is 
\begin_inset Formula 
\[
s_{\infty}(a,b)=\int_{0}^{2\pi}\left(f(x)-h(x)\right)^{2}dx+C
\]

\end_inset

 The theorem regarding the consistency of extremum estimators (Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

) tells us that 
\begin_inset Formula $\hat{a}$
\end_inset

 and 
\begin_inset Formula $\hat{b}$
\end_inset

 will converge almost surely to the values that minimize the limiting objective function.
 Solving the first order conditions
\begin_inset Foot
status open

\begin_layout Plain Layout
The following results were obtained using the free computer algebra system (CAS) 
\begin_inset CommandInset href
LatexCommand href
name "Maxima"
target "http://maxima.sourceforge.net/"
literal "false"

\end_inset

.
 Unfortunately,
 I have lost the source code to get the results.
 It's not hard to do,
 though:
 see 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Consistency-of-OLS"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\end_inset

 reveals that 
\begin_inset Formula $s_{\infty}(a,b)$
\end_inset

 obtains its minimum at 
\begin_inset Formula $\left\{ a^{0}=\frac{7}{6},b^{0}=\frac{1}{\pi}\right\} .$
\end_inset

 The estimated approximating function 
\begin_inset Formula $\hat{h}(x)$
\end_inset

 therefore tends almost surely to
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\infty}(x)=7/6+x/\pi
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-and-simple"
nolink "false"

\end_inset

 we see the true function and the limit of the approximation to see the asymptotic bias as a function of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-and-simple"

\end_inset

True and simple approximating functions
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/linearfit.png
	width 4in

\end_inset


\end_layout

\end_inset

(The approximating model is the straight line,
 the true model has curvature.) Note that the approximating model is in general inconsistent,
 even at the approximation point.
 This shows that ``flexible functional forms
\begin_inset Quotes erd
\end_inset

 based upon Taylor's series approximations do not in general lead to consistent estimation of functions.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The approximating model seems to fit the true model fairly well,
 asymptotically,
 so maybe the approximation problem is not too important?
 
\end_layout

\begin_layout Itemize
However,
 we are interested in the elasticity of the function.
 Recall that the elasticity is an elasticity is the marginal function divided by the average function:
 
\begin_inset Formula 
\[
\varepsilon(x)=\frac{f^{\prime}(x)}{f(x)/x}
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Good approximation of the elasticity over the range of 
\begin_inset Formula $x$
\end_inset

 will require a good approximation of both 
\begin_inset Formula $f(x)$
\end_inset

 and 
\begin_inset Formula $f^{\prime}(x)$
\end_inset

 over the range of 
\begin_inset Formula $x.$
\end_inset

 The approximating elasticity is 
\begin_inset Formula 
\[
\eta(x)=\frac{h^{\prime}(x)}{h(x)/x}
\]

\end_inset


\end_layout

\begin_layout Itemize
The question is:
 how well does 
\begin_inset Formula $\eta(x)$
\end_inset

 approximate 
\begin_inset Formula $\varepsilon(x)$
\end_inset

?
\begin_inset Newpage newpage
\end_inset

 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-and-approximating"
nolink "false"

\end_inset

 we see the true elasticity and the elasticity obtained from the limiting approximating model.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-and-approximating"

\end_inset

True and approximating elasticities
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/linearelasticity.png
	width 4in

\end_inset


\end_layout

\end_inset

The true elasticity is the line that has negative slope for large 
\begin_inset Formula $x.$
\end_inset

 Visually we see that the elasticity is not approximated so well.
 Root mean squared error in the approximation of the elasticity is 
\begin_inset Formula 
\[
\left(\int_{0}^{2\pi}\left(\varepsilon(x)-\eta(x)\right)^{2}dx\right)^{1/2}=0.31546
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Now suppose we use the leading terms of a trigonometric series as the approximating model.
 The reason for using a trigonometric series as an approximating model is motivated by the asymptotic properties of the Fourier flexible functional form (Gallant,
 1981,
 1982),
 which is an example of a 
\emph on
sieve estimator
\emph default
.
 Normally with this type of model the number of basis functions is an increasing function of the sample size.
 Here we hold the set of basis function fixed.
 We will consider the asymptotic behavior of a fixed model,
 which we interpret as an approximation to the estimator's behavior in finite samples.
 Consider the set of basis functions:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z(x)=\left[\begin{array}{cccccc}
1 & x & \cos(x) & \sin(x) & \cos(2x) & \sin(2x)\end{array}\right].
\]

\end_inset

 The approximating model is the linear in parameters model
\begin_inset Formula 
\[
g_{K}(x)=Z(x)\alpha
\]

\end_inset

which can be estimated by OLS.
 Maintaining these basis functions as the sample size increases,
 we find that the limiting objective function is minimized at 
\begin_inset Formula 
\[
\left\{ a_{1}=\frac{7}{6},a_{2}=\frac{1}{\pi},a_{3}=-\frac{1}{\pi^{2}},a_{4}=0,a_{5}=-\frac{1}{4\pi^{2}},a_{6}=0\right\} .
\]

\end_inset

 Substituting these values into 
\begin_inset Formula $g_{K}(x)$
\end_inset

 we obtain the almost sure limit of the approximation 
\begin_inset Formula 
\begin{equation}
g_{\infty}(x)=7/6+x/\pi+\left(\cos x\right)\left(-\frac{1}{\pi^{2}}\right)+\left(\sin x\right)0+\left(\cos2x\right)\left(-\frac{1}{4\pi^{2}}\right)+\left(\sin2x\right)0\label{g}
\end{equation}

\end_inset


\begin_inset Newpage newpage
\end_inset

 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-function-and"
nolink "false"

\end_inset

 we have the approximation and the true function:
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-function-and"

\end_inset

True function and more flexible approximation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/fourierfit.png
	width 4in

\end_inset


\end_layout

\end_inset

Clearly the truncated trigonometric series model offers a better approximation,
 asymptotically,
 than does the linear model.
 
\begin_inset Newpage newpage
\end_inset

In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-elasticity-and"
nolink "false"

\end_inset

 we have the more flexible approximation's elasticity and that of the true function:
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-elasticity-and"

\end_inset

True elasticity and more flexible approximation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/fourierelasticity.png
	width 4in

\end_inset


\end_layout

\end_inset

On average,
 the fit is better,
 though there is some implausible waviness in the estimate.
 
\begin_inset Newpage newpage
\end_inset

Root mean squared error in the approximation of the elasticity is 
\begin_inset Formula 
\[
\left(\int_{0}^{2\pi}\left(\varepsilon(x)-\frac{g_{\infty}^{\prime}(x)x}{g_{\infty}(x)}\right)^{2}dx\right)^{1/2}=0.16213,
\]

\end_inset

about half that of the RMSE when the first order approximation is used.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize

\emph on
Sieve estimators
\emph default
 are a form of 
\emph on
semi-nonparametric
\emph default
 estimator.
 They allow the number of regressors to grow as the sample size grows.
 
\end_layout

\begin_deeper
\begin_layout Itemize
they look like parametric estimators,
 but they can behave as nonparametric estimators
\end_layout

\begin_layout Itemize
this must be done in a controlled way:
 there is a variance/bias tradeoff:
\end_layout

\begin_deeper
\begin_layout Itemize
more regressors -> less bias
\end_layout

\begin_layout Itemize
more regressors-> more variance
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
It can be shown that if we introduce sine and cosine regressors at a slow rate,
 it is possible to drive the RMSE of the approximation to the true regression function and to the true elasticity to zero,
 as the sample size grows.
 Such an approach is known as the Fourier flexible form (
\begin_inset CommandInset citation
LatexCommand citet
key "gallant_1987"
literal "false"

\end_inset

).
\end_layout

\begin_layout Itemize
This is why sieve estimators (and other (semi-)nonparametric regression estimators) are of interest:
 we can obtain consistent estimates of regression functions without knowledge of the functional form of the true regression line.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Generally speaking,
 nonparametric estimators have a larger variance than do correctly specified parametric estimators.
 So,
 nonparametric estimators will usually be more appropriate in cases where the sample size is reasonably large,
 to control the variance.
\end_layout

\begin_layout Itemize
Nonparametric estimators also need 
\begin_inset Quotes sld
\end_inset

tuning
\begin_inset Quotes srd
\end_inset

,
 which complicates their use.
 Tuning refers to issues such as 
\end_layout

\begin_deeper
\begin_layout Itemize
selecting how many sine and cosine terms to use for the Fourier form
\end_layout

\begin_layout Itemize
selecting the bandwidth and kernel for kernel regression or kernel density estimators
\end_layout

\begin_layout Itemize
selecting the net architecture and training method for neural net estimators
\end_layout

\end_deeper
\begin_layout Itemize
Finding methods that allow for reliable and uncomplicated tuning is desirable,
 of course.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Estimation of regression functions
\end_layout

\begin_layout Standard
Here,
 we will see two examples of methods of estimating regression functions without knowledge of the true functional form:
 kernel regression and neural nets.
 There are other methods,
 for example sieve estimators,
 
\begin_inset CommandInset href
LatexCommand href
name "nearest neighbors"
target "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
literal "false"

\end_inset

,
 etc.
\end_layout

\begin_layout Subsection
Kernel regression estimators
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
:
 
\begin_inset CommandInset citation
LatexCommand cite
key "li2007nonparametric"
literal "true"

\end_inset

,
 Ch.
 2;
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Ch.
 9;
 
\begin_inset CommandInset citation
LatexCommand citet
key "bierens1987kernel"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Kernel regression estimation is an example of fully nonparametric estimation (others are splines,
 nearest neighbors,
 etc.).
 We'll consider the Nadaraya-Watson kernel regression estimator in a simple case.
\end_layout

\begin_layout Itemize
Suppose we have an iid sample from the joint density 
\begin_inset Formula $f(x,y),$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula $k$
\end_inset

 -dimensional.
 The model is 
\begin_inset Formula 
\[
y_{t}=g(x_{t})+\varepsilon_{t},
\]

\end_inset

 where 
\begin_inset Formula 
\[
E(\varepsilon_{t}|x_{t})=0.
\]

\end_inset


\end_layout

\begin_layout Itemize
The conditional expectation of 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula $g(x).$
\end_inset

 By definition of the conditional expectation,
 we have 
\begin_inset Formula 
\begin{eqnarray*}
g(x) & = & \int y\frac{f(x,y)}{h(x)}dy\\
 & = & \frac{1}{h(x)}\int yf(x,y)dy,
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $h(x)$
\end_inset

 is the marginal density of 
\begin_inset Formula $x:$
\end_inset


\begin_inset Formula 
\[
h(x)=\int f(x,y)dy.
\]

\end_inset


\end_layout

\begin_layout Itemize
This suggests that we could estimate 
\begin_inset Formula $g(x)$
\end_inset

 by estimating 
\begin_inset Formula $h(x)$
\end_inset

 and 
\begin_inset Formula $\int yf(x,y)dy.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Estimation of the denominator
\end_layout

\begin_layout Standard
A kernel estimator for the marginal density 
\begin_inset Formula $h(x)$
\end_inset

 has the form 
\begin_inset Formula 
\[
\hat{h}(x)=\frac{1}{n}\sum_{t=1}^{n}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}},
\]

\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the sample size and 
\begin_inset Formula $k$
\end_inset

 is the dimension of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Itemize
The function 
\begin_inset Formula $K(\cdot)$
\end_inset

 (the kernel) is absolutely integrable:
 
\begin_inset Formula 
\[
\int|K(x)|dx<\infty,
\]

\end_inset

 and 
\begin_inset Formula $K(\cdot)$
\end_inset

 integrates to 
\begin_inset Formula $1:$
\end_inset


\begin_inset Formula 
\[
\int K(x)dx=1.
\]

\end_inset

 In this respect,
 
\begin_inset Formula $K(\cdot)$
\end_inset

 is like a density function,
 but we do not necessarily restrict 
\begin_inset Formula $K(\cdot)$
\end_inset

 to be nonnegative.
\end_layout

\begin_layout Itemize
The 
\emph on
window width
\emph default
 parameter,
 
\begin_inset Formula $\gamma_{n}$
\end_inset

 is a sequence of positive numbers that satisfies 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}\gamma_{n} & = & 0\\
\lim_{n\rightarrow\infty}n\gamma_{n}^{k} & = & \infty
\end{eqnarray*}

\end_inset

 So,
 the window width must tend to zero,
 but not too quickly.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
To show pointwise consistency of 
\begin_inset Formula $\hat{h}(x)$
\end_inset

 for 
\begin_inset Formula $h(x),$
\end_inset

 first consider the expectation of the estimator (because the estimator is an average of iid terms,
 we only need to consider the expectation of a representative term):
 
\begin_inset Formula 
\[
E\left[\hat{h}(x)\right]=\int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]h(z)dz.
\]

\end_inset

 Change variables as 
\begin_inset Formula $z^{*}=(x-z)/\gamma_{n},$
\end_inset

 so 
\begin_inset Formula $z=x-\gamma_{n}z^{*}$
\end_inset

 and 
\begin_inset Formula $|\frac{dz}{dz^{*\prime}}|=\gamma_{n}^{k},$
\end_inset

 we obtain
\begin_inset Formula 
\begin{eqnarray*}
E\left[\hat{h}(x)\right] & = & \int\gamma_{n}^{-k}K\left(z^{*}\right)h(x-\gamma_{n}z^{*})\gamma_{n}^{k}dz^{*}\\
 & = & \int K\left(z^{*}\right)h(x-\gamma_{n}z^{*})dz^{*}.
\end{eqnarray*}

\end_inset

 Now,
 asymptotically,
 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}E\left[\hat{h}(x)\right] & = & \lim_{n\rightarrow\infty}\int K\left(z^{*}\right)h(x-\gamma_{n}z^{*})dz^{*}\\
 & = & \int\lim_{n\rightarrow\infty}K\left(z^{*}\right)h(x-\gamma_{n}z^{*})dz^{*}\\
 & = & \int K\left(z^{*}\right)h(x)dz^{*}\\
 & = & h(x)\int K\left(z^{*}\right)dz^{*}\\
 & = & h(x),
\end{eqnarray*}

\end_inset

 since 
\begin_inset Formula $\gamma_{n}\rightarrow0$
\end_inset

 and 
\begin_inset Formula $\int K\left(z^{*}\right)dz^{*}=1$
\end_inset

 by assumption.
 (Note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

that we can pass the limit through the integral is a result of the dominated convergence theorem.
 For this to hold we need that 
\begin_inset Formula $h(\cdot)$
\end_inset

 be dominated by an absolutely integrable function.)
\end_layout

\begin_layout Itemize
So,
 the estimator is asymptotically unbiased.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Next,
 considering the variance of 
\begin_inset Formula $\hat{h}(x),$
\end_inset

 we have,
 due to the iid assumption 
\begin_inset Formula 
\begin{eqnarray*}
n\gamma_{n}^{k}V\left[\hat{h}(x)\right] & = & n\gamma_{n}^{k}\frac{1}{n^{2}}\sum_{t=1}^{n}V\left\{ \frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}\right\} \\
 & = & \gamma_{n}^{-k}\frac{1}{n}\sum_{t=1}^{n}V\left\{ K\left[\left(x-x_{t}\right)/\gamma_{n}\right]\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
By the representative term argument,
 this is
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
n\gamma_{n}^{k}V\left[\hat{h}(x)\right]=\gamma_{n}^{-k}V\left\{ K\left[\left(x-z\right)/\gamma_{n}\right]\right\} 
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Also,
 since 
\begin_inset Formula $V(x)=E(x^{2})-E(x)^{2}$
\end_inset

 we have 
\begin_inset Formula 
\begin{eqnarray*}
n\gamma_{n}^{k}V\left[\hat{h}(x)\right] & = & \gamma_{n}^{-k}E\left\{ \left(K\left[\left(x-z\right)/\gamma_{n}\right]\right)^{2}\right\} -\gamma_{n}^{-k}\left\{ E\left(K\left[\left(x-z\right)/\gamma_{n}\right]\right)\right\} ^{2}\\
 & = & \int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]^{2}h(z)dz-\gamma_{n}^{k}\left\{ \int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]h(z)dz\right\} ^{2}\\
 & = & \int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]^{2}h(z)dz-\gamma_{n}^{k}E\left[\widehat{h}(x)\right]^{2}
\end{eqnarray*}

\end_inset

 The second term converges to zero:
 
\begin_inset Formula 
\[
\gamma_{n}^{k}E\left[\widehat{h}(x)\right]^{2}\rightarrow0,
\]

\end_inset

 by the previous result regarding the expectation and the fact that 
\begin_inset Formula $\gamma_{n}\rightarrow0.$
\end_inset

 Therefore,
 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}n\gamma_{n}^{k}V\left[\hat{h}(x)\right]=\lim_{n\rightarrow\infty}\int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]^{2}h(z)dz.
\]

\end_inset

 Using exactly the same change of variables as before,
 this can be shown to be 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}n\gamma_{n}^{k}V\left[\hat{h}(x)\right]=h(x)\int\left[K(z^{*})\right]^{2}dz^{*}.
\]

\end_inset

 Since both 
\begin_inset Formula $\int\left[K(z^{*})\right]^{2}dz^{*}$
\end_inset

 and 
\begin_inset Formula $h(x)$
\end_inset

 are bounded,
 the RHS is bounded,
 and since 
\begin_inset Formula $n\gamma_{n}^{k}\rightarrow\infty$
\end_inset

 by assumption,
 we have that 
\begin_inset Formula 
\[
V\left[\hat{h}(x)\right]\rightarrow0.
\]

\end_inset


\end_layout

\begin_layout Itemize
Since the bias and the variance both go to zero,
 we have pointwise consistency (convergence in quadratic mean implies convergence in probability).
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Estimation of the numerator
\end_layout

\begin_layout Standard
To estimate 
\begin_inset Formula $\int yf(x,y)dy,$
\end_inset

 we need an estimator of 
\begin_inset Formula $f(x,y).$
\end_inset

 The estimator has the same form as the estimator for 
\begin_inset Formula $h(x),$
\end_inset

 only with one dimension more:
 
\begin_inset Formula 
\[
\hat{f}(x,y)=\frac{1}{n}\sum_{t=1}^{n}\frac{K_{*}\left[\left(y-y_{t}\right)/\gamma_{n},\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k+1}}
\]

\end_inset

 The kernel 
\begin_inset Formula $K_{*}\left(\cdot\right)$
\end_inset

 is required to have mean zero:
 
\begin_inset Formula 
\[
\int yK_{*}\left(y,x\right)dy=0
\]

\end_inset

 and to marginalize to the previous kernel for 
\begin_inset Formula $h(x):$
\end_inset


\begin_inset Formula 
\[
\int K_{*}\left(y,x\right)dy=K(x).
\]

\end_inset

 With this kernel,
 we have (not obviously,
 see Li and Racine,
 Ch.
 2,
 section 2.1) 
\begin_inset Formula 
\[
\int y\hat{f}(y,x)dy=\frac{1}{n}\sum_{t=1}^{n}y_{t}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}
\]

\end_inset

 by marginalization of the kernel,
 so we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\hat{g}(x) & := & \frac{1}{\hat{h}(x)}\int y\hat{f}(y,x)dy\\
 & = & \frac{\frac{1}{n}\sum_{t=1}^{n}y_{t}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}}{\frac{1}{n}\sum_{t=1}^{n}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}}\\
 & = & \frac{\sum_{t=1}^{n}y_{t}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}
\end{eqnarray*}

\end_inset

 This is the Nadaraya-Watson kernel regression estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Discussion
\end_layout

\begin_layout Itemize
defining 
\begin_inset Formula 
\[
w_{t}=\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]},
\]

\end_inset

the kernel regression estimator for 
\begin_inset Formula $g(x_{t})$
\end_inset

 can be written as
\begin_inset Formula 
\begin{align*}
\hat{g}(x) & =\sum_{t=1}^{n}y_{t}w_{t},
\end{align*}

\end_inset

a weighted average of the 
\begin_inset Formula $y_{j},\,j=1,2,...,n$
\end_inset

,
 where higher weights are associated with points that are closer to 
\begin_inset Formula $x_{t}.$
\end_inset

 The weights sum to 1.
 See this 
\begin_inset CommandInset href
LatexCommand href
name "link for a graphic interpretation."
target "https://en.wikipedia.org/wiki/Kernel_smoother"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
The window width parameter 
\begin_inset Formula $\gamma_{n}$
\end_inset

 imposes smoothness.
 The estimator is increasingly flat as 
\begin_inset Formula $\gamma_{n}\rightarrow\infty,$
\end_inset

 since in this case each weight tends to 
\begin_inset Formula $1/n.$
\end_inset


\end_layout

\begin_layout Itemize
A large window width reduces the variance (strong imposition of flatness),
 but increases the bias.
\end_layout

\begin_layout Itemize
A small window width reduces the bias,
 but makes very little use of information except points that are in a small neighborhood of 
\begin_inset Formula $x_{t}.$
\end_inset

 Since relatively little information is used,
 the variance is large when the window width is small.
\end_layout

\begin_layout Itemize
The standard normal density is a popular choice for 
\begin_inset Formula $K(.)\;$
\end_inset

 and 
\begin_inset Formula $K_{*}(y,x),$
\end_inset

 though there are possibly better alternatives.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Choice of the window width:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

Cross-validation
\end_layout

\begin_layout Standard
The selection of an appropriate window width is important.
 One popular method is cross validation.
 This consists of splitting the sample into two parts (e.g.,
 50%-50%).
 The first part is the 
\begin_inset Quotes eld
\end_inset

in sample
\begin_inset Quotes erd
\end_inset

 data,
 which is used for estimation,
 and the second part is the 
\begin_inset Quotes eld
\end_inset

out of sample
\begin_inset Quotes erd
\end_inset

 data,
 used for evaluation of the fit though RMSE or some other criterion.
 The steps are:
\end_layout

\begin_layout Enumerate
Split the data.
 The out of sample data is 
\begin_inset Formula $y^{out}$
\end_inset

 and 
\begin_inset Formula $x^{out}$
\end_inset

 (these are the first 
\begin_inset Formula $n_{out}$
\end_inset

 observations,
 say-
\end_layout

\begin_layout Enumerate
Choose a window width 
\begin_inset Formula $\gamma$
\end_inset

.
\end_layout

\begin_layout Enumerate
With the in sample data,
 fit 
\begin_inset Formula $\hat{y}_{t}^{out}(\gamma)$
\end_inset

 corresponding to each 
\begin_inset Formula $x_{t}^{out}.$
\end_inset

 This fitted value is a function of the window width,
 the in sample data,
 as well as the evaluation point 
\begin_inset Formula $x_{t}^{out}$
\end_inset

,
 but it does not involve 
\begin_inset Formula $y_{t}^{out}.$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat for all out of sample points.
\end_layout

\begin_layout Enumerate
Calculate RMSE
\begin_inset Formula $(\gamma)=\sqrt{\frac{1}{n_{out}}\sum_{t=1}^{n_{out}}\left(y_{t}^{out}-\hat{y}_{t}^{out}(\gamma)\right)^{2}}$
\end_inset


\end_layout

\begin_layout Enumerate
Go to step 
\begin_inset Formula $2,$
\end_inset

 or to the next step if enough window widths have been tried.
\end_layout

\begin_layout Enumerate
Select the 
\begin_inset Formula $\gamma$
\end_inset

 that minimizes RMSE(
\begin_inset Formula $\gamma)$
\end_inset

 (Verify that a minimum has been found,
 for example by plotting RMSE as a function of 
\begin_inset Formula $\gamma).$
\end_inset


\end_layout

\begin_layout Enumerate
Re-estimate using the best 
\begin_inset Formula $\gamma$
\end_inset

 and all of the data.
 
\end_layout

\begin_layout Itemize
there is a variation known as leave-one-out cross validation,
 where each 
\begin_inset Formula $y_{t}^{out}$
\end_inset

 is fit in turn using all of the remaining observations,
 omitting the 
\begin_inset Formula $t^{th}$
\end_inset

 observation.
 This is the recommended procedure.
 It is somewhat more demanding computationally,
 but works better.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Example:

\series default
 from Julia,
 and after doing 
\family typewriter
using Econometrics
\family default
,
 run npreg().
 It will give you a figure similar to 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Examples/Nonparametric/npreg.png
	width 15cm

\end_inset


\begin_inset Newline newline
\end_inset

You see that the kernel regression mean and median are close to the true function,
 and the 90% CI defined by nonparametric quantiles contains the true function,
 everywhere,
 in this case.
 Edit the code to see what's going on.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Neural nets
\end_layout

\begin_layout Standard
Neural networks are a well known tool in many fields,
 and there are many presentations,
 both academic and more informal,
 of various structures that can be used.
 For this reason,
 the presentation here is brief.
 For more details and references,
 see 
\begin_inset CommandInset citation
LatexCommand citet
key "KuanWhiteNNSurvey1994"
literal "true"

\end_inset

.
 A very useful practical guide is given by 
\begin_inset CommandInset citation
LatexCommand citet
key "LeCun2012"
literal "true"

\end_inset

.
 A good 
\begin_inset CommandInset href
LatexCommand href
name "practical introduction is here"
target "https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/understanding-deep-learning-parameter-tuning-with-mxnet-h2o-package-in-r/tutorial/"
literal "false"

\end_inset

.
 Papers by 
\begin_inset CommandInset citation
LatexCommand cite
key "GallantWhiteNeural88"
literal "true"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Hornik1989MFN:70405.70408"
literal "true"

\end_inset

 show that some types of neural networks can be thought of as as nonparametric regression estimators,
 but this discussion seems to still be open,
 in the case of the 
\begin_inset Quotes sld
\end_inset

deep learning
\begin_inset Quotes srd
\end_inset

 nets that are popular nowadays.
 The discussion below is based on 
\begin_inset CommandInset citation
LatexCommand citet
key "creel2017neural"
literal "false"

\end_inset

,
 and code for the example below is at 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/mcreel/NeuralNetsForIndirectInference.jl"
target "https://github.com/mcreel/NeuralNetsForIndirectInference.jl"
literal "false"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Suppose we are interested in the regression model 
\begin_inset Formula $y=g(x)+\epsilon,$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $K$
\end_inset

-vector and 
\begin_inset Formula $y$
\end_inset

 is a 
\begin_inset Formula $G$
\end_inset

-vector.
 
\end_layout

\begin_deeper
\begin_layout Itemize
This is a multivariate (more than one dependent variable) multiple (more than one regressor) regression model.
 
\end_layout

\begin_layout Itemize
Because we don't specify the form of 
\begin_inset Formula $g(x)=E(y|x),$
\end_inset

 it is a nonparametric regression model.
\end_layout

\begin_layout Itemize
Let's model this using a neural net.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Consider a simple feed forward neural net for regression of an output in 
\begin_inset Formula $R^{K}$
\end_inset

 upon an input in 
\begin_inset Formula $R^{G}$
\end_inset

.
 A typical feed forward net is depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:A-simple-neural"
nolink "false"

\end_inset

,
 which maps 3 inputs to 2 outputs.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The inputs to the net,
 
\begin_inset Formula $I_{1}$
\end_inset

,
 
\begin_inset Formula $I_{2}$
\end_inset

,
 and 
\begin_inset Formula $I_{3}$
\end_inset

,
 are scalar real numbers,
 as are the outputs 
\begin_inset Formula $O_{1}$
\end_inset

 and 
\begin_inset Formula $O_{2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The net has two hidden layers,
 formed by 4 and 3 hidden nodes or neurons,
 
\begin_inset Formula $h_{1}$
\end_inset

,
\begin_inset Formula $h_{2}$
\end_inset

,...,
\begin_inset Formula $h_{7}$
\end_inset

,
 
\end_layout

\begin_layout Itemize
and an output layer,
 which gives the values of the two outputs 
\begin_inset Formula $O_{1}$
\end_inset

 and 
\begin_inset Formula $O_{2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The values 
\begin_inset Formula $\alpha_{1},\,\alpha_{2}$
\end_inset

 and 
\begin_inset Formula $\alpha_{3}$
\end_inset

 are vectors of 
\begin_inset Quotes eld
\end_inset

bias
\begin_inset Quotes erd
\end_inset

 parameters,
 which are discussed below.
 
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:A-simple-neural"

\end_inset

A simple neural net
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/neuralnet.pdf
	width 14cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In general,
 a net is a series of transformations of the inputs.
 
\end_layout

\begin_layout Itemize
Each of the transformations is referred to as a layer.
 
\end_layout

\begin_layout Itemize
The inputs themselves constitute the zero-th layer,
 and the final result of the transformations is the output layer.
 
\end_layout

\begin_layout Itemize
A layer,
 
\begin_inset Formula $H_{j}$
\end_inset

,
 is a vector of real numbers,
 which is the result of the 
\begin_inset Formula $j^{th}$
\end_inset

 in the series of transformations.
\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $H_{0}$
\end_inset

 be the 
\begin_inset Formula $G$
\end_inset

 dimensional vector of inputs.
 
\end_layout

\begin_layout Itemize
Suppose that there are 
\begin_inset Formula $P$
\end_inset

 layers.
 
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $n_{j}$
\end_inset

 be the number of neurons in the 
\begin_inset Formula $j^{th}$
\end_inset

 layer,
 
\begin_inset Formula $j=1,2,...,P.$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
The value taken by a neuron in the 
\begin_inset Formula $j^{th}$
\end_inset

 layer is the result of the layer's 
\begin_inset Quotes eld
\end_inset

activation function
\begin_inset Quotes erd
\end_inset

,
 
\begin_inset Formula $f_{j}(\cdot)$
\end_inset

,
 applied on an element-by-element basis to an affine function of the inputs to the layer.
 The relationship between the layers is given by 
\begin_inset Formula 
\begin{align}
H_{j} & =f_{j}(\alpha_{j}+\beta_{j}H_{j-1}),\,j=1,2,...,P,\label{eq:LayersOfNet}
\end{align}

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\alpha_{j}$
\end_inset

 is a 
\begin_inset Formula $n_{j}$
\end_inset

 dimensional vector of parameters (these are known as bias parameters in the neural net literature)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{j}$
\end_inset

 is a 
\begin_inset Formula $n_{j}\times n_{j-1}$
\end_inset

 matrix of parameters.
 
\end_layout

\begin_layout Itemize
The layers 
\begin_inset Formula $1,\,2,...,\,P-1$
\end_inset

 are referred to as hidden layers
\end_layout

\begin_layout Itemize
layer 
\begin_inset Formula $P$
\end_inset

 is the output layer.
\end_layout

\begin_layout Itemize
The input to the first hidden layer,
 known as the input layer,
 is simply the input data,
 
\begin_inset Formula $H_{0}\in\mathbb{R}^{G}$
\end_inset

.
 The output of the net is the final layer,
 
\begin_inset Formula $H_{P}\in\mathbb{\mathbb{R}}^{K}$
\end_inset

.
 When using a net for regression,
 the last activation function,
 
\begin_inset Formula $f_{P}(\cdot)$
\end_inset

,
 is simply an identity function,
 so that 
\begin_inset Formula $H_{P}=\alpha_{P}+\beta_{P}H_{P-1}.$
\end_inset

 The reason that an activation function is used with the hidden layers is that this is what allows the net to approximate a nonlinear mapping.
 If all activation functions were identity functions,
 the entire net would reduce to an over-parameterized linear regression model.
 In this paper,
 the activation function that is used for the hidden layers is the 
\begin_inset Quotes eld
\end_inset

rectified linear unit
\begin_inset Quotes erd
\end_inset

 (ReLU) function,
 
\begin_inset Formula $f(x)=\max(0,\,x)$
\end_inset

,
 a very widely used choice in modern deep learning applications.
\end_layout

\end_deeper
\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

A neural net may contain many,
 many hidden parameters
\end_layout

\begin_layout Itemize
Suppose the number of inputs,
 
\begin_inset Formula $G,$
\end_inset

 is 40,
 and the number of outputs,
 
\begin_inset Formula $K,$
\end_inset

 is 9.
\end_layout

\begin_layout Itemize
Suppose the net has two hidden layers,
 of size 300 and 40,
 respectively.
 
\end_layout

\begin_layout Itemize
Then there are 
\begin_inset Formula $300\times40$
\end_inset

 parameters in the 
\begin_inset Formula $\beta_{1}$
\end_inset

 matrix of the first layer and 40 elements in the 
\begin_inset Formula $\alpha_{1}$
\end_inset

 vector.
 
\end_layout

\begin_layout Itemize
Similarly,
 in the second hidden layer,
 there are 
\begin_inset Formula $40\times300+40$
\end_inset

 parameters
\end_layout

\begin_layout Itemize
there are 
\begin_inset Formula $9\times40+9$
\end_inset

 parameters corresponding to the output layer.
 
\end_layout

\begin_layout Itemize
Thus,
 the total number of parameters is 24449.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
A neural net is a nonlinear regression model that may be highly parameterized
\end_layout

\begin_deeper
\begin_layout Itemize
may be more parameters than observations in a single sample
\end_layout

\begin_layout Itemize
lack of identification:
 neurons can be reordered
\end_layout

\begin_layout Itemize
multiple local minima
\end_layout

\end_deeper
\begin_layout Standard
Partial solutions:
\end_layout

\begin_layout Itemize
for a simulable model,
 we can generate multiple data sets to train the net.
 With much data,
 even a large net can be trained well.
\end_layout

\begin_layout Itemize
For the multiple local minima problem,
 
\begin_inset Quotes sld
\end_inset

stochastic gradient descent
\begin_inset Quotes srd
\end_inset

 and techniques related to cross validation can help a lot:
\end_layout

\begin_deeper
\begin_layout Itemize
compute the gradient using a small number of observations from the training set.
 This is called a stochastic gradient,
 because it depends on the observations that were chosen.
\end_layout

\begin_layout Itemize
take a small step in that direction.
 The step size is called the 
\begin_inset Quotes sld
\end_inset

learning rate
\begin_inset Quotes srd
\end_inset

 in the NN literature.
\end_layout

\begin_layout Itemize
evaluate the new fit using a testing set
\end_layout

\begin_layout Itemize
iterate gradient/learning,
 with the learning rate (step size) getting smaller as learning proceeds,
 until the fit to the testing set no longer improves.
 
\end_layout

\end_deeper
\begin_layout Standard
Modern software exists to make this quite easy to do.
 For Julia,
 
\begin_inset CommandInset href
LatexCommand href
name "see this page"
target "https://juliacomputing.com/domains/ml-and-ai.html"
literal "false"

\end_inset

 to get started.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
For this to work well,
 you need a lot of data,
 to train the net.
\end_layout

\begin_layout Itemize
Simulation based econometric methods can give us a lot of simulated data,
 so using neural nets when doing simulation based estimation is very natural
\end_layout

\begin_layout Itemize
A neural net indirect inference estimator is not an extremum estimator:
 how to test hypotheses?
 
\end_layout

\begin_deeper
\begin_layout Itemize
bootstrapping?
 
\emph on
Update
\emph default
:
 based on my experimentation,
 no.
 Inference requires accurate estimation of tail quantiles,
 and this is difficult to do based on a training sample drawn from the prior.
 Likewise,
 neural quantile regression does not lead to good estimation of tail quantiles,
 for the same reason (one would need an enormous sample from the prior).
\end_layout

\begin_layout Itemize
One can use the NN estimator as a statistic for indirect inference or related methods,
 and then use the asymptotic theory for those methods.
 This works pretty well - see 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/mcreel/SNM"
target "https://github.com/mcreel/SNM"
literal "false"

\end_inset

 and the working paper referenced there .
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Density function estimation
\end_layout

\begin_layout Subsection
Kernel density estimation
\end_layout

\begin_layout Standard
The previous discussion suggests that a kernel density estimator may easily be constructed.
 We have already seen how joint densities may be estimated.
 If were interested in a conditional density,
 for example of 
\begin_inset Formula $y$
\end_inset

 conditional on 
\begin_inset Formula $x$
\end_inset

,
 then the kernel estimate of the conditional density is simply
\begin_inset Formula 
\begin{eqnarray*}
\widehat{f}_{y|x} & = & \frac{\hat{f}(x,y)}{\hat{h}(x)}\\
 & = & \frac{\frac{1}{n}\sum_{t=1}^{n}\frac{K_{*}\left[\left(y-y_{t}\right)/\gamma_{n},\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k+1}}}{\frac{1}{n}\sum_{t=1}^{n}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}}\\
 & = & \frac{1}{\gamma_{n}}\frac{\sum_{t=1}^{n}K_{*}\left[\left(y-y_{t}\right)/\gamma_{n},\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}
\end{eqnarray*}

\end_inset

where we obtain the expressions for the joint and marginal densities from the section on kernel regression.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{./Examples/Nonparametric/ExampleKernelDensity.jl}{ExampleKernelDensity.jl}
\end_layout

\end_inset

 draws data from a 
\begin_inset Formula $\chi^{2}(3)$
\end_inset

 distribution and plots a kernel density fit,
 plus the true density.
 We see that they're pretty close,
 when the sample size is large enough for the kernel estimate to be precise.
 Try playing around with a smaller sample,
 and see what happens.
\end_layout

\begin_layout Example
\begin_inset Graphics
	filename Examples/Nonparametric/NPdensity.png
	width 10cm

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Semi-nonparametric maximum likelihood
\begin_inset CommandInset label
LatexCommand label
name "subsec:Semi-nonparametric-maximum-likel"

\end_inset


\end_layout

\begin_layout Standard

\series bold
Readings:

\series default
 Gallant and Nychka,
 
\emph on
Econometrica
\emph default
,
 1987.
 For a Fortran program to do this and a useful discussion in the user's guide,
 see 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{http://www.econ.duke.edu/~get/snp.html}{this link}
\end_layout

\end_inset

.
 See also Cameron and Johansson,
 
\emph on
Journal of Applied Econometrics
\emph default
,
 V.
 12,
 1997.
\end_layout

\begin_layout Standard
MLE is the estimation method of choice when we are confident about specifying the density.
 Is is possible to obtain the benefits of MLE when we're not so confident about the specification?
 In part,
 yes.
\end_layout

\begin_layout Standard
Suppose we're interested in the density of 
\begin_inset Formula $y$
\end_inset

 conditional on 
\begin_inset Formula $x$
\end_inset

 (both may be vectors).
 Suppose that the density 
\begin_inset Formula $f(y|x,\phi)$
\end_inset

 is a reasonable starting approximation to the true density.
 This density can be reshaped by multiplying it by a squared polynomial.
 The new density is 
\begin_inset Formula 
\[
g_{p}(y|x,\phi,\gamma)=\frac{h_{p}^{2}(y|\gamma)f(y|x,\phi)}{\eta_{p}(x,\phi,\gamma)}
\]

\end_inset

 where 
\begin_inset Formula 
\[
h_{p}(y|\gamma)=\sum_{k=0}^{p}\gamma_{k}y^{k}
\]

\end_inset

 and 
\begin_inset Formula $\eta_{p}(x,\phi,\gamma)$
\end_inset

 is a normalizing factor to make the density integrate (sum) to one.
 Because 
\begin_inset Formula $h_{p}^{2}(y|\gamma)/\eta_{p}(x,\phi,\gamma)$
\end_inset

 is a homogenous function of 
\begin_inset Formula $\theta$
\end_inset

 it is necessary to impose a normalization to identify the parameters:
 
\begin_inset Formula $\gamma_{0}$
\end_inset

 is set to 1.
 The normalization factor 
\begin_inset Formula $\eta_{p}(\phi,\gamma)$
\end_inset

 is calculated (following Cameron and Johansson) using
\begin_inset Formula 
\begin{eqnarray*}
E(Y^{r}) & = & \sum_{y=0}^{\infty}y^{r}f_{Y}(y|\phi,\gamma)\\
 & = & \sum_{y=0}^{\infty}y^{r}\frac{\left[h_{p}\left(y|\gamma\right)\right]^{2}}{\eta_{p}(\phi,\gamma)}f_{Y}(y|\phi)\\
 & = & \sum_{y=0}^{\infty}\sum_{k=0}^{p}\sum_{l=0}^{p}y^{r}f_{Y}(y|\phi)\gamma_{k}\gamma_{l}y^{k}y^{l}/\eta_{p}(\phi,\gamma)\\
 & = & \sum_{k=0}^{p}\sum_{l=0}^{p}\gamma_{k}\gamma_{l}\left\{ \sum_{y=0}^{\infty}y^{r+k+l}f_{Y}(y|\phi)\right\} /\eta_{p}(\phi,\gamma)\\
 & = & \sum_{k=0}^{p}\sum_{l=0}^{p}\gamma_{k}\gamma_{l}m_{k+l+r}/\eta_{p}(\phi,\gamma).
\end{eqnarray*}

\end_inset

By setting 
\begin_inset Formula $r=0$
\end_inset

 we get that the normalizing factor is
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand ref
reference "normfactor"
nolink "false"

\end_inset


\begin_inset Formula 
\begin{equation}
\eta_{p}(\phi,\gamma)=\sum_{k=0}^{p}\sum_{l=0}^{p}\gamma_{k}\gamma_{l}m_{k+l}\label{normfactor}
\end{equation}

\end_inset

Recall that 
\begin_inset Formula $\gamma_{0}$
\end_inset

 is set to 1 to achieve identification.
 The 
\begin_inset Formula $m_{r}$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "normfactor"
nolink "false"

\end_inset

 are the raw moments of the baseline density.
 Gallant and Nychka (1987) give conditions under which such a density may be treated as correctly specified,
 asymptotically.
 Basically,
 the order of the polynomial must increase as the sample size increases.
 However,
 there are technicalities.
\end_layout

\begin_layout Standard
Similarly to Cameron and Johannson (1997),
 we may develop a negative binomial polynomial (NBP) density for count data.
 The negative binomial baseline density may be written (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:negbindensity"
nolink "false"

\end_inset

) as
\begin_inset Formula 
\[
f_{Y}(y|\phi)=\frac{\Gamma(y+\psi)}{\Gamma(y+1)\Gamma(\psi)}\left(\frac{\psi}{\psi+\lambda}\right)^{\psi}\left(\frac{\lambda}{\psi+\lambda}\right)^{y}
\]

\end_inset

 where 
\begin_inset Formula $\phi=\{\lambda,\psi\},$
\end_inset

 
\begin_inset Formula $\lambda>0$
\end_inset

 and 
\begin_inset Formula $\psi>0$
\end_inset

.
 The usual means of incorporating conditioning variables 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the parameterization 
\begin_inset Formula $\lambda=e^{\mathbf{x}^{\prime}\beta}$
\end_inset

.
 When 
\begin_inset Formula $\psi=\lambda/\alpha$
\end_inset

 we have the negative binomial-I model (NB-I).
 When 
\begin_inset Formula $\psi=1/\alpha$
\end_inset

 we have the negative binomial-II (NP-II) model.
 For the NB-I density,
 
\begin_inset Formula $V(Y)=\lambda+\alpha\lambda$
\end_inset

.
 In the case of the NB-II model,
 we have 
\begin_inset Formula $V(Y)=\lambda+\alpha\lambda^{2}$
\end_inset

.
 For both forms,
 
\begin_inset Formula $E(Y)=\lambda$
\end_inset

.
 
\end_layout

\begin_layout Standard
The reshaped density,
 with normalization to sum to one,
 is
\begin_inset Formula 
\begin{equation}
f_{Y}(y|\phi,\gamma)=\frac{\left[h_{p}\left(y|\gamma\right)\right]^{2}}{\eta_{p}(\phi,\gamma)}\frac{\Gamma(y+\psi)}{\Gamma(y+1)\Gamma(\psi)}\left(\frac{\psi}{\psi+\lambda}\right)^{\psi}\left(\frac{\lambda}{\psi+\lambda}\right)^{y}.\label{NBP}
\end{equation}

\end_inset

To get the normalization factor,
 we need the moment generating function:
\begin_inset Formula 
\begin{equation}
M_{Y}(t)=\psi^{\psi}\left(\lambda-e^{t}\lambda+\psi\right)^{-\psi}.\label{nbmgf}
\end{equation}

\end_inset

To illustrate,
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Negative-binomial-raw"
nolink "false"

\end_inset

 shows calculation of the first four raw moments of the NB density,
 calculated using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{http://www.mupad.org}{MuPAD}
\end_layout

\end_inset

,
 which is a Computer Algebra System that (used to be?) free for personal use.
 These are the moments you would need to use a second order polynomial 
\begin_inset Formula $(p=2)$
\end_inset

.
 MuPAD will output these results in the form of C code,
 which is relatively easy to edit to write the likelihood function for the model.
 This has been done in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Nonparametric/NegBinSNP.cc}{NegBinSNP.cc}
\end_layout

\end_inset

,
 which is a C++ version of this model that can be compiled to use with octave using the 
\family typewriter
mkoctfile
\family default
 command.
 Note the impressive length of the expressions when the degree of the expansion is 4 or 5!
 This is an example of a model that would be difficult to formulate without the help of a program like 
\emph on
MuPAD.
\emph default

\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Negative-binomial-raw"

\end_inset

Negative binomial raw moments
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/mupad.png
	width 5in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is possible that there is conditional heterogeneity such that the appropriate reshaping should be more local.
 This can be accomodated by allowing the 
\begin_inset Formula $\gamma_{k}$
\end_inset

 parameters to depend upon the conditioning variables,
 for example using polynomials.
\end_layout

\begin_layout Standard
Gallant and Nychka,
 
\emph on
Econometrica
\emph default
,
 1987 prove that this sort of density can approximate a wide variety of densities arbitrarily well as the degree of the polynomial increases with the sample size.
 This approach is not without its drawbacks:
 the sample objective function can have an 
\emph on
extremely
\emph default
 large number of local maxima that can lead to numeric difficulties.
 If someone could figure out how to do in a way such that the sample objective function was nice and smooth,
 they would probably get the paper published in a good journal.
 Any ideas?
\end_layout

\begin_layout Standard
Here's a plot of true and the limiting SNP approximations (with the order of the polynomial fixed) to four different count data densities,
 which variously exhibit over and underdispersion,
 as well as excess zeros.
 The baseline model is a negative binomial density.
\end_layout

\begin_layout Standard
\begin_inset VSpace 0.5001cm
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Examples/Figures/SNP.pdf
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 0.5001cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Standard
Some of these examples are old,
 using Octave code.
 I may try to get around to translating them.
\end_layout

\begin_layout Subsection
MEPS health care usage data
\end_layout

\begin_layout Standard
We'll use the MEPS OBDV data to illustrate kernel regression and semi-nonparametric maximum likelihood.
\end_layout

\begin_layout Subsubsection
Kernel regression estimation
\end_layout

\begin_layout Standard
Let's try a kernel regression fit for the OBDV data.
 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Nonparametric/OBDVkernel.m}{OBDVkernel.m} 
\end_layout

\end_inset

 loads the MEPS OBDV data,
 computes kernel regression estimates using the same conditioning variables as in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"
nolink "false"

\end_inset

,
 and plots the fitted OBDV usage versus AGE and INCOME.
 The plots are in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kernel-regression-fits,"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that usage increases with age,
 just as we've seen with the parametric models.
 
\end_layout

\begin_layout Itemize
Note that for income,
 there is a U shape.
 Previously,
 we found that income appeared to be insignificant (run EstimatePoisson to see it again).
 
\end_layout

\begin_deeper
\begin_layout Itemize
Perhaps that insignificance was due to omitting a nonlinear effect (e.g.,
 quadratic).
\end_layout

\begin_layout Itemize
The U shape could also be due to ignoring endogeneity of income.
 If a person is seriously ill,
 they may make more doctor visits,
 but may also suffer loss of income due to reduces work hours.
\end_layout

\begin_layout Itemize
Another explanation might be that kernel regression has a high variance in regions of data sparseness,
 so that for very low or high incomes,
 an outlier or two can have a big impact 
\end_layout

\end_deeper
\begin_layout Itemize
Nonparametric analysis can help us to learn what might be appropriate parametric models,
 by helping to identify potential problems with a parametric model 
\end_layout

\begin_layout Itemize
Once could use bootstrapping or other methods to generate a confidence intervals for the fits.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kernel-regression-fits,"

\end_inset

Kernel regression fits,
 OBDV health care usage versus AGE and INCOME
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Kernel-fitted-OBDV"

\end_inset

Kernel fitted OBDV usage versus AGE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/kernelfit.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Kernel-fitted-OBDV-income"

\end_inset

Kernel fitted OBDV usage versus INCOME
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/OBDVvsIncome.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Financial data and volatility
\end_layout

\begin_layout Standard
The data set 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Nonparametric/SpotRate/rates}{rates}
\end_layout

\end_inset

 contains the growth rate (100
\begin_inset Formula $\times$
\end_inset

log difference) of the daily spot $/euro and $/yen exchange rates at New York,
 noon,
 from January 04,
 1999 to February 12,
 2008.
 There are 2291 observations.
 See the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Nonparametric/SpotRate/README}{README}
\end_layout

\end_inset

 file for details.
 Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dollar-Euro"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dollar-Yen"
nolink "false"

\end_inset

 show the data and their histograms.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dollar-Euro"

\end_inset

Dollar-Euro
\end_layout

\end_inset


\begin_inset Graphics
	filename Examples/Nonparametric/dollar_euro_historgram.png
	lyxscale 10
	width 6cm

\end_inset


\begin_inset Graphics
	filename Examples/Nonparametric/dollar_euro_series.png
	lyxscale 10
	width 6cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dollar-Yen"

\end_inset

Dollar-Yen
\end_layout

\end_inset


\begin_inset Graphics
	filename Examples/Nonparametric/dollar_yen_histogram.png
	lyxscale 10
	width 6cm

\end_inset


\begin_inset Graphics
	filename Examples/Nonparametric/dollar_yen_series.png
	lyxscale 10
	width 6cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
at the center of the histograms,
 the bars extend above the normal density that best fits the data,
 and the tails are fatter than those of the best fit normal density.
 This feature of the data is known as 
\emph on
leptokurtosis
\begin_inset Index idx
range none
pageformat default
status open

\begin_layout Plain Layout
leptokurtosis
\end_layout

\end_inset


\emph default
.
\end_layout

\begin_layout Itemize
in the series plots,
 we can see that the variance of the growth rates is not constant over time.
 Volatility clusters are apparent,
 alternating between periods of stability and periods of more wild swings.
 This is known as 
\begin_inset Index idx
range none
pageformat default
status open

\begin_layout Plain Layout
conditional heteroscedasticity
\end_layout

\end_inset


\emph on
conditional heteroscedasticity
\emph default
.
 
\begin_inset Index idx
range none
pageformat default
status open

\begin_layout Plain Layout
ARCH
\end_layout

\end_inset

ARCH and 
\begin_inset Index idx
range none
pageformat default
status open

\begin_layout Plain Layout
GARCH
\end_layout

\end_inset

GARCH well-known models that are often applied to this sort of data.
\end_layout

\begin_layout Itemize
Many structural economic models often cannot generate data that exhibits conditional heteroscedasticity without directly assuming shocks that are conditionally heteroscedastic.
 It would be nice to have an economic explanation for how conditional heteroscedasticity,
 leptokurtosis,
 and other (leverage,
 etc.) features of financial data result from the behavior of economic agents,
 rather than from a black box that provides shocks.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Nonparametric/SpotRate/kernelfit.m}{kernelfit.m}
\end_layout

\end_inset

 performs kernel regression to fit 
\begin_inset Formula $E(y_{t}^{2}|y_{t-1,}^{2}y_{t-2}^{2})$
\end_inset

,
 and generates the plots in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kernel reg spot rates"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Itemize
From the point of view of learning the practical aspects of kernel regression,
 note how the data is compactified in the example script.
\end_layout

\begin_layout Itemize
In the Figure,
 note how current volatility depends on lags of the squared return rate - it is high when both of the lags are high,
 but drops off quickly when either of the lags is low.
\end_layout

\begin_layout Itemize
The fact that the plots are not flat suggests that this conditional moment contain information about the process that generates the data.
 Perhaps attempting to match this moment might be a means of estimating the parameters of the dgp.
 We'll come back to this later.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kernel reg spot rates"

\end_inset

Kernel regression fitted conditional second moments,
 Yen/Dollar and Euro/Dollar
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Yen/Dollar
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/SpotRate/yendollar.png
	width 8cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Euro/Dollar
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/SpotRate/eurodollar.png
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Additional kernel regression examples
\end_layout

\begin_layout Standard
There is a basic example of kernel regression and kernel density estimation in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./MyOctaveFiles/Econometrics/Kernel/kernel
\backslash
_example.m}{kernel
\backslash
_example.m} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
There is another example of local constant and local linear kernel regression in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./MyOctaveFiles/Econometrics/Kernel/kernel
\backslash
_local
\backslash
_linear
\backslash
_example.m}{kernel
\backslash
_local
\backslash
_linear
\backslash
_example.m} 
\end_layout

\end_inset

.
 With that,
 you can experiment with different bandwidths.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Seminonparametric ML estimation and the MEPS data
\end_layout

\begin_layout Standard
Now let's estimate a seminonparametric density for the OBDV data.
 We'll reshape a negative binomial density,
 as discussed above.
 The program 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{./Examples/Nonparametric/EstimateNBSNP.m}{EstimateNBSNP.m} 
\end_layout

\end_inset

 loads the MEPS OBDV data and estimates the model,
 using a NB-I baseline density and a 2nd order polynomial expansion.
 The output is:
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/Nonparametric/NBSNP.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that the CAIC and BIC are lower for this model than for the models presented in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Information-Criteria,-OBDV"
nolink "false"

\end_inset

.
 This model fits well,
 still being parsimonious.
 You can play around trying other use measures,
 using a NP-II baseline density,
 and using other orders of expansions.
 Density functions formed in this way may have 
\series bold
MANY
\series default
 local maxima,
 so you need to be careful before accepting the results of a casual run.
 To guard against having converged to a local maximum,
 one can try using multiple starting values,
 or one could try simulated annealing as an optimization method.
 If you uncomment the relevant lines in the program,
 you can use SA to do the minimization.
 This will take a 
\emph on
lot
\emph default
 of time,
 compared to the default BFGS minimization.
 The chapter on parallel computations might be interesting to read before trying this.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Limited-information-nonparametri"

\end_inset

Limited information nonparametric filtering
\end_layout

\begin_layout Standard
Add discussion from JEF paper.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
In Octave,
 type 
\begin_inset Quotes sld
\end_inset


\family typewriter
edit kernel_example
\family default

\begin_inset Quotes srd
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
Look this script over,
 and describe in words what it does.
\end_layout

\begin_layout Enumerate
Run the script and interpret the output.
\end_layout

\begin_layout Enumerate
Experiment with different bandwidths,
 and comment on the effects of choosing small and large values.
\end_layout

\end_deeper
\begin_layout Enumerate
In Octave,
 type 
\begin_inset Quotes sld
\end_inset


\family typewriter
help kernel_regression
\family default

\begin_inset Quotes srd
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
How can a kernel fit be done without supplying a bandwidth?
\end_layout

\begin_layout Enumerate
How is the bandwidth chosen if a value is not provided?
\end_layout

\begin_layout Enumerate
What is the default kernel used?
\end_layout

\end_deeper
\begin_layout Enumerate
Using the Octave script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
href{./Examples/Nonparametric/OBDVkernel.m}{OBDVkernel.m} 
\end_layout

\end_inset

 as a model,
 plot kernel regression fits for OBDV visits as a function of income and education.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Quantile regression
\end_layout

\begin_layout Standard
References:
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

,
 Chapter 4,
 
\begin_inset CommandInset citation
LatexCommand citet
key "koenker1978"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand citet
key "koenker2001quantile"
literal "true"

\end_inset

,
 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHansen"
literal "true"

\end_inset

,
 and Chernozhukov's MIT OpenCourseWare notes,
 lecture 8 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "Chernozhukov's quantile reg notes"
target "http://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/lecture08.pdf"
literal "false"

\end_inset


\bar default
.
\end_layout

\begin_layout Standard
This chapter gives an outline of quantile regression.
 The quantile IV estimator provides an opportunity to explore MCMC methods.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Conditional quantile,
 definition
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\alpha$
\end_inset

 quantile of a random variable 
\begin_inset Formula $Y$
\end_inset

,
 conditional on 
\begin_inset Formula $X=x$
\end_inset

 (notation:
 
\begin_inset Formula $Y_{\alpha|X=x})$
\end_inset

 is the smallest value 
\begin_inset Formula $z$
\end_inset

 such that 
\begin_inset Formula $Pr(Y\leq z|X=x)=\alpha$
\end_inset

.
 
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $F_{Y|X=x}$
\end_inset

 is the conditional CDF of 
\begin_inset Formula $Y,$
\end_inset

 then the 
\begin_inset Formula $\alpha$
\end_inset

-conditional quantile is 
\begin_inset Formula 
\[
Y_{\alpha|X=x}=\text{\ensuremath{\inf}\,}y:\alpha\leq F_{Y|X=x}(y).
\]

\end_inset

 
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $\alpha=0.5,$
\end_inset

 we are talking about the conditional median 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $Y_{0.5|X=x}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
,
 but we could be interested in other quantiles,
 too.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The linear regression model is focused on the conditional mean of the dependent variable.
 
\end_layout

\begin_layout Itemize
However,
 when looking at economic policies,
 we're often interested in distributional effects:
\end_layout

\begin_deeper
\begin_layout Itemize
we may like to know how the rich and poor may be differentially affected by a policy that provides a public good
\end_layout

\begin_layout Itemize
or we might like to know how a training program affects low-performing students compared to high-performing students 
\end_layout

\end_deeper
\begin_layout Itemize
For these sorts of issues,
 we're not concerned with the average agent:
 we want to know about the extremes,
 too.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Quantiles of the linear regression model
\end_layout

\begin_layout Standard
The classical linear regression model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\epsilon_{t}$
\end_inset

 with normal errors implies that the distribution of 
\begin_inset Formula $y_{t}$
\end_inset

 conditional on 
\begin_inset Formula $x_{t}$
\end_inset

 is
\begin_inset Formula 
\[
y_{t}\sim N(x_{t}^{\prime}\beta,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $Pr(Y<x^{\prime}\beta|X=x)=0.5$
\end_inset

 when the model follows the classical assumptions with normal errors,
 because the normal distribution is symmetric about the mean,
 so the mean and the median are the same,
 that is,
 
\begin_inset Formula $Y_{0.5|X=x}=x^{\prime}\beta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
One can estimate the conditional median just by using the fitted conditional mean,
 because the mean and median are the same,
 given normality.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
How about other quantiles?
 We have 
\begin_inset Formula $y=x^{\prime}\beta+\epsilon$
\end_inset

 and 
\begin_inset Formula $\epsilon\sim N(0,\sigma^{2})$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Conditional on 
\begin_inset Formula $x$
\end_inset

,
 
\begin_inset Formula $x^{\prime}\beta$
\end_inset

 is given,
 and the distribution of 
\begin_inset Formula $\epsilon$
\end_inset

 does not depend on 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\epsilon/\sigma$
\end_inset

 is standard normal,
 and the 
\begin_inset Formula $\alpha$
\end_inset

 quantile of 
\begin_inset Formula $\epsilon/\sigma$
\end_inset

 is simply the inverse of the standard normal CDF evaulated at 
\begin_inset Formula $\alpha,$
\end_inset

 
\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

,
 where 
\begin_inset Formula $\Phi$
\end_inset

 is the standard normal CDF function.
 
\end_layout

\begin_layout Itemize
The probit function 
\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

 is tabulated (or can be found in Julia using 
\family typewriter
using Distributions;
 y = quantile.(Normal(),range(0.001,stop=0.999,length=200))
\family default
.
 It is plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inverse-CDF-for"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Inverse-CDF-for"

\end_inset

Inverse CDF for N(0,1)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/norminv.png
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\alpha$
\end_inset

 quantile of 
\begin_inset Formula $\epsilon$
\end_inset

 is 
\begin_inset Formula $\sigma\Phi^{-1}(\alpha).$
\end_inset

 Thus,
 the 
\begin_inset Formula $\alpha$
\end_inset

 conditional quantile of 
\begin_inset Formula $y$
\end_inset

 is 
\begin_inset Formula $Y_{\alpha|X=x}=x^{\prime}\beta+$
\end_inset


\begin_inset Formula $\sigma$
\end_inset


\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

.
 Some quantiles are pictured in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Quantiles-of-classical"
nolink "false"

\end_inset

.
 These give confidence intervals for the the fitted value,
 
\begin_inset Formula $x^{\prime}\beta$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Quantiles-of-classical"

\end_inset

Quantiles of classical linear regression model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/quantiles.jpg
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The conditional quantiles for the classical model are 
\emph on
parallel
\emph default
,
 
\emph on
linear
\emph default
 functions of 
\begin_inset Formula $x$
\end_inset


\end_layout

\begin_layout Itemize
all have the same slope:
 the only thing that changes with 
\begin_inset Formula $\alpha$
\end_inset

 is the intercept 
\begin_inset Formula $\sigma$
\end_inset


\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

.
\end_layout

\begin_layout Itemize
If the error is heteroscedastic,
 so that 
\begin_inset Formula $\sigma=\sigma(x)$
\end_inset

,
 quantiles can have 
\bar under
different slopes
\bar default
,
 and given quantiles may be nonlinear functions of 
\begin_inset Formula $x$
\end_inset

,
 depending on the form of the heteroscedasticity.
 
\emph on
Draw a picture.
\emph default

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Fully nonparametric conditional quantiles
\end_layout

\begin_layout Standard
To compute conditional quantiles for the classical linear model,
 we used the assumption of normality.
 Can we estimate conditional quantiles without making distributional assumptions?
 Yes,
 we can!
 (nod to Obama) (a note from 2018:
 those were the good old days!).
 You can do fully nonparametric conditional density estimation,
 as in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Nonparametric-inference"
nolink "false"

\end_inset

,
 and use the fitted conditional density to compute quantiles.
\end_layout

\begin_layout Itemize
Note that estimating quantiles where 
\begin_inset Formula $\alpha$
\end_inset

 is close to 0 or 1 is difficult,
 because you have few observations that lie in the neighborhood of the quantile,
 so you should expect a large variance if you go the nonparametric route.
 For more central quantiles,
 like the median,
 this will be less of a problem.
\end_layout

\begin_layout Itemize
For this reason,
 we may go the 
\emph on
semi-parametric
\emph default
 route,
 which imposes more structure.
 When people talk about quantile regression,
 they usually mean the semi-parametric approach.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Quantile regression as a semi-parametric estimator
\end_layout

\begin_layout Standard
The most widely used method does not take either of the extreme positions,
 it is not fully parametric,
 like the linear regression model with known distribution of errors,
 but some parametric restrictions are made,
 to improve efficiency compared to the fully nonparametric approach.
\end_layout

\begin_layout Itemize
The assumption is that the 
\begin_inset Formula $\alpha$
\end_inset

-conditional quantile of the dependent variable 
\begin_inset Formula $Y$
\end_inset

 is a linear function of the conditioning variables 
\begin_inset Formula $X$
\end_inset

:
 
\begin_inset Formula $Y_{\alpha|X=x}=x^{\prime}\beta_{\alpha}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
This is a generalization of what we get from the classical model with normality,
 where the slopes of the quantiles with respect to the regressors are constant for all 
\begin_inset Formula $\alpha$
\end_inset

:
 
\end_layout

\begin_deeper
\begin_layout Itemize
For the classical model with normality,
 
\begin_inset Formula $\frac{\partial}{\partial x}Y_{\alpha|X=x}=\beta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
With the assumption of linear quantiles without distributional assumptions,
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\frac{\partial}{\partial x}Y_{\alpha|X=x}=\beta_{\alpha}$
\end_inset

,
 so the slopes (and constants) are allowed to change with 
\begin_inset Formula $\alpha$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
This is a step in the direction of flexibility,
 but it also means we need to estimate many parameters if we're interested in many quantiles:
 there may be an efficiency loss due to using many parameters to avoid distributional assumptions.
\end_layout

\begin_layout Itemize
The question is how to estimate 
\begin_inset Formula $\beta_{\alpha}$
\end_inset

 when we don't make distributional assumptions.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
It turns out that the problem can be expressed as an extremum estimator:
 
\begin_inset Formula $\widehat{\beta_{\alpha}}=\arg\min s_{n}(\beta)$
\end_inset

 where
\begin_inset Formula 
\[
s_{n}(\beta)=\sum_{i=1}^{n}\left[1(y_{i}\geq x_{i}^{\prime}\beta_{\alpha})\alpha+1(y_{i}<x_{i}^{\prime}\beta_{\alpha})(1-\alpha)\right]\left|y_{i}-x_{i}^{\prime}\beta_{\alpha}\right|
\]

\end_inset

First,
 suppose that 
\begin_inset Formula $\alpha=0.5,$
\end_inset

 so we are estimating the median.
 Then the objective simplifies to minimizing the absolute deviations:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{n}(\beta)=\sum_{i=1}^{n}\left|y_{i}-x_{i}^{\prime}\beta_{\alpha}\right|
\]

\end_inset


\end_layout

\begin_layout Standard
The presence of the weights in the general version accounts for the fact that if we're estimating the 
\begin_inset Formula $\alpha=0.1$
\end_inset

 quantile,
 we expect 90% of the 
\begin_inset Formula $y_{i}$
\end_inset

 to be greater than 
\begin_inset Formula $x_{i}^{\prime}\beta_{\alpha}$
\end_inset

,
 and only 10% to be smaller.
 We need to down-weight the likely events and up-weight the unlikely events so that the objective function minimizes at the appropriate place.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
One note is that median regression may be a useful means of dealing with data that satisfies the classical assumptions,
 except for contamination by outliers.
 
\emph on
In class,
 use Gretl to show this.
\end_layout

\begin_layout Itemize
Note that the quantile regression objective function is discontinuous.
 Minimization can be done quickly using linear programming.
 BFGS won't work.
\end_layout

\begin_layout Itemize
the asymptotic distribution is normal,
 with the sandwich form typical of extremum estimators.
 Estimation of the terms is not completely straightforward,
 so methods like bootstrapping may be preferable.
\end_layout

\begin_layout Itemize
the asymptotic variance depends upon which quantile we're estimating.
 When 
\begin_inset Formula $\alpha$
\end_inset

 is close to 0 or 1,
 the asymptotic variance becomes large,
 and the asymptotic approximation is unreliable for the small sample distribution.
\end_layout

\begin_layout Itemize
Extreme quantiles are hard to estimate with precision,
 because the data is sparse in those regions.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The artificial data set 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Quantile/quantile.gdt}{quantile.gdt} 
\end_layout

\end_inset

 allows you to explore quantile regression with GRETL,
 and to see how median regression can help to deal with data contamination.
 
\end_layout

\begin_layout Itemize
If you do quantile regression of the variable y versus x,
 we are in a situation where the assumptions of the classical model hold.
 Quantiles all have approximately the same slope (the true value is 1).
\end_layout

\begin_layout Itemize
With heteroscedastic data,
 the quantiles have different slopes.
\end_layout

\begin_layout Itemize
see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Quantile-regression-results"
nolink "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Quantile-regression-results"

\end_inset

Quantile regression results
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
homoscedastic data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/qreg1.png
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
heteroscedastic data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/qreg2.png
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Suppose that 
\begin_inset Formula $y$
\end_inset

 depends on a single regressor,
 
\begin_inset Formula $x$
\end_inset

.
 Think about how you could do quantile regression estimation where the quantiles are nonlinear functions of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Returns to schooling:
 quantile regression,
 quantile IV regression,
 and Bayesian GMM via MCMC
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "card1993using"
literal "true"

\end_inset

 presents an analysis of returns to schooling using the data from the National Longitudinal Survey of Young Men,
 for those interviewed in 1976.
 Card presents OLS and instrumental variables estimates for a number of specifications,
 using college proximity as an instrument for years of education,
 and age as an instrument for experience.
 Here,
 we work with the simple model from column (1) of Card's Table 2.
 Let's consider estimation of conditional quantiles for the model.
 The model is 
\begin_inset Formula 
\begin{align*}
Q_{\ln W|X}(\tau) & =\beta_{0}(\tau)+\beta_{EDUC}(\tau)EDUC+\beta_{X}(\tau)EXP+\beta_{EXP^{2}}(\tau)\frac{EXP^{2}}{100}\\
 & +\beta_{BLACK}(\tau)BLACK+\beta_{SMSA}(\tau)SMSA+\beta_{SOUTH}(\tau)SOUTH\\
 & \equiv X\beta(\tau)
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
the dependent variable 
\begin_inset Formula $\ln W$
\end_inset

 is log hourly earnings (in cents)
\end_layout

\begin_layout Itemize
the regressors are years of education (EDUC),
 experience (EXP),
 experience squared divided by 100,
 a black indicator (BLACK),
 a metropolitan area indicator (SMSA),
 and a South indicator (SOUTH).
\end_layout

\begin_layout Itemize
We explore estimation of quantiles treating all variables as exogenous,
 or treating education and experience as endogenous,
 and the others as exogenous.
\end_layout

\begin_layout Itemize
When education and experience are treated as endogenous,
 we use proximity to an accredited four year college (NEARC4) as an instrumental variable.
 EXPER is defined as EXPER = AGE-EDUC-6,
 so if EDUC is endogenous,
 so is EXPER.
 We use AGE as an instrument for EXPER.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
If all variables are taken as exogenous,
 then quantile regression (QR) estimates may be obtained by standard methods,
 as implemented in the GRETL software package.
 
\end_layout

\begin_layout Itemize
The Card data set is provided with the Wooldridge data set for GRETL,
 see the GRETL web page.
 A version prepared for the model used here is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Data/card.gdt}{card.gdt} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Itemize
QR results from GRETL for EDUC are in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:QR-results-for"
nolink "false"

\end_inset

.
 Note that the QR results are pretty close to the OLS results,
 for all quantiles,
 and there's no clear pattern of the effect of education differing across quantiles.
 
\end_layout

\begin_layout Itemize
The effect of an additional year of education on earnings is about 7-8%,
 all across the distribution.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:QR-results-for"

\end_inset

QR results for the Card data,
 
\begin_inset Formula $\tau$
\end_inset

 sequence
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{EDUC}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

,
 with 95% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/QReduc.pdf
	width 12cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{0}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

,
 with 95% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/QRconst.pdf
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
If education and experience are taken as endogenous,
 ordinary quantile regression will give biased estimates,
 just as OLS is biased and inconsistent with endogenous regressors.
 
\end_layout

\begin_layout Itemize
We may use an instrumental variables version of quantile regression,
 due to 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHansen"
literal "true"

\end_inset

.
 They show that the moment conditions 
\begin_inset Formula 
\[
m_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}Z_{i}\left(\tau-1\left[y_{i}\le X_{i}\beta(\tau)\right]\right)
\]

\end_inset

(where 
\begin_inset Formula $\theta=\beta(\tau))$
\end_inset

 have expectation zero at the true parameter values,
 and thus can be used for GMM estimation.
 
\end_layout

\begin_layout Itemize
We can show that,
 at the true parameter values 
\begin_inset Formula 
\[
\sqrt{n}m_{n}(\theta_{0})\rightarrow^{d}N(0,(\tau-\tau^{2})Q_{Z})
\]

\end_inset

so an estimate of the efficient weight matrix is the inverse of 
\begin_inset Formula $\hat{\Sigma}=\frac{(\tau-\tau^{2})}{n}\sum_{i}Z_{i}Z_{i}^{\prime}.$
\end_inset

 
\end_layout

\begin_layout Itemize
The problem is that these moment conditions are discontinuous functions of the parameters,
 due to the indicator function,
 so gradient-based optimization methods will not work for computing the GMM estimates.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
To deal with this problem,
 we can consider using the MCMC methods proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHong2003"
literal "true"

\end_inset

 to compute a Bayesian version of the GMM estimator.
 
\end_layout

\begin_layout Itemize
This estimator works with the asymptotic distribution of the moment conditions to define the likelihood used in MCMC,
 rather than the full sample likelihood function,
 but otherwise,
 it is standard MCMC.
\end_layout

\begin_deeper
\begin_layout Itemize
the use of moment conditions is a dimension reducing operation:
 the full sample likelihood requires knowing the distribution of 
\begin_inset Formula $n$
\end_inset

 (growing with the sample size) random variables,
 while the use of moment conditions and their asymptotic distribution only requires knowing the (asymptotic) distribution of 
\begin_inset Formula $G$
\end_inset

 (fixed and finite) random variables
\end_layout

\begin_layout Itemize
thus,
 GMM is like a limited information ML estimator,
 with the asymptotic distribution substituting the actual small sample distribution.
\end_layout

\end_deeper
\begin_layout Itemize
The model is implemented in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Quantile/QIVmodel.jl}{QIVmodel.jl} 
\end_layout

\end_inset

,
 and the estimation by MCMC is done in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Quantile/QIVbyMCMC.jl}{QIVbyMCMC} 
\end_layout

\end_inset

.
 It may be of interest to examine the code to see how posterior means and 90% confidence intervals are computed using the Chernozhukhov-Hong method.
\end_layout

\begin_layout Itemize
For those of you interested in MCMC,
 there is the file 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{./Examples/Quantile/PlayWithMCMC.jl}{PlayWithMCMC.jl} 
\end_layout

\end_inset

,
 which studies the MCMC chain a bit.
 The basic proposal draws the parameter from independent normal densities.
 The second proposal draws the parameters from a joint normal density that accounts for correlations in the posterior.
 The empirical results reported below don't depend on which proposal is used,
 though,
 as a long enough chain was used so that the difference washes out.
 This is an issue of computational efficiency,
 not one of statistical reliability.
 To obtain reliable results with a shorter chain,
 the proposal density should be chosen to ensure good mixing (sampling from the whole support of the posterior).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Two-chains-for"

\end_inset

Two chains for 
\begin_inset Formula $\beta_{0}(\tau=0.5),$
\end_inset

 independent and correlated proposals
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Independent proposals,
 poor mixing
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/chain1.png
	width 20cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Correlated proposals,
 better mixing
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/chain2.png
	width 20cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The results are in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:IV-QR-results"
nolink "false"

\end_inset

,
 for 
\begin_inset Formula $\beta_{EDUC}(\tau)$
\end_inset

 and 
\begin_inset Formula $\beta_{0}(\tau)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
We can see that the IVQR results are substantially different from the ordinary QR results in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:QR-results-for"
nolink "false"

\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The effect of education,
 according to the IVQR results,
 is substantially larger,
 for all quantiles,
 with an additional year of education increasing all quantiles,
 except the 40th,
 by more than 20%.
 This is good news for the people in the U.S.
 that have to take out enormous student loans.
 Given the cost of college tuition in the U.S.,
 the miserable 7% return that OLS and ordinary QR find would probably not be enough to induce people to take out loans.
 So,
 we have external reasons to believe that this higher number may be more realistic.
 It would be interesting to study the evolution of returns over time,
 and compare them to the cost of education.
\end_layout

\begin_layout Itemize
There is a U shape,
 with a greater effect at the lower and higher quantiles.
 
\end_layout

\end_deeper
\begin_layout Itemize
The confidence bands are broader for the IV version,
 which is to be expected.
 This is similar to what happens with ordinary IV and OLS.
\end_layout

\begin_layout Itemize
The results are quite similar to those of 
\begin_inset CommandInset citation
LatexCommand citet
key "chernozhukovHansen2006instrumental"
literal "true"

\end_inset

,
 who estimate a similar model using the 
\begin_inset CommandInset citation
LatexCommand citet
key "angrist1991does"
literal "true"

\end_inset

 data (this is the influential paper that used quarter of birth as an instrument for education).
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:IV-QR-results"

\end_inset

IV-QR results
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{EDUC}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

,
 with 90% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/Educ.png
	width 12cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{0}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

,
 with 90% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/Constant.png
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Notation and Review
\end_layout

\begin_layout Itemize
All vectors will be column vectors,
 unless they have a transpose symbol (or I forget to apply this rule - your help catching typos and er0rors is much appreciated).
 For example,
 if 
\begin_inset Formula $x_{t}$
\end_inset

 is a 
\begin_inset Formula $p\times1$
\end_inset

 vector,
 
\begin_inset Formula $x_{t}^{\prime}$
\end_inset

 is a 
\begin_inset Formula $1\times p$
\end_inset

 vector.
 When I refer to a 
\begin_inset Formula $p$
\end_inset

-vector,
 I mean a column vector.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Notation-for-differentiation"

\end_inset

Notation for differentiation of vectors and matrices
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "GallantNonlinearStatisticalModels"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $s(\cdot):\Re^{p}\rightarrow\Re$
\end_inset

 be a real valued function of the 
\begin_inset Formula $p$
\end_inset

-vector 
\begin_inset Formula $\theta.$
\end_inset

 Then 
\begin_inset Formula $\frac{\partial s(\theta)}{\partial\theta}$
\end_inset

 is organized as a 
\begin_inset Formula $p$
\end_inset

-vector,
 
\begin_inset Formula 
\[
\frac{\partial s(\theta)}{\partial\theta}=\left[\begin{array}{c}
\frac{\partial s(\theta)}{\partial\theta_{1}}\\
\frac{\partial s(\theta)}{\partial\theta_{2}}\\
\vdots\\
\frac{\partial s(\theta)}{\partial\theta_{p}}
\end{array}\right]
\]

\end_inset

 Following this convention,
\begin_inset Formula $\frac{\partial s(\theta)}{\partial\theta^{\prime}}$
\end_inset

is a 
\begin_inset Formula $1\times p$
\end_inset

 vector
\begin_inset Formula $,$
\end_inset

 and 
\begin_inset Formula $\frac{\partial^{2}s(\theta)}{\partial\theta\partial\theta^{\prime}}$
\end_inset

 is a 
\begin_inset Formula $p\times p$
\end_inset

 matrix.
 Also,
\begin_inset Formula 
\[
\frac{\partial^{2}s(\theta)}{\partial\theta\partial\theta^{\prime}}=\frac{\partial}{\partial\theta}\left(\frac{\partial s(\theta)}{\partial\theta^{\prime}}\right)=\frac{\partial}{\partial\theta^{\prime}}\left(\frac{\partial s(\theta)}{\partial\theta}\right).
\]

\end_inset


\end_layout

\begin_layout Exercise
For 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 both 
\begin_inset Formula $p$
\end_inset

-vectors,
 show that 
\begin_inset Formula $\frac{\partial a^{\prime}x}{\partial x}=a$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f(\theta)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 be a 
\begin_inset Formula $n$
\end_inset

-vector valued function of the 
\begin_inset Formula $p$
\end_inset

-vector 
\begin_inset Formula $\theta$
\end_inset

.
 Let 
\begin_inset Formula $f(\theta)^{\prime}$
\end_inset

 be the 
\begin_inset Formula $1\times n$
\end_inset

 valued transpose of 
\begin_inset Formula $f$
\end_inset

 .
 Then 
\begin_inset Formula $\left(\frac{\partial}{\partial\theta}f(\theta)^{\prime}\right)^{\prime}=\frac{\partial}{\partial\theta^{\prime}}f(\theta).$
\end_inset

 
\end_layout

\begin_layout Definition*
Product rule.
 
\begin_inset CommandInset label
LatexCommand label
name "def: Product-rule.-Let"

\end_inset

Let 
\begin_inset Formula $f(\theta)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 and 
\begin_inset Formula $h(\theta)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 be 
\begin_inset Formula $n$
\end_inset

-vector valued functions of the 
\begin_inset Formula $p$
\end_inset

-vector 
\begin_inset Formula $\theta$
\end_inset

.
 Then 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta^{\prime}}h(\theta)^{\prime}f(\theta)=h^{\prime}\left(\frac{\partial}{\partial\theta^{\prime}}f\right)+f^{\prime}\left(\frac{\partial}{\partial\theta^{\prime}}h\right)
\]

\end_inset

 has dimension 
\begin_inset Formula $1\times p.$
\end_inset

 Applying the transposition rule we get 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}h(\theta)^{\prime}f(\theta)=\left(\frac{\partial}{\partial\theta}f^{\prime}\right)h+\left(\frac{\partial}{\partial\theta}h^{\prime}\right)f
\]

\end_inset

 which has dimension 
\begin_inset Formula $p\times1.$
\end_inset


\end_layout

\begin_layout Exercise
For 
\begin_inset Formula $A$
\end_inset

 a 
\begin_inset Formula $p\times p$
\end_inset

 matrix and 
\begin_inset Formula $x$
\end_inset

 a 
\begin_inset Formula $p\times1$
\end_inset

 vector,
 show that 
\begin_inset Formula $\frac{\partial x^{\prime}Ax}{\partial x}=\left(A+A^{\prime}\right)x$
\end_inset

.
 Also,
 what is the result if 
\begin_inset Formula $A$
\end_inset

 is symmetric?
\end_layout

\begin_layout Definition
Chain rule.
\begin_inset CommandInset label
LatexCommand label
name "def: Chain-rule.-Let"

\end_inset

 Let 
\begin_inset Formula $f(\cdot)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 a 
\begin_inset Formula $n$
\end_inset

-vector valued function of a 
\begin_inset Formula $p$
\end_inset

-vector argument,
 and let 
\begin_inset Formula $g()$
\end_inset

:
\begin_inset Formula $\Re^{r}\rightarrow\Re^{p}$
\end_inset

 be a 
\begin_inset Formula $p$
\end_inset

-vector valued function of an 
\begin_inset Formula $r$
\end_inset

-vector valued argument 
\begin_inset Formula $\rho$
\end_inset

.
 Then 
\begin_inset Formula 
\[
\frac{\partial}{\partial\rho^{\prime}}f\left[g\left(\rho\right)\right]=\left.\frac{\partial}{\partial\theta^{\prime}}f(\theta)\right|_{\theta=g(\rho)}\frac{\partial}{\partial\rho^{\prime}}g(\rho)
\]

\end_inset


\end_layout

\begin_layout Standard
has dimension 
\begin_inset Formula $n\times r.$
\end_inset


\end_layout

\begin_layout Exercise
For 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors,
 show that 
\begin_inset Formula $\frac{\partial\exp(x^{\prime}\beta)}{\partial\beta}=\exp(x^{\prime}\beta)x$
\end_inset

.
\end_layout

\begin_layout Section
Convergence modes 
\end_layout

\begin_layout Standard

\series bold
Readings:

\series default
 Davidson,
 R.
 and J.G.
 MacKinnon,
 
\emph on
Econometric Theory and Methods
\emph default
,
 Ch.
 4;
 Gallant,
 A.R.,
 
\emph on
An Introduction to Econometric Theory,
 
\emph default
Ch.
 4.
\end_layout

\begin_layout Standard
We will consider several modes of convergence.
 The first three modes discussed are simply for background.
 The stochastic modes are those which will be used later in the course.
\end_layout

\begin_layout Definition
 A sequence is a mapping from the natural numbers 
\begin_inset Formula $\{1,2,...\}=\{n\}_{n=1}^{\infty}=\{n\}$
\end_inset

 to some other set,
 so that the set is ordered according to the natural numbers associated with its elements.
 
\end_layout

\begin_layout Subsection*
Real-valued sequences:
\end_layout

\begin_layout Definition

\emph on
[Convergence]
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
Convergence,
 ordinary
\end_layout

\end_inset


\emph default
 A real-valued sequence of vectors 
\begin_inset Formula $\{a_{n}\}$
\end_inset

 
\emph on
converges
\emph default
 to the vector 
\begin_inset Formula $a$
\end_inset

 if for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

 there exists an integer 
\begin_inset Formula $N_{\varepsilon}$
\end_inset

 such that for all 
\begin_inset Formula $n>N_{\varepsilon},\parallel a_{n}-a\parallel<\varepsilon$
\end_inset

 .
 
\begin_inset Formula $a$
\end_inset

 is the 
\emph on
limit
\emph default
 of 
\begin_inset Formula $a_{n},$
\end_inset

 written 
\begin_inset Formula $a_{n}\rightarrow a.$
\end_inset


\end_layout

\begin_layout Subsection*
Deterministic real-valued functions
\end_layout

\begin_layout Standard
Consider a sequence of functions 
\begin_inset Formula $\{f_{n}(\omega)\}$
\end_inset

 where 
\begin_inset Formula 
\[
f_{n}:\Omega\rightarrow T\subseteq\Re.
\]

\end_inset

 
\begin_inset Formula $\Omega$
\end_inset

 may be an arbitrary set.
\end_layout

\begin_layout Definition

\emph on
[Pointwise convergence]
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
convergence,
 pointwise
\end_layout

\end_inset


\emph default
 A sequence of functions 
\begin_inset Formula $\{f_{n}(\omega)\}$
\end_inset

 
\emph on
converges pointwise
\emph default
 on
\emph on

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 {}
\end_layout

\end_inset


\emph default

\begin_inset Formula $\Omega$
\end_inset

 to the function 
\begin_inset Formula $f$
\end_inset

(
\begin_inset Formula $\omega)$
\end_inset

 if for all 
\begin_inset Formula $\varepsilon>0$
\end_inset

 and 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 there exists an integer 
\begin_inset Formula $N_{\varepsilon\omega}$
\end_inset

 such that 
\begin_inset Formula 
\[
|f_{n}(\omega)-f(\omega)|<\varepsilon,\forall n>N_{\varepsilon\omega}.
\]

\end_inset


\end_layout

\begin_layout Standard
It's important to note that 
\begin_inset Formula $N_{\varepsilon\omega}$
\end_inset

 depends upon 
\begin_inset Formula $\omega,$
\end_inset

 so that converge may be much more rapid for certain 
\begin_inset Formula $\omega$
\end_inset

 than for others.
 Uniform convergence requires a similar rate of convergence throughout 
\begin_inset Formula $\Omega.$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Uniform convergence]
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
convergence,
 uniform
\end_layout

\end_inset


\emph default
 A sequence of functions 
\begin_inset Formula $\{f_{n}(\omega)\}$
\end_inset

 
\emph on
converges uniformly
\emph default
 on
\emph on

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 {}
\end_layout

\end_inset


\emph default

\begin_inset Formula $\Omega$
\end_inset

 to the function 
\begin_inset Formula $f$
\end_inset

(
\begin_inset Formula $\omega)$
\end_inset

 if for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

 there exists an integer 
\begin_inset Formula $N$
\end_inset

 such that 
\begin_inset Formula 
\[
\sup_{\omega\in\Omega}|f_{n}(\omega)-f(\omega)|<\varepsilon,\forall n>N.
\]

\end_inset

 (insert a diagram here showing the envelope around 
\begin_inset Formula $f(\omega)$
\end_inset

 in which 
\begin_inset Formula $f_{n}(\omega)$
\end_inset

 must lie).
\end_layout

\begin_layout Subsection*
Stochastic sequences
\end_layout

\begin_layout Standard
In econometrics,
 we typically deal with stochastic sequences.
 Given a probability space 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right),$
\end_inset

 recall that a random variable maps the sample space to the real line
\shape italic
,

\shape default
 
\shape italic
\emph on
i.e.
\emph default
,

\shape default
 
\begin_inset Formula $X(\omega):\Omega\rightarrow\Re.$
\end_inset

 A sequence of random variables 
\begin_inset Formula $\{X_{n}(\omega)\}$
\end_inset

 is a collection of such mappings,
 
\emph on
i.e.,
\shape italic
\emph default

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 {}
\end_layout

\end_inset


\shape default
each 
\begin_inset Formula $X_{n}(\omega)$
\end_inset

 is a random variable with respect to the probability space 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right).$
\end_inset

 For example,
 given the model 
\begin_inset Formula $Y=X\beta_{0}+\varepsilon,$
\end_inset

 the OLS estimator 
\begin_inset Formula $\hat{\beta}_{n}=\left(X^{\prime}X\right)^{-1}X^{\prime}Y,$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the sample size,
 can be used to form a sequence of random vectors 
\begin_inset Formula $\{\hat{\beta}_{n}\}$
\end_inset

.
 A number of modes of convergence are in use when dealing with sequences of random variables.
 Several such modes of convergence should already be familiar:
\end_layout

\begin_layout Definition

\emph on
[Convergence in probability]
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
convergence,
 in probability
\end_layout

\end_inset


\emph default
 Let 
\begin_inset Formula $X_{n}(\omega)$
\end_inset

 be a sequence of random variables,
 and let 
\begin_inset Formula $X(\omega)$
\end_inset

 be a random variable.
 Let 
\begin_inset Formula $\mathcal{A}_{n}=\{\omega:|X_{n}(\omega)-X(\omega)|>\varepsilon\}$
\end_inset

.
 Then 
\begin_inset Formula $\{X_{n}(\omega)\}$
\end_inset

 converges in probability to 
\begin_inset Formula $X(\omega)$
\end_inset

 if 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\mathcal{A}_{n}\right)=0,\forall\varepsilon>0.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
Convergence in probability is written as 
\begin_inset Formula $X_{n}\stackrel{p}{\rightarrow}X,$
\end_inset

 or plim 
\begin_inset Formula $X_{n}=X.$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Almost sure convergence]
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
convergence,
 almost sure
\end_layout

\end_inset


\emph default
 Let 
\begin_inset Formula $X_{n}(\omega)$
\end_inset

 be a sequence of random variables,
 and let 
\begin_inset Formula $X(\omega)$
\end_inset

 be a random variable.
 Let 
\begin_inset Formula $\mathcal{A}=\{\omega:\lim_{n\rightarrow\infty}X_{n}(\omega)=X(\omega)\}$
\end_inset

.
 Then 
\begin_inset Formula $\{X_{n}(\omega)\}$
\end_inset

 converges almost surely to 
\begin_inset Formula $X(\omega)$
\end_inset

 if 
\begin_inset Formula 
\[
P\left(\mathcal{A}\right)=1.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
In other words,
 
\begin_inset Formula $X_{n}(\omega)\rightarrow X(\omega)$
\end_inset

 (ordinary convergence of the two functions) except on a set 
\begin_inset Formula $C=\Omega-\mathcal{A}$
\end_inset

 such that 
\begin_inset Formula $P(C)=0.$
\end_inset

 Almost sure convergence is written as 
\begin_inset Formula $X_{n}\stackrel{a.s.}{\rightarrow}X,$
\end_inset

 or 
\begin_inset Formula $X_{n}\rightarrow X,\,a.s.$
\end_inset

 One can show that 
\begin_inset Formula 
\[
X_{n}\stackrel{a.s.}{\rightarrow}X\Rightarrow X_{n}\stackrel{p}{\rightarrow}X.
\]

\end_inset


\end_layout

\begin_layout Definition

\emph on
[Convergence in distribution]
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
convergence,
 in distribution
\end_layout

\end_inset


\emph default
 Let the r.v.
 
\begin_inset Formula $X_{n}$
\end_inset

 have distribution function 
\begin_inset Formula $F_{n}$
\end_inset

 and the r.v.
 
\begin_inset Formula $X_{n}$
\end_inset

 have distribution function 
\begin_inset Formula $F.$
\end_inset

 If 
\begin_inset Formula $F_{n}\rightarrow F$
\end_inset

 at every continuity point of 
\begin_inset Formula $F,$
\end_inset

 then 
\begin_inset Formula $X_{n}$
\end_inset

 converges in distribution to 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Standard
\noindent
Convergence in distribution is written as 
\begin_inset Formula $X_{n}\stackrel{d}{\rightarrow}X.$
\end_inset

 It can be shown that convergence in probability implies convergence in distribution.
\end_layout

\begin_layout Subsection*
Stochastic functions
\end_layout

\begin_layout Standard
Simple laws of large numbers (LLN's) allow us to directly conclude that 
\begin_inset Formula $\hat{\beta}_{n}\stackrel{a.s.}{\rightarrow}\beta_{0}$
\end_inset

 in the OLS example,
 since 
\begin_inset Formula 
\[
\hat{\beta}_{n}=\beta_{0}+\left(\frac{X^{\prime}X}{n}\right)^{-1}\left(\frac{X^{\prime}\varepsilon}{n}\right),
\]

\end_inset

 and 
\begin_inset Formula $\frac{X^{\prime}\varepsilon}{n}\stackrel{a.s.}{\rightarrow0}$
\end_inset

 by a SLLN.
 Note that this term is not a function of the parameter 
\begin_inset Formula $\beta.$
\end_inset

 This easy proof is a result of the linearity of the model,
 which allows us to express the estimator in a way that separates parameters from random functions.
 In general,
 this is not possible.
 We often deal with the more complicated situation where the stochastic sequence depends on parameters in a manner that is not reducible to a simple sequence of random variables.
 In this case,
 we have a sequence of random functions that depend on 
\begin_inset Formula $\theta$
\end_inset

:
 
\begin_inset Formula $\{X_{n}(\omega,\theta)\},$
\end_inset

 where each 
\begin_inset Formula $X_{n}(\omega,\theta)$
\end_inset

 is a random variable with respect to a probability space 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right)$
\end_inset

 and the parameter 
\begin_inset Formula $\theta$
\end_inset

 belongs to a parameter space 
\begin_inset Formula $\theta\in\Theta.$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Uniform almost sure convergence]
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
convergence,
 uniform almost sure
\end_layout

\end_inset


\emph default
 
\begin_inset Formula $\{X_{n}(\omega,\theta)\}$
\end_inset

 converges uniformly almost surely in 
\begin_inset Formula $\Theta$
\end_inset

 to 
\begin_inset Formula $X(\omega,\theta)$
\end_inset

 if 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{\theta\in\Theta}|X_{n}(\omega,\theta)-X(\omega,\theta)|=0,\text{(a.s.)}
\]

\end_inset


\end_layout

\begin_layout Standard
Implicit is the assumption that all 
\begin_inset Formula $X_{n}(\omega,\theta)$
\end_inset

 and 
\begin_inset Formula $X(\omega,\theta)$
\end_inset

 are random variables w.r.t.
 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right)$
\end_inset

 for all 
\begin_inset Formula $\theta\in\Theta.$
\end_inset

 We'll indicate uniform almost sure convergence by 
\begin_inset Formula $\stackrel{u.a.s.}{\rightarrow}$
\end_inset

 and uniform convergence in probability by 
\begin_inset Formula $\stackrel{u.p.}{\rightarrow}.$
\end_inset


\end_layout

\begin_layout Itemize
An equivalent definition,
 based on the fact that 
\begin_inset Quotes eld
\end_inset

almost sure
\begin_inset Quotes erd
\end_inset

 means 
\begin_inset Quotes eld
\end_inset

with probability one
\begin_inset Quotes erd
\end_inset

 is 
\begin_inset Formula 
\[
\Pr\left(\lim_{n\rightarrow\infty}\sup_{\theta\in\Theta}|X_{n}(\omega,\theta)-X(\omega,\theta)|=0\right)=1
\]

\end_inset

 This has a form similar to that of the definition of a.s.
 convergence - the essential difference is the addition of the 
\begin_inset Formula $\sup$
\end_inset

.
 
\end_layout

\begin_layout Section
Rates of convergence and asymptotic equality
\end_layout

\begin_layout Standard
It's often useful to have notation for the relative magnitudes of quantities.
 Quantities that are small relative to others can often be ignored,
 which simplifies analysis.
\end_layout

\begin_layout Definition

\emph on
[Little-o]
\emph default
 Let 
\begin_inset Formula $f(n)$
\end_inset

 and 
\begin_inset Formula $g(n)$
\end_inset

 be two real-valued functions.
 The notation 
\begin_inset Formula $f(n)=o(g(n))$
\end_inset

 means 
\begin_inset Formula $\lim_{n\rightarrow\infty}\frac{f(n)}{g(n)}=0.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Big-O]
\emph default
 Let 
\begin_inset Formula $f(n)$
\end_inset

 and 
\begin_inset Formula $g(n)$
\end_inset

 be two real-valued functions.
 The notation 
\begin_inset Formula $f(n)=O(g(n))$
\end_inset

 means there exists some 
\begin_inset Formula $N$
\end_inset

 such that for 
\begin_inset Formula $n>N,\left|\frac{f(n)}{g(n)}\right|<K,$
\end_inset

 where 
\begin_inset Formula $K$
\end_inset

 is a finite constant.
 
\end_layout

\begin_layout Standard
This definition doesn't require that 
\begin_inset Formula $\frac{f(n)}{g(n)}$
\end_inset

 have a limit (it may fluctuate boundedly).
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\left\{ f_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ g_{n}\right\} $
\end_inset

 are sequences of random variables analogous definitions are
\end_layout

\begin_layout Definition
The notation 
\begin_inset Formula $f(n)=o_{p}(g(n))$
\end_inset

 means 
\begin_inset Formula $\frac{f(n)}{g(n)}\stackrel{p}{\rightarrow}0.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
 The least squares estimator 
\begin_inset Formula $\hat{\theta}=(X^{\prime}X)^{-1}X^{\prime}Y=(X^{\prime}X)^{-1}X^{\prime}\left(X\theta_{0}+\varepsilon\right)=\theta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon.$
\end_inset

 Since plim
\begin_inset Formula $\frac{(X^{\prime}X)^{-1}X^{\prime}\varepsilon}{1}=0,$
\end_inset

 we can write 
\begin_inset Formula $(X^{\prime}X)^{-1}X^{\prime}\varepsilon=o_{p}(1)$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}=\theta_{0}+o_{p}(1).$
\end_inset

 Asymptotically,
 the term 
\begin_inset Formula $o_{p}(1)$
\end_inset

 is negligible.
 This is just a way of indicating that the LS estimator is consistent.
 
\end_layout

\begin_layout Definition
 The notation 
\begin_inset Formula $f(n)=O_{p}(g(n))$
\end_inset

 means there exists some 
\begin_inset Formula $N_{\varepsilon}$
\end_inset

 such that for 
\begin_inset Formula $\varepsilon>0$
\end_inset

 and all 
\begin_inset Formula $n>N_{\varepsilon},$
\end_inset


\begin_inset Formula 
\[
P\left(\left|\frac{f(n)}{g(n)}\right|<K_{\varepsilon}\right)>1-\varepsilon,
\]

\end_inset

 where 
\begin_inset Formula $K_{\varepsilon}$
\end_inset

 is a finite constant.
 
\end_layout

\begin_layout Example
 
\begin_inset CommandInset label
LatexCommand label
name "normop1"

\end_inset

If 
\begin_inset Formula $X_{n}\sim N(0,1)$
\end_inset

 then 
\begin_inset Formula $X_{n}=O_{p}(1),$
\end_inset

 since,
 given 
\begin_inset Formula $\varepsilon,$
\end_inset

 there is always some 
\begin_inset Formula $K_{\varepsilon}$
\end_inset

 such that 
\begin_inset Formula $P\left(\left|X_{n}\right|<K_{\varepsilon}\right)>1-\varepsilon.$
\end_inset


\end_layout

\begin_layout Standard
Useful rules:
\end_layout

\begin_layout Itemize
\begin_inset Formula $O_{p}(n^{p})O_{p}(n^{q})=O_{p}(n^{p+q})$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $o_{p}(n^{p})o_{p}(n^{q})=o_{p}(n^{p+q})$
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "centered"

\end_inset

Consider a random sample of iid r.v.'s with mean 0 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The estimator of the mean 
\begin_inset Formula $\hat{\theta}=1/n\sum_{i=1}^{n}x_{i}$
\end_inset

 is asymptotically normally distributed,
 e.g.,
 
\begin_inset Formula $n^{1/2}\hat{\theta}\stackrel{A}{\sim}N(0,\sigma^{2}).$
\end_inset

 So 
\begin_inset Formula $n^{1/2}\hat{\theta}=O_{p}(1),$
\end_inset

 so 
\begin_inset Formula $\hat{\theta}=O_{p}(n^{-1/2}).$
\end_inset

 Before we had 
\begin_inset Formula $\hat{\theta}=o_{p}(1),$
\end_inset

 now we have have the stronger result that relates the rate of convergence to the sample size..
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
 
\begin_inset CommandInset label
LatexCommand label
name "uncentered"

\end_inset

Now consider a random sample of iid r.v.'s with mean 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The estimator of the mean 
\begin_inset Formula $\hat{\theta}=1/n\sum_{i=1}^{n}x_{i}$
\end_inset

 is asymptotically normally distributed,
 e.g.,
 
\begin_inset Formula $n^{1/2}\left(\hat{\theta}-\mu\right)\stackrel{A}{\sim}N(0,\sigma^{2}).$
\end_inset

 So 
\begin_inset Formula $n^{1/2}\left(\hat{\theta}-\mu\right)=O_{p}(1),$
\end_inset

 so 
\begin_inset Formula $\hat{\theta}-\mu=O_{p}(n^{-1/2}),$
\end_inset

 so 
\begin_inset Formula $\hat{\theta}=O_{p}(1).$
\end_inset


\end_layout

\begin_layout Standard
These two examples show that averages of centered (mean zero) quantities typically have plim 0,
 while averages of uncentered quantities have finite nonzero plims.
 Note that the definition of 
\begin_inset Formula $O_{p}$
\end_inset

 does not mean that 
\begin_inset Formula $f(n)$
\end_inset

 and 
\begin_inset Formula $g(n)$
\end_inset

 are of the same order.
 Asymptotic equality ensures that this is the case.
\end_layout

\begin_layout Definition
Two sequences of random variables 
\begin_inset Formula $\left\{ f_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ g_{n}\right\} $
\end_inset

 are asymptotically equal
\begin_inset Index idx
range none
pageformat default
status collapsed

\begin_layout Plain Layout
asymptotic equality
\end_layout

\end_inset

 (written 
\begin_inset Formula $f_{n}\stackrel{a}{=}g_{n})$
\end_inset

 if 
\begin_inset Formula 
\[
plim\left(\frac{f(n)}{g(n)}\right)=1
\]

\end_inset


\end_layout

\begin_layout Standard
Finally,
 analogous almost sure versions of 
\begin_inset Formula $o_{p}$
\end_inset

 and 
\begin_inset Formula $O_{p}$
\end_inset

 are defined in the obvious way.
 
\end_layout

\begin_layout Section
Slutsky Theorem and Continuous Mapping Theorem
\end_layout

\begin_layout Standard
The following two theorems are important for getting the asymptotic distribution of estimators,
 and for test statistics that are derived from transformations of estimators.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "true"

\end_inset

,
 Theorems 4.6 and 4.7.
 Statement of the theorems are here:
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "Slutsky Theorem"
target "https://en.wikipedia.org/wiki/Slutsky%27s_theorem"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "Continuous Mapping Theorem"
target "https://en.wikipedia.org/wiki/Continuous_mapping_theorem"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors,
 show that 
\begin_inset Formula $D_{x}a^{\prime}x=a$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $A$
\end_inset

 a 
\begin_inset Formula $p\times p$
\end_inset

 matrix and 
\begin_inset Formula $x$
\end_inset

 a 
\begin_inset Formula $p\times1$
\end_inset

 vector,
 show that 
\begin_inset Formula $D_{x}^{2}x^{\prime}Ax=A+A^{\prime}$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors,
 show that 
\begin_inset Formula $D_{\beta}\exp x^{\prime}\beta=\exp(x^{\prime}\beta)x$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors,
 find the analytic expression for 
\begin_inset Formula $D_{\beta}^{2}\exp x^{\prime}\beta$
\end_inset

.
\end_layout

\begin_layout Enumerate
Write an Octave program that verifies each of the previous results by taking numeric derivatives.
 For a hint,
 type 
\family typewriter
help numgradient
\family default
 and 
\family typewriter
help numhessian
\family default
 inside octave.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
The attic
\end_layout

\begin_layout Standard
This holds material that is not really ready to be incorporated into the main body,
 or that I believe distracts from the flow,
 but that I don't want to lose.
 Basically,
 ignore it.
\end_layout

\begin_layout Section
Efficient method of moments (EMM)
\begin_inset CommandInset label
LatexCommand label
name "sec:Efficient-method-of"

\end_inset


\end_layout

\begin_layout Standard
Note:
 this is a specific type of MSM estimator.
 I moved this out of the main text,
 as it will be of interest to a reduced group of students.
 
\end_layout

\begin_layout Standard
The choice of which moments upon which to base a GMM estimator can have very pronounced effects upon the efficiency of the estimator.
\end_layout

\begin_layout Itemize
A poor choice of moment conditions may lead to very inefficient estimators,
 and can even cause identification problems (as we've seen with the GMM problem set).
\end_layout

\begin_layout Itemize
The drawback of the above approach MSM is that the moment conditions used in estimation are selected arbitrarily.
 The asymptotic efficiency of the estimator may be low.
\end_layout

\begin_layout Itemize
The asymptotically optimal choice of moments would be the score vector of the likelihood function,
 
\begin_inset Formula 
\[
m_{t}(\theta)=D_{\theta}\ln p_{t}(\theta\mid I_{t})
\]

\end_inset

 As before,
 this choice is unavailable.
\end_layout

\begin_layout Standard
The efficient method of moments (EMM) (see 
\begin_inset CommandInset citation
LatexCommand cite
key "emm"
literal "true"

\end_inset

) seeks to provide moment conditions that closely mimic the score vector.
 If the approximation is very good,
 the resulting estimator will be very nearly fully efficient.
 
\end_layout

\begin_layout Standard
The DGP is characterized by random sampling from the density 
\begin_inset Formula 
\[
p(y_{t}|x_{t},\theta_{0})\equiv p_{t}(\theta_{0})
\]

\end_inset


\end_layout

\begin_layout Standard
We can define an auxiliary model,
 called the 
\begin_inset Quotes eld
\end_inset

score generator
\begin_inset Quotes erd
\end_inset

,
 which simply provides a (misspecified) parametric density 
\begin_inset Formula 
\[
f(y|x_{t},\lambda)\equiv f_{t}(\lambda)
\]

\end_inset


\end_layout

\begin_layout Itemize
This density is known up to a parameter 
\begin_inset Formula $\lambda.$
\end_inset

 We assume that this density function 
\emph on
is
\emph default
 calculable.
 Therefore quasi-ML estimation is possible.
 Specifically,
 
\begin_inset Formula 
\[
\hat{\lambda}=\arg\max_{\Lambda}s_{n}(\lambda)=\frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\lambda).
\]

\end_inset


\end_layout

\begin_layout Itemize
After determining 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 we can calculate the score functions 
\begin_inset Formula $D_{\lambda}\ln f(y_{t}|x_{t},\hat{\lambda})$
\end_inset

.
\end_layout

\begin_layout Itemize
The important point is that even if the density is misspecified,
 there is a pseudo-true 
\begin_inset Formula $\lambda^{0}$
\end_inset

 for which the true expectation,
 taken with respect to the true but unknown density of 
\begin_inset Formula $y,$
\end_inset

 
\begin_inset Formula $p(y|x_{t},\theta_{0}),$
\end_inset

 and then marginalized over 
\begin_inset Formula $x$
\end_inset

 is zero:
 
\begin_inset Formula 
\[
\exists\lambda^{0}:\mathcal{E}_{X}\mathcal{E}_{Y|X}\left[D_{\lambda}\ln f(y|x,\lambda^{0})\right]=\int_{X}\int_{Y|X}D_{\lambda}\ln f(y|x,\lambda^{0})p(y|x,\theta_{0})dyd\mu(x)=0
\]

\end_inset


\end_layout

\begin_layout Itemize
We have seen in the section on QML that 
\begin_inset Formula $\hat{\lambda}\stackrel{p}{\rightarrow}\lambda^{0}$
\end_inset

;
 this suggests using the moment conditions 
\begin_inset Formula 
\begin{equation}
\bar{m}_{n}(\theta,\hat{\lambda})=\frac{1}{n}\sum_{t=1}^{n}\int D_{\lambda}\ln f_{t}(\hat{\lambda})p_{t}(\theta)dy\label{iimomcond}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
These moment conditions are not calculable,
 since 
\begin_inset Formula $p_{t}(\theta)$
\end_inset

 is not available,
 but they are simulable using 
\begin_inset Formula 
\[
\widetilde{\bar{m}_{n}}(\theta,\hat{\lambda})=\frac{1}{n}\sum_{t=1}^{n}\frac{1}{H}\sum_{h=1}^{H}D_{\lambda}\ln f(\widetilde{y}_{t}^{h}|x_{t},\hat{\lambda})
\]

\end_inset

 where 
\begin_inset Formula $\tilde{y}_{t}^{h}$
\end_inset

 is a draw from 
\begin_inset Formula $DGP(\theta),$
\end_inset

 holding 
\begin_inset Formula $x_{t}$
\end_inset

 fixed.
 By the LLN and the fact that 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 converges to 
\begin_inset Formula $\lambda^{0}$
\end_inset

,
 
\begin_inset Formula 
\[
\widetilde{m}_{\infty}(\theta_{0},\lambda^{0})=0.
\]

\end_inset

 This is not the case for other values of 
\begin_inset Formula $\theta$
\end_inset

,
 assuming that 
\begin_inset Formula $\lambda^{0}$
\end_inset

 is identified.
\end_layout

\begin_layout Itemize
The advantage of this procedure is that if 
\begin_inset Formula $f(y_{t}|x_{t},\lambda)$
\end_inset

 closely approximates 
\begin_inset Formula $p(y|x_{t},\theta),$
\end_inset

 then 
\begin_inset Formula $\widetilde{m}_{n}(\theta,\hat{\lambda})$
\end_inset

 will closely approximate the optimal moment conditions which characterize maximum likelihood estimation,
 which is fully efficient.
\end_layout

\begin_layout Itemize
If one has prior information that a certain density approximates the data well,
 it would be a good choice for 
\begin_inset Formula $f(\cdot).$
\end_inset


\end_layout

\begin_layout Itemize
If one has no density in mind,
 there exist good ways of approximating unknown distributions parametrically:
 Philips' ERA's (
\shape italic
Econometrica
\shape default
,
 1983) and Gallant and Nychka's (
\shape italic
Econometrica,
 1987)
\shape default
 SNP density estimator which we saw before.
 Since the SNP density is consistent,
 the efficiency of the indirect estimator is the same as the infeasible ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\end_layout

\end_inset

estimator.
 
\end_layout

\begin_layout Subsection
Optimal weighting matrix
\end_layout

\begin_layout Standard
I will present the theory for 
\begin_inset Formula $H$
\end_inset

 finite,
 and possibly small.
 This is done because it is sometimes impractical to estimate with 
\begin_inset Formula $H$
\end_inset

 very large.
 Gallant and Tauchen give the theory for the case of 
\begin_inset Formula $H$
\end_inset

 so large that it may be treated as infinite (the difference being irrelevant given the numerical precision of a computer).
 The theory for the case of 
\begin_inset Formula $H$
\end_inset

 infinite follows directly from the results presented here.
\end_layout

\begin_layout Standard
The moment condition 
\begin_inset Formula $\widetilde{m}(\theta,\hat{\lambda})$
\end_inset

 depends on the pseudo-ML estimate 
\begin_inset Formula $\hat{\lambda}.$
\end_inset

 We can apply Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

 to conclude that 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\lambda}-\lambda^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}(\lambda^{0})^{-1}\mathcal{I}(\lambda^{0})\mathcal{J}(\lambda^{0})^{-1}\right]\label{lamdist}
\end{equation}

\end_inset

 If the density 
\begin_inset Formula $f(y_{t}|x_{t},\hat{\lambda})$
\end_inset

 were in fact the true density 
\begin_inset Formula $p(y|x_{t},\theta),$
\end_inset

 then 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 would be the maximum likelihood estimator,
 and 
\begin_inset Formula $\mathcal{J}(\lambda^{0})^{-1}\mathcal{I}(\lambda^{0})$
\end_inset

 would be an identity matrix,
 due to the information matrix equality.
 However,
 in the present case we assume that 
\begin_inset Formula $f(y_{t}|x_{t},\hat{\lambda})$
\end_inset

 is only an approximation to 
\begin_inset Formula $p(y|x_{t},\theta),$
\end_inset

 so there is no cancellation.
\end_layout

\begin_layout Standard
Recall that 
\begin_inset Formula $\mathcal{J}(\lambda^{0})\equiv p\lim\left(\frac{\partial^{2}}{\partial\lambda\partial\lambda^{\prime}}s_{n}(\lambda^{0})\right).$
\end_inset

 Comparing the definition of 
\begin_inset Formula $s_{n}(\lambda)$
\end_inset

 with the definition of the moment condition in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "iimomcond"
nolink "false"

\end_inset

,
 we see that 
\begin_inset Formula 
\[
\mathcal{J}(\lambda^{0})=D_{\lambda^{\prime}}m(\theta_{0},\lambda^{0}).
\]

\end_inset

 As in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

,
 
\begin_inset Formula 
\[
\mathcal{I}(\lambda^{0})=\lim_{n\rightarrow\infty}\mathcal{E}\left[n\left.\frac{\partial s_{n}(\lambda)}{\partial\lambda}\right|_{\lambda^{0}}\left.\frac{\partial s_{n}(\lambda)}{\partial\lambda^{\prime}}\right|_{\lambda^{0}}\right].
\]

\end_inset

 In this case,
 this is simply the asymptotic variance covariance matrix of the moment conditions,
 
\begin_inset Formula $\Omega.$
\end_inset

 Now take a first order Taylor's series approximation to 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\theta_{0},\hat{\lambda})$
\end_inset

 about 
\begin_inset Formula $\lambda^{0}$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{n}\tilde{m}_{n}(\theta_{0},\hat{\lambda})=\sqrt{n}\tilde{m}_{n}(\theta_{0},\lambda^{0})+\sqrt{n}D_{\lambda^{\prime}}\tilde{m}(\theta_{0},\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)+o_{p}(1)
\]

\end_inset


\end_layout

\begin_layout Standard
First consider 
\begin_inset Formula $\sqrt{n}\tilde{m}_{n}(\theta_{0},\lambda^{0})$
\end_inset

.
 It is straightforward but somewhat tedious to show that the asymptotic variance of this term is 
\begin_inset Formula $\frac{1}{H}I_{\infty}(\lambda^{0})$
\end_inset

.
 
\end_layout

\begin_layout Standard
Next consider the second term 
\begin_inset Formula $\sqrt{n}D_{\lambda^{\prime}}\tilde{m}(\theta_{0},\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)$
\end_inset

.
 Note that 
\begin_inset Formula $D_{\lambda^{\prime}}\tilde{m}_{n}(\theta_{0},\lambda^{0})\stackrel{a.s.}{\rightarrow}\mathcal{J}(\lambda^{0}),$
\end_inset

 so we have
\begin_inset Formula 
\[
\sqrt{n}D_{\lambda^{\prime}}\tilde{m}(\theta_{0},\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)=\sqrt{n}\mathcal{J}(\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right),a.s.
\]

\end_inset

 But noting equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "lamdist"
nolink "false"

\end_inset


\begin_inset Formula 
\[
\sqrt{n}\mathcal{J}(\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)\stackrel{a}{\sim}N\left[0,\mathcal{I}(\lambda^{0})\right]
\]

\end_inset

Now,
 combining the results for the first and second terms,
 
\begin_inset Formula 
\[
\sqrt{n}\tilde{m}_{n}(\theta_{0},\hat{\lambda})\stackrel{a}{\sim}N\left[0,\left(1+\frac{1}{H}\right)\mathcal{I}(\lambda^{0})\right]
\]

\end_inset

 Suppose that 
\begin_inset Formula $\widehat{\mathcal{I}(\lambda^{0})}$
\end_inset

 is a consistent estimator of the asymptotic variance-covariance matrix of the moment conditions.
 This may be complicated if the score generator is a poor approximator,
 since the individual score contributions may not have mean zero in this case (see the section on QML) .
 Even if this is the case,
 the individuals means can be calculated by simulation,
 so it is always possible to consistently estimate 
\begin_inset Formula $\mathcal{I}(\lambda^{0})$
\end_inset

 when the model is simulable.
 On the other hand,
 if the score generator is taken to be correctly specified,
 the ordinary estimator of the information matrix is consistent.
 Combining this with the result on the efficient GMM weighting matrix in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "efficient weighting matrix"
nolink "false"

\end_inset

,
 we see that defining 
\begin_inset Formula $\hat{\theta}$
\end_inset

 as 
\begin_inset Formula 
\[
\hat{\theta}=\arg\min_{\Theta}\bar{m}_{n}(\theta,\hat{\lambda})^{\prime}\left[\left(1+\frac{1}{H}\right)\widehat{\mathcal{I}(\lambda^{0})}\right]^{-1}\bar{m}_{n}(\theta,\hat{\lambda})
\]

\end_inset

 is the GMM estimator with the efficient choice of weighting matrix.
\end_layout

\begin_layout Itemize
If one has used the Gallant-Nychka ML estimator as the auxiliary model,
 the appropriate weighting matrix is simply the information matrix of the auxiliary model,
 since the scores are uncorrelated.
 (e.g.,
 it really is ML estimation asymptotically,
 since the score generator can approximate the unknown density arbitrarily well).
 
\end_layout

\begin_layout Subsection
Asymptotic distribution
\end_layout

\begin_layout Standard
Since we use the optimal weighting matrix,
 the asymptotic distribution is as in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "gmm distribution with optimal weighting matrix"
nolink "false"

\end_inset

,
 so we have (using the result in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "lamdist"
nolink "false"

\end_inset

):
 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}\left[\left(1+\frac{1}{H}\right)\mathcal{I}(\lambda^{0})\right]^{-1}D_{\infty}^{\prime}\right)^{-1}\right],
\]

\end_inset

 where 
\begin_inset Formula 
\[
D_{\infty}=\lim_{n\rightarrow\infty}\mathcal{E}\left[D_{\theta}\bar{m}_{n}(\theta_{0},\lambda^{0})\right].
\]

\end_inset

 This can be consistently estimated using 
\begin_inset Formula 
\[
\hat{D}=D_{\theta}\bar{m}_{n}(\hat{\theta},\hat{\lambda})
\]

\end_inset


\end_layout

\begin_layout Subsection
Diagnostic testing
\end_layout

\begin_layout Standard
The fact that 
\begin_inset Formula 
\[
\sqrt{n}\bar{m}_{n}(\theta_{0},\hat{\lambda})\stackrel{a}{\sim}N\left[0,\left(1+\frac{1}{H}\right)\mathcal{I}(\lambda^{0})\right]
\]

\end_inset

 implies that 
\begin_inset Formula 
\[
n\bar{m}_{n}(\hat{\theta},\hat{\lambda})^{\prime}\left[\left(1+\frac{1}{H}\right)\mathcal{I}(\hat{\lambda})\right]^{-1}\bar{m}_{n}(\hat{\theta},\hat{\lambda})\stackrel{a}{\sim}\chi^{2}(q)
\]

\end_inset

 where 
\begin_inset Formula $q$
\end_inset

 is 
\begin_inset Formula $\dim(\lambda)-\dim(\theta),$
\end_inset

 since without 
\begin_inset Formula $\dim(\theta)$
\end_inset

 moment conditions the model is not identified,
 so testing is impossible.
 One test of the model is simply based on this statistic:
 if it exceeds the 
\begin_inset Formula $\chi^{2}(q)$
\end_inset

 critical point,
 something may be wrong (the small sample performance of this sort of test would be a topic worth investigating).
\end_layout

\begin_layout Itemize
Information about what is wrong can be gotten from the pseudo-t-statistics:
 
\begin_inset Formula 
\[
\left(\text{diag}\left[\left(1+\frac{1}{H}\right)\mathcal{I}(\hat{\lambda})\right]^{1/2}\right)^{-1}\sqrt{n}\bar{m}_{n}(\hat{\theta},\hat{\lambda})
\]

\end_inset

 can be used to test which moments are not well modeled.
 Since these moments are related to parameters of the score generator,
 which are usually related to certain features of the model,
 this information can be used to revise the model.
 These aren't actually distributed as 
\begin_inset Formula $N(0,1),$
\end_inset

 since 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\theta_{0},\hat{\lambda})$
\end_inset

 and 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\hat{\theta},\hat{\lambda})$
\end_inset

 have different distributions (that of 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\hat{\theta},\hat{\lambda})$
\end_inset

 is somewhat more complicated).
 It can be shown that the pseudo-t statistics are biased toward nonrejection.
 See Gourieroux 
\emph on
et.
 al.

\emph default
 or Gallant and Long,
 1995,
 for more details.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "cha:Parallel-programming-for"

\end_inset

Parallel programming for econometrics
\end_layout

\begin_layout Standard
The following borrows heavily from Creel (2005).
 
\end_layout

\begin_layout Standard
Parallel computing can offer an important reduction in the time to complete computations.
 This is well-known,
 but it bears emphasis since it is the main reason that parallel computing may be attractive to users.
 To illustrate,
 the Intel Pentium IV (Willamette) processor,
 running at 1.5GHz,
 was introduced in November of 2000.
 The Pentium IV (Northwood-HT) processor,
 running at 3.06GHz,
 was introduced in November of 2002.
 An approximate doubling of the performance of a commodity CPU took place in two years.
 Extrapolating this admittedly rough snapshot of the evolution of the performance of commodity processors,
 one would need to wait more than 6.6 years and then purchase a new computer to obtain a 10-fold improvement in computational performance.
 The examples in this chapter show that a 10-fold improvement in performance can be achieved immediately,
 using distributed parallel computing on available computers.
\end_layout

\begin_layout Standard
Recent (this is written in 2005) developments that may make parallel computing attractive to a broader spectrum of researchers who do computations.
 The first is the fact that setting up a cluster of computers for distributed parallel computing is not difficult.
 If you are using the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{ParallelKnoppix}{http://pareto.uab.es/mcreel/ParallelKnoppix}
\end_layout

\end_inset

 bootable CD that accompanies these notes,
 you are less than 10 minutes away from creating a cluster,
 supposing you have a second computer at hand and a crossover ethernet cable.
 See the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{ParallelKnoppix tutorial}{http://pareto.uab.es/mcreel/ParallelKnoppix/ParallelKnoppixTutorial.html}
\end_layout

\end_inset

.
 A second development is the existence of extensions to some of the high-level matrix programming (HLMP) languages
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
By 
\begin_inset Quotes sld
\end_inset

high-level matrix programming language
\begin_inset Quotes srd
\end_inset

 I mean languages such as MATLAB (TM the Mathworks,
 Inc.),
 Ox (TM OxMetrics Technologies,
 Ltd.),
 and GNU Octave (
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

www.octave.org
\end_layout

\end_inset

),
 for example.
\end_layout

\end_inset

 that allow the incorporation of parallelism into programs written in these languages.
 A third is the spread of dual and quad-core CPUs,
 so that an ordinary desktop or laptop computer can be made into a mini-cluster.
 Those cores won't work together on a single problem unless they are told how to.
\end_layout

\begin_layout Standard
Following are examples of parallel implementations of several mainstream problems in econometrics.
 A focus of the examples is on the possibility of hiding parallelization from end users of programs.
 If programs that run in parallel have an interface that is nearly identical to the interface of equivalent serial versions,
 end users will find it easy to take advantage of parallel computing's performance.
 We continue to use Octave,
 taking advantage of the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{MPI Toolbox (MPITB) for Octave}{http://atc.ugr.es/javier-bin/mpitb}
\end_layout

\end_inset

,
 by by Fernández Baldomero 
\emph on
et al.

\emph default
 (2004).
 There are also parallel packages for Ox,
 R,
 and Python which may be of interest to econometricians,
 but as of this writing,
 the following examples are the most accessible introduction to parallel programming for econometricians.
\end_layout

\begin_layout Subsection
Example problems
\end_layout

\begin_layout Standard
This section introduces example problems from econometrics,
 and shows how they can be parallelized in a natural way.
\end_layout

\begin_layout Subsubsection
Monte Carlo
\end_layout

\begin_layout Standard
A Monte Carlo study involves repeating a random experiment many times under identical conditions.
 Several authors have noted that Monte Carlo studies are obvious candidates for parallelization (Doornik 
\emph on
et al.

\emph default
 2002;
 Bruche,
 2003) since blocks of replications can be done independently on different computers.
 To illustrate the parallelization of a Monte Carlo study,
 we use same trace test example as do Doornik,
 
\emph on
et.
 al.

\emph default
 (2002).
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{tracetest.m}{./Examples/Parallel/montecarlo/tracetest.m}
\end_layout

\end_inset

 is a function that calculates the trace test statistic for the lack of cointegration of integrated time series.
 This function is illustrative of the format that we adopt for Monte Carlo simulation of a function:
 it receives a single argument of cell type,
 and it returns a row vector that holds the results of one random simulation.
 The single argument in this case is a cell array that holds the length of the series in its first position,
 and the number of series in the second position.
 It generates a random result though a process that is internal to the function,
 and it reports some output in a row vector (in this case the result is a scalar).
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{mc
\backslash
_example1.m}{./Examples/Parallel/montecarlo/mc
\backslash
_example1.m}
\end_layout

\end_inset

 is an Octave script that executes a Monte Carlo study of the trace test by repeatedly evaluating the 
\family typewriter
tracetest.m
\family default
 function.
 The main thing to notice about this script is that lines 7 and 10 call the function 
\family typewriter
montecarlo.m.

\family default
 When called with 3 arguments,
 as in line 7,
 
\family typewriter
montecarlo.m
\family default
 executes serially on the computer it is called from.
 In line 10,
 there is a fourth argument.
 When called with four arguments,
 the last argument is the number of slave hosts to use.
 We see that running the Monte Carlo study on one or more processors is transparent to the user - he or she must only indicate the number of slave computers to be used.
\end_layout

\begin_layout Subsubsection
ML
\end_layout

\begin_layout Standard
For a sample 
\begin_inset Formula $\left\{ (y_{t},x_{t})\right\} _{n}$
\end_inset

 of 
\begin_inset Formula $n$
\end_inset

 observations of a set of dependent and explanatory variables,
 the maximum likelihood estimator of the parameter 
\begin_inset Formula $\theta$
\end_inset

 can be defined as 
\begin_inset Formula 
\[
\hat{\theta}=\arg\max s_{n}(\theta)
\]

\end_inset

where
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}\ln f(y_{t}|x_{t},\theta)
\]

\end_inset

Here,
 
\begin_inset Formula $y_{t}$
\end_inset

 may be a vector of random variables,
 and the model may be dynamic since 
\begin_inset Formula $x_{t}$
\end_inset

 may contain lags of 
\begin_inset Formula $y_{t}$
\end_inset

.
 As Swann (2002) points out,
 this can be broken into sums over blocks of observations,
 for example two blocks:
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\left\{ \left(\sum_{t=1}^{n_{1}}\ln f(y_{t}|x_{t},\theta)\right)+\left(\sum_{t=n_{1}+1}^{n}\ln f(y_{t}|x_{t},\theta)\right)\right\} 
\]

\end_inset

Analogously,
 we can define up to 
\begin_inset Formula $n$
\end_inset

 blocks.
 Again following Swann,
 parallelization can be done by calculating each block on separate computers.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{mle
\backslash
_example1.m}{./Examples/Parallel/mle/mle
\backslash
_example1.m}
\end_layout

\end_inset

 is an Octave script that calculates the maximum likelihood estimator of the parameter vector of a model that assumes that the dependent variable is distributed as a Poisson random variable,
 conditional on some explanatory variables.
 In lines 1-3 the data is read,
 the name of the density function is provided in the variable 
\family typewriter
model
\family default
,
 and the initial value of the parameter vector is set.
 In line 5,
 the function 
\family typewriter
mle_estimate
\family default
 performs ordinary serial calculation of the ML estimator,
 while in line 7 the same function is called with 6 arguments.
 The fourth and fifth arguments are empty placeholders where options to 
\family typewriter
mle_estimate
\family default
 may be set,
 while the sixth argument is the number of slave computers to use for parallel execution,
 1 in this case.
 A person who runs the program sees no parallel programming code - the parallelization is transparent to the end user,
 beyond having to select the number of slave computers.
 When executed,
 this script prints out the estimates 
\family typewriter
theta_s
\family default
 and 
\family typewriter
theta_p
\family default
,
 which are identical.
\end_layout

\begin_layout Standard
It is worth noting that a different likelihood function may be used by making the 
\family typewriter
model
\family default
 variable point to a different function.
 The likelihood function itself is an ordinary Octave function that is not parallelized.
 The 
\family typewriter
mle_estimate
\family default
 function is a generic function that can call any likelihood function that has the appropriate input/output syntax for evaluation either serially or in parallel.
 Users need only learn how to write the likelihood function using the Octave language.
 
\end_layout

\begin_layout Subsubsection
GMM
\end_layout

\begin_layout Standard
For a sample as above,
 the GMM estimator of the parameter 
\begin_inset Formula $\theta$
\end_inset

 can be defined as
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}s_{n}(\theta)
\]

\end_inset

where 
\begin_inset Formula 
\[
s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta)
\]

\end_inset

 and 
\begin_inset Formula 
\[
\bar{m}_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}m_{t}(y_{t}|x_{t},\theta)
\]

\end_inset

 Since 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 is an average,
 it can obviously be computed blockwise,
 using for example 2 blocks:
\begin_inset Formula 
\begin{equation}
\bar{m}_{n}(\theta)=\frac{1}{n}\left\{ \left(\sum_{t=1}^{n_{1}}m_{t}(y_{t}|x_{t},\theta)\right)+\left(\sum_{t=n_{1}+1}^{n}m_{t}(y_{t}|x_{t},\theta)\right)\right\} \label{eq:gmm moment contributions}
\end{equation}

\end_inset

Likewise,
 we may define up to 
\begin_inset Formula $n$
\end_inset

 blocks,
 each of which could potentially be computed on a different machine.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{gmm
\backslash
_example1.m}{./Examples/Parallel/gmm/gmm
\backslash
_example1.m}
\end_layout

\end_inset

 is a script that illustrates how GMM estimation may be done serially or in parallel.
 When this is run,
 
\family typewriter
theta_s
\family default
 and 
\family typewriter
theta_p
\family default
 are identical up to the tolerance for convergence of the minimization routine.
 The point to notice here is that an end user can perform the estimation in parallel in virtually the same way as it is done serially.
 Again,
 
\family typewriter
gmm_estimate
\family default
,
 used in lines 8 and 10,
 is a generic function that will estimate any model specified by the 
\family typewriter
moments
\family default
 variable - a different model can be estimated by changing the value of the 
\family typewriter
moments
\family default
 variable.
 The function that 
\family typewriter
moments
\family default
 points to is an ordinary Octave function that uses no parallel programming,
 so users can write their models using the simple and intuitive HLMP syntax of Octave.
 Whether estimation is done in parallel or serially depends only the seventh argument to 
\family typewriter
gmm_estimate
\family default
 - when it is missing or zero,
 estimation is by default done serially with one processor.
 When it is positive,
 it specifies the number of slave nodes to use.
\end_layout

\begin_layout Subsubsection
Kernel regression
\end_layout

\begin_layout Standard
The Nadaraya-Watson kernel regression estimator of a function 
\begin_inset Formula $g(x)$
\end_inset

 at a point 
\begin_inset Formula $x$
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
\hat{g}(x) & = & \frac{\sum_{t=1}^{n}y_{t}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}\\
 & \equiv & \sum_{t=1}^{n}w_{t}y_{y}
\end{eqnarray*}

\end_inset

We see that the weight depends upon every data point in the sample.
 To calculate the fit at every point in a sample of size 
\begin_inset Formula $n,$
\end_inset

 on the order of 
\begin_inset Formula $n^{2}k$
\end_inset

 calculations must be done,
 where 
\begin_inset Formula $k$
\end_inset

 is the dimension of the vector of explanatory variables,
 
\begin_inset Formula $x$
\end_inset

.
 Racine (2002) demonstrates that MPI parallelization can be used to speed up calculation of the kernel regression estimator by calculating the fits for portions of the sample on different computers.
 We follow this implementation here.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
href{kernel
\backslash
_example1.m}{./Examples/Parallel/kernel/kernel
\backslash
_example1.m}
\end_layout

\end_inset

 is a script for serial and parallel kernel regression.
 
\end_layout

\begin_layout Standard
The example programs show that parallelization may be mostly hidden from end users.
 Users can benefit from parallelization without having to write or understand parallel code.
 The speedups one can obtain are highly dependent upon the specific problem at hand,
 as well as the size of the cluster,
 the efficiency of the network,
 
\emph on
etc.

\emph default
 Some examples of speedups are presented in Creel (2005).
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Speedups-from-parallelization"
nolink "false"

\end_inset

 reproduces speedups for some econometric problems on a cluster of 12 desktop computers.
 The speedup for 
\begin_inset Formula $k$
\end_inset

 nodes is the time to finish the problem on a single node divided by the time to finish the problem on 
\begin_inset Formula $k$
\end_inset

 nodes.
 Note that you can get 10X speedups,
 as claimed in the introduction.
 It's pretty obvious that much greater speedups could be obtained using a larger cluster,
 for the 
\begin_inset Quotes sld
\end_inset

embarrassingly parallel
\begin_inset Quotes srd
\end_inset

 problems.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Speedups-from-parallelization"

\end_inset

Speedups from parallelization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/speedup.pdf

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Duration data and the Weibull model
\end_layout

\begin_layout Standard
In some cases the dependent variable may be the time that passes between the occurrence of two events.
 For example,
 it may be the duration of a strike,
 or the time needed to find a job once one is unemployed.
 Such variables take on values on the positive real line,
 and are referred to as duration data.
\end_layout

\begin_layout Standard
A 
\emph on
spell
\emph default
 is the period of time between the occurrence of initial event and the concluding event.
 For example,
 the initial event could be the loss of a job,
 and the final event is the finding of a new job.
 The spell is the period of unemployment.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $t_{0}$
\end_inset

 be the time the initial event occurs,
 and 
\begin_inset Formula $t_{1}$
\end_inset

 be the time the concluding event occurs.
 For simplicity,
 assume that time is measured in years.
 The random variable 
\begin_inset Formula $D$
\end_inset

 is the duration of the spell,
 
\begin_inset Formula $D=t_{1}-t_{0}$
\end_inset

.
 Define the density function of 
\begin_inset Formula $D,$
\end_inset

 
\begin_inset Formula $f_{D}(t),$
\end_inset

 with distribution function 
\begin_inset Formula $F_{D}(t)=\Pr(D<t).$
\end_inset


\end_layout

\begin_layout Standard
Several questions may be of interest.
 For example,
 one might wish to know the expected time one has to wait to find a job given that one has already waited 
\begin_inset Formula $s$
\end_inset

 years.
 The probability that a spell lasts more than 
\begin_inset Formula $s$
\end_inset

 years is 
\begin_inset Formula 
\[
\Pr(D>s)=1-\Pr(D\leq s)=1-F_{D}(s).
\]

\end_inset

 The density of 
\begin_inset Formula $D$
\end_inset

 conditional on the spell being longer than 
\begin_inset Formula $s$
\end_inset

 years is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{D}(t|D>s)=\frac{f_{D}(t)}{1-F_{D}(s)}.
\]

\end_inset

 The expected additional time required for the spell to end given that is has already lasted 
\begin_inset Formula $s$
\end_inset

 years is the expectation of 
\begin_inset Formula $D$
\end_inset

 with respect to this density,
 minus 
\begin_inset Formula $s.$
\end_inset


\begin_inset Formula 
\[
E=\mathcal{E}(D|D>s)-s=\left(\int_{t}^{\infty}z\frac{f_{D}(z)}{1-F_{D}(s)}dz\right)-s
\]

\end_inset


\end_layout

\begin_layout Standard
To estimate this function,
 one needs to specify the density 
\begin_inset Formula $f_{D}(t)$
\end_inset

 as a parametric density,
 then estimate by maximum likelihood.
 There are a number of possibilities including the exponential density,
 the lognormal,
 
\emph on
etc.

\emph default
 A reasonably flexible model that is a generalization of the exponential density is the Weibull density
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{D}(t|\theta)=e^{-\left(\lambda t\right)^{\gamma}}\lambda\gamma(\lambda t)^{\gamma-1}.
\]

\end_inset

 According to this model,
 
\begin_inset Formula $\mathcal{E}(D)=\lambda^{-\gamma}.$
\end_inset

 The log-likelihood is just the product of the log densities.
\end_layout

\begin_layout Standard
To illustrate application of this model,
 402 observations on the lifespan of dwarf mongooses in Serengeti National Park (Tanzania) were used to fit a Weibull model.
 The 
\begin_inset Quotes sld
\end_inset

spell
\begin_inset Quotes srd
\end_inset

 in this case is the lifetime of an individual mongoose.
 The parameter estimates and standard errors are 
\begin_inset Formula $\hat{\lambda}=0.559\,(0.034)$
\end_inset

 and 
\begin_inset Formula $\hat{\gamma}=0.867\,(0.033)$
\end_inset

 and the log-likelihood value is -659.3.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Life-expectancy-of"
nolink "false"

\end_inset

 presents fitted life expectancy (expected additional years of life) as a function of age,
 with 95% confidence bands.
 The plot is accompanied by a nonparametric Kaplan-Meier estimate of life-expectancy.
 This nonparametric estimator simply averages all spell lengths greater than age,
 and then subtracts age.
 This is consistent by the LLN.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Life-expectancy-of"

\end_inset

Life expectancy of mongooses,
 Weibull model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/weibull.pdf
	width 6in

\end_inset


\end_layout

\end_inset

In the figure one can see that the model doesn't fit the data well,
 in that it predicts life expectancy quite differently than does the nonparametric model.
 For ages 4-6,
 the nonparametric estimate is outside the confidence interval that results from the parametric model,
 which casts doubt upon the parametric model.
 Mongooses that are between 2-6 years old seem to have a lower life expectancy than is predicted by the Weibull model,
 whereas young mongooses that survive beyond infancy have a higher life expectancy,
 up to a bit beyond 2 years.
 Due to the dramatic change in the death rate as a function of 
\begin_inset Formula $t$
\end_inset

,
 one might specify 
\begin_inset Formula $f_{D}(t)$
\end_inset

 as a mixture of two Weibull densities,
 
\begin_inset Formula 
\[
f_{D}(t|\theta)=\delta\left(e^{-\left(\lambda_{1}t\right)^{\gamma_{1}}}\lambda_{1}\gamma_{1}(\lambda_{1}t)^{\gamma_{1}-1}\right)+\left(1-\delta\right)\left(e^{-\left(\lambda_{2}t\right)^{\gamma_{2}}}\lambda_{2}\gamma_{2}(\lambda_{2}t)^{\gamma_{2}-1}\right).
\]

\end_inset

 The parameters 
\begin_inset Formula $\gamma_{i}$
\end_inset

 and 
\begin_inset Formula $\lambda_{i},i=1,2$
\end_inset

 are the parameters of the two Weibull densities,
 and 
\begin_inset Formula $\delta$
\end_inset

 is the parameter that mixes the two.
\end_layout

\begin_layout Standard
With the same data,
 
\begin_inset Formula $\theta$
\end_inset

 can be estimated using the mixed model.
 The results are a log-likelihood = -623.17.
 Note that a standard likelihood ratio test cannot be used to chose between the two models,
 since under the null that 
\begin_inset Formula $\delta=1$
\end_inset

 (single density),
 the two parameters 
\begin_inset Formula $\lambda_{2}$
\end_inset

 and 
\begin_inset Formula $\gamma_{2}$
\end_inset

 are not identified.
 It is possible to take this into account,
 but this topic is out of the scope of this course.
 Nevertheless,
 the improvement in the likelihood function is considerable.
 The parameter estimates are
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features firstHeadEmpty="true" tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt">
<column alignment="left" valignment="top" width="0pt">
<column alignment="left" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 Parameter 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 Estimate 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 St.
 Error 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\lambda_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.233 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.016 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\gamma_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 1.722 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.166 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\lambda_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 1.731 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.101 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\gamma_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 1.522 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.096 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\delta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.428 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.035 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that the mixture parameter is highly significant.
 This model leads to the fit in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "mixed weibull"
nolink "false"

\end_inset

.
 Note that the parametric and nonparametric fits are quite close to one another,
 up to around 
\begin_inset Formula $6$
\end_inset

 years.
 The disagreement after this point is not too important,
 since less than 5% of mongooses live more than 6 years,
 which implies that the Kaplan-Meier nonparametric estimate has a high variance (since it's an average of a small number of observations).
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mixed weibull"

\end_inset

Life expectancy of mongooses,
 mixed Weibull model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/mixed.pdf
	width 6in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Mixture models are often an effective way to model complex responses,
 though they can suffer from overparameterization.
 Alternatives will be discussed later.
\end_layout

\begin_layout Standard
For examples of MLE using the Poisson model applied to count data,
 see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"
nolink "false"

\end_inset

 in the chapter on Numerical Optimization.
 You should examine the scripts and run them to see how MLE is actually done,
 and how parameter standard errors are estimated.
 
\end_layout

\begin_layout Section
Quasi-ML
\end_layout

\begin_layout Standard
Quasi-ML is the estimator one obtains when a misspecified probability model is used to calculate an 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimator.
\end_layout

\begin_layout Standard
Given a sample of size 
\begin_inset Formula $n$
\end_inset

 of a random vector 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and a vector of conditioning variables 
\begin_inset Formula $\mathbf{x},$
\end_inset

 suppose the joint density of 
\begin_inset Formula $\mathbf{Y}=\left(\begin{array}{ccc}
\mathbf{y}_{1} & \ldots & \mathbf{y}_{n}\end{array}\right)$
\end_inset

 conditional on 
\begin_inset Formula $\mathbf{X}=\left(\begin{array}{ccc}
\mathbf{x}_{1} & \ldots & \mathbf{x}_{n}\end{array}\right)$
\end_inset

 is a member of the parametric family 
\begin_inset Formula $p_{\mathcal{Y}}(\mathbf{Y}|\mathbf{X},\rho),$
\end_inset

 
\begin_inset Formula $\rho\in\Xi.$
\end_inset

 The true joint density is associated with the vector 
\begin_inset Formula $\rho_{0}:$
\end_inset


\begin_inset Formula 
\[
p_{\mathcal{Y}}(\mathbf{Y}|\mathbf{X},\rho_{0}).
\]

\end_inset


\begin_inset Newline newline
\end_inset

As long as the marginal density of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 doesn't depend on 
\begin_inset Formula $\rho_{0},$
\end_inset

 this conditional density fully characterizes the random characteristics of samples:
 i.e.,
 it fully describes the probabilistically important features of the d.g.p.
 The 
\emph on
likelihood function
\emph default
 is just this density evaluated at other values 
\begin_inset Formula $\rho$
\end_inset


\begin_inset Formula 
\[
L(\mathbf{Y}|\mathbf{X},\rho)=p_{\mathcal{Y}}(\mathbf{Y}|\mathbf{X},\rho),\rho\in\Xi.
\]

\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\mathbf{Y}_{t-1}=\left(\begin{array}{ccc}
\mathbf{y}_{1} & \ldots & \mathbf{y}_{t-1}\end{array}\right)$
\end_inset

,
 
\begin_inset Formula $\mathbf{Y}_{0}=0,$
\end_inset

 and let 
\begin_inset Formula $\mathbf{X}_{t}=\left(\begin{array}{ccc}
\mathbf{x}_{1} & \ldots & \mathbf{x}_{t}\end{array}\right)$
\end_inset

 The likelihood function,
 taking into account possible dependence of observations,
 can be written as 
\begin_inset Formula 
\begin{eqnarray*}
L(\mathbf{Y}|\mathbf{X},\rho) & = & \prod_{t=1}^{n}p_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\rho)\\
 & \equiv & \prod_{t=1}^{n}p_{t}(\rho)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The average log-likelihood function is:
 
\begin_inset Formula 
\[
s_{n}(\rho)=\frac{1}{n}\ln L(\mathbf{Y}|\mathbf{X},\rho)=\frac{1}{n}\sum_{t=1}^{n}\ln p_{t}(\rho)
\]

\end_inset


\end_layout

\begin_layout Itemize
Suppose that we do not have knowledge of the family of densities 
\begin_inset Formula $p_{t}(\rho).$
\end_inset

 Mistakenly,
 we may assume that the conditional density of 
\begin_inset Formula $\mathbf{y}_{t}$
\end_inset

 is a member of the family 
\begin_inset Formula $f_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\theta),$
\end_inset

 
\begin_inset Formula $\theta\in\Theta,$
\end_inset

 where there is no 
\begin_inset Formula $\theta_{0}$
\end_inset

 such that 
\begin_inset Formula $f_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\theta_{0})=p_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\rho_{0}),\forall t$
\end_inset

 (this is what we mean by 
\begin_inset Quotes eld
\end_inset

misspecified
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Itemize
This setup allows for heterogeneous time series data,
 with dynamic misspecification.
 
\end_layout

\begin_layout Standard
The QML estimator is the argument that maximizes the 
\series bold
misspecified
\series default
 average log likelihood,
 which we refer to as the quasi-log likelihood function.
 This objective function is 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\theta) & = & \frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\theta_{0})\\
 & \equiv & \frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\theta)
\end{eqnarray*}

\end_inset

 and the QML is 
\begin_inset Formula 
\[
\hat{\theta}_{n}=\arg\max_{\Theta}s_{n}(\theta)
\]

\end_inset

 A SLLN for dependent sequences applies (we assume),
 so that 
\begin_inset Formula 
\[
s_{n}(\theta)\stackrel{a.s.}{\rightarrow}\lim_{n\rightarrow\infty}\mathcal{E}\frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\theta)\equiv s_{\infty}(\theta)
\]

\end_inset

 We assume that this can be strengthened to uniform convergence,
 a.s.,
 following the previous arguments.
 The 
\begin_inset Quotes eld
\end_inset

pseudo-true
\begin_inset Quotes erd
\end_inset

 value of 
\begin_inset Formula $\theta$
\end_inset

 is the value that maximizes 
\begin_inset Formula $\bar{s}(\theta)$
\end_inset

:
 
\begin_inset Formula 
\[
\theta_{0}=\arg\max_{\Theta}s_{\infty}(\theta)
\]

\end_inset

 Given assumptions so that theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

 is applicable,
 we obtain 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\hat{\theta}_{n}=\theta_{0},\text{a.s.}
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Applying the asymptotic normality theorem,
 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]
\]

\end_inset

 where 
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\theta_{0})=\lim_{n\rightarrow\infty}\mathcal{E}D_{\theta}^{2}s_{n}(\theta_{0})
\]

\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta_{0})=\lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta_{0}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that asymptotic normality only requires that the additional assumptions regarding 
\begin_inset Formula $\mathcal{J}$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}$
\end_inset

 hold in a neighborhood of 
\begin_inset Formula $\theta_{0}$
\end_inset

 for 
\begin_inset Formula $\mathcal{J}$
\end_inset

 and at 
\begin_inset Formula $\theta_{0},$
\end_inset

 for 
\begin_inset Formula $\mathcal{I},$
\end_inset

 not throughout 
\begin_inset Formula $\Theta.$
\end_inset

 In this sense,
 asymptotic normality is a local property.
 
\end_layout

\begin_layout Subsection
Consistent Estimation of Variance Components
\end_layout

\begin_layout Standard
Consistent estimation of 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 is straightforward.
 Assumption (b) of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

 implies that 
\begin_inset Formula 
\[
\mathcal{J}_{n}(\hat{\theta}_{n})=\frac{1}{n}\sum_{t=1}^{n}D_{\theta}^{2}\ln f_{t}(\hat{\theta}_{n})\stackrel{a.s.}{\rightarrow}\lim_{n\rightarrow\infty}\mathcal{E}\frac{1}{n}\sum_{t=1}^{n}D_{\theta}^{2}\ln f_{t}(\theta_{0})=\mathcal{J}_{\infty}(\theta_{0}).
\]

\end_inset

 That is,
 just calculate the Hessian using the estimate 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 in place of 
\begin_inset Formula $\theta_{0}.$
\end_inset


\end_layout

\begin_layout Standard
Consistent estimation of 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

 is more difficult,
 and may be impossible.
\end_layout

\begin_layout Itemize

\series bold
Notation
\series default
:
 Let 
\begin_inset Formula $g_{t}\equiv D_{\theta}f_{t}(\theta_{0})$
\end_inset


\end_layout

\begin_layout Standard
We need to estimate 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta_{0}) & = & \lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta_{0})\\
 & = & \lim_{n\rightarrow\infty}Var\sqrt{n}\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f_{t}(\theta_{0})\\
 & = & \lim_{n\rightarrow\infty}\frac{1}{n}Var\sum_{t=1}^{n}g_{t}\\
 & = & \lim_{n\rightarrow\infty}\frac{1}{n}\mathcal{E}\left\{ \left(\sum_{t=1}^{n}\left(g_{t}-\mathcal{E}g_{t}\right)\right)\left(\sum_{t=1}^{n}\left(g_{t}-\mathcal{E}g_{t}\right)\right)^{\prime}\right\} 
\end{eqnarray*}

\end_inset

 This is going to contain a term 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{t=1}^{n}\left(\mathcal{E}g_{t}\right)\left(\mathcal{E}g_{t}\right)^{\prime}
\]

\end_inset

 which will not tend to zero,
 in general.
 This term is not consistently estimable in general,
 since it requires calculating an expectation using the true density under the d.g.p.,
 which is unknown.
\end_layout

\begin_layout Itemize
There are important cases where 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

 
\emph on
is
\emph default
 consistently estimable.
 For example,
 suppose that the data come from a random sample (
\emph on
i.e.,

\emph default
 they are iid).
 This would be the case with cross sectional data,
 for example.
 (Note:
 under i.i.d.
 sampling,
 the joint distribution of 
\begin_inset Formula $(y_{t},x_{t})$
\end_inset

 is identical.
 This does not imply that the conditional density 
\begin_inset Formula $f(y_{t}|x_{t})$
\end_inset

 is identical).
\end_layout

\begin_layout Itemize
With random sampling,
 the limiting objective function is simply 
\begin_inset Formula 
\[
s_{\infty}(\theta_{0})=\mathcal{E}_{X}\mathcal{E}_{0}\ln f(y|x,\theta_{0})
\]

\end_inset

 where 
\begin_inset Formula $\mathcal{E}_{0}$
\end_inset

 means expectation of 
\begin_inset Formula $y|x$
\end_inset

 and 
\begin_inset Formula $\mathcal{E}_{X}$
\end_inset

 means expectation respect to the marginal density of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Itemize
By the requirement that the limiting objective function be maximized at 
\begin_inset Formula $\theta_{0}$
\end_inset

 we have 
\begin_inset Formula 
\[
D_{\theta}\mathcal{E}_{X}\mathcal{E}_{0}\ln f(y|x,\theta_{0})=D_{\theta}s_{\infty}(\theta_{0})=0
\]

\end_inset


\end_layout

\begin_layout Itemize
The dominated convergence theorem allows switching the order of expectation and differentiation,
 so 
\begin_inset Formula 
\[
D_{\theta}\mathcal{E}_{X}\mathcal{E}_{0}\ln f(y|x,\theta_{0})=\mathcal{E}_{X}\mathcal{E}_{0}D_{\theta}\ln f(y|x,\theta_{0})=0
\]

\end_inset

 The CLT implies that 
\begin_inset Formula 
\[
\frac{1}{\sqrt{n}}\sum_{t=1}^{n}D_{\theta}\ln f(y|x,\theta_{0})\stackrel{d}{\rightarrow}N(0,\mathcal{I}_{\infty}(\theta_{0})).
\]

\end_inset

 That is,
 it's not necessary to subtract the individual means,
 since they are zero.
 Given this,
 and due to independent observations,
 a consistent estimator is 
\begin_inset Formula 
\[
\widehat{\mathcal{I}}=\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f_{t}(\hat{\theta})D_{\theta^{\prime}}\ln f_{t}(\hat{\theta})
\]

\end_inset


\end_layout

\begin_layout Standard
This is an important case where consistent estimation of the covariance matrix is possible.
 Other cases exist,
 even for dynamically misspecified time series models.
\end_layout

\begin_layout Section
Nonlinear simultaneous equations
\end_layout

\begin_layout Standard
Taken out of GMM chapter.
 GMM provides a convenient way to estimate nonlinear systems of simultaneous equations.
 We have a system of equations of the form 
\begin_inset Formula 
\begin{eqnarray*}
y_{1t} & = & f_{1}(\mathbf{z}_{t},\theta_{1}^{0})+\varepsilon_{1t}\\
y_{2t} & = & f_{2}(\mathbf{z}_{t},\theta_{2}^{0})+\varepsilon_{2t}\\
 &  & \vdots\\
y_{Gt} & = & f_{G}(\mathbf{z}_{t},\theta_{G}^{0})+\varepsilon_{Gt},
\end{eqnarray*}

\end_inset

 or in compact notation 
\begin_inset Formula 
\[
y_{t}=f(\mathbf{z}_{t},\theta_{0})+\varepsilon_{t},
\]

\end_inset

 where 
\begin_inset Formula $f(\cdot)$
\end_inset

 is a 
\begin_inset Formula $G$
\end_inset

 -vector valued function,
 and 
\begin_inset Formula $\theta_{0}=(\theta_{1}^{0\prime},\theta_{2}^{0\prime},\cdots,\theta_{G}^{0\prime})^{\prime}.$
\end_inset

 We assume that 
\begin_inset Formula $\mathbf{z}_{t}$
\end_inset

 contains the current period endogenous variables,
 so we have a simultaneity problem.
\end_layout

\begin_layout Standard
We need to find an 
\begin_inset Formula $A_{i}\times1$
\end_inset

 vector of instruments 
\begin_inset Formula $\mathbf{x}_{it},$
\end_inset

 for each equation,
 that are uncorrelated with 
\begin_inset Formula $\varepsilon_{it}.$
\end_inset

 Typical instruments would be low order monomials in the exogenous variables in 
\begin_inset Formula $\mathbf{z}_{t},$
\end_inset

 with their lagged values.
 Then we can define the 
\begin_inset Formula $\left(\sum_{i=1}^{G}A_{i}\right)\times1$
\end_inset

 orthogonality conditions 
\begin_inset Formula 
\[
m_{t}(\theta)=\left[\begin{array}{c}
\left(y_{1t}-f_{1}(\mathbf{z}_{t},\theta_{1})\right)\mathbf{x}_{1t}\\
\left(y_{2t}-f_{2}(\mathbf{z}_{t},\theta_{2})\right)\mathbf{x}_{2t}\\
\vdots\\
\left(y_{Gt}-f_{G}(\mathbf{z}_{t},\theta_{G})\right)\mathbf{x}_{Gt}
\end{array}\right].
\]

\end_inset


\end_layout

\begin_layout Itemize
once we have gotten this far,
 we can just proceed with GMM estimation,
 one-step,
 two-step,
 CUE,
 or whatever.
\end_layout

\begin_layout Itemize
A note on identification:
 selection of instruments that ensure identification is a non-trivial problem.
 Identification in nonlinear models is not as easy to check as it is with linear models,
 where counting zero restrictions works.
\end_layout

\begin_layout Itemize
A note on efficiency:
 the selected set of instruments has important effects on the efficiency of estimation.
 There are some papers that study this problem,
 but the results are fairly complicated and difficult to implement.
 I think it's safe to say that the great majority of applied work does not attempt to use optimal instruments.
\end_layout

\begin_layout Section
Example:
 The MEPS data
\end_layout

\begin_layout Standard
Taken out of the GMM chapter,
 distracting,
 and not a great example.
 The MEPS data on health care usage discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"
nolink "false"

\end_inset

 estimated a Poisson model by 
\begin_inset Quotes sld
\end_inset

maximum likelihood
\begin_inset Quotes srd
\end_inset

 (probably misspecified).
 Perhaps the same latent factors (e.g.,
 chronic illness) that induce one to make doctor visits also influence the decision of whether or not to purchase insurance.
 If this is the case,
 the PRIV variable could well be endogenous,
 in which case,
 the Poisson 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimator would be inconsistent,
 even if the conditional mean were correctly specified.
 Suppose that 
\begin_inset Formula 
\[
y=\exp(X\beta+Z\gamma)v
\]

\end_inset

where 
\begin_inset Formula $E(v|X)=1$
\end_inset

 but 
\begin_inset Formula $v$
\end_inset

 may be related to 
\begin_inset Formula $Z,$
\end_inset

 so 
\begin_inset Formula $Z$
\end_inset

 is endogenous.
 Then 
\begin_inset Formula $E(y/\exp(X\beta+Z\gamma)-1|X)=0.$
\end_inset

 This expression can be used to define moment conditions.
 The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
href{meps.m}{./Examples/GMM/MEPS/meps.m} 
\end_layout

\end_inset

 estimates the parameters of the model presented in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Poisson model OBDV"
nolink "false"

\end_inset

,
 using Poisson 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 (better thought of as quasi-ML),
 and IV estimation
\begin_inset Foot
status open

\begin_layout Plain Layout
The validity of the instruments used may be debatable,
 but real data sets often don't contain ideal instruments.
\end_layout

\end_inset

.
 Both estimation methods are implemented using a GMM form.
 Running that script gives the output 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GMM/MEPS/meps.out"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Note how the Poisson QML results,
 estimated here using a GMM routine,
 are the same as were obtained using the ML estimation routine (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:PoissonOBDV_results"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

).
 This is an example of how (Q)ML may be represented as a GMM estimator.
 Also note that the IV and QML results are considerably different.
 Treating PRIV as potentially endogenous causes the sign of its coefficient to change.
 Perhaps it is logical that people who own private insurance make fewer visits,
 if they have to make a co-payment.
 Note that income becomes positive and significant when PRIV is treated as endogenous.
\end_layout

\begin_layout Standard
Perhaps the difference in the results depending upon whether or not PRIV is treated as endogenous can suggest a method for testing exogeneity....
\end_layout

\begin_layout Subsubsection
Invertibility of AR process
\end_layout

\begin_layout Standard
To begin with,
 define the lag operator 
\begin_inset Formula $L$
\end_inset


\begin_inset Formula 
\[
Ly_{t}=y_{t-1}
\]

\end_inset

 The lag operator is defined to behave just as an algebraic quantity,
 e.g.,
 
\begin_inset Formula 
\begin{eqnarray*}
L^{2}y_{t} & =L(Ly_{t})\\
 & =Ly_{t-1}\\
 & =y_{t-2}
\end{eqnarray*}

\end_inset

 or 
\begin_inset Formula 
\begin{eqnarray*}
(1-L)(1+L)y_{t} & = & 1-Ly_{t}+Ly_{t}-L^{2}y_{t}\\
 & = & 1-y_{t-2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
A mean-zero AR(p) process can be written as 
\begin_inset Formula 
\[
y_{t}-\phi_{1}y_{t-1}-\phi_{2}y_{t-2}-\cdots-\phi_{p}y_{t-p}=\varepsilon_{t}
\]

\end_inset

 or 
\begin_inset Formula 
\[
y_{t}(1-\phi_{1}L-\phi_{2}L^{2}-\cdots-\phi_{p}L^{p})=\varepsilon_{t}
\]

\end_inset

 Factor this polynomial as 
\begin_inset Formula 
\[
1-\phi_{1}L-\phi_{2}L^{2}-\cdots-\phi_{p}L^{p}=(1-\lambda_{1}L)(1-\lambda_{2}L)\cdots(1-\lambda_{p}L)
\]

\end_inset

 For the moment,
 just assume that the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 are coefficients to be determined.
 Since 
\begin_inset Formula $L\;$
\end_inset

is defined to operate as an algebraic quantity,
 determination of the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 is the same as determination of the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 such that the following two expressions are the same for all 
\begin_inset Formula $z:$
\end_inset


\begin_inset Formula 
\[
1-\phi_{1}z-\phi_{2}z^{2}-\cdots-\phi_{p}z^{p}=(1-\lambda_{1}z)(1-\lambda_{2}z)\cdots(1-\lambda_{p}z)
\]

\end_inset

 Multiply both sides by 
\begin_inset Formula $z^{-p}$
\end_inset


\begin_inset Formula 
\[
z^{-p}-\phi_{1}z^{1-p}-\phi_{2}z^{2-p}-\cdots\phi_{p-1}z^{-1}-\phi_{p}=(z^{-1}-\lambda_{1})(z^{-1}-\lambda_{2})\cdots(z^{-1}-\lambda_{p})
\]

\end_inset

 and now define 
\begin_inset Formula $\lambda=z^{-1}$
\end_inset

 so we get 
\begin_inset Formula 
\[
\lambda^{p}-\phi_{1}\lambda^{p-1}-\phi_{2}\lambda^{p-2}-\cdots-\phi_{p-1}\lambda-\phi_{p}=(\lambda-\lambda_{1})(\lambda-\lambda_{2})\cdots(\lambda-\lambda_{p})
\]

\end_inset

 The LHS is precisely the determinantal polynomial that gives the eigenvalues of 
\begin_inset Formula $F.$
\end_inset

 Therefore,
 the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 that are the coefficients of the factorization are simply the eigenvalues of the matrix 
\begin_inset Formula $F.$
\end_inset


\end_layout

\begin_layout Standard
Now consider a different stationary process 
\begin_inset Formula 
\[
(1-\phi L)y_{t}=\varepsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Itemize
Stationarity,
 as above,
 implies that 
\begin_inset Formula $|\phi|<1.$
\end_inset


\end_layout

\begin_layout Standard
Multiply both sides by 
\begin_inset Formula $1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}$
\end_inset

 to get 
\begin_inset Formula 
\[
\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)(1-\phi L)y_{t}=\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 or,
 multiplying the polynomials on the LHS,
 we get
\begin_inset Formula 
\[
\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}-\phi L-\phi^{2}L^{2}-...-\phi^{j}L^{j}-\phi^{j+1}L^{j+1}\right)y_{t}=\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

and with cancellations we have 
\begin_inset Formula 
\[
\left(1-\phi^{j+1}L^{j+1}\right)y_{t}=\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 so 
\begin_inset Formula 
\[
y_{t}=\phi^{j+1}L^{j+1}y_{t}+\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 Now as 
\begin_inset Formula $j\rightarrow\infty,$
\end_inset

 
\begin_inset Formula $\phi^{j+1}L^{j+1}y_{t}\rightarrow0,$
\end_inset

 since 
\begin_inset Formula $|\phi|<1,$
\end_inset

 so 
\begin_inset Formula 
\[
y_{t}\cong\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 and the approximation becomes better and better as 
\begin_inset Formula $j$
\end_inset

 increases.
 However,
 we started with 
\begin_inset Formula 
\[
(1-\phi L)y_{t}=\varepsilon_{t}
\]

\end_inset

 Substituting this into the above equation we have 
\begin_inset Formula 
\[
y_{t}\cong\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)(1-\phi L)y_{t}
\]

\end_inset

 so 
\begin_inset Formula 
\[
\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)(1-\phi L)\cong1
\]

\end_inset

 and the approximation becomes arbitrarily good as 
\begin_inset Formula $j$
\end_inset

 increases arbitrarily.
 Therefore,
 for 
\begin_inset Formula $|\phi|<1,$
\end_inset

 define 
\begin_inset Formula 
\[
(1-\phi L)^{-1}=\sum_{j=0}^{\infty}\phi^{j}L^{j}
\]

\end_inset

 Recall that our mean zero AR(p) process 
\begin_inset Formula 
\[
y_{t}(1-\phi_{1}L-\phi_{2}L^{2}-\cdots-\phi_{p}L^{p})=\varepsilon_{t}
\]

\end_inset

 can be written using the factorization 
\begin_inset Formula 
\[
y_{t}(1-\lambda_{1}L)(1-\lambda_{2}L)\cdots(1-\lambda_{p}L)=\varepsilon_{t}
\]

\end_inset

 where the 
\begin_inset Formula $\lambda$
\end_inset

 are the eigenvalues of 
\begin_inset Formula $F,$
\end_inset

 and given stationarity,
 all the 
\begin_inset Formula $|\lambda_{i}|<1.$
\end_inset

 Therefore,
 we can invert each first order polynomial on the LHS to get 
\begin_inset Formula 
\[
y_{t}=\left(\sum_{j=0}^{\infty}\lambda_{1}^{j}L^{j}\right)\left(\sum_{j=0}^{\infty}\lambda_{2}^{j}L^{j}\right)\cdots\left(\sum_{j=0}^{\infty}\lambda_{p}^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 The RHS is a product of infinite-order polynomials in 
\begin_inset Formula $L,$
\end_inset

 which can be represented as 
\begin_inset Formula 
\[
y_{t}=(1+\psi_{1}L+\psi_{2}L^{2}+\cdots)\varepsilon_{t}
\]

\end_inset

 where the 
\begin_inset Formula $\psi_{i}$
\end_inset

 are real-valued and absolutely summable.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\psi_{i}$
\end_inset

 are formed of products of powers of the 
\begin_inset Formula $\lambda_{i}$
\end_inset

,
 which are in turn functions of the 
\begin_inset Formula $\phi_{i}.$
\end_inset


\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\psi_{i}$
\end_inset

 are real-valued because any complex-valued 
\begin_inset Formula $\lambda_{i}$
\end_inset

 always occur in conjugate pairs.
 This means that if 
\begin_inset Formula $a+bi$
\end_inset

 is an eigenvalue of 
\begin_inset Formula $F,$
\end_inset

 then so is 
\begin_inset Formula $a-bi.$
\end_inset

 In multiplication 
\begin_inset Formula 
\begin{eqnarray*}
\left(a+bi\right)(a-bi) & = & a^{2}-abi+abi-b^{2}i^{2}\\
 & = & a^{2}+b^{2}
\end{eqnarray*}

\end_inset

 which is real-valued.
\end_layout

\begin_layout Itemize
This shows that an AR(p) process is representable as an infinite-order MA(q) process.
\end_layout

\begin_layout Itemize
Recall before that by recursive substitution,
 an AR(p) process can be written as 
\begin_inset Formula 
\[
Y_{t+j}=C+FC+\cdots+F^{j}C+F^{j+1}Y_{t-1}+F^{j}E_{t}+F^{j-1}E_{t+1}+\cdots+FE_{t+j-1}+E_{t+j}
\]

\end_inset

 If the process is mean zero,
 then everything with a 
\begin_inset Formula $C$
\end_inset

 drops out.
 Take this and lag it by 
\begin_inset Formula $j$
\end_inset

 periods to get 
\begin_inset Formula 
\[
Y_{t}=F^{j+1}Y_{t-j-1}+F^{j}E_{t-j}+F^{j-1}E_{t-j+1}+\cdots+FE_{t-1}+E_{t}
\]

\end_inset

 As 
\begin_inset Formula $j\rightarrow\infty,$
\end_inset

 the lagged 
\begin_inset Formula $Y$
\end_inset

 on the RHS drops out.
 The 
\begin_inset Formula $E_{t-s}$
\end_inset

 are vectors of zeros except for their first element,
 so we see that the first equation here,
 in the limit,
 is just 
\begin_inset Formula 
\[
y_{t}=\sum_{j=0}^{\infty}\left(F^{j}\right)_{1,1}\varepsilon_{t-j}
\]

\end_inset

 which makes explicit the relationship between the 
\begin_inset Formula $\psi_{i}$
\end_inset

 and the 
\begin_inset Formula $\phi_{i}$
\end_inset

 (and the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 as well,
 recalling the previous factorization of 
\begin_inset Formula $F^{j}).$
\end_inset


\end_layout

\begin_layout Subsection
Invertibility of MA(q) process
\end_layout

\begin_layout Standard
An MA(q) can be written as 
\begin_inset Formula 
\[
y_{t}-\mu=(1+\theta_{1}L+...+\theta_{q}L^{q})\varepsilon_{t}
\]

\end_inset

 As before,
 the polynomial on the RHS can be factored as 
\begin_inset Formula 
\[
(1+\theta_{1}L+...+\theta_{q}L^{q})=(1-\eta_{1}L)(1-\eta_{2}L)...(1-\eta_{q}L)
\]

\end_inset

 and each of the 
\begin_inset Formula $(1-\eta_{i}L)$
\end_inset

 can be inverted as long as each of the 
\begin_inset Formula $|\eta_{i}|<1.$
\end_inset

 If this is the case,
 then we can write 
\begin_inset Formula 
\[
(1+\theta_{1}L+...+\theta_{q}L^{q})^{-1}(y_{t}-\mu)=\varepsilon_{t}
\]

\end_inset

 where 
\begin_inset Formula 
\[
(1+\theta_{1}L+...+\theta_{q}L^{q})^{-1}
\]

\end_inset

 will be an infinite-order polynomial in 
\begin_inset Formula $L,$
\end_inset

 so we get 
\begin_inset Formula 
\[
\sum_{j=0}^{\infty}-\delta_{j}L^{j}(y_{t-j}-\mu)=\varepsilon_{t}
\]

\end_inset

 with 
\begin_inset Formula $\delta_{0}=-1,$
\end_inset

 or 
\begin_inset Formula 
\[
(y_{t}-\mu)-\delta_{1}(y_{t-1}-\mu)-\delta_{2}(y_{t-2}-\mu)+...=\varepsilon_{t}
\]

\end_inset

 or 
\begin_inset Formula 
\[
y_{t}=c+\delta_{1}y_{t-1}+\delta_{2}y_{t-2}+...+\varepsilon_{t}
\]

\end_inset

 where 
\begin_inset Formula 
\[
c=\mu+\delta_{1}\mu+\delta_{2}\mu+...
\]

\end_inset

 So we see that an MA(q) has an infinite AR representation,
 as long as the 
\begin_inset Formula $|\eta_{i}|<1,$
\end_inset

 
\begin_inset Formula $i=1,2,...,q.$
\end_inset


\end_layout

\begin_layout Itemize
It turns out that one can always manipulate the parameters of an MA(q) process to find an invertible representation.
 For example,
 the two MA(1) processes 
\begin_inset Formula 
\[
y_{t}-\mu=(1-\theta L)\varepsilon_{t}
\]

\end_inset

 and 
\begin_inset Formula 
\[
y_{t}^{\ast}-\mu=(1-\theta^{-1}L)\varepsilon_{t}^{\ast}
\]

\end_inset

 have exactly the same moments if 
\begin_inset Formula 
\[
\sigma_{\varepsilon^{\ast}}^{2}=\sigma_{\varepsilon}^{2}\theta^{2}
\]

\end_inset

 For example,
 we've seen that 
\begin_inset Formula 
\[
\gamma_{0}=\sigma^{2}(1+\theta^{2}).
\]

\end_inset

 Given the above relationships amongst the parameters,
 
\begin_inset Formula 
\[
\gamma_{0}^{\ast}=\sigma_{\varepsilon}^{2}\theta^{2}(1+\theta^{-2})=\sigma^{2}(1+\theta^{2})
\]

\end_inset

 so the variances are the same.
 It turns out that 
\emph on
all
\emph default
 the autocovariances will be the same,
 as is easily checked.
 This means that the two MA processes are 
\emph on
observationally equivalent
\emph default
.
 As before,
 it's impossible to distinguish between observationally equivalent processes on the basis of data.
\end_layout

\begin_layout Itemize
For a given MA(q) process,
 it's always possible to manipulate the parameters to find an invertible representation (which is unique).
\end_layout

\begin_layout Itemize
It's important to find an invertible representation,
 since it's the only representation that allows one to represent 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 as a function of past 
\begin_inset Formula $y's.$
\end_inset

 The other representations express 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 as a function of future 
\begin_inset Formula $y's$
\end_inset


\end_layout

\begin_layout Itemize
Why is invertibility important?
 The most important reason is that it provides a justification for the use of parsimonious models.
 Since an AR(1) process has an MA(
\begin_inset Formula $\infty)$
\end_inset

 representation,
 one can reverse the argument and note that at least some MA(
\begin_inset Formula $\infty)$
\end_inset

 processes have an AR(1) representation.
 Likewise,
 some AR(
\begin_inset Formula $\infty)$
\end_inset

 processes have an MA(1) representation.
 At the time of estimation,
 it's a lot easier to estimate the single AR(1) or MA(1) coefficient rather than the infinite number of coefficients associated with the MA(
\begin_inset Formula $\infty)$
\end_inset

 or AR(
\begin_inset Formula $\infty)$
\end_inset

 representation.
\end_layout

\begin_layout Itemize
This is the reason that ARMA models are popular.
 Combining low-order AR and MA models can usually offer a satisfactory representation of univariate time series data using a reasonable number of parameters.
\end_layout

\begin_layout Itemize
Stationarity and invertibility of ARMA models is similar to what we've seen - we won't go into the details.
 Likewise,
 calculating moments is similar.
 
\end_layout

\begin_layout Exercise
Calculate the autocovariances of an ARMA(1,1) model:
\begin_inset Formula $(1+\phi L)y_{t}=c+(1+\theta L)\epsilon_{t}$
\end_inset


\end_layout

\begin_layout Subsection
Optimal instruments for GMM
\end_layout

\begin_layout Standard
PLEASE IGNORE THE REST OF THIS SECTION:
 there is a flaw in the argument that needs correction.
 In particular,
 it may be the case that 
\begin_inset Formula $E(Z_{t}\epsilon_{t})\ne0$
\end_inset

 if instruments are chosen in the way suggested here.
 
\end_layout

\begin_layout Standard
An interesting question that arises is how one should choose the instrumental variables 
\begin_inset Formula $Z(w_{t})$
\end_inset

 to achieve maximum efficiency.
\end_layout

\begin_layout Standard
Note that with this choice of moment conditions,
 we have that 
\begin_inset Formula $D_{n}\equiv\frac{\partial}{\partial\theta}m^{\prime}(\theta)$
\end_inset

 (a 
\begin_inset Formula $K\times g$
\end_inset

 matrix) is 
\begin_inset Formula 
\begin{eqnarray*}
D_{n}(\theta) & = & \frac{\partial}{\partial\theta}\frac{1}{n}\left(Z_{n}^{\prime}h_{n}(\theta)\right)^{\prime}\\
 & = & \frac{1}{n}\left(\frac{\partial}{\partial\theta}h_{n}^{\prime}\left(\theta\right)\right)Z_{n}
\end{eqnarray*}

\end_inset

 which we can define to be 
\begin_inset Formula 
\[
D_{n}(\theta)=\frac{1}{n}H_{n}Z_{n}.
\]

\end_inset

where 
\begin_inset Formula $H_{n}$
\end_inset

 is a 
\begin_inset Formula $K\times n$
\end_inset

 matrix that has the derivatives of the individual moment conditions as its columns.
 Likewise,
 define the var-cov.
 of the moment conditions 
\begin_inset Formula 
\begin{eqnarray*}
\Omega_{n} & = & \mathcal{E}\left[n\bar{m}_{n}(\theta_{0})\bar{m}_{n}(\theta_{0})^{\prime}\right]\\
 & = & \mathcal{E}\left[\frac{1}{n}Z_{n}^{\prime}h_{n}(\theta_{0})h_{n}(\theta_{0})^{\prime}Z_{n}\right]\\
 & = & Z_{n}^{\prime}\mathcal{E}\left(\frac{1}{n}h_{n}(\theta_{0})h_{n}(\theta_{0})^{\prime}\right)Z_{n}\\
 & \equiv & Z_{n}^{\prime}\frac{\Phi_{n}}{n}Z_{n}
\end{eqnarray*}

\end_inset

where we have defined 
\begin_inset Formula $\Phi_{n}=V\left(h_{n}(\theta_{0})\right).$
\end_inset

 Note that the dimension of this matrix is growing with the sample size,
 so it is not consistently estimable without additional assumptions.
\end_layout

\begin_layout Standard
The asymptotic normality theorem above says that the GMM estimator using the optimal weighting matrix is distributed as 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N(0,V_{\infty})
\]

\end_inset

 where 
\begin_inset Formula 
\begin{equation}
V_{\infty}=\lim_{n\rightarrow\infty}\left(\left(\frac{H_{n}Z_{n}}{n}\right)\left(\frac{Z_{n}^{\prime}\Phi_{n}Z_{n}}{n}\right)^{-1}\left(\frac{Z_{n}^{\prime}H_{n}^{\prime}}{n}\right)\right)^{-1}.\label{var-covcondmoments,nonoptimal}
\end{equation}

\end_inset

Using an argument similar to that used to prove that 
\begin_inset Formula $\Omega_{\infty}^{-1}$
\end_inset

 is the efficient weighting matrix,
 we can show that putting 
\begin_inset Formula 
\[
Z_{n}=\Phi_{n}^{-1}H_{n}^{\prime}
\]

\end_inset

 causes the above var-cov matrix to simplify to 
\begin_inset Formula 
\begin{equation}
V_{\infty}=\lim_{n\rightarrow\infty}\left(\frac{H_{n}\Phi_{n}^{-1}H_{n}^{\prime}}{n}\right)^{-1}.\label{simplevarcov,condmoments}
\end{equation}

\end_inset

 and furthermore,
 this matrix is smaller that the limiting var-cov for any other choice of instrumental variables.
 (To prove this,
 examine the difference of the inverses of the var-cov matrices with the optimal intruments and with non-optimal instruments.
 As above,
 you can show that the difference is positive semi-definite).
\end_layout

\begin_layout Itemize
Note that both 
\begin_inset Formula $H_{n},$
\end_inset

 which we should write more properly as 
\begin_inset Formula $H_{n}(\theta_{0}),$
\end_inset

 since it depends on 
\begin_inset Formula $\theta_{0},$
\end_inset

 and 
\begin_inset Formula $\Phi$
\end_inset

 must be consistently estimated to apply this.
\end_layout

\begin_layout Itemize
Usually,
 estimation of 
\begin_inset Formula $H_{n}$
\end_inset

 is straightforward - one just uses 
\begin_inset Formula 
\[
\widehat{H}=\frac{\partial}{\partial\theta}h_{n}^{\prime}\left(\tilde{\theta}\right),
\]

\end_inset

 where 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is some initial consistent estimator based on non-optimal instruments.
\end_layout

\begin_layout Itemize
Estimation of 
\begin_inset Formula $\Phi_{n}$
\end_inset

 may not be possible.
 It is an 
\begin_inset Formula $n\times n$
\end_inset

 matrix,
 so it has more unique elements than 
\begin_inset Formula $n,$
\end_inset

 the sample size,
 so without restrictions on the parameters it can't be estimated consistently.
 Basically,
 you need to provide a parametric specification of the covariances of the 
\begin_inset Formula $h_{t}(\theta)\;$
\end_inset

in order to be able to use optimal instruments.
 A solution is to approximate this matrix parametrically to define the instruments.
 Note that the simplified var-cov matrix in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "simplevarcov,condmoments"
nolink "false"

\end_inset

 will not apply if approximately optimal instruments are used - it will be necessary to use an estimator based upon equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "var-covcondmoments,nonoptimal"
nolink "false"

\end_inset

,
 where the term 
\begin_inset Formula $n^{-1}Z_{n}^{\prime}\Phi_{n}Z_{n}$
\end_inset

 must be estimated consistently apart,
 for example by the Newey-West procedure.
 
\end_layout

\begin_layout Section
Hurdle models
\end_layout

\begin_layout Standard
Returning to the Poisson model,
 lets look at actual and fitted count probabilities.
 Actual relative frequencies are 
\begin_inset Formula $f(y=j)=\sum_{i}1(y_{i}=j)/n$
\end_inset

 and fitted frequencies are 
\begin_inset Formula $\hat{f}(y=j)=\sum_{i=1}^{n}f_{Y}(j|x_{i},\hat{\theta})/n$
\end_inset


\begin_inset Float table
placement htbp
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Actual and Poisson fitted frequencies
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.83
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.14
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.19
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.004
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.052
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0002
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.032
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.4e-5
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

We see that for the OBDV measure,
 there are many more actual zeros than predicted.
 For ERV,
 there are somewhat more actual zeros than fitted,
 but the difference is not too important.
 
\end_layout

\begin_layout Standard
Why might OBDV not fit the zeros well?
 What if people made the decision to contact the doctor for a first visit,
 they are sick,
 then the 
\emph on
doctor
\emph default
 decides on whether or not follow-up visits are needed.
 This is a principal/agent type situation,
 where the total number of visits depends upon the decision of both the patient and the doctor.
 Since different parameters may govern the two decision-makers choices,
 we might expect that different parameters govern the probability of zeros versus the other counts.
 Let 
\begin_inset Formula $\lambda_{p}$
\end_inset

 be the parameters of the patient's demand for visits,
 and let 
\begin_inset Formula $\lambda_{d}$
\end_inset

 be the paramter of the doctor's 
\begin_inset Quotes eld
\end_inset

demand
\begin_inset Quotes erd
\end_inset

 for visits.
 The patient will initiate visits according to a discrete choice model,
 for example,
 a logit model:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Pr(Y=0) & =f_{Y}(0,\lambda_{p})= & 1-1/\left[1+\exp(-\lambda_{p})\right]\\
\Pr(Y>0) & = & 1/\left[1+\exp(-\lambda_{p})\right],
\end{eqnarray*}

\end_inset

 The above probabilities are used to estimate the binary 0/1 hurdle process.
 Then,
 for the observations where visits are positive,
 a truncated Poisson density is estimated.
 This density is
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y,\lambda_{d}|y>0) & = & \frac{f_{Y}(y,\lambda_{d})}{\Pr(y>0)}\\
 & = & \frac{f_{Y}(y,\lambda_{d})}{1-\exp(-\lambda_{d})}
\end{eqnarray*}

\end_inset

since according to the Poisson model with the doctor's paramaters,
\begin_inset Formula 
\[
\Pr(y=0)=\frac{\exp(-\lambda_{d})\lambda_{d}^{0}}{0!}.
\]

\end_inset

Since the hurdle and truncated components of the overall density for 
\begin_inset Formula $Y$
\end_inset

 share no parameters,
 they may be estimated separately,
 which is computationally more efficient than estimating the overall model.
 (Recall that the BFGS algorithm,
 for example,
 will have to invert the approximated Hessian.
 The computational overhead is of order 
\begin_inset Formula $K^{2}$
\end_inset

 where 
\begin_inset Formula $K$
\end_inset

 is the number of parameters to be estimated) .
 The expectation of 
\begin_inset Formula $Y$
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
E(Y|x) & = & \Pr(Y>0|x)E(Y|Y>0,x)\\
 & = & \left(\frac{1}{1+\exp(-\lambda_{p})}\right)\left(\frac{\lambda_{d}}{1-\exp(-\lambda_{d})}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Here are hurdle Poisson estimation results for OBDV,
 obtained from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{this estimation program}{./Examples/MEPS-II/estimate
\backslash
_hpoisson.ox} 
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data,
 OBDV
\end_layout

\begin_layout Standard

\family typewriter
logit results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.58939
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.5502
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.5709
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.5269
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.5560
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.0519
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0520
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0027
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0384
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.45867
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7289
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.6924
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7166
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.63570
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0873
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1677
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1366
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.018614
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1547
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1969
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1807
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.039606
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.0467
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.98710
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.0222
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.077446
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7655
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1672
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.9601
\end_layout

\begin_layout Standard

\family typewriter
Information Criteria 
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 639.89
\end_layout

\begin_layout Standard

\family typewriter
Schwartz 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 632.89
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 614.96
\end_layout

\begin_layout Standard

\family typewriter
Akaike 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 603.39
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

The results for the truncated part:
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data,
 OBDV
\end_layout

\begin_layout Standard

\family typewriter
tpoisson results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.7042
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.54254
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.4291
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1747
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.2323
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.31001
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.5708
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7573
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7183
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.014382
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.29433
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.10438
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.18112
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.19075
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 10.293
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1890
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.6942
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.016683
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 16.148
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.5262
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.9814
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.016286
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 4.2144
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.56547
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.6353
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.0079016
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.3186
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.35309
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.96078
\end_layout

\begin_layout Standard

\family typewriter
Information Criteria 
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2754.7
\end_layout

\begin_layout Standard

\family typewriter
Schwartz
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2747.7
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2729.8
\end_layout

\begin_layout Standard

\family typewriter
Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2718.2
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Fitted and actual probabilites (NB-II fits are provided as well) are:
\end_layout

\begin_layout Standard
\begin_inset Float table
placement htbp
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Actual and Hurdle Poisson fitted frequencies
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="7">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted HP
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted NB-II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted HP
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted NB-II
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.34
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.035
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.16
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.071
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.08
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.004
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.006
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.006
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.052
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.032
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0005
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.001
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

For the Hurdle Poisson models,
 the ERV fit is very accurate.
 The OBDV fit is not so good.
 Zeros are exact,
 but 1's and 2's are underestimated,
 and higher counts are overestimated.
 For the NB-II fits,
 performance is at least as good as the hurdle Poisson model,
 and one should recall that many fewer parameters are used.
 Hurdle version of the negative binomial model are also widely used.
\end_layout

\begin_layout Section
Finite mixture models
\end_layout

\begin_layout Standard
The following are results for a mixture of 2 negative binomial (NB-I) models,
 for the OBDV data,
 which you can replicate using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{this estimation program}{./Examples/MEPS-II/estimate
\backslash
_mixnegbin.ox} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data,
 OBDV
\end_layout

\begin_layout Standard

\family typewriter
mixnegbin results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.2312
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.64852
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3851
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3226
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.4358
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.062139
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.23188
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.13802
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.18729
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.093396
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.46948
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.33046
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.40854
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.39785
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.6121
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.2148
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.4882
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.015969
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5173
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5475
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.7151
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.049175
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.8013
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.7061
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.8036
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.015880
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.58386
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.76782
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.73281
\end_layout

\begin_layout Standard

\family typewriter
ln_alpha
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.69961
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.3456
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.0396
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.4029
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -3.6130
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.6126
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.7365
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.8411
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.3456
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7527
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7677
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.6519
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.77431
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.73854
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1366
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.97338
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.34886
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.80035
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.74016
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.81892
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.021425
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1354
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3032
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3387
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.22461
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.0922
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7826
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1470
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.019227
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.20453
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.40854
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.36313
\end_layout

\begin_layout Standard

\family typewriter
ln_alpha
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.8419
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.2497
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.8702
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.6182
\end_layout

\begin_layout Standard

\family typewriter
logit_inv_mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.85186
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7096
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.4827
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7883
\end_layout

\begin_layout Standard

\family typewriter
Information Criteria
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2353.8
\end_layout

\begin_layout Standard

\family typewriter
Schwartz
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2336.8
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2293.3
\end_layout

\begin_layout Standard

\family typewriter
Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2265.2
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
Delta method for mix parameter st.
 err.
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 se_mix
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.70096
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.12043
\end_layout

\begin_layout Itemize
The 95% confidence interval for the mix parameter is perilously close to 1,
 which suggests that there may really be only one component density,
 rather than a mixture.
 Again,
 this is 
\emph on
not
\emph default
 the way to test this - it is merely suggestive.
\end_layout

\begin_layout Itemize
Education is interesting.
 For the subpopulation that is 
\begin_inset Quotes eld
\end_inset

healthy
\begin_inset Quotes erd
\end_inset

,
 i.e.,
 that makes relatively few visits,
 education seems to have a positive effect on visits.
 For the 
\begin_inset Quotes eld
\end_inset

unhealthy
\begin_inset Quotes erd
\end_inset

 group,
 education has a negative effect on visits.
 The other results are more mixed.
 A larger sample could help clarify things.
\end_layout

\begin_layout Standard
The following are results for a 2 component constrained mixture negative binomial model where all the slope parameters in 
\begin_inset Formula $\lambda_{j}=e^{\mathbf{x}\beta_{j}}$
\end_inset

 are the same across the two components.
 The constants and the overdispersion parameters 
\begin_inset Formula $\alpha_{j}$
\end_inset

 are allowed to differ for the two components.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data,
 OBDV
\end_layout

\begin_layout Standard

\family typewriter
cmixnegbin results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.2441
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.34153
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.94203
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.91456
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.97943
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.45320
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.6206
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5088
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.7067
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.20663
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.4258
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3105
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3895
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.37714
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1948
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.4929
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.5319
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.015822
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1212
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7806
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7042
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.011784
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.65887
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.50362
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.58331
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.014088
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.69088
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.96831
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.83408
\end_layout

\begin_layout Standard

\family typewriter
ln_alpha
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1798
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 4.6140
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.2462
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.4293
\end_layout

\begin_layout Standard

\family typewriter
const_2
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.2621
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.47525
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5219
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.5060
\end_layout

\begin_layout Standard

\family typewriter
lnalpha_2
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.7769
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.5539
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.4918
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 4.2243
\end_layout

\begin_layout Standard

\family typewriter
logit_inv_mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.4888
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.60073
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7224
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.9693
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard

\family typewriter
Information Criteria
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2323.5
\end_layout

\begin_layout Standard

\family typewriter
Schwartz
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2312.5
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2284.3
\end_layout

\begin_layout Standard

\family typewriter
Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2266.1
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
Delta method for mix parameter st.
 err.
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 se_mix
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.92335
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.047318
\end_layout

\begin_layout Itemize
Now the mixture parameter is even closer to 1.
\end_layout

\begin_layout Itemize
The slope parameter estimates are pretty close to what we got with the NB-I model.
\end_layout

\begin_layout Section
Nonlinear least squares (NLS)
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
:
 Davidson and MacKinnon,
 Ch.
 2
\begin_inset Formula $^{*}$
\end_inset

 and 5
\begin_inset Formula $^{*}$
\end_inset

;
 Gallant,
 Ch.
 1
\end_layout

\begin_layout Subsection
Introduction and definition
\end_layout

\begin_layout Standard
Nonlinear least squares (NLS) is a means of estimating the parameter of the model 
\begin_inset Formula 
\[
y_{t}=f(\mathbf{x}_{t},\theta_{0})+\varepsilon_{t}.
\]

\end_inset


\end_layout

\begin_layout Itemize
In general,
 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 will be heteroscedastic and autocorrelated,
 and possibly nonnormally distributed.
 However,
 dealing with this is exactly as in the case of linear models,
 so we'll just treat the iid case here,
 
\begin_inset Formula 
\[
\varepsilon_{t}\sim iid(0,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
If we stack the observations vertically,
 defining 
\begin_inset Formula 
\[
\mathbf{y}=(y_{1},y_{2},...,y_{n})^{\prime}
\]

\end_inset

 
\begin_inset Formula 
\[
\mathbf{f}=(f(x_{1},\theta),f(x_{1},\theta),...,f(x_{1},\theta))^{\prime}
\]

\end_inset

 and 
\begin_inset Formula 
\[
\varepsilon=(\varepsilon_{1},\varepsilon_{2},...,\varepsilon_{n})^{\prime}
\]

\end_inset

 we can write the 
\begin_inset Formula $n$
\end_inset

 observations as 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta)+\varepsilon
\]

\end_inset

 Using this notation,
 the NLS estimator can be defined as 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}s_{n}(\theta)=\frac{1}{n}\left[\mathbf{y}-\mathbf{f}(\theta)\right]^{\prime}\left[\mathbf{y}-\mathbf{f}(\theta)\right]=\frac{1}{n}\parallel\mathbf{y}-\mathbf{f}(\theta)\parallel^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
The estimator minimizes the weighted sum of squared errors,
 which is the same as minimizing the Euclidean distance between 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{f}(\theta).$
\end_inset


\end_layout

\begin_layout Standard
The objective function can be written as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\left[\mathbf{y}^{\prime}\mathbf{y}-2\mathbf{y}^{\prime}\mathbf{f}(\theta)+\mathbf{f}(\theta)^{\prime}\mathbf{f}(\theta)\right],
\]

\end_inset

 which gives the first order conditions
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
-\left[\frac{\partial}{\partial\theta}\mathbf{f}(\hat{\theta})^{\prime}\right]\mathbf{y}+\left[\frac{\partial}{\partial\theta}\mathbf{f}(\hat{\theta})^{\prime}\right]\mathbf{f}(\hat{\theta})\equiv0.
\]

\end_inset

 Define the 
\begin_inset Formula $n\times K$
\end_inset

 matrix 
\begin_inset Formula 
\begin{equation}
\mathbf{F}(\hat{\theta})\equiv D_{\theta^{\prime}}\mathbf{f}(\hat{\theta}).\label{nlsderiv}
\end{equation}

\end_inset

 In shorthand,
 use 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 in place of 
\begin_inset Formula $\mathbf{F}(\hat{\theta}).$
\end_inset

 Using this,
 the first order conditions can be written as 
\begin_inset Formula 
\[
-\hat{\mathbf{F}}^{\prime}\mathbf{y}+\hat{\mathbf{F}}^{\prime}\mathbf{f}(\hat{\theta})\equiv0,
\]

\end_inset

 or 
\begin_inset Formula 
\begin{equation}
\hat{\mathbf{F}}^{\prime}\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]\equiv0.\label{nlsfoc}
\end{equation}

\end_inset

 This bears a good deal of similarity to the f.o.c.
 for the linear model - the derivative of the prediction is orthogonal to the prediction error.
 If 
\begin_inset Formula $\mathbf{f}(\theta)=\mathbf{X}\theta,$
\end_inset

 then 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 is simply 
\begin_inset Formula $\mathbf{X},$
\end_inset

 so the f.o.c.
 (with spherical errors) simplify to 
\begin_inset Formula 
\[
\mathbf{X}^{\prime}\mathbf{y}-\mathbf{X}^{\prime}\mathbf{X}\beta=0,
\]

\end_inset

 the usual 0LS f.o.c.
\end_layout

\begin_layout Standard
We can interpret this geometrically:
 
\shape italic
INSERT drawings of geometrical depiction of OLS and NLS (see Davidson and MacKinnon,
 pgs.
 8,13 and 46).
\end_layout

\begin_layout Itemize
Note that the nonlinearity of the manifold leads to potential multiple local maxima,
 minima and saddlepoints:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the objective function 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is not necessarily well-behaved and may be difficult to minimize.
 
\end_layout

\begin_layout Subsection
Identification
\end_layout

\begin_layout Standard
As before,
 identification can be considered conditional on the sample,
 and asymptotically.
 The condition for asymptotic identification is that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 tend to a limiting function 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 such that 
\begin_inset Formula $s_{\infty}(\theta_{0})<s_{\infty}(\theta),$
\end_inset

 
\begin_inset Formula $\forall\theta\neq\theta_{0}.$
\end_inset

 This will be the case if 
\begin_inset Formula $s_{\infty}(\theta_{0})$
\end_inset

 is strictly convex at 
\begin_inset Formula $\theta_{0},$
\end_inset

 which requires that 
\begin_inset Formula $D_{\theta}^{2}s_{\infty}(\theta_{0})$
\end_inset

 be positive definite.
 Consider the objective function:
 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\theta) & = & \frac{1}{n}\sum_{t=1}^{n}\left[y_{t}-f(\mathbf{x}_{t},\theta)\right]^{2}\\
 & = & \frac{1}{n}\sum_{t=1}^{n}\left[f(\mathbf{x}_{t},\theta_{0})+\varepsilon_{t}-f_{t}(\mathbf{x}_{t},\theta)\right]^{2}\\
 & = & \frac{1}{n}\sum_{t=1}^{n}\left[f_{t}(\theta_{0})-f_{t}(\theta)\right]^{2}+\frac{1}{n}\sum_{t=1}^{n}\left(\varepsilon_{t}\right)^{2}\\
 & - & \frac{2}{n}\sum_{t=1}^{n}\left[f_{t}(\theta_{0})-f_{t}(\theta)\right]\varepsilon_{t}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
As in example 
\begin_inset CommandInset ref
LatexCommand ref
reference "eeolsexample"
nolink "false"

\end_inset

,
 which illustrated the consistency of extremum estimators using OLS,
 we conclude that the second term will converge to a constant which does not depend upon 
\begin_inset Formula $\theta.$
\end_inset


\end_layout

\begin_layout Itemize
A LLN can be applied to the third term to conclude that it converges pointwise to 0,
 as long as 
\begin_inset Formula $\mathbf{f}(\theta)\;$
\end_inset

and 
\begin_inset Formula $\varepsilon$
\end_inset

 are uncorrelated.
\end_layout

\begin_layout Itemize
Next,
 pointwise convergence needs to be strengthened to uniform almost sure convergence.
 There are a number of possible assumptions one could use.
 Here,
 we'll just assume it holds.
\end_layout

\begin_layout Itemize
Turning to the first term,
 we'll assume a pointwise law of large numbers applies,
 so 
\begin_inset Formula 
\begin{equation}
\frac{1}{n}\sum_{t=1}^{n}\left[f_{t}(\theta_{0})-f_{t}(\theta)\right]^{2}\stackrel{a.s.}{\rightarrow}\int\left[f(z,\theta_{0})-f(z,\theta)\right]^{2}d\mu(z),\label{nlslim}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\mu(x)$
\end_inset

 is the distribution function of 
\begin_inset Formula $x.$
\end_inset

 In many cases,
 
\begin_inset Formula $f(x,\theta)$
\end_inset

 
\emph on
will
\emph default
 be bounded and continuous,
 for all 
\begin_inset Formula $\theta\in\Theta,$
\end_inset

 so strengthening to uniform almost sure convergence is immediate.
 For example if 
\begin_inset Formula $f(x,\theta)=\left[1+\exp(-x\theta)\right]^{-1},$
\end_inset

 
\begin_inset Formula $f:\Re^{K}\rightarrow\left(0,1\right),$
\end_inset

 a bounded range,
 and the function is continuous in 
\begin_inset Formula $\theta.$
\end_inset


\end_layout

\begin_layout Standard
Given these results,
 it is clear that a minimizer is 
\begin_inset Formula $\theta_{0}.$
\end_inset

 When considering identification (asymptotic),
 the question is whether or not there may be some other minimizer.
 A local condition for identification is that 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{\infty}(\theta)=\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}\int\left[f(x,\theta_{0})-f(x,\theta)\right]^{2}d\mu(x)
\]

\end_inset

 be positive definite at 
\begin_inset Formula $\theta_{0}.$
\end_inset

 Evaluating this derivative,
 we obtain (after a little work)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left.\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}\int\left[f(x,\theta_{0})-f(x,\theta)\right]^{2}d\mu(x)\right|_{\theta_{0}}=2\int\left[D_{\theta}f(z,\theta_{0})^{\prime}\right]\left[D_{\theta^{\prime}}f(z,\theta_{0})\right]^{\prime}d\mu(z)
\]

\end_inset

 the expectation of the outer product of the gradient of the regression function evaluated at 
\begin_inset Formula $\theta_{0}.$
\end_inset

 (Note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the uniform boundedness we have already assumed allows passing the derivative through the integral,
 by the dominated convergence theorem.) This matrix will be positive definite (wp1) as long as the gradient vector is of full rank (wp1).
 The tangent space to the regression manifold must span a 
\begin_inset Formula $K$
\end_inset

 -dimensional space if we are to consistently estimate a 
\begin_inset Formula $K$
\end_inset

 -dimensional parameter vector.
 This is analogous to the requirement that there be no perfect colinearity in a linear model.
 This is a necessary condition for identification.
 Note that the LLN implies that the above expectation is equal to 
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\theta_{0})=2\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n}
\]

\end_inset


\end_layout

\begin_layout Subsection
Consistency
\end_layout

\begin_layout Standard
We simply assume that the conditions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

 hold,
 so the estimator is consistent.
 Given that the strong stochastic equicontinuity conditions hold,
 as discussed above,
 and given the above identification conditions an a compact estimation space (the closure of the parameter space 
\begin_inset Formula $\Theta),$
\end_inset

 the consistency proof's assumptions are satisfied.
\end_layout

\begin_layout Subsection
Asymptotic normality
\end_layout

\begin_layout Standard
As in the case of GMM,
 we also simply assume that the conditions for asymptotic normality as in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"
nolink "false"

\end_inset

 hold.
 The only remaining problem is to determine the form of the asymptotic variance-covariance matrix.
 Recall that the result of the asymptotic normality theorem is 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right],
\]

\end_inset

 where 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 is the almost sure limit of 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{n}(\theta)$
\end_inset

 evaluated at 
\begin_inset Formula $\theta_{0},$
\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta_{0})=\lim Var\sqrt{n}D_{\theta}s_{n}(\theta_{0})
\]

\end_inset

 The objective function is 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}\left[y_{t}-f(\mathbf{x}_{t},\theta)\right]^{2}
\]

\end_inset

 So 
\begin_inset Formula 
\[
D_{\theta}s_{n}(\theta)=-\frac{2}{n}\sum_{t=1}^{n}\left[y_{t}-f(\mathbf{x}_{t},\theta)\right]D_{\theta}f(\mathbf{x}_{t},\theta).
\]

\end_inset

 Evaluating at 
\begin_inset Formula $\theta_{0},$
\end_inset


\begin_inset Formula 
\[
D_{\theta}s_{n}(\theta_{0})=-\frac{2}{n}\sum_{t=1}^{n}\varepsilon_{t}D_{\theta}f(\mathbf{x}_{t},\theta_{0}).
\]

\end_inset

 Note that the expectation of this is zero,
 since 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

 are assumed to be uncorrelated.
 So to calculate the variance,
 we can simply calculate the second moment about zero.
 Also note that 
\begin_inset Formula 
\begin{eqnarray*}
\sum_{t=1}^{n}\varepsilon_{t}D_{\theta}f(\mathbf{x}_{t},\theta_{0}) & = & \frac{\partial}{\partial\theta}\left[\mathbf{f}(\theta_{0})\right]^{\prime}\varepsilon\\
 & = & \mathbf{F}^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

With this we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta_{0}) & = & \lim Var\sqrt{n}D_{\theta}s_{n}(\theta_{0})\\
 & = & \lim n\mathcal{E}\frac{4}{n^{2}}\mathbf{F}^{\prime}\varepsilon\varepsilon^{\textrm{'}}\mathbf{F}\\
 & = & 4\sigma^{2}\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n}
\end{eqnarray*}

\end_inset

We've already seen that 
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\theta_{0})=2\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n},
\]

\end_inset

 where the expectation is with respect to the joint density of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon.$
\end_inset

 Combining these expressions for 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0}),$
\end_inset

 and the result of the asymptotic normality theorem,
 we get 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left(0,\left(\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n}\right)^{-1}\sigma^{2}\right).
\]

\end_inset

 We can consistently estimate the variance covariance matrix using 
\begin_inset Formula 
\begin{equation}
\left(\frac{\hat{\mathbf{F}}^{\prime}\hat{\mathbf{F}}}{n}\right)^{-1}\hat{\sigma}^{2},\label{nlsvcov}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 is defined as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlsderiv"
nolink "false"

\end_inset

 and 
\begin_inset Formula 
\[
\hat{\sigma}^{2}=\frac{\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]^{\prime}\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]}{n},
\]

\end_inset

 the obvious estimator.
 Note the close correspondence to the results for the linear model.
\end_layout

\begin_layout Subsection
Example:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

The Poisson model for count data
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $y_{t}$
\end_inset

 conditional on 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

 is independently distributed Poisson.
 A Poisson random variable is a 
\emph on
count data
\emph default
 variable,
 which means it can take the values {0,1,2,...}.
 This sort of model has been used to study visits to doctors per year,
 number of patents registered by businesses per year,
 
\emph on
etc.
\end_layout

\begin_layout Standard
The Poisson density is 
\begin_inset Formula 
\[
f(y_{t})=\frac{\exp(-\lambda_{t})\lambda_{t}^{y_{t}}}{y_{t}!},y_{t}\in\{0,1,2,...\}.
\]

\end_inset

 The mean of 
\begin_inset Formula $y_{t}$
\end_inset

 is 
\begin_inset Formula $\lambda_{t},$
\end_inset

 as is the variance.
 Note that 
\begin_inset Formula $\lambda_{t}$
\end_inset

 must be positive.
 Suppose that the true mean is 
\begin_inset Formula 
\[
\lambda_{t}^{0}=\exp(\mathbf{x}_{t}^{\prime}\beta_{0}),
\]

\end_inset

 which enforces the positivity of 
\begin_inset Formula $\lambda_{t}.$
\end_inset

 Suppose we estimate 
\begin_inset Formula $\beta_{0}$
\end_inset

 by nonlinear least squares:
 
\begin_inset Formula 
\[
\hat{\beta}=\arg\min s_{n}(\beta)=\frac{1}{T}\sum_{t=1}^{n}\left(y_{t}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
We can write 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\beta) & = & \frac{1}{T}\sum_{t=1}^{n}\left(\exp(\mathbf{x}_{t}^{\prime}\beta_{0}+\varepsilon_{t}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)^{2}\\
 & = & \frac{1}{T}\sum_{t=1}^{n}\left(\exp(\mathbf{x}_{t}^{\prime}\beta_{0}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)^{2}+\frac{1}{T}\sum_{t=1}^{n}\varepsilon_{t}^{2}+2\frac{1}{T}\sum_{t=1}^{n}\varepsilon_{t}\left(\exp(\mathbf{x}_{t}^{\prime}\beta_{0}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)
\end{eqnarray*}

\end_inset

 The last term has expectation zero since the assumption that 
\begin_inset Formula $\mathcal{E}(y_{t}|\mathbf{x}_{t})=\exp(\mathbf{x}_{t}^{\prime}\beta_{0})$
\end_inset

 implies that 
\begin_inset Formula $\mathcal{E}\left(\varepsilon_{t}|\mathbf{x}_{t}\right)=0,$
\end_inset

 which in turn implies that functions of 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

 are uncorrelated with 
\begin_inset Formula $\varepsilon_{t}.$
\end_inset

 Applying a strong LLN,
 and noting that the objective function is continuous on a compact parameter space,
 we get 
\begin_inset Formula 
\[
s_{\infty}(\beta)=\mathcal{E}_{\mathbf{x}}\left(\exp(\mathbf{x}^{\prime}\beta_{0}-\exp(\mathbf{x}^{\prime}\beta)\right)^{2}+\mathcal{E}_{\mathbf{x}}\exp(\mathbf{x}^{\prime}\beta_{0})
\]

\end_inset

 where the last term comes from the fact that the conditional variance of 
\begin_inset Formula $\varepsilon$
\end_inset

 is the same as the variance of 
\begin_inset Formula $y.$
\end_inset

 This function is clearly minimized at 
\begin_inset Formula $\beta=\beta_{0},$
\end_inset

 so the NLS estimator is consistent as long as identification holds.
\end_layout

\begin_layout Exercise
Determine the limiting distribution of 
\begin_inset Formula $\sqrt{n}\left(\hat{\beta}-\beta_{0}\right).$
\end_inset

 This means finding the the specific forms of
\begin_inset Formula $\frac{\partial^{2}}{\partial\beta\partial\beta^{\prime}}s_{n}(\beta)$
\end_inset

,
 
\begin_inset Formula $\mathcal{J}(\beta_{0}),\left.\frac{\partial s_{n}(\beta)}{\partial\beta}\right|,$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}(\beta_{0}).$
\end_inset

 Again,
 use a CLT as needed,
 no need to verify that it can be applied.
\end_layout

\begin_layout Subsection
The Gauss-Newton algorithm
\end_layout

\begin_layout Standard

\series bold
Readings:
\series default

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

Davidson and MacKinnon,
 Chapter 6,
 pgs.
 201-207
\begin_inset Formula $^{*}$
\end_inset

.
\end_layout

\begin_layout Standard
The Gauss-Newton optimization technique is specifically designed for nonlinear least squares.
 The idea is to linearize the nonlinear model,
 rather than the objective function.
 The model is 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta_{0})+\varepsilon.
\]

\end_inset

 At some 
\begin_inset Formula $\theta$
\end_inset

 in the parameter space,
 not equal to 
\begin_inset Formula $\theta_{0},$
\end_inset

 we have 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta)+\nu
\]

\end_inset

 where 
\begin_inset Formula $\nu$
\end_inset

 is a combination of the fundamental error term 
\begin_inset Formula $\varepsilon$
\end_inset

 and the error due to evaluating the regression function at 
\begin_inset Formula $\theta$
\end_inset

 rather than the true value 
\begin_inset Formula $\theta_{0}.$
\end_inset

 Take a first order Taylor's series approximation around a point 
\begin_inset Formula $\theta^{1}:$
\end_inset


\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta^{1})+\left[D_{\theta^{\prime}}\mathbf{f}\left(\theta^{1}\right)\right]\left(\theta-\theta^{1}\right)+\nu+\text{approximation error.}
\]

\end_inset

 Define 
\begin_inset Formula $\mathbf{z}\equiv\mathbf{y}-\mathbf{f}(\theta^{1})$
\end_inset

 and 
\begin_inset Formula $b\equiv(\theta-\theta^{1}).$
\end_inset

 Then the last equation can be written as 
\begin_inset Formula 
\[
\mathbf{z}=\mathbf{F}(\theta^{1})b+\omega\text{,}
\]

\end_inset

 where,
 as above,
 
\begin_inset Formula $\mathbf{F}(\theta^{1})\equiv D_{\theta^{\prime}}\mathbf{f}(\theta^{1})$
\end_inset

 is the 
\begin_inset Formula $n\times K$
\end_inset

 matrix of derivatives of the regression function,
 evaluated at 
\begin_inset Formula $\theta^{1},$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

 is 
\begin_inset Formula $\nu$
\end_inset

 plus approximation error from the truncated Taylor's series.
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\mathbf{F}$
\end_inset

 is known,
 given 
\begin_inset Formula $\theta^{1}.$
\end_inset


\end_layout

\begin_layout Itemize
Note that one could estimate 
\begin_inset Formula $b$
\end_inset

 simply by performing OLS on the above equation.
\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $\hat{b},$
\end_inset

 we calculate a new round estimate of 
\begin_inset Formula $\theta_{0}$
\end_inset

 as 
\begin_inset Formula $\theta^{2}=\hat{b}+\theta^{1}.$
\end_inset

 With this,
 take a new Taylor's series expansion around 
\begin_inset Formula $\theta^{2}$
\end_inset

 and repeat the process.
 Stop when 
\begin_inset Formula $\hat{b}=0$
\end_inset

 (to within a specified tolerance).
 
\end_layout

\begin_layout Standard
To see why this might work,
 consider the above approximation,
 but evaluated at the NLS estimator:
 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\hat{\theta})+\mathbf{F}(\hat{\theta})\left(\theta-\hat{\theta}\right)+\omega
\]

\end_inset

 The OLS estimate of 
\begin_inset Formula $b\equiv\theta-\hat{\theta}$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{b}=\left(\hat{\mathbf{F}}^{\prime}\hat{\mathbf{F}}\right)^{-1}\hat{\mathbf{F}}^{\prime}\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right].
\]

\end_inset

 This must be zero,
 since 
\begin_inset Formula 
\[
\hat{\mathbf{F}}^{\prime}\left(\hat{\theta}\right)\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]\equiv0
\]

\end_inset

 by definition of the NLS estimator (these are the normal equations as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlsfoc"
nolink "false"

\end_inset

,
 Since 
\begin_inset Formula $\hat{b}$
\end_inset

 
\begin_inset Formula $\equiv0$
\end_inset

 when we evaluate at 
\begin_inset Formula $\hat{\theta},$
\end_inset

 updating would stop.
\end_layout

\begin_layout Itemize
The Gauss-Newton method doesn't require second derivatives,
 as does the Newton-Raphson method,
 so it's faster.
\end_layout

\begin_layout Itemize
The varcov estimator,
 as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlsvcov"
nolink "false"

\end_inset

 is simple to calculate,
 since we have 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 as a by-product of the estimation process (
\emph on
i.e.,

\emph default
 it's just the last round 
\begin_inset Quotes eld
\end_inset

regressor matrix
\begin_inset Quotes erd
\end_inset

).
 In fact,
 a normal OLS program will give the NLS varcov estimator directly,
 since it's just the OLS varcov estimator from the last iteration.
\end_layout

\begin_layout Itemize
The method can suffer from convergence problems since 
\begin_inset Formula $\mathbf{F}(\theta)^{\prime}\mathbf{F}(\theta),$
\end_inset

 may be very nearly singular,
 even with an asymptotically identified model,
 especially if 
\begin_inset Formula $\theta$
\end_inset

 is very far from 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 Consider the example 
\begin_inset Formula 
\[
y=\beta_{1}+\beta_{2}x_{t}\beta^{3}+\varepsilon_{t}
\]

\end_inset

 When evaluated at 
\begin_inset Formula $\beta_{2}\approx0,$
\end_inset

 
\begin_inset Formula $\beta_{3}$
\end_inset

 has virtually no effect on the NLS objective function,
 so 
\begin_inset Formula $\mathbf{F}$
\end_inset

 will have rank that is 
\begin_inset Quotes eld
\end_inset

essentially
\begin_inset Quotes erd
\end_inset

 2,
 rather than 3.
 In this case,
 
\begin_inset Formula $\mathbf{F}^{\prime}\mathbf{F}$
\end_inset

 will be nearly singular,
 so 
\begin_inset Formula $(\mathbf{F}^{\prime}\mathbf{F})^{-1}$
\end_inset

 will be subject to large roundoff errors.
 
\end_layout

\begin_layout Subsection
Application:
 Limited dependent variables and sample selection
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
:
 Davidson and MacKinnon,
 Ch.
 15
\begin_inset Formula $^{*}$
\end_inset

 (a quick reading is sufficient),
 J.
 Heckman,
 
\begin_inset Quotes eld
\end_inset

Sample Selection Bias as a Specification Error
\begin_inset Quotes erd
\end_inset

,
 
\shape italic
Econometrica
\shape default
,
 1979 (This is a classic article,
 not required for reading,
 and which is a bit out-dated.
 Nevertheless it's a good place to start if you encounter sample selection problems in your research).
\end_layout

\begin_layout Standard
Sample selection is a common problem in applied research.
 The problem occurs when observations used in estimation are sampled non-randomly,
 according to some selection scheme.
\end_layout

\begin_layout Subsection
Example:
 Labor Supply
\end_layout

\begin_layout Standard
Labor supply of a person is a positive number of hours per unit time supposing the offer wage is higher than the reservation wage,
 which is the wage at which the person prefers not to work.
 The model (very simple,
 with 
\begin_inset Formula $t$
\end_inset

 subscripts suppressed):
\end_layout

\begin_layout Itemize
Characteristics of individual:
 
\begin_inset Formula $\mathbf{x}$
\end_inset


\end_layout

\begin_layout Itemize
Latent labor supply:
 
\begin_inset Formula $s^{*}=\mathbf{x}^{\prime}\beta+\omega$
\end_inset


\end_layout

\begin_layout Itemize
Offer wage:
 
\begin_inset Formula $w^{o}=\mathbf{z}^{\prime}\gamma+\nu$
\end_inset


\end_layout

\begin_layout Itemize
Reservation wage:
 
\begin_inset Formula $w^{r}=\mathbf{q}^{\prime}\delta+\eta$
\end_inset


\end_layout

\begin_layout Standard
Write the wage differential as 
\begin_inset Formula 
\begin{eqnarray*}
w^{*} & = & \left(\mathbf{z}^{\prime}\gamma+\nu\right)-\left(\mathbf{q}^{\prime}\delta+\eta\right)\\
 & \equiv & \mathbf{r}^{\prime}\theta+\varepsilon
\end{eqnarray*}

\end_inset

 We have the set of equations 
\begin_inset Formula 
\begin{eqnarray*}
s^{*} & = & \mathbf{x}^{\prime}\beta+\omega\\
w^{*} & = & \mathbf{r}^{\prime}\theta+\varepsilon.
\end{eqnarray*}

\end_inset

 Assume that 
\begin_inset Formula 
\[
\left[\begin{array}{c}
\omega\\
\varepsilon
\end{array}\right]\sim N\left(\left[\begin{array}{c}
0\\
0
\end{array}\right],\left[\begin{array}{cc}
\sigma^{2} & \rho\sigma\\
\rho\sigma & 1
\end{array}\right]\right).
\]

\end_inset

 We assume that the offer wage and the reservation wage,
 as well as the latent variable 
\begin_inset Formula $s^{*}$
\end_inset

 are unobservable.
 What is observed is 
\begin_inset Formula 
\begin{eqnarray*}
w & = & 1\left[w^{*}>0\right]\\
s & = & ws^{*}.
\end{eqnarray*}

\end_inset

 In other words,
 we observe whether or not a person is working.
 If the person is working,
 we observe labor supply,
 which is equal to latent labor supply,
 
\begin_inset Formula $s^{*}.$
\end_inset

 Otherwise,
 
\begin_inset Formula $s=0\neq s^{*}.$
\end_inset

 Note that we are using a simplifying assumption that individuals can freely choose their weekly hours of work.
\end_layout

\begin_layout Standard
Suppose we estimated the model 
\begin_inset Formula 
\[
s^{*}=\mathbf{x}^{\prime}\beta+\text{residual}
\]

\end_inset

 using only observations for which 
\begin_inset Formula $s>0.$
\end_inset

 The problem is that these observations are those for which 
\begin_inset Formula $w^{*}>0,$
\end_inset

 or equivalently,
 
\begin_inset Formula $-\varepsilon<\mathbf{r}^{\prime}\theta$
\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{E}\left[\omega|-\varepsilon<\mathbf{r}^{\prime}\theta\right]\neq0,
\]

\end_inset

 since 
\begin_inset Formula $\varepsilon$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

 are dependent.
 Furthermore,
 this expectation will in general depend on 
\begin_inset Formula $\mathbf{x}$
\end_inset

 since elements of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can enter in 
\begin_inset Formula $\mathbf{r}.$
\end_inset

 Because of these two facts,
 least squares estimation is biased and inconsistent.
\end_layout

\begin_layout Standard
Consider more carefully 
\begin_inset Formula $\mathcal{E}\left[\omega|-\varepsilon<\mathbf{r}^{\prime}\theta\right].$
\end_inset

 Given the joint normality of 
\begin_inset Formula $\omega$
\end_inset

 and 
\begin_inset Formula $\varepsilon,$
\end_inset

 we can write (see for example Spanos 
\shape italic
Statistical Foundations of Econometric Modelling,

\shape default
 pg.
 122) 
\begin_inset Formula 
\[
\omega=\rho\sigma\varepsilon+\eta,
\]

\end_inset

 where 
\begin_inset Formula $\eta$
\end_inset

 has mean zero and is independent of 
\begin_inset Formula $\varepsilon$
\end_inset

.
 With this we can write 
\begin_inset Formula 
\[
s^{*}=\mathbf{x}^{\prime}\beta+\rho\sigma\varepsilon+\eta.
\]

\end_inset

 If we condition this equation on 
\begin_inset Formula $-\varepsilon<\mathbf{r}^{\prime}\theta$
\end_inset

 we get 
\begin_inset Formula 
\[
s=\mathbf{x}^{\prime}\beta+\rho\sigma\mathcal{E}(\varepsilon|-\varepsilon<\mathbf{r}^{\prime}\theta)+\eta
\]

\end_inset

which may be written as 
\begin_inset Formula 
\[
s=\mathbf{x}^{\prime}\beta+\rho\sigma\mathcal{E}(\varepsilon|\varepsilon>-\mathbf{r}^{\prime}\theta)+\eta
\]

\end_inset


\end_layout

\begin_layout Itemize
A useful result is that for 
\begin_inset Formula 
\[
z\sim N(0,1)
\]

\end_inset

 
\begin_inset Formula 
\[
E(z|z>z^{*})=\frac{\phi(z^{*})}{\Phi(-z^{*})},
\]

\end_inset

 where 
\begin_inset Formula $\phi\left(\cdot\right)$
\end_inset

 and 
\begin_inset Formula $\Phi\left(\cdot\right)$
\end_inset

 are the standard normal density and distribution function,
 respectively.
 The quantity on the RHS above is known as the 
\emph on
inverse Mill's ratio:

\emph default
 
\begin_inset Formula 
\[
IMR(\mathbf{z}^{*})=\frac{\phi(z^{*})}{\Phi(-z^{*})}
\]

\end_inset

 With this we can write (making use of the fact that the standard normal density is symmetric about zero,
 so that 
\begin_inset Formula $\phi(-a)=\phi(a)$
\end_inset

):
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
s & = & \mathbf{x}^{\prime}\beta+\rho\sigma\frac{\phi\left(\mathbf{r}^{\prime}\theta\right)}{\Phi\left(\mathbf{r}^{\prime}\theta\right)}+\eta\label{aa}\\
 & \equiv & \left[\begin{array}{cc}
\mathbf{x}^{\prime} & \frac{\phi\left(\mathbf{r}^{\prime}\theta\right)}{\Phi\left(\mathbf{r}^{\prime}\theta\right)}\end{array}\right]\left[\begin{array}{c}
\beta\\
\zeta
\end{array}\right]+\eta.\label{bb}
\end{eqnarray}

\end_inset

 where 
\begin_inset Formula $\zeta=\rho\sigma$
\end_inset

.
 The error term 
\begin_inset Formula $\eta$
\end_inset

 has conditional mean zero,
 and is uncorrelated with the regressors 
\begin_inset Formula $\begin{array}{cc}
\mathbf{x}^{\prime} & \frac{\phi\left(\mathbf{r}^{\prime}\theta\right)}{\Phi\left(\mathbf{r}^{\prime}\theta\right)}\end{array}.$
\end_inset

 At this point,
 we can estimate the equation by NLS.
 
\end_layout

\begin_layout Itemize
Heckman showed how one can estimate this in a two step procedure where first 
\begin_inset Formula $\theta$
\end_inset

 is estimated,
 then equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "bb"
nolink "false"

\end_inset

 is estimated by least squares using the estimated value of 
\begin_inset Formula $\theta$
\end_inset

 to form the regressors.
 This is inefficient and estimation of the covariance is a tricky issue.
 It is probably easier (and more efficient) just to do MLE.
\end_layout

\begin_layout Itemize
The model presented above depends strongly on joint normality.
 There exist many alternative models which weaken the maintained assumptions.
 It is possible to estimate consistently without distributional assumptions.
 See Ahn and Powell,
 
\shape italic
Journal of Econometrics
\shape default
,
 1994.
 
\end_layout

\begin_layout Section
The Fourier functional form
\end_layout

\begin_layout Standard
This material was removed from the chapter on nonparametric regression,
 to make that chapter easier to read,
 and to focus on the main ideas.
 
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
:
 Gallant,
 1987,
 
\begin_inset Quotes eld
\end_inset

Identification and consistency in semi-nonparametric regression,
\begin_inset Quotes erd
\end_inset

 in 
\emph on
Advances in Econometrics,
 Fifth World Congress
\shape italic
\emph default
,

\shape default
 V.
 1,
 Truman Bewley,
 ed.,
 Cambridge.
\end_layout

\begin_layout Standard
Suppose we have a multivariate model 
\begin_inset Formula 
\[
y=f(\mathbf{x})+\varepsilon,
\]

\end_inset

 where 
\begin_inset Formula $f(x)$
\end_inset

 is of unknown form and 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $P-$
\end_inset

dimensional vector.
 For simplicity,
 assume that 
\begin_inset Formula $\varepsilon$
\end_inset

 is a classical error.
 Let us take the estimation of the vector of elasticities with typical element 
\begin_inset Formula 
\[
\xi_{x_{i}}=\frac{\mathbf{x}_{i}}{f(\mathbf{x})}\frac{\partial f(\mathbf{x})}{\partial x_{i}f(x)},
\]

\end_inset

 at an arbitrary point 
\begin_inset Formula $\mathbf{x}_{i}.$
\end_inset


\end_layout

\begin_layout Standard
The Fourier form,
 following Gallant (1982),
 but with a somewhat different parameterization,
 may be written as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g_{K}(\mathbf{x}\mid\theta_{K})=\alpha+\mathbf{x}^{\prime}\beta+1/2\mathbf{x}^{\prime}\mathbf{Cx}+\sum_{\alpha=1}^{A}\sum_{j=1}^{J}\left(u_{j\alpha}\cos(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})-v_{j\alpha}\sin(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})\right).\label{FourierForm}
\end{equation}

\end_inset

 where the 
\begin_inset Formula $K$
\end_inset

-dimensional parameter vector 
\begin_inset Formula 
\begin{equation}
\theta_{K}=\{\alpha,\beta^{\prime},vec^{*}(C)^{\prime},u_{11},v_{11},\ldots,u_{JA},v_{JA}\}^{\prime}.\label{thetak}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
We assume that the conditioning variables 
\begin_inset Formula $\mathbf{x}$
\end_inset

 have each been transformed to lie in an interval that is shorter than 
\begin_inset Formula $2\pi.$
\end_inset

 This is required to avoid periodic behavior of the approximation,
 which is desirable since economic functions aren't periodic.
 For example,
 subtract sample means,
 divide by the maxima of the conditioning variables,
 and multiply by 
\begin_inset Formula $2\pi-eps,$
\end_inset

 where 
\begin_inset Formula $eps$
\end_inset

 is some positive number less than 
\begin_inset Formula $2\pi$
\end_inset

 in value.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $k_{\alpha}$
\end_inset

 are 
\begin_inset Quotes erd
\end_inset

elementary multi-indices
\begin_inset Quotes erd
\end_inset

 which are simply 
\begin_inset Formula $P-$
\end_inset

 vectors formed of integers (negative,
 positive and zero).
 The 
\begin_inset Formula $k_{\alpha}$
\end_inset

,
 
\begin_inset Formula $\alpha=1,2,...,A$
\end_inset

 are required to be linearly independent,
 and we follow the convention that the first non-zero element be positive.
 For example 
\begin_inset Formula 
\[
\left[\begin{array}{ccccc}
0 & 1 & -1 & 0 & 1\end{array}\right]^{\prime}
\]

\end_inset

 is a potential multi-index to be used,
 but 
\begin_inset Formula 
\[
\left[\begin{array}{ccccc}
0 & -1 & -1 & 0 & 1\end{array}\right]^{\prime}
\]

\end_inset

 is not since its first nonzero element is negative.
 Nor is 
\begin_inset Formula 
\[
\left[\begin{array}{ccccc}
0 & 2 & -2 & 0 & 2\end{array}\right]^{\prime}
\]

\end_inset

 a multi-index we would use,
 since it is a scalar multiple of the original multi-index.
\end_layout

\begin_layout Itemize
We parameterize the matrix 
\begin_inset Formula $C$
\end_inset

 differently than does Gallant because it simplifies things in practice.
 The cost of this is that we are no longer able to test a quadratic specification using nested testing.
 
\end_layout

\begin_layout Standard
The vector of first partial derivatives is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{x}g_{K}(\mathbf{x}\mid\theta_{K})=\beta+\mathbf{Cx}+\sum_{\alpha=1}^{A}\sum_{j=1}^{J}\left[\left(-u_{j\alpha}\sin(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})-v_{j\alpha}\cos(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})\right)j\mathbf{k}_{\alpha}\right]\label{firstderivative}
\end{equation}

\end_inset

and the matrix of second partial derivatives is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{x}^{2}g_{K}(\mathbf{x}|\theta_{K})=\mathbf{C}+\sum_{\alpha=1}^{A}\sum_{j=1}^{J}\left[\left(-u_{j\alpha}\cos(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})+v_{j\alpha}\sin(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})\right)j^{2}\mathbf{k}_{\alpha}\mathbf{k}_{\alpha}^{\prime}\right]\label{secondderivative}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
To define a compact notation for partial derivatives,
 let 
\begin_inset Formula $\lambda$
\end_inset

 be an 
\begin_inset Formula $N$
\end_inset

-dimensional multi-index with no negative elements.
 Define 
\begin_inset Formula $\mid\lambda\mid^{*}$
\end_inset

 as the sum of the elements of 
\begin_inset Formula $\lambda$
\end_inset

.
 If we have 
\begin_inset Formula $N$
\end_inset

 arguments 
\begin_inset Formula $\mathbf{x}$
\end_inset

 of the (arbitrary) function 
\begin_inset Formula $h(\mathbf{x})$
\end_inset

,
 use 
\begin_inset Formula $D^{\lambda}h(\mathbf{x})$
\end_inset

 to indicate a certain partial derivative:
 
\begin_inset Formula 
\[
D^{\lambda}h(\mathbf{x})\equiv\frac{\partial^{\mid\lambda\mid^{*}}}{\partial x_{1}^{\lambda_{1}}\partial x_{2}^{\lambda_{2}}\cdots\partial x_{N}^{\lambda_{N}}}h(\mathbf{x})
\]

\end_inset

 When 
\begin_inset Formula $\lambda$
\end_inset

 is the zero vector,
 
\begin_inset Formula $D^{\lambda}h(\mathbf{x})\equiv h(\mathbf{x})$
\end_inset

.
 Taking this definition and the last few equations into account,
 we see that it is possible to define 
\begin_inset Formula $\left(1\times K\right)$
\end_inset

 vector 
\begin_inset Formula $Z^{\lambda}(\mathbf{x})$
\end_inset

 so that 
\begin_inset Formula 
\begin{equation}
D^{\lambda}g_{K}(\mathbf{x}|\theta_{K})=\mathbf{z}^{\lambda}(\mathbf{x})^{\prime}\theta_{K}.\label{Znotation}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
Both the approximating model and the derivatives of the approximating model are linear in the parameters.
 
\end_layout

\begin_layout Itemize
For the approximating model to the function (not derivatives),
 write 
\begin_inset Formula $g_{K}(\mathbf{x}|\theta_{K})=\mathbf{z}^{\prime}\theta_{K}$
\end_inset

 for simplicity.
\end_layout

\begin_layout Standard
The following theorem can be used to prove the consistency of the Fourier form.
\end_layout

\begin_layout Theorem
[Gallant and Nychka,
 1987] Suppose that 
\begin_inset Formula $\hat{h}_{n}$
\end_inset

 is obtained by maximizing a sample objective function 
\begin_inset Formula $s_{n}(h)$
\end_inset

 over 
\begin_inset Formula $\mathcal{H}_{K_{n}}$
\end_inset

 where 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is a subset of some function space 
\begin_inset Formula $\mathcal{H}$
\end_inset

 on which is defined a norm 
\begin_inset Formula $\parallel h\parallel$
\end_inset

.
 Consider the following conditions:
\end_layout

\begin_layout Theorem
(a) Compactness:
 The closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 is compact in the relative topology defined by 
\begin_inset Formula $\parallel h\parallel$
\end_inset

.
\end_layout

\begin_layout Theorem
(b) Denseness:
 
\begin_inset Formula $\cup_{K}\mathcal{H}_{K}$
\end_inset

,
 
\begin_inset Formula $K=1,2,3,...$
\end_inset

 is a dense subset of the closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 and 
\begin_inset Formula $\mathcal{H}_{K}\subset\mathcal{H}_{K+1}$
\end_inset

.
\end_layout

\begin_layout Theorem
(c) Uniform convergence:
 There is a point 
\begin_inset Formula $h^{*}$
\end_inset

 in 
\begin_inset Formula $\mathcal{H}$
\end_inset

 and there is a function 
\begin_inset Formula $s_{\infty}(h,h^{*})$
\end_inset

 that is continuous in 
\begin_inset Formula $h$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{\overline{\mathcal{H}}}\mid s_{n}(h)-s_{\infty}(h,h^{*})\mid=0
\]

\end_inset

 almost surely.
\end_layout

\begin_layout Theorem
(d) Identification:
 Any point 
\begin_inset Formula $h$
\end_inset

 in the closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 with 
\begin_inset Formula $s_{\infty}(h,h^{*})\geq s_{\infty}(h^{*},h^{*})$
\end_inset

 must have 
\begin_inset Formula $\parallel h-h^{*}\parallel=0$
\end_inset

.
\end_layout

\begin_layout Theorem
Under these conditions 
\begin_inset Formula $\lim_{n\rightarrow\infty}\parallel h^{*}-\hat{h}_{n}\parallel=0$
\end_inset

 almost surely,
 provided that 
\begin_inset Formula $\lim_{n\rightarrow\infty}K_{n}=\infty$
\end_inset

 almost surely.
 
\end_layout

\begin_layout Standard
The modification of the original statement of the theorem that has been made is to set the parameter space 
\begin_inset Formula $\Theta$
\end_inset

 in Gallant and Nychka's (1987) Theorem 0 to a single point and to state the theorem in terms of maximization rather than minimization.
\end_layout

\begin_layout Standard
This theorem is very similar in form to Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

.
 The main differences are:
\end_layout

\begin_layout Enumerate
A generic norm 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 is used in place of the Euclidean norm.
 This norm may be stronger than the Euclidean norm,
 so that convergence with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 implies convergence w.r.t the Euclidean norm.
 Typically we will want to make sure that the norm is strong enough to imply convergence of all functions of interest.
\end_layout

\begin_layout Enumerate
The 
\begin_inset Quotes eld
\end_inset

estimation space
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mathcal{H}$
\end_inset

 is a function space.
 It plays the role of the parameter space 
\begin_inset Formula $\Theta$
\end_inset

 in our discussion of parametric estimators.
 There is no restriction to a parametric family,
 only a restriction to a space of functions that satisfy certain conditions.
 This formulation is much less restrictive than the restriction to a parametric family.
\end_layout

\begin_layout Enumerate
There is a denseness assumption that was not present in the other theorem.
 
\end_layout

\begin_layout Standard
We will not prove this theorem (the proof is quite similar to the proof of theorem [
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"
nolink "false"

\end_inset

],
 see Gallant,
 1987) but we will discuss its assumptions,
 in relation to the Fourier form as the approximating model.
\end_layout

\begin_layout Paragraph
Sobolev norm
\end_layout

\begin_layout Standard
Since all of the assumptions involve the norm 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 ,
 we need to make explicit what norm we wish to use.
 We need a norm that guarantees that the errors in approximation of the functions we are interested in are accounted for.
 Since we are interested in first-order elasticities in the present case,
 we need close approximation of both the function 
\begin_inset Formula $f(x)$
\end_inset

 and its first derivative 
\begin_inset Formula $f^{\prime}(x),$
\end_inset

 throughout the range of 
\begin_inset Formula $x.$
\end_inset

 Let 
\begin_inset Formula $\mathcal{X}$
\end_inset

 be an open set that contains all values of 
\begin_inset Formula $x$
\end_inset

 that we're interested in.
 The Sobolev norm is appropriate in this case.
 It is defined,
 making use of our notation for partial derivatives,
 as:
 
\begin_inset Formula 
\[
\parallel h\parallel_{m,\mathcal{X}}=\max_{\left|\lambda^{*}\right|\leq m}\sup_{\mathcal{X}}\left|D^{\lambda}h(x)\right|
\]

\end_inset

 To see whether or not the function 
\begin_inset Formula $f(x)$
\end_inset

 is well approximated by an approximating model 
\begin_inset Formula $g_{K}(x\mid\theta_{K})$
\end_inset

,
 we would evaluate
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\parallel f(\mathbf{x})-g_{K}(\mathbf{x}\mid\theta_{K})\parallel_{m,\mathcal{X}}.
\]

\end_inset

 We see that this norm takes into account errors in approximating the function and partial derivatives up to order 
\begin_inset Formula $m.$
\end_inset

 If we want to estimate first order elasticities,
 as is the case in this example,
 the relevant 
\begin_inset Formula $m$
\end_inset

 would be 
\begin_inset Formula $m=1.$
\end_inset

 Furthermore,
 since we examine the 
\begin_inset Formula $\sup$
\end_inset

 over 
\begin_inset Formula $\mathcal{X},$
\end_inset

 convergence w.r.t.
 the Sobolev means 
\shape italic
uniform
\shape default
 convergence,
 so that we obtain consistent estimates for all values of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Paragraph
Compactness
\end_layout

\begin_layout Standard
Verifying compactness with respect to this norm is quite technical and unenlightening.
 It is proven by Elbadawi,
 Gallant and Souza,
 
\shape italic
Econometrica
\shape default
,
 1983.
 The basic requirement is that if we need consistency w.r.t.
 
\begin_inset Formula $\parallel h\parallel_{m,\mathcal{X}},$
\end_inset

 then the functions of interest must belong to a Sobolev space which takes into account derivatives of order 
\begin_inset Formula $m+1$
\end_inset

.
 A Sobolev space is the set of functions 
\begin_inset Formula 
\[
\mathcal{W}_{m,\mathcal{X}}(D)=\{h(\mathbf{x}):\parallel h(\mathbf{x})\parallel_{m,\mathcal{X}}<D\},
\]

\end_inset

 where 
\begin_inset Formula $D$
\end_inset

 is a finite constant.
 In plain words,
 the functions must have bounded partial derivatives of one order higher than the derivatives we seek to estimate.
\end_layout

\begin_layout Paragraph
The estimation space and the estimation subspace
\end_layout

\begin_layout Standard
Since in our case we're interested in consistent estimation of first-order elasticities,
 we'll define the estimation space as follows:
\end_layout

\begin_layout Definition
[Estimation space] The estimation space 
\begin_inset Formula $\mathcal{H}=\mathcal{W}_{2,\mathcal{X}}(D).$
\end_inset

 The estimation space is an open set,
 and we presume that 
\begin_inset Formula $h^{*}\in\mathcal{H}.$
\end_inset


\end_layout

\begin_layout Standard
So we are assuming that the function to be estimated has bounded second derivatives throughout 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
\end_layout

\begin_layout Standard
With seminonparametric estimators,
 we don't actually optimize over the estimation space.
 Rather,
 we optimize over a subspace,
 
\begin_inset Formula $\mathcal{H}_{K_{n}},$
\end_inset

 defined as:
\end_layout

\begin_layout Definition
[Estimation subspace] The estimation subspace 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is defined as 
\begin_inset Formula 
\[
\mathcal{H}_{K}=\{g_{K}(\mathbf{x}|\theta_{K}):g_{K}(\mathbf{x}|\theta_{K})\in\mathcal{W}_{2,\mathcal{Z}}(D),\theta_{K}\in\Re^{K}\},
\]

\end_inset

 where 
\begin_inset Formula $g_{K}(\mathbf{x},\theta_{K})$
\end_inset

 is the Fourier form approximation as defined in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "FourierForm"
nolink "false"

\end_inset

.
 
\end_layout

\begin_layout Paragraph
Denseness
\end_layout

\begin_layout Standard
The important point here is that 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is a space of functions that is indexed by a finite dimensional parameter (
\begin_inset Formula $\theta_{K}$
\end_inset

 has 
\begin_inset Formula $K$
\end_inset

 elements,
 as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "thetak"
nolink "false"

\end_inset

).
 With 
\begin_inset Formula $n$
\end_inset

 observations,
 
\begin_inset Formula $n>K,$
\end_inset

 this parameter is estimable.
 Note that the true function 
\begin_inset Formula $h^{*}$
\end_inset

 is not necessarily an element of 
\begin_inset Formula $\mathcal{H}_{K},$
\end_inset

 so optimization over 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 may not lead to a consistent estimator.
 In order for optimization over 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 to be equivalent to optimization over 
\begin_inset Formula $\mathcal{H},$
\end_inset

 at least asymptotically,
 we need that:
\end_layout

\begin_layout Enumerate
The dimension of the parameter vector,
 
\begin_inset Formula $\dim\theta_{K_{n}}\rightarrow\infty$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 This is achieved by making 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $J$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "FourierForm"
nolink "false"

\end_inset

 increasing functions of 
\begin_inset Formula $n,$
\end_inset

 the sample size.
 It is clear that 
\begin_inset Formula $K$
\end_inset

 will have to grow more slowly than 
\begin_inset Formula $n$
\end_inset

.
 The second requirement is:
\end_layout

\begin_layout Enumerate
We need that the 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 be dense subsets of 
\begin_inset Formula $\mathcal{H}.$
\end_inset


\end_layout

\begin_layout Standard
The estimation subspace 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

,
 defined above,
 is a subset of the closure of the estimation space,
 
\begin_inset Formula $\overline{\mathcal{H}}$
\end_inset

 .
 A set of subsets 
\begin_inset Formula $\mathcal{A}_{a}$
\end_inset

 of a set 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

dense
\begin_inset Quotes erd
\end_inset

 if the closure of the countable union of the subsets is equal to the closure of 
\begin_inset Formula $\mathcal{A}$
\end_inset

:
 
\begin_inset Formula 
\[
\overline{\cup_{a=1}^{\infty}\mathcal{A}_{a}}=\overline{\mathcal{A}}
\]

\end_inset

 
\emph on
Use a picture here.
 The rest of the discussion of denseness is provided just for completeness:
 there's no need to study it in detail
\emph default
.
 To show that 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is a dense subset of 
\begin_inset Formula $\overline{\mathcal{H}}$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}},$
\end_inset

 it is useful to apply Theorem 1 of Gallant (1982),
 who in turn cites Edmunds and Moscatelli (1977).
 We reproduce the theorem as presented by Gallant,
 with minor notational changes,
 for convenience of reference:
\end_layout

\begin_layout Theorem
[Edmunds and Moscatelli,
 1977] 
\begin_inset CommandInset label
LatexCommand label
name "EdMosctheorem"

\end_inset

Let the real-valued function 
\begin_inset Formula $h^{*}(\mathbf{x})$
\end_inset

 be continuously differentiable up to order 
\begin_inset Formula $m$
\end_inset

 on an open set containing the closure of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 Then it is possible to choose a triangular array of coefficients 
\begin_inset Formula $\theta_{1},\theta_{2},\ldots\theta_{K},\ldots,$
\end_inset

 such that for every 
\begin_inset Formula $q$
\end_inset

 with 
\begin_inset Formula $0\leq q<m$
\end_inset

,
 and every 
\begin_inset Formula $\varepsilon>0,$
\end_inset

 
\begin_inset Formula $\parallel h^{*}(\mathbf{x})-h_{K}(\mathbf{x}|\theta_{K})\parallel_{q,\mathcal{X}}=o(K^{-m+q+\varepsilon})$
\end_inset

 as 
\begin_inset Formula $K\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
In the present application,
 
\begin_inset Formula $q=1$
\end_inset

,
 and 
\begin_inset Formula $m=2$
\end_inset

.
 By definition of the estimation space,
 the elements of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 are once continuously differentiable on 
\begin_inset Formula $\mathcal{X}$
\end_inset

,
 which is open and contains the closure of 
\begin_inset Formula $\mathcal{X}$
\end_inset

,
 so the theorem is applicable.
 Closely following Gallant and Nychka (1987),
 
\begin_inset Formula $\cup_{\infty}\mathcal{H}_{K}$
\end_inset

 is the countable union of the 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

.
 The implication of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "EdMosctheorem"
nolink "false"

\end_inset

 is that there is a sequence of {
\begin_inset Formula $h_{K}$
\end_inset

} from 
\begin_inset Formula $\cup_{\infty}\mathcal{H}_{K}$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{K\rightarrow\infty}\parallel h^{*}-h_{K}\parallel_{1,\mathcal{X}}=0,
\]

\end_inset

 for all 
\begin_inset Formula $h^{*}\in\mathcal{H}$
\end_inset

.
 Therefore,
 
\begin_inset Formula 
\[
\mathcal{H}\subset\overline{\cup_{\infty}\mathcal{H}_{K}}.
\]

\end_inset

However,
 
\begin_inset Formula 
\[
\cup_{\infty}\mathcal{H}_{K}\subset\mathcal{H},
\]

\end_inset

 so 
\begin_inset Formula 
\[
\overline{\cup_{\infty}\mathcal{H}_{K}}\subset\overline{\mathcal{H}}.
\]

\end_inset

Therefore 
\begin_inset Formula 
\[
\overline{\mathcal{H}}=\overline{\cup_{\infty}\mathcal{H}_{K}},
\]

\end_inset

 so 
\begin_inset Formula $\cup_{\infty}\mathcal{H}_{K}$
\end_inset

 is a dense subset of 
\begin_inset Formula $\mathcal{H}$
\end_inset

,
 with respect to the norm 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Uniform convergence
\end_layout

\begin_layout Standard
We now turn to the limiting objective function.
 We estimate by OLS.
 The sample objective function stated in terms of maximization is 
\begin_inset Formula 
\[
s_{n}(\theta_{K})=-\frac{1}{n}\sum_{t=1}^{n}\left(y_{t}-g_{K}(\mathbf{x}_{t}\mid\theta_{K})\right)^{2}
\]

\end_inset

With random sampling,
 as in the case of Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "olslim"
nolink "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlslim"
nolink "false"

\end_inset

,
 the limiting objective function is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
s_{\infty}\left(g,f\right)=-\int_{\mathcal{X}}\left(f(\mathbf{x})-g(\mathbf{x})\right)^{2}d\mu x-\sigma_{\varepsilon}^{2}.\label{limobjfn}
\end{equation}

\end_inset

 where the true function 
\begin_inset Formula $f(x)$
\end_inset

 takes the place of the generic function 
\begin_inset Formula $h^{*}$
\end_inset

 in the presentation of the theorem.
 Both 
\begin_inset Formula $g(x)$
\end_inset

 and 
\begin_inset Formula $f(x)$
\end_inset

 are elements of 
\begin_inset Formula $\overline{\cup_{\infty}\mathcal{H}_{K}}$
\end_inset

.
\end_layout

\begin_layout Standard
The pointwise convergence of the objective function needs to be strengthened to uniform convergence.
 We will simply assume that this holds,
 since the way to verify this depends upon the specific application.
 We also have continuity of the objective function in 
\begin_inset Formula $g,$
\end_inset

 with respect to the norm 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}}$
\end_inset

 since 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \lim_{\parallel g^{1}-g^{0}\parallel_{1,\mathcal{X}}\rightarrow0}\left\{ s_{\infty}\left(g^{1},f)\right)-s_{\infty}\left(g^{0},f)\right)\right\} \\
 & = & \lim_{\parallel g^{1}-g^{0}\parallel_{1,\mathcal{X}}\rightarrow0}\int_{\mathcal{X}}\left[\left(g^{1}(\mathbf{x})-f(\mathbf{x})\right)^{2}-\left(g^{0}(\mathbf{x})-f(\mathbf{x})\right)^{2}\right]d\mu x.
\end{eqnarray*}

\end_inset

By the dominated convergence theorem (which applies since the finite bound 
\begin_inset Formula $D$
\end_inset

 used to define 
\begin_inset Formula $\mathcal{W}_{2,\mathcal{Z}}(D)$
\end_inset

 is dominated by an integrable function),
 the limit and the integral can be interchanged,
 so by inspection,
 the limit is zero.
\end_layout

\begin_layout Paragraph
Identification
\end_layout

\begin_layout Standard
The identification condition requires that for any point 
\begin_inset Formula $(g,f)$
\end_inset

 in 
\begin_inset Formula $\overline{\mathcal{H}}\times\overline{\mathcal{H}},$
\end_inset

 
\begin_inset Formula $s_{\infty}(g,f)\geq s_{\infty}(f,f)$
\end_inset

 
\begin_inset Formula $\Rightarrow$
\end_inset

 
\begin_inset Formula $\parallel g-f\parallel_{1,\mathcal{X}}=0$
\end_inset

.
 This condition is clearly satisfied given that 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $f$
\end_inset

 are once continuously differentiable (by the assumption that defines the estimation space).
\end_layout

\begin_layout Paragraph
Review of concepts
\end_layout

\begin_layout Standard
For the example of estimation of first-order elasticities,
 the relevant concepts are:
\end_layout

\begin_layout Itemize
Estimation space 
\begin_inset Formula $\mathcal{H}=\mathcal{W}_{2,\mathcal{X}}(D)$
\end_inset

:
 the function space in the closure of which the true function must lie.
\end_layout

\begin_layout Itemize
Consistency norm 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}}.$
\end_inset

 The closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 is compact with respect to this norm.
\end_layout

\begin_layout Itemize
Estimation subspace 
\begin_inset Formula $\mathcal{H}_{K}.$
\end_inset

 The estimation subspace is the subset of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 that is representable by a Fourier form with parameter 
\begin_inset Formula $\theta_{K}.$
\end_inset

 These are dense subsets of 
\begin_inset Formula $\mathcal{H}.$
\end_inset


\end_layout

\begin_layout Itemize
Sample objective function 
\begin_inset Formula $s_{n}(\theta_{K}),$
\end_inset

 the negative of the sum of squares.
 By standard arguments this converges uniformly to the
\end_layout

\begin_layout Itemize
Limiting objective function 
\begin_inset Formula $s_{\infty}($
\end_inset

 
\begin_inset Formula $g,f),$
\end_inset

 which is continuous in 
\begin_inset Formula $g$
\end_inset

 and has a global maximum in its first argument,
 over the closure of the infinite union of the estimation subpaces,
 at 
\begin_inset Formula $g=f.$
\end_inset


\end_layout

\begin_layout Itemize
As a result of this,
 first order elasticities 
\begin_inset Formula 
\[
\frac{\mathbf{x}_{i}}{f(\mathbf{x})}\frac{\partial f(\mathbf{x})}{\partial x_{i}f(x)}
\]

\end_inset

 are consistently estimated for all 
\begin_inset Formula $\mathbf{x}\in\mathcal{X}.$
\end_inset


\end_layout

\begin_layout Paragraph
Discussion
\end_layout

\begin_layout Standard
Consistency requires that the number of parameters used in the expansion increase with the sample size,
 tending to infinity.
 If parameters are added at a high rate,
 the bias tends relatively rapidly to zero.
 A basic problem is that a high rate of inclusion of additional parameters causes the variance to tend more slowly to zero.
 The issue of how to chose the rate at which parameters are added and which to add first is fairly complex.
 A problem is that the allowable rates for asymptotic normality to obtain (Andrews 1991;
 Gallant and Souza,
 1991) are very strict.
 Supposing we stick to these rates,
 our approximating model is:
 
\begin_inset Formula 
\[
g_{K}(\mathbf{x}|\theta_{K})=\mathbf{z}^{\prime}\theta_{K}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Define 
\begin_inset Formula $\mathbf{Z}_{K}$
\end_inset

 as the 
\begin_inset Formula $n\times K$
\end_inset

 matrix of regressors obtained by stacking observations.
 The LS estimator is 
\begin_inset Formula 
\[
\hat{\theta}_{K}=\left(\mathbf{Z}_{K}^{\prime}\mathbf{Z}_{K}\right)^{+}\mathbf{Z}_{K}^{\prime}y,
\]

\end_inset

 where 
\begin_inset Formula $\left(\cdot\right)^{+}$
\end_inset

 is the Moore-Penrose generalized inverse.
\end_layout

\begin_deeper
\begin_layout Itemize
This is used since 
\begin_inset Formula $\mathbf{Z}_{K}^{\prime}\mathbf{Z}_{K}$
\end_inset

 may be singular,
 as would be the case for 
\begin_inset Formula $K(n)$
\end_inset

 large enough when some indicator variables are included.
 
\end_layout

\end_deeper
\begin_layout Itemize
.
 The prediction,
 
\begin_inset Formula $\mathbf{z}^{\prime}\hat{\theta}_{K},$
\end_inset

 of the unknown function 
\begin_inset Formula $f(\mathbf{x})$
\end_inset

 is asymptotically normally distributed:
 
\begin_inset Formula 
\[
\sqrt{n}\left(\mathbf{z}^{\prime}\hat{\theta}_{K}-f(x)\right)\stackrel{d}{\rightarrow}N(0,AV),
\]

\end_inset

 where 
\begin_inset Formula 
\[
AV=\lim_{n\rightarrow\infty}E\left[\mathbf{z}^{\prime}\left(\frac{\mathbf{Z}_{K}^{\prime}\mathbf{Z}_{K}}{n}\right)^{+}\mathbf{z}\hat{\sigma}^{2}\right].
\]

\end_inset

 Formally,
 this is exactly the same as if we were dealing with a parametric linear model.
 I emphasize,
 though,
 that this is only valid if 
\begin_inset Formula $K$
\end_inset

 grows very slowly as 
\begin_inset Formula $n$
\end_inset

 grows.
 If we can't stick to acceptable rates,
 we should probably use some other method of approximating the small sample distribution.
 Bootstrapping is a possibility.
 We'll discuss this in the section on simulation.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "econometrics"
options "bibtotoc,plainnm"

\end_inset


\begin_inset CommandInset index_print
LatexCommand printindex
type "idx"
name "Index"
literal "true"

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
