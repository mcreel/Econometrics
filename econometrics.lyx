#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extreport
\begin_preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{dcolumn}
%\usepackage[ps2pdf,pdftitle={Econometrics},urlcolor=blue,linktocpage,a4paper,colorlinks=true]{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{setspace}
%
\definecolor{hellgelb}{rgb}{1,1,0.8}
\definecolor{colKeys}{rgb}{0,0,1}
\definecolor{colIdentifier}{rgb}{0,0,0}
\definecolor{colComments}{rgb}{1,0,0}
\definecolor{colString}{rgb}{0,0.5,0}

\lstset{%
   morekeywords={AND,ASC,avg,CHECK,COMMIT,count,DECODE,DESC,DISTINCT,%
                 GROUP,IN,LIKE,NUMBER,ROLLBACK,SUBSTR,sum,VARCHAR2}%
}
\lstset{%
    float=hbp,%
    basicstyle=\ttfamily\small, %
    identifierstyle=\color{colIdentifier}, %
    keywordstyle=\color{colKeys}, %
    stringstyle=\color{colString}, %
    commentstyle=\color{colComments}, %
    columns=flexible, %
    tabsize=2, %
    frame=single, %
    extendedchars=true, %
    showspaces=false, %
    showstringspaces=false, %
    numbers=left, %
    numberstyle=\tiny, %
    breaklines=true, %
    backgroundcolor=\color{hellgelb}, %
    breakautoindent=true, %
    captionpos=b%
}
\end_preamble
\options authordate
\use_default_options false
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 17
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "colorlinks=true,citecolor=blue"
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation landscape
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 2cm
\rightmargin 3cm
\bottommargin 2cm
\secnumdepth 1
\tocdepth 1
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle empty
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Econometrics
\end_layout

\begin_layout Author

\size small
Michael Creel
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename logoUAB.jpg
	width 8cm

\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Chapter
About this document
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Prerequisites
\end_layout

\begin_layout Standard
These notes have been prepared under the assumption that the reader understands
 basic statistics, linear algebra, and mathematical optimization.
 There are many sources for this material, one are the appendices to 
\emph on
Introductory Econometrics: A Modern Approach
\emph default
 by Jeffrey Wooldridge.
 It is the student's responsibility to get up to speed on this material,
 it will not be covered in class.
\end_layout

\begin_layout Standard
This document integrates lecture notes for a one year graduate level course
 with computer programs that illustrate and apply the methods that are studied.
 The immediate availability of executable (and modifiable) example programs
 when using the PDF version of the document is a distinguishing feature
 of these notes.
 If printed, the document is a somewhat terse approximation to a textbook.
 These notes are not intended to be a perfect substitute for a printed textbook.
 If you are a student of mine, please note that last sentence carefully.
 There are many good textbooks available.
 Students taking my courses should read the appropriate sections from at
 least one of the following books (or other textbooks with similar level
 and content)
\end_layout

\begin_layout Itemize
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

 Cameron, A.C.
 and P.K.
 Trivedi, 
\emph on
Microeconometrics - Methods and Applications.
 
\emph default
This is the book I recommend to use, if you don't have some reason to choose
 a different one.
\end_layout

\begin_layout Itemize
Davidson, R.
 and J.G.
 MacKinnon, 
\emph on
Econometric Theory and Methods
\end_layout

\begin_layout Itemize
Gallant, A.R., 
\emph on
An Introduction to Econometric Theory
\end_layout

\begin_layout Itemize
Hamilton, J.D., 
\emph on
Time Series Analysis
\end_layout

\begin_layout Standard
Some more advanced books:
\end_layout

\begin_layout Itemize
Davidson, R.
 and J.G.
 MacKinnon (1993) 
\emph on
Estimation and Inference in Econometrics
\emph default
, Oxford Univ.
 Press.
 
\end_layout

\begin_layout Itemize
Gallant, 
\emph on
Nonlinear Statistical Models
\emph default
.
 
\end_layout

\begin_layout Standard
Undergraduate level texts, if you need to catch up with some concepts 
\end_layout

\begin_layout Itemize
Wooldridge (2003), 
\emph on
Introductory Econometrics: A Modern Approach
\emph default
 (undergraduate level, for supplementary use only.
 Be sure to see the appendices, which give good coverage of foundations).
\end_layout

\begin_layout Itemize
Stock and Watson, 
\emph on

\begin_inset CommandInset href
LatexCommand href
name "Introduction to Econometrics"
target "https://www.pearson.com/us/higher-education/program/Stock-Introduction-to-Econometrics-Update-Plus-NEW-My-Lab-Economics-with-Pearson-e-Text-Access-Card-Package-3rd-Edition/PGM214413.html"
literal "false"

\end_inset

.
 
\emph default
This is the book used at the UAB for undergraduate courses in econometrics.
\end_layout

\begin_layout Section
Contents
\end_layout

\begin_layout Standard
With respect to contents, the emphasis is on estimation and inference within
 the world of stationary data.
 If you take a moment to read the licensing information in the next section,
 you'll see that you are free to copy and modify the document.
 If anyone would like to contribute material that expands the contents,
 it would be very welcome.
 Error corrections and other additions are also welcome.
\end_layout

\begin_layout Standard
The integrated examples and the support files (available online at the 
\begin_inset CommandInset href
LatexCommand href
name "github repository"
target "https://github.com/mcreel/Econometrics"
literal "false"

\end_inset

) are an important part of these notes.
 Julia 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{(julialang.org)}{https://julialang.org}
\end_layout

\end_inset

 has been used for most of the example programs, which are scattered though
 the document.
 The examples and code use the current stable version of Julia, version
 1.x.
 This choice is motivated by several factors.
 Julia runs on all of the popular operating systems, it is free, and it
 is fast, thanks to just-in-time compilation.
 It is a relatively new language, but is already quite stable.
 The fundamental tools (manipulation of matrices, statistical functions,
 minimization, 
\emph on
etc.
\emph default
) exist and are implemented in a way that make extending them fairly easy,
 plus new packages for more advanced applications are appearing constantly.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Julia"

\end_inset

 shows Julia running one of the examples from this document.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Julia"

\end_inset

Julia
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename example.png
	lyxscale 25
	width 10in

\end_inset


\end_layout

\end_inset

 There are also some examples which use 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{Gretl}{http://gretl.sourceforge.net}
\end_layout

\end_inset

, the Gnu Regression, Econometrics, and Time-Series Library.
 This is an easy to use program, available in a number of languages, and
 it comes with a lot of data ready to use.
 It runs on the major operating systems.
 Sometimes, simple is better.
\end_layout

\begin_layout Standard
The main document was prepared using \SpecialChar LyX
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{(www.lyx.org)}{http://www.lyx.org}
\end_layout

\end_inset

.
 \SpecialChar LyX
 is a free
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Quotes sld
\end_inset

Free
\begin_inset Quotes srd
\end_inset

 is used in the sense of 
\begin_inset Quotes sld
\end_inset

freedom
\begin_inset Quotes srd
\end_inset

, but \SpecialChar LyX
 is also free of charge (free as in 
\begin_inset Quotes sld
\end_inset

free beer
\begin_inset Quotes srd
\end_inset

).
\end_layout

\end_inset

 
\begin_inset Quotes eld
\end_inset

what you see is what you mean
\begin_inset Quotes erd
\end_inset

 word processor, basically working as a graphical frontend to \SpecialChar LaTeX
.
 It (with help from other applications) can export your work in \SpecialChar LaTeX
, HTML,
 PDF and several other forms.
 It will run on Linux, Windows, and MacOS systems.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Picture of LyX"

\end_inset

 shows \SpecialChar LyX
 editing this document.
 
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Picture of LyX"

\end_inset

\SpecialChar LyX

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/lyx.png
	width 6in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
License
\end_layout

\begin_layout Standard
All materials are copyrighted by Michael Creel with the date that appears
 above, under the MIT license.
 See the file License.md
\end_layout

\begin_layout Section
Obtaining the materials
\end_layout

\begin_layout Standard
The materials are available from a 
\begin_inset CommandInset href
LatexCommand href
name "github repository"
target "https://github.com/mcreel/Econometrics"
literal "false"

\end_inset

.
 In addition to the final product, which you're probably looking at in some
 form now, you can obtain the editable \SpecialChar LyX
 sources, which will allow you to
 create your own version, if you like, or send error corrections and contributio
ns.
 To run the examples embedded in the document, you need to install the 
\begin_inset CommandInset href
LatexCommand href
name "Julia language"
target "https://julialang.org/"
literal "false"

\end_inset

, and you also need to add files of the 
\begin_inset CommandInset href
LatexCommand href
name "github repository"
target "https://github.com/mcreel/Econometrics"
literal "false"

\end_inset

 as a Julia package.
 See the README.md file on the web page.you need to install Julia and then
 install this repository as a Julia package.
 Do this as follows:
\end_layout

\begin_layout Enumerate
git clone the repository
\end_layout

\begin_layout Enumerate
add the package to Julia by doing 
\family typewriter
] add <path where you downloaded the repo>
\family default
, for example, if you cloned it into the git directory of your home directory,
 you would do 
\family typewriter
] add ~/git/Econometrics
\end_layout

\begin_layout Enumerate
then do 
\family typewriter
using Econometrics
\family default
 in Julia to use the package.
 To run examples, cd into the relevant subdirectory of Econometrics/Examples,
 and then just include the script you would like to run.
\end_layout

\begin_layout Standard

\series bold
To make a local version of the pdf file
\series default
, so that the links open the example code from your hard drive, rather than
 from the github page:
\end_layout

\begin_layout Itemize
install 
\begin_inset CommandInset href
LatexCommand href
name " LyX"
target "https://www.lyx.org/"
literal "false"

\end_inset

 on your computer
\end_layout

\begin_layout Itemize
in the econometrics_local.lyx file, while editing using LyX, search for all
 instances of 
\family typewriter
https://github.com/mcreel/Econometrics/blob/master
\family default
, and replace them with 
\family typewriter
file:///<directory where the econometrics_local.lyx file is installed>
\end_layout

\begin_layout Itemize
build a local pdf by executing, from the command prompt, 
\family typewriter
lyx -e pdf2 econometrics_local.lyx
\end_layout

\begin_layout Itemize
the result will be an econometrics_local.pdf file
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Introduction to Julia
\end_layout

\begin_layout Standard
This document uses the 
\begin_inset CommandInset href
LatexCommand href
name " Julia programming language"
target "https://julialang.org/"
literal "false"

\end_inset

 for most of the examples.
 This chapter gives a very bare bones introduction to Julia.
 There are much better introductory materials from other sources, some of
 which are noted below.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Why Julia?
\end_layout

\begin_layout Itemize
free: free in terms of $$$, and also, source code is free, so you can know
 exactly what it does, and you can modify it and contribute to it
\end_layout

\begin_layout Itemize
multi-platform: runs on all the popular operating systems.
 For teaching econometrics, this is nice, because all students have equal
 access to the materials.
\end_layout

\begin_layout Itemize
fast: speed of well-written code is close to C or Fortan.
 Code is relatively easy to write and to read, similar to Matlab or other
 matrix scripting languages
\end_layout

\begin_layout Itemize
the above 3 considerations are essentially necessary for a language for
 modern science, which requires accessibility, verifiability, and performance
\end_layout

\begin_layout Itemize
also, it's fun: the ecosystem is still developing rapidly, plenty of room
 to contribute
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Why not Julia?
\end_layout

\begin_layout Itemize
Julia code is compiled before it's run.
 This means that first calls to functions take a bit of time, as they are
 compiled.
 The second call will be much faster-Interactive use may frustrate quite
 a bit, at least until you learn to work around this particularity.
\end_layout

\begin_deeper
\begin_layout Itemize
this will get better over time, as support for pre-compilation improves
\end_layout

\begin_layout Itemize
can be dealt with quite easily by warming up functions with toy usages,
 which you might include in your startup file.
\end_layout

\begin_layout Itemize
Also, keep your Julia session running, and the things you use often will
 already be compiled from previous uses.
\end_layout

\end_deeper
\begin_layout Itemize
the speed is only needed if your work is computationally demanding.
 Dividing epsilon by 2 is not very important when epsilon is small.
 For getting fast results for linear models, you may prefer a more complete
 canned package, e.g., Stata, etc.
\end_layout

\begin_layout Itemize
the ecosystem is still developing, some features like DataFrames and plotting
 are not as complete or stable as in other languages.
 However, I have been using it happily since 2014, and have been getting
 work done.
 
\strikeout on
Once the 1.0 release is out, 
\begin_inset Quotes sld
\end_inset

real soon now
\begin_inset Quotes srd
\end_inset

, things will stabilize
\strikeout default
 Strike that! Julia 1.0 was released in August 2018.
 The basic language is now quite stable, though packages are still developing
 at a quick rate.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Resources
\end_layout

\begin_layout Itemize
Julia language: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://julialang.org/
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
tutorials:
\end_layout

\begin_deeper
\begin_layout Itemize
Recommended! 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/PaulSoderlind/JuliaTutorial
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
More detailed.
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://ucidatascienceinitiative.github.io/IntroToJulia/
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Recommended! Quantitative Economics: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://lectures.quantecon.org/jl/index.html
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Installation of Julia and packages
\end_layout

\begin_layout Itemize
install Julia stable version from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://julialang.org/downloads/
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Getting started: 
\begin_inset CommandInset href
LatexCommand href
target "https://docs.julialang.org/en/v1/manual/getting-started/"

\end_inset


\end_layout

\begin_layout Itemize
Package manager documentation: 
\begin_inset CommandInset href
LatexCommand href
target "https://docs.julialang.org/en/v1/stdlib/Pkg/"

\end_inset


\end_layout

\begin_layout Itemize
The first thing you will need to do to make full use of this document is
 to install the examples and the support code.
 The main commands for packages:
\end_layout

\begin_deeper
\begin_layout Itemize
from the REPL, press ] to enter package mode.
\end_layout

\begin_layout Itemize

\family typewriter
]?
\family default
 : help for package mode.
\end_layout

\begin_layout Itemize

\family typewriter
] add
\family default
 : Add a package.
 e.g., to add the package that supports these notes, do the following
\family typewriter
: ] add https://github.com/mcreel/Econometrics.git
\end_layout

\begin_deeper
\begin_layout Itemize
That will install the code and the required packages.
 If you see an error about missing some package, just install what's missing.
\end_layout

\end_deeper
\begin_layout Itemize
Recommended packages (amongst many others):
\family typewriter
 ] add OhMyREPL Revise
\end_layout

\begin_layout Itemize
Recommended: put 
\family typewriter
using Revise; using OhMyREPL; using Econometrics
\family default
 in your 
\family typewriter
~/.julia/config/startup.jl
\family default
 file so that they are automatically used when you start Julia
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Section
Running Julia and the work flow
\end_layout

\begin_layout Subsection
REPL and text editor
\end_layout

\begin_layout Itemize
The REPL (
\begin_inset Quotes sld
\end_inset

read-eval-print loop
\begin_inset Quotes srd
\end_inset

 ), or in more plain parlance, the Julia command prompt, is my main way
 of working for research.
 Simple and easy to replicate.
 On Linux, just open a terminal and type 
\begin_inset Quotes sld
\end_inset

julia
\begin_inset Quotes srd
\end_inset

.
 You can run your code in one window, and edit it in another.
\end_layout

\begin_layout Subsection
IJulia/Jupyter notebooks
\end_layout

\begin_layout Itemize
Good for presentations, but I find it cumbersome for research and daily
 work
\end_layout

\begin_layout Itemize
Show it by doing 
\family typewriter
] add IJulia using IJulia; notebook()
\end_layout

\begin_layout Subsection
Juno/Atom
\end_layout

\begin_layout Itemize
If you want something more modern (?) and integrated looking than the REPL
 and a text editor, check out the Juno editor :
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://junolab.org/
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Loading/saving data
\end_layout

\begin_layout Standard
The examples that follow in later chapters provide some examples.
 Relevant commands are:
\end_layout

\begin_layout Itemize
using DelimitedFiles; ?readdlm, ?writedlm
\end_layout

\begin_layout Itemize
using CSV;
\end_layout

\begin_deeper
\begin_layout Itemize
?CSV.read for reading CSV files into with variable names in the first row
 into a dataframe
\end_layout

\begin_layout Itemize
?CSV.write
\end_layout

\end_deeper
\begin_layout Itemize
For more information, see 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/PaulSoderlind/JuliaTutorial/blob/master/Tutorial_09_LoadSaveData.ipynb"
target "https://github.com/PaulSoderlind/JuliaTutorial/blob/master/Tutorial_09_LoadSaveData.ipynb"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Example data sets to practice on:
\end_layout

\begin_layout Standard
CSV with names: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/mcreel/Econometrics/blob/master/Examples/Data/card.csv
\end_layout

\end_inset

, and
\end_layout

\begin_layout Standard
plain text, space delimited: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/mcreel/Econometrics/blob/master/Examples/Data/nerlove.data
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exploratory analysis and plotting
\end_layout

\begin_layout Standard
This section needs work, but the following two examples can give you ideas:
\end_layout

\begin_layout Itemize
Using DataFrames and StatPlots for exploratory analysis: 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{BasicDataAnalysis.jl}{https://github.com/mcreel/Econometrics/blo
b/master/Examples/Julia/BasicDataAnalysis.jl} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Data wrangling: transforming, adding variables, etc.: 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{DataWrangling.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/Julia/DataWrangling.jl} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://www.juliabloggers.com/data-wrangling-in-julia-based-on-dplyr-flights-tutor
ials/
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/piever/JuliaDBTutorial/blob/master/hflights.ipynb
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Introduction: Economic and econometric models
\end_layout

\begin_layout Standard
Here's some 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{data}{https://github.com/mcreel/Econometrics/blob/master/Exampl
es/Intro/data.txt}
\end_layout

\end_inset

: observations on 3 economic variables.
 
\end_layout

\begin_layout Standard

\series bold
\emph on
Draw a data block.
\end_layout

\begin_layout Standard
Let's do some exploratory analysis using Gretl:
\end_layout

\begin_layout Itemize
histograms
\end_layout

\begin_layout Itemize
correlations
\end_layout

\begin_layout Itemize
x-y scatterplots
\end_layout

\begin_layout Standard
So, what can we say? Correlations? Yes.
 Causality? Who knows?
\end_layout

\begin_layout Itemize

\emph on
What are these variables
\emph default
? So far, we don't know, so we have no mental model to sort out which variables
 might be causing others.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
We are missing a theoretical model! 
\end_layout

\begin_layout Itemize
A theoretical model is a key ingredient to assign causal relationships (which
 we might subsequently try to test).
 Without a model (or the ability to do experiments) we can't distinguish
 correlation from causality.
\end_layout

\begin_layout Itemize
It turns out that the variables we're looking at are QUANTITY (q), PRICE
 (p), and INCOME (m), and the data were generated using 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{this script}{https://github.com/mcreel/Econometrics/blob/master
/Examples/Intro/SupplyDemand.jl}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Economic theory tells us that the quantity of a good that consumers will
 purchase (the demand function) is something like: 
\begin_inset Formula 
\[
q=d(p,m,z)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $q$
\end_inset

 is the quantity demanded
\end_layout

\begin_layout Itemize
\begin_inset Formula $p$
\end_inset

 is the price of the good
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

 is income
\end_layout

\begin_layout Itemize
\begin_inset Formula $z$
\end_inset

 is a vector of other variables that may affect demand
\end_layout

\begin_layout Standard
The supply of the good to the market is the aggregation of the firms' supply
 functions.
 The market supply function is something like
\begin_inset Formula 
\[
q=s(p,z)
\]

\end_inset

Suppose we have a sample consisting of a number of observations on 
\begin_inset Formula $q$
\end_inset

 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $m$
\end_inset

 at different time periods 
\begin_inset Formula $t=1,2,...,n$
\end_inset

.
 Supply and demand in each period is
\begin_inset Formula 
\begin{align*}
q_{t} & =d(p_{t},m_{t},z_{t})\\
q_{t} & =s(p_{t},z_{t})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Draw a theory block.

\emph default
 (draw some graphs showing roles of 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
This is the basic economic model of supply and demand: 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 are determined in the market equilibrium, given by the intersection of
 the two curves.
 
\end_layout

\begin_layout Itemize
These two variables are determined jointly by the model, and are the 
\emph on
endogenous variables
\emph default
.
 Income (
\begin_inset Formula $m$
\end_inset

) is not determined by this model, its value is determined independently
 of 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 by some other process.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

 is an 
\emph on
exogenous variable
\emph default
.
 So, 
\begin_inset Formula $m$
\end_inset

 causes 
\begin_inset Formula $q$
\end_inset

, though the demand function.
 Because 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 are jointly determined, 
\begin_inset Formula $m$
\end_inset

 also causes 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 do not cause 
\begin_inset Formula $m$
\end_inset

, according to this theoretical model.
 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 have a joint causal relationship.
\end_layout

\begin_layout Itemize
Economic theory can help us to determine the causality relationships between
 correlated variables.
 According to theory, income does not affect the supply equation, so when
 income changes, supply stays the same.
 You can see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Price-and-Quantity,"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that when income increases, the upward movement of demand is tracing out
 the slope of the supply equation.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Price-and-Quantity,"

\end_inset

Price and Quantity, colored by income (blue is low, violet is high)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Intro/PriceQuantity.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The model is essentially a theoretical construct up to now:
\end_layout

\begin_layout Itemize
We don't know the forms of the functions 
\begin_inset Formula $s$
\end_inset

 and 
\begin_inset Formula $d.$
\end_inset


\end_layout

\begin_layout Itemize
Some components of 
\begin_inset Formula $z_{t}$
\end_inset

 may not be observable.
 For example, people don't eat the same lunch every day, and you can't tell
 what they will order just by looking at them.
 There are unobservable components to supply and demand, and we can model
 them as random variables.
 Suppose we can break 
\begin_inset Formula $z_{t}$
\end_inset

 into two unobservable components 
\begin_inset Formula $\varepsilon_{t1}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Theory can make some predictions, too.
 For example, theory tells us that demand functions are homogeneous of degree
 zero in prices and income.
 Also, the compensated demand functions have a negative slope with respect
 to price.
 But theory gives us 
\emph on
qualitative information, 
\emph default
signs of effects and so forth, but not the actual values in a given economy,
 not the magnitudes.
 So, theory by itself has some limitations, just as data by itself has limitatio
n.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An econometric model attempts to 
\series bold
quantify
\series default
 the relationship more precisely.
 A step toward an estimable econometric model is to suppose that the model
 may be written as
\begin_inset Formula 
\begin{align*}
q_{t} & =\alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}m_{t}+\varepsilon_{t1}\\
q_{t} & =\beta_{1}+\beta_{2}p_{t}+\varepsilon_{t1}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
The functions 
\begin_inset Formula $s$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 have been specified to be linear functions
\end_layout

\begin_layout Itemize
The parameters (
\begin_inset Formula $\alpha_{1},$
\end_inset

 
\begin_inset Formula $\beta_{2},$
\end_inset

 etc.) are constant over time.
\end_layout

\begin_layout Itemize
There is a single unobservable component in each equation, and it is additive.
\end_layout

\begin_layout Standard
If we assume nothing about the error terms 
\begin_inset Formula $\epsilon_{t1}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t2}$
\end_inset

, we can always write the last two equations, as the errors simply make
 up the difference between the true demand and supply functions and the
 assumed forms.
 But in order for the 
\begin_inset Formula $\beta$
\end_inset

 coefficients to exist in a sense that has 
\emph on
economic meaning
\emph default
, and in order to be able to use sample data to make reliable inferences
 about their values, we need to make additional assumptions.
 
\series bold
\emph on
Draw an assumptions block.

\emph default
 
\series default
Such assumptions might be something like:
\end_layout

\begin_layout Itemize
\begin_inset Formula $E(\epsilon_{tj})=0,\,j=1,2$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $E(m_{t}\epsilon_{tj})=0,\,j=1,2$
\end_inset


\end_layout

\begin_layout Standard
These are assertions that the errors have mean zero and are uncorrelated
 with income, and such assertions may or may not be reasonable.
 Later we will see how such assumptions may be used and/or tested.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
We can now use econometric methods to learn about the parameters.
 
\series bold
Draw an econometric model block.
 
\end_layout

\begin_layout Itemize
All of the last six bulleted points have 
\series bold
no theoretical basis
\series default
, in that the theory of supply and demand doesn't imply these conditions.
 
\end_layout

\begin_layout Itemize
The validity of any econometric results we obtain using an econometric model
 will be contingent on these additional restrictions being at least approximatel
y correct.
 
\end_layout

\begin_layout Itemize
For this reason, 
\emph on
specification testing
\emph default
 will be needed, to check that the model seems to be reasonable.
 
\end_layout

\begin_layout Itemize
Only when we are convinced that the model is at least approximately correct
 should we use it for economic analysis.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Given that we know the variable names of the above data, estimate the supply
 equation by two stage least squares, if you know how to.
 Compare the coefficient estimates with the values that generated the data.
 Note that the estimates are not bad, and get very close to the true values
 if you increase the sample size.
 This is because the model is correctly specified, and the 2SLS estimator
 is consistent in this case.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Ordinary Least Squares
\end_layout

\begin_layout Section
The Linear Model
\end_layout

\begin_layout Standard
Consider approximating a variable 
\begin_inset Formula $y$
\end_inset

 using the variables 
\begin_inset Formula $x_{1},x_{2},...,x_{k}$
\end_inset

.
 We can consider a model that is a linear approximation:
\end_layout

\begin_layout Standard

\series bold
Linearity
\series default
: the model is a linear function of the parameter vector 
\begin_inset Formula $\beta^{0}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
y & = & \beta_{1}^{0}x_{1}+\beta_{2}^{0}x_{2}+...+\beta_{k}^{0}x_{k}+\epsilon
\end{eqnarray*}

\end_inset

or, using vector notation: 
\begin_inset Formula 
\[
y=\mathbf{x}^{\prime}\beta^{0}+\epsilon
\]

\end_inset

 The dependent variable 
\begin_inset Formula $y$
\end_inset

 is a scalar random variable, 
\begin_inset Formula $\mathbf{x}=(\begin{array}{cccc}
x_{1} & x_{2} & \cdots & x_{k})^{'}\end{array}$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-vector of explanatory variables, and 
\begin_inset Formula $\beta^{0}=(\begin{array}{cccc}
\beta_{1}^{0} & \beta_{2}^{0} & \cdots & \beta_{k}^{0})^{'}\end{array}.$
\end_inset

 The superscript 
\begin_inset Quotes eld
\end_inset

0
\begin_inset Quotes erd
\end_inset

 in 
\begin_inset Formula $\beta^{0}$
\end_inset

 means this is the 
\begin_inset Quotes sld
\end_inset

true value
\begin_inset Quotes srd
\end_inset

 of the unknown parameter.
 It will be defined more precisely later, and usually suppressed when it's
 not necessary for clarity.
 
\end_layout

\begin_layout Standard
Suppose that we want to use data to try to determine the best linear approximati
on to 
\begin_inset Formula $y$
\end_inset

 using the variables 
\begin_inset Formula $\mathbf{x}.$
\end_inset

 The data 
\begin_inset Formula $\left\{ (y_{t},\mathbf{x}_{t})\right\} ,t=1,2,...,n$
\end_inset

 are obtained by some form of sampling
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
For example, cross-sectional data may be obtained by random sampling.
 Time series data accumulate historically.
\end_layout

\end_inset

.
 An individual observation is
\begin_inset Formula 
\[
y_{t}=\mathbf{x}_{t}^{\prime}\beta+\varepsilon_{t}
\]

\end_inset

 The 
\begin_inset Formula $n$
\end_inset

 observations can be written in matrix form as 
\begin_inset Formula 
\begin{equation}
\mathbf{y}=\mathbf{X}\beta+\mathbf{\varepsilon},
\end{equation}

\end_inset

 where 
\begin_inset Formula $\mathbf{y}=\left(\begin{array}{cccc}
y_{1} & y_{2} & \cdots & y_{n}\end{array}\right)^{\prime}$
\end_inset

 is 
\begin_inset Formula $n\times1$
\end_inset

 and 
\begin_inset Formula $\mathbf{X}=\left(\begin{array}{cccc}
\mathbf{x}_{1} & \mathbf{x}_{2} & \cdots & \mathbf{x}_{n}\end{array}\right)^{\prime}$
\end_inset

.
\end_layout

\begin_layout Standard
Linear models are more general than they might first appear, since one can
 employ nonlinear transformations of the variables: 
\begin_inset Formula 
\[
\varphi_{0}(z)=\left[\begin{array}{cccc}
\varphi_{1}(w) & \varphi_{2}(w) & \cdots & \varphi_{p}(w)\end{array}\right]\beta+\varepsilon
\]

\end_inset

 where the 
\begin_inset Formula $\phi_{i}()$
\end_inset

 are known functions.
 Defining 
\begin_inset Formula $y=\varphi_{0}(z),$
\end_inset

 
\begin_inset Formula $x_{1}=\varphi_{1}(w),$
\end_inset

 
\emph on
etc
\emph default
.
 leads to a model in the form of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: linearity"

\end_inset

.
 For example, the Cobb-Douglas model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Cobb-Douglas model
\end_layout

\end_inset

 
\begin_inset Formula 
\[
z=Aw_{2}^{\beta_{2}}w_{3}^{\beta_{3}}\exp(\varepsilon)
\]

\end_inset

 can be transformed logarithmically to obtain 
\begin_inset Formula 
\[
\ln z=\ln A+\beta_{2}\ln w_{2}+\beta_{3}\ln w_{3}+\varepsilon.
\]

\end_inset

If we define 
\begin_inset Formula $y=\ln z,$
\end_inset

 
\begin_inset Formula $\beta_{1}=\ln A,$
\end_inset

 
\emph on
etc.,
\emph default
 we can put the model in the form needed.
 The approximation is linear in the parameters, but not necessarily linear
 in the variables.
\end_layout

\begin_layout Section
Estimation by least squares
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Typical-data,-Classical"

\end_inset

, obtained by running 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{TypicalData.jl}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/OLS/TypicalData.jl}
\end_layout

\end_inset

 shows some data that follows the linear model 
\begin_inset Formula $y_{t}=\beta_{1}+\beta_{2}x_{t2}+\epsilon_{t}$
\end_inset

.
 The blue line is the 
\begin_inset Quotes sld
\end_inset

true
\begin_inset Quotes srd
\end_inset

 regression line 
\begin_inset Formula $\beta_{1}+\beta_{2}x_{t2}$
\end_inset

, and the orange dots are the data points 
\begin_inset Formula $(x_{t2},y_{t}),$
\end_inset

 where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is a random error that has mean zero and is independent of 
\begin_inset Formula $x_{t2}$
\end_inset

.
 Exactly how the blue line is defined will become clear later.
 In practice, we only have the data, and we don't know where the blue line
 lies.
 We need to gain information about the straight line that best fits the
 data points.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Typical-data,-Classical"

\end_inset

Typical data, Classical Model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/TypicalData.svg

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
ordinary least squares
\emph default
 (OLS) estimator is defined as the value that minimizes the sum of the squared
 errors: 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \arg\min s(\beta)
\end{eqnarray*}

\end_inset

 where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
s(\beta) & = & \sum_{t=1}^{n}\left(y_{t}-\mathbf{x}_{t}^{\prime}\beta\right)^{2}\label{eq:OLS criterion function}\\
 & = & \left(\mathbf{y}-\mathbf{X}\beta\right)^{\prime}\left(\mathbf{y}-\mathbf{X}\beta\right)\nonumber \\
 & = & \mathbf{y}^{\prime}\mathbf{y}-2\mathbf{y}^{\prime}\mathbf{X}\beta+\beta^{\prime}\mathbf{X}^{\prime}\mathbf{X}\beta\nonumber \\
 & = & \parallel\mathbf{y}-\mathbf{X}\beta\parallel^{2}\nonumber 
\end{eqnarray}

\end_inset

This last expression makes it clear how the OLS estimator
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
estimator, OLS
\end_layout

\end_inset

 is defined: it minimizes the Euclidean distance between 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $X\beta.$
\end_inset

 The fitted OLS coefficients are those that give the best linear approximation
 to 
\begin_inset Formula $y$
\end_inset

 using 
\begin_inset Formula $\mathbf{x}$
\end_inset

 as basis functions, where 
\begin_inset Quotes sld
\end_inset

best
\begin_inset Quotes srd
\end_inset

 means minimum Euclidean distance.
 One could think of other estimators based upon other metrics.
 For example, the 
\emph on
minimum absolute distance
\emph default
 (MAD) minimizes 
\begin_inset Formula $\sum_{t=1}^{n}\left|y_{t}-\mathbf{x}_{t}^{\prime}\beta\right|$
\end_inset

.
 Later, we will see that which estimator is best in terms of their statistical
 properties, rather than in terms of the metrics that define them, depends
 upon the properties of 
\begin_inset Formula $\epsilon$
\end_inset

, about which we have as yet made no assumptions.
\end_layout

\begin_layout Itemize
To minimize the criterion 
\begin_inset Formula $s(\beta),$
\end_inset

 find the derivative with respect to 
\begin_inset Formula $\beta$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
D_{\beta}s(\beta) & = & -2\mathbf{X}^{\prime}\mathbf{y}+2\mathbf{X}^{\prime}\mathbf{X}\beta
\end{eqnarray*}

\end_inset

Then setting it to zeros gives
\begin_inset Formula 
\[
D_{\beta}s(\hat{\beta})=-2\mathbf{X}^{\prime}\mathbf{y}+2\mathbf{X}^{\prime}\mathbf{X}\hat{\beta}\equiv0
\]

\end_inset

 so
\begin_inset Formula 
\[
\hat{\beta}=(\mathbf{X}^{\prime}\mathbf{X})^{-1}\mathbf{X}^{\prime}\mathbf{y}.
\]

\end_inset


\end_layout

\begin_layout Itemize
To verify that this is a minimum, check the second order sufficient condition:
 
\begin_inset Formula 
\[
D_{\beta}^{2}s(\hat{\beta})=2\mathbf{X}^{\prime}\mathbf{X}
\]

\end_inset

 Since 
\begin_inset Formula $\rho(\mathbf{X})=K,$
\end_inset

 this matrix is positive definite, since it's a quadratic form in a p.d.
 matrix (identity matrix of order 
\begin_inset Formula $n)$
\end_inset

, so 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is in fact a minimizer.
\end_layout

\begin_layout Itemize
The 
\emph on
fitted values
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
fitted values
\end_layout

\end_inset


\emph default
 are the vector 
\begin_inset Formula $\hat{\mathbf{y}}=\mathbf{X}\hat{\beta}.$
\end_inset


\end_layout

\begin_layout Itemize
The 
\emph on
residuals
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
residuals
\end_layout

\end_inset


\emph default
 are the vector 
\begin_inset Formula $\hat{\varepsilon}=\mathbf{y}-\mathbf{X}\hat{\beta}$
\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{y} & = & \mathbf{X}\beta+\varepsilon\\
 & = & \mathbf{X}\hat{\beta}+\hat{\varepsilon}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Also, the first order conditions can be written as 
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{X}^{\prime}\mathbf{y}-\mathbf{X}^{\prime}\mathbf{X}\hat{\beta} & = & 0\\
\mathbf{X}^{\prime}\left(\mathbf{y}-\mathbf{X}\hat{\beta}\right) & = & 0\\
\mathbf{X}^{\prime}\hat{\varepsilon} & = & 0
\end{eqnarray*}

\end_inset

which is to say, the OLS residuals are orthogonal to 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 Let's look at this more carefully.
\end_layout

\begin_layout Section
Geometric interpretation of least squares estimation
\end_layout

\begin_layout Subsection
In 
\begin_inset Formula $X,Y$
\end_inset

 Space
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fitted in X,Y space"

\end_inset

 shows a typical fit to data, along with the true regression line.
 Note that the true line and the estimated line are different.
 This figure was created by running the Julia program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{OlsFit.jl}{https://github.com/mcreel/Econometrics/blob/master/Ex
amples/OLS/OlsFit.jl} 
\end_layout

\end_inset

.
 You can experiment with changing the parameter values to see how this affects
 the fit, and to see how the fitted line will sometimes be close to the
 true line, and sometimes rather far away.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fitted in X,Y space"

\end_inset

Example OLS Fit
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/OlsFit.svg

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
In Observation Space
\end_layout

\begin_layout Standard
If we want to plot in observation space, we'll need to use only two or three
 observations, or we'll encounter some limitations of the blackboard.
 If we try to use 3, we'll encounter the limits of my artistic ability,
 so let's use two.
 With only two observations, we can't have 
\begin_inset Formula $K>1.$
\end_inset

 
\begin_inset Float figure
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The fit in observation space
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/regression_obs_space.pdf
	width 6in

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Itemize
We can decompose 
\begin_inset Formula $y$
\end_inset

 into two components: the orthogonal projection onto the 
\begin_inset Formula $K-$
\end_inset

dimensional space spanned by 
\begin_inset Formula $X$
\end_inset

, 
\begin_inset Formula $X\hat{\beta},$
\end_inset

 and the component that is the orthogonal projection onto the 
\begin_inset Formula $n-K$
\end_inset

 subpace that is orthogonal to the span of 
\begin_inset Formula $X,$
\end_inset

 
\begin_inset Formula $\hat{\varepsilon}.$
\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is chosen to make 
\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 as short as possible, 
\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 will be orthogonal to the space spanned by 
\begin_inset Formula $X.$
\end_inset

 Since 
\begin_inset Formula $X$
\end_inset

 is in this space, 
\begin_inset Formula $X^{\prime}\hat{\varepsilon}=0.$
\end_inset

 Note that the f.o.c.
 that define the least squares estimator imply that this is so.
\end_layout

\begin_layout Subsection
Projection Matrices
\end_layout

\begin_layout Standard
\begin_inset Formula $X\hat{\beta}$
\end_inset

 is the projection of 
\begin_inset Formula $y$
\end_inset

 onto the span of 
\begin_inset Formula $X,$
\end_inset

 or 
\begin_inset Formula 
\[
X\hat{\beta}=X\left(X^{\prime}X\right)^{-1}X^{\prime}y
\]

\end_inset

 Therefore, the matrix
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
matrix, projection
\end_layout

\end_inset

 that projects 
\begin_inset Formula $y$
\end_inset

 onto the span of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula 
\[
P_{X}=X(X^{\prime}X)^{-1}X^{\prime}
\]

\end_inset

 since 
\begin_inset Formula 
\[
X\hat{\beta}=P_{X}y.
\]

\end_inset


\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 is the projection of 
\begin_inset Formula $y$
\end_inset

 onto the 
\begin_inset Formula $N-K$
\end_inset

 dimensional space that is orthogonal to the span of 
\begin_inset Formula $X$
\end_inset

.
 We have that 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\varepsilon} & = & y-X\hat{\beta}\\
 & = & y-X(X^{\prime}X)^{-1}X^{\prime}y\\
 & = & \left[I_{n}-X(X^{\prime}X)^{-1}X^{\prime}\right]y.
\end{eqnarray*}

\end_inset

 So the matrix that projects 
\begin_inset Formula $y$
\end_inset

 onto the space orthogonal to the span of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
M_{X} & = & I_{n}-X(X^{\prime}X)^{-1}X^{\prime}\\
 & = & I_{n}-P_{X}.
\end{eqnarray*}

\end_inset

 We have 
\begin_inset Formula 
\[
\hat{\varepsilon}=M_{X}y.
\]

\end_inset

Therefore 
\begin_inset Formula 
\begin{eqnarray*}
y & = & P_{X}y+M_{X}y\\
 & = & X\hat{\beta}+\hat{\varepsilon}.
\end{eqnarray*}

\end_inset

These two projection matrices decompose the 
\begin_inset Formula $n$
\end_inset

 dimensional vector 
\begin_inset Formula $y$
\end_inset

 into two orthogonal components - the portion that lies in the 
\begin_inset Formula $K$
\end_inset

 dimensional space defined by 
\begin_inset Formula $X,$
\end_inset

 and the portion that lies in the orthogonal 
\begin_inset Formula $n-K$
\end_inset

 dimensional space.
\end_layout

\begin_layout Itemize
Note that both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are 
\emph on
symmetric
\emph default
 and 
\emph on
idempotent
\emph default
.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A symmetric matrix
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
matrix, symmetric
\end_layout

\end_inset

 
\begin_inset Formula $A$
\end_inset

 is one such that 
\begin_inset Formula $A=A^{\prime}.$
\end_inset


\end_layout

\begin_layout Itemize
An idempotent matrix
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
matrix, idempotent
\end_layout

\end_inset

 
\begin_inset Formula $A$
\end_inset

 is one such that 
\begin_inset Formula $A=AA.$
\end_inset


\end_layout

\begin_layout Itemize
The only nonsingular idempotent matrix is the identity matrix.
 
\end_layout

\end_deeper
\begin_layout Section
Influential observations
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
observations, influential
\end_layout

\end_inset

 and outliers
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
outliers
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The OLS estimator of the 
\begin_inset Formula $i^{th}$
\end_inset

 element of the vector 
\begin_inset Formula $\beta_{0}$
\end_inset

 is simply 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{i} & = & \left[(X^{\prime}X)^{-1}X^{\prime}\right]_{i\cdot}y\\
 & = & c_{i}^{\prime}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is how we define a linear estimator
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
estimator, linear
\end_layout

\end_inset

 - it's a linear function of the dependent variable.
 Since it's a linear combination of the observations on the dependent variable,
 where the weights are determined by the observations on the regressors,
 some observations may have more influence than others.
\end_layout

\begin_layout Standard
To investigate this, let 
\begin_inset Formula $e_{t}$
\end_inset

 be an 
\begin_inset Formula $n$
\end_inset

 vector of zeros with a 
\begin_inset Formula $1$
\end_inset

 in the t
\begin_inset Formula $^{th}$
\end_inset

 position, 
\emph on
i.e.,
\emph default
 it's the 
\begin_inset Formula $t\textrm{th column of the matrix \ensuremath{I_{n}}}$
\end_inset

.
 Define 
\begin_inset Formula 
\begin{eqnarray*}
h_{t} & = & \left(P_{X}\right)_{tt}\\
 & = & e_{t}^{\prime}P_{X}e_{t}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula $h_{t}$
\end_inset

 is the t
\begin_inset Formula $^{th}$
\end_inset

 element on the main diagonal of 
\begin_inset Formula $P_{X}$
\end_inset

.
 Note that 
\begin_inset Formula 
\begin{eqnarray*}
h_{t} & = & \parallel P_{X}e_{t}\parallel^{2}
\end{eqnarray*}

\end_inset

so 
\begin_inset Formula 
\[
h_{t}\leq\parallel e_{t}\parallel^{2}=1
\]

\end_inset

So 
\begin_inset Formula $0<h_{t}<1$
\end_inset

.
 Also, 
\begin_inset Formula 
\[
TrP_{X}=K\Rightarrow\overline{h}=K/n.
\]

\end_inset

So the average of the 
\begin_inset Formula $h_{t}$
\end_inset

 is 
\begin_inset Formula $K/n$
\end_inset

.
 The value 
\begin_inset Formula $h_{t}$
\end_inset

 is referred to as the 
\emph on
leverage
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
leverage
\end_layout

\end_inset


\emph default
 of the observation.
 If the leverage is much higher than average, the observation has the potential
 to affect the OLS fit importantly.
 However, an observation may also be influential due to the value of 
\begin_inset Formula $y_{t}$
\end_inset

, rather than the weight it is multiplied by, which only depends on the
 
\begin_inset Formula $x_{t}$
\end_inset

's.
\end_layout

\begin_layout Standard
To account for this, consider estimation of 
\begin_inset Formula $\beta$
\end_inset

 without using the 
\begin_inset Formula $t^{th}$
\end_inset

 observation (designate this estimator as 
\begin_inset Formula $\hat{\beta}^{(t)}).$
\end_inset

 One can show (see Davidson and MacKinnon, pp.
 32-5 for proof) that 
\begin_inset Formula 
\[
\hat{\beta}^{(t)}=\hat{\beta}-\left(\frac{1}{1-h_{t}}\right)(X^{\prime}X)^{-1}X_{t}^{\prime}\hat{\varepsilon}_{t}
\]

\end_inset

 so the change in the 
\begin_inset Formula $t^{th}$
\end_inset

 observations fitted value is 
\begin_inset Formula 
\[
\mathbf{x}_{t}^{\prime}\hat{\beta}-\mathbf{x}_{t}^{\prime}\hat{\beta}^{(t)}=\left(\frac{h_{t}}{1-h_{t}}\right)\hat{\varepsilon}_{t}
\]

\end_inset

 While an observation may be influential if it doesn't affect its own fitted
 value, it certainly 
\emph on
is
\emph default
 influential if it does.
 A fast means of identifying influential observations is to plot 
\begin_inset Formula $\left(\frac{h_{t}}{1-h_{t}}\right)\hat{\varepsilon}_{t}$
\end_inset

 (which I will refer to as the 
\emph on
own influence
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
own influence
\end_layout

\end_inset


\emph default
 of the observation) as a function of 
\begin_inset Formula $t$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Detection-of-influential"

\end_inset

 gives an example plot of data, fit, leverage and influence.
 The Julia program is 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{InfluentialObservation.jl}{https://github.com/mcreel/Econometric
s/blob/master/Examples/OLS/InfluentialObservation.jl}
\end_layout

\end_inset

.
 If you re-run the program you will see that the leverage of the last observatio
n (an outlying value of x) is always high, and the influence is sometimes
 high.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Detection-of-influential"

\end_inset

Detection of influential observations
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/InfluentialObservation.svg

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
After influential observations are detected, one needs to determine 
\emph on
why
\emph default
 they are influential.
 Possible causes include:
\end_layout

\begin_layout Itemize
data entry error, which can easily be corrected once detected.
 Data entry errors 
\emph on
are very common.
\end_layout

\begin_layout Itemize
special economic factors that affect some observations.
 These would need to be identified and incorporated in the model.
 This is the idea behind 
\emph on
structural change
\emph default
: the parameters may not be constant across all observations.
\end_layout

\begin_layout Itemize
pure randomness may have caused us to sample a low-probability observation.
\end_layout

\begin_layout Standard
There exist 
\emph on
robust
\emph default
 estimation methods that downweight outliers.
\end_layout

\begin_layout Section
Goodness of fit
\end_layout

\begin_layout Standard
The fitted model is 
\begin_inset Formula 
\[
y=X\hat{\beta}+\hat{\varepsilon}
\]

\end_inset

 Take the inner product:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y^{\prime}y=\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}+2\hat{\beta}^{\prime}X^{\prime}\hat{\varepsilon}+\hat{\varepsilon}^{\prime}\hat{\varepsilon}
\]

\end_inset

 But the middle term of the RHS is zero since 
\begin_inset Formula $X^{\prime}\hat{\varepsilon}=0$
\end_inset

, so 
\begin_inset Formula 
\begin{equation}
y^{\prime}y=\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}+\hat{\varepsilon}^{\prime}\hat{\varepsilon}\label{rsquare development}
\end{equation}

\end_inset

 The 
\emph on
uncentered
\emph default
 
\begin_inset Formula $R_{u}^{2}$
\end_inset

 is defined as 
\begin_inset Formula 
\begin{eqnarray*}
R_{u}^{2} & = & 1-\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{y^{\prime}y}\\
 & = & \frac{\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}}{y^{\prime}y}\\
 & = & \frac{\parallel P_{X}y\parallel^{2}}{\parallel y\parallel^{2}}\\
 & = & \cos^{2}(\phi),
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\phi$
\end_inset

 is the angle between 
\begin_inset Formula $y$
\end_inset

 and the span of 
\begin_inset Formula $X$
\end_inset

 .
\end_layout

\begin_layout Itemize
The uncentered 
\begin_inset Formula $R^{2}$
\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
R- squared, uncentered
\end_layout

\end_inset

 changes if we add a constant to 
\begin_inset Formula $y,$
\end_inset

 since this changes 
\begin_inset Formula $\phi$
\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Uncentered--R^{2}"

\end_inset

, the yellow vector is a constant, since it's on the 
\begin_inset Formula $45$
\end_inset

 degree line in observation space).
 
\begin_inset Float figure
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Uncentered--R^{2}"

\end_inset

Uncentered 
\begin_inset Formula $R^{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/Figures/UncenteredRSquare.png
	width 5in
	height 4in
	rotateOrigin center

\end_inset


\end_layout

\end_inset

Another, more common definition measures the contribution of the variables,
 other than the constant term, to explaining the variation in 
\begin_inset Formula $y.$
\end_inset

 Thus it measures the ability of the model to explain the variation of 
\begin_inset Formula $y$
\end_inset

 about its unconditional sample mean.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\iota=(1,1,...,1)^{\prime},$
\end_inset

 a 
\begin_inset Formula $n$
\end_inset

 -vector.
 So 
\begin_inset Formula 
\begin{eqnarray*}
M_{\iota} & = & I_{n}-\iota(\iota^{\prime}\iota)^{-1}\iota^{\prime}\\
 & = & I_{n}-\iota\iota^{\prime}/n
\end{eqnarray*}

\end_inset

 
\begin_inset Formula $M_{\iota}y$
\end_inset

 just returns the vector of deviations from the mean.
 In terms of deviations from the mean, equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "rsquare development"

\end_inset

 becomes
\begin_inset Formula 
\[
y^{\prime}M_{\iota}y=\hat{\beta}^{\prime}X^{\prime}M_{\iota}X\hat{\beta}+\hat{\varepsilon}^{\prime}M_{\iota}\hat{\varepsilon}
\]

\end_inset

 
\end_layout

\begin_layout Standard
The 
\emph on
centered
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
R-squared, centered
\end_layout

\end_inset


\emph default
 
\begin_inset Formula $R_{c}^{2}$
\end_inset

 is defined as 
\begin_inset Formula 
\[
R_{c}^{2}=1-\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{y^{\prime}M_{\iota}y}=1-\frac{ESS}{TSS}
\]

\end_inset

where 
\begin_inset Formula $ESS=\hat{\varepsilon}^{\prime}\hat{\varepsilon}$
\end_inset

 and 
\begin_inset Formula $TSS=y^{\prime}M_{\iota}y$
\end_inset

=
\begin_inset Formula $\sum_{t=1}^{n}(y_{t}-\bar{y})^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
Supposing that 
\begin_inset Formula $X$
\end_inset

 contains a column of ones (
\emph on
i.e.,
\emph default
 there is a constant term), 
\begin_inset Formula 
\[
X^{\prime}\hat{\varepsilon}=0\Rightarrow\sum_{t}\hat{\varepsilon}_{t}=0
\]

\end_inset

 so 
\begin_inset Formula $M_{\iota}\hat{\varepsilon}$
\end_inset

 
\begin_inset Formula $=\hat{\varepsilon}.$
\end_inset

 In this case 
\begin_inset Formula 
\[
y^{\prime}M_{\iota}y=\hat{\beta}^{\prime}X^{\prime}M_{\iota}X\hat{\beta}+\hat{\varepsilon}^{\prime}\hat{\varepsilon}
\]

\end_inset

 So 
\begin_inset Formula 
\[
R_{c}^{2}=\frac{RSS}{TSS}
\]

\end_inset

where 
\begin_inset Formula $RSS=\hat{\beta}^{\prime}X^{\prime}M_{\iota}X\hat{\beta}$
\end_inset

 
\end_layout

\begin_layout Itemize
Supposing that a column of ones is in the space spanned by 
\begin_inset Formula $X$
\end_inset

 (
\begin_inset Formula $P_{X}\iota=\iota),$
\end_inset

 then one can show that 
\begin_inset Formula $0\leq R_{c}^{2}\leq1.$
\end_inset


\end_layout

\begin_layout Section
The classical linear regression model
\begin_inset CommandInset label
LatexCommand label
name "sec:The-classical-linear"

\end_inset


\end_layout

\begin_layout Standard
Up to this point the model is empty of content beyond the definition of
 a best linear approximation to 
\begin_inset Formula $y$
\end_inset

 and some geometrical properties.
 There is no economic content to the model, and the regression parameters
 have no economic interpretation.
 For example, what is the partial derivative of 
\begin_inset Formula $y$
\end_inset

 with respect to 
\begin_inset Formula $x_{j}$
\end_inset

? The linear approximation is
\begin_inset Formula 
\[
y=\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{k}x_{k}+\epsilon
\]

\end_inset

The partial derivative is 
\begin_inset Formula 
\[
\frac{\partial y}{\partial x_{j}}=\beta_{j}+\frac{\partial\epsilon}{\partial x_{j}}
\]

\end_inset

Up to now, there's no guarantee that 
\begin_inset Formula $\frac{\partial\epsilon}{\partial x_{j}}$
\end_inset

=0.
 For the 
\begin_inset Formula $\beta$
\end_inset

 to have an economic meaning, we need to make additional assumptions.
 The assumptions that are appropriate to make depend on the data under considera
tion.
 We'll start with the classical linear regression model, which incorporates
 some assumptions that are clearly not realistic for economic data.
 This is to be able to explain some concepts with a minimum of confusion
 and notational clutter.
 Later we'll adapt the results to what we can get with more realistic assumption
s.
\end_layout

\begin_layout Standard

\series bold
Linearity
\series default
: the model is a linear function of the parameter vector 
\begin_inset Formula $\beta^{0}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray}
y & = & \beta_{1}^{0}x_{1}+\beta_{2}^{0}x_{2}+...+\beta_{k}^{0}x_{k}+\epsilon\label{assumption: linearity}
\end{eqnarray}

\end_inset

or, using vector notation: 
\begin_inset Formula 
\[
y=\mathbf{x}^{\prime}\beta^{0}+\epsilon
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Nonstochastic linearly independent regressors
\series default
: 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a fixed matrix of constants, it has rank 
\begin_inset Formula $K$
\end_inset

 equal to its number of columns, and 
\begin_inset Formula 
\begin{align}
\lim\frac{1}{n}\mathbf{X}^{\prime}\mathbf{X} & =Q_{X}\label{assumption: linearly independent regressors}
\end{align}

\end_inset

where 
\begin_inset Formula $Q_{X}$
\end_inset

 is a finite positive definite matrix.
 This is needed to be able to identify the individual effects of the explanatory
 variables.
\end_layout

\begin_layout Standard

\series bold
Independently and identically distributed errors
\series default
:
\begin_inset Formula 
\begin{equation}
\epsilon\sim IID(0,\sigma^{2}I_{n})\label{assumption: IID errors}
\end{equation}

\end_inset


\begin_inset Formula $\varepsilon$
\end_inset

 is jointly distributed IID.
 This implies the following two properties:
\end_layout

\begin_layout Standard

\series bold
Homoscedastic errors
\series default
:
\begin_inset Formula 
\begin{equation}
V(\varepsilon_{t})=\sigma_{0}^{2},\forall t\label{assumption: homoscedasticity}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\series bold
Nonautocorrelated errors:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathcal{E}(\varepsilon_{t}\epsilon_{s})=0,\forall t\neq s\label{assumption: nonautocorrelation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Optionally, we will sometimes assume that the errors are normally distributed.
\end_layout

\begin_layout Standard

\series bold
Normally distributed errors:
\series default

\begin_inset Formula 
\begin{equation}
\epsilon\sim N(0,\sigma^{2}I_{n})\label{assumption: normal errors}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Small sample statistical properties of the least squares estimator
\end_layout

\begin_layout Standard
Up to now, we have only examined numeric properties of the OLS estimator,
 that always hold.
 Now we will examine statistical properties.
 The statistical properties depend upon the assumptions we make.
\end_layout

\begin_layout Subsection
Unbiasedness
\end_layout

\begin_layout Standard
We have 
\begin_inset Formula $\hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y$
\end_inset

.
 By linearity, 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & (X^{\prime}X)^{-1}X^{\prime}\left(X\beta+\varepsilon\right)\\
 & = & \beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: linearly independent regressors"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: IID errors"

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
E(X^{\prime}X)^{-1}X^{\prime}\varepsilon & = & E(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & (X^{\prime}X)^{-1}X^{\prime}E\varepsilon\\
 & = & 0
\end{eqnarray*}

\end_inset

 so the OLS estimator is unbiased under the assumptions of the classical
 model.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-unbiasedness"

\end_inset

 shows the results of a small Monte Carlo experiment where the OLS estimator
 was calculated for 10000 samples from the classical model with 
\begin_inset Formula $y=1+2x+\varepsilon$
\end_inset

, where 
\begin_inset Formula $n=20$
\end_inset

, 
\begin_inset Formula $\sigma_{\varepsilon}^{2}=9$
\end_inset

, and 
\begin_inset Formula $x$
\end_inset

 is fixed across samples.
 We can see that the 
\begin_inset Formula $\beta_{2}$
\end_inset

 appears to be estimated without bias.
 The program that generates the plot is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Unbiased.jl}{https://github.com/mcreel/Econometrics/blob/master/
Examples/OLS/Unbiased.jl} 
\end_layout

\end_inset

, if you would like to experiment with this.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "figure-unbiasedness"

\end_inset

Unbiasedness of OLS under classical assumptions: replications of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 minus true 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/Unbiased.svg

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
With time series data, the OLS estimator will often be biased.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-biasedness"

\end_inset

 shows the results of a small Monte Carlo experiment where the OLS estimator
 was calculated for 1000 samples from the AR(1) model with 
\begin_inset Formula $y_{t}=0+0.9y_{t-1}+\varepsilon_{t}$
\end_inset

, where 
\begin_inset Formula $n=20$
\end_inset

 and 
\begin_inset Formula $\sigma_{\varepsilon}^{2}=1$
\end_inset

.
 In this case, assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: linearly independent regressors"

\end_inset

 does not hold: the regressors are stochastic.
 We can see that the bias in the estimation of 
\begin_inset Formula $\beta_{2}$
\end_inset

 is about -0.2.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "figure-biasedness"

\end_inset

Biasedness of OLS when an assumption fails: : replications of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 minus true 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/Biased.svg

\end_inset


\end_layout

\end_inset

The program that generates the plot is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Biased.jl}{https://github.com/mcreel/Econometrics/blob/master/Ex
amples/OLS/Biased.jl} 
\end_layout

\end_inset

 , if you would like to experiment with this.
\end_layout

\begin_layout Subsection
Normality
\end_layout

\begin_layout Standard
With the linearity assumption, we have 
\begin_inset Formula $\hat{\beta}=\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon.$
\end_inset

 This is a linear function of 
\begin_inset Formula $\varepsilon$
\end_inset

.
 Adding the assumption of normality (
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption: normal errors"

\end_inset

, which implies strong exogeneity), then
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\beta}\sim N\left(\beta,(X^{\prime}X)^{-1}\sigma_{0}^{2}\right)
\]

\end_inset

since a linear function of a normal random vector is also normally distributed.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-unbiasedness"

\end_inset

 you can see that the estimator appears to be normally distributed.
 It in fact is normally distributed, since the DGP (see the Octave program)
 has normal errors.
 Even when the data may be taken to be IID, the assumption of normality
 is often questionable or simply untenable.
 For example, if the dependent variable is the number of automobile trips
 per week, it is a count variable with a discrete distribution, and is thus
 not normally distributed.
 Many variables in economics can take on only nonnegative values, which,
 strictly speaking, rules out normality.
\begin_inset Foot
status open

\begin_layout Plain Layout
Normality may be a good model nonetheless, as long as the probability of
 a negative value occurring is negligible under the model.
 This depends upon the mean being large enough in relation to the variance.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The variance of the OLS estimator and the Gauss-Markov theorem
\end_layout

\begin_layout Standard
Now let's make all the classical assumptions except the assumption of normality.
 We have 
\begin_inset Formula $\hat{\beta}=\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon$
\end_inset

 and we know that 
\begin_inset Formula $E(\hat{\beta})=\beta$
\end_inset

.
 So
\begin_inset Formula 
\begin{eqnarray*}
Var(\hat{\beta}) & = & E\left\{ \left(\mathbf{\hat{\beta}-\beta}\right)\left(\mathbf{\hat{\beta}-\beta}\right)^{\prime}\right\} \\
 & = & E\left\{ (X^{\prime}X)^{-1}X^{\prime}\varepsilon\varepsilon^{\prime}X(X^{\prime}X)^{-1}\right\} \\
 & = & (X^{\prime}X)^{-1}\sigma_{0}^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is a 
\emph on
linear estimator
\emph default

\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
estimator, linear
\end_layout

\end_inset

, which means that it is a linear function of the dependent variable, 
\begin_inset Formula $y.$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \left[(X^{\prime}X)^{-1}X^{\prime}\right]y\\
 & = & Cy
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is a function of the explanatory variables only, not the dependent variable.
 It is also 
\emph on
unbiased
\emph default
 under the present assumptions, as we proved above.
 One could consider other weights 
\begin_inset Formula $W$
\end_inset

 that are a function of 
\begin_inset Formula $X$
\end_inset

 that define some other linear estimator.
 We'll still insist upon unbiasedness.
 Consider 
\begin_inset Formula $\tilde{\beta}=Wy,$
\end_inset

 where 
\begin_inset Formula $W=W(X)$
\end_inset

 is some 
\begin_inset Formula $k\times n$
\end_inset

 matrix function of 
\begin_inset Formula $X.$
\end_inset

 Note that since 
\begin_inset Formula $W$
\end_inset

 is a function of 
\begin_inset Formula $X,$
\end_inset

 it is nonstochastic, too.
 If the estimator is unbiased, then we must have 
\begin_inset Formula $WX=I_{K}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(Wy) & = & \mathcal{E}(WX\beta_{0}+W\varepsilon)\\
 & = & WX\beta_{0}\\
 & = & \beta_{0}\\
 & \Rightarrow\\
WX & = & I_{K}
\end{eqnarray*}

\end_inset

 The variance of 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 is 
\begin_inset Formula 
\[
V(\tilde{\beta})=WW^{\prime}\sigma_{0}^{2}.
\]

\end_inset

 Define 
\begin_inset Formula 
\[
D=W-(X^{\prime}X)^{-1}X^{\prime}
\]

\end_inset

 so 
\begin_inset Formula 
\[
W=D+(X^{\prime}X)^{-1}X^{\prime}
\]

\end_inset

 Since 
\begin_inset Formula $WX=I_{K},$
\end_inset

 
\begin_inset Formula $DX=0,$
\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
V(\tilde{\beta}) & = & \left(D+(X^{\prime}X)^{-1}X^{\prime}\right)\left(D+(X^{\prime}X)^{-1}X^{\prime}\right)^{\prime}\sigma_{0}^{2}\\
 & = & \left(DD^{\prime}+\left(X^{\prime}X\right)^{-1}\right)\sigma_{0}^{2}
\end{eqnarray*}

\end_inset

 So 
\begin_inset Formula 
\[
V(\tilde{\beta})\geq V(\hat{\beta})
\]

\end_inset

 The inequality is a shorthand means of expressing, more formally, that
 
\begin_inset Formula $V(\tilde{\beta})-V(\hat{\beta})$
\end_inset

 is a positive semi-definite matrix.
 This is a proof of the Gauss-Markov Theorem.
 The OLS estimator is the 
\begin_inset Quotes sld
\end_inset

best linear unbiased estimator
\begin_inset Quotes srd
\end_inset

 (BLUE).
\end_layout

\begin_layout Itemize
It is worth emphasizing again that we have not used the normality assumption
 in any way to prove the Gauss-Markov theorem, so it is valid if the errors
 are not normally distributed, as long as the other assumptions hold.
 
\end_layout

\begin_layout Standard
To illustrate the Gauss-Markov result, consider the estimator that results
 from splitting the sample into 
\begin_inset Formula $p$
\end_inset

 equally-sized parts, estimating using each part of the data separately
 by OLS, then averaging the 
\begin_inset Formula $p$
\end_inset

 resulting estimators.
 You should be able to show that this estimator is unbiased, but inefficient
 with respect to the OLS estimator.
 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Efficiency.jl}{https://github.com/mcreel/Econometrics/blob/maste
r/Examples/OLS/Efficiency.jl} 
\end_layout

\end_inset

 illustrates this using a small Monte Carlo experiment, which compares the
 OLS estimator and a 3-way split sample estimator.
 The data generating process follows the classical model, with 
\begin_inset Formula $n=21$
\end_inset

.
 The true parameter value is 
\begin_inset Formula $\beta=2.$
\end_inset

 In Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Gauss-Markov-Result: OLS"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Gauss-Markov-Result: split sample"

\end_inset

 we can see that the OLS estimator is more efficient, since the tails of
 its histogram are more narrow.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Gauss-Markov-Result: OLS"

\end_inset

Gauss-Markov Result: The OLS estimator
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/efficiency-1.svg
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Gauss-Markov Resul
\begin_inset CommandInset label
LatexCommand label
name "cap:Gauss-Markov-Result: split sample"

\end_inset

: The split sample estimator
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/OLS/efficiency-2.svg
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We have that 
\begin_inset Formula $E(\hat{\beta})=\beta$
\end_inset

 and 
\begin_inset Formula $Var(\hat{\beta})=\left(X^{'}X\right)^{-1}\sigma_{0}^{2},$
\end_inset

 but we still need to estimate the variance of 
\begin_inset Formula $\epsilon$
\end_inset

, 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

, in order to have an idea of the precision of the estimates of 
\begin_inset Formula $\beta$
\end_inset

.
 A commonly used estimator of 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\sigma_{0}^{2}}=\frac{1}{n-K}\hat{\varepsilon}^{\prime}\hat{\varepsilon}
\]

\end_inset

This estimator is unbiased:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\widehat{\sigma_{0}^{2}} & = & \frac{1}{n-K}\hat{\varepsilon}^{\prime}\hat{\varepsilon}\\
 & = & \frac{1}{n-K}\varepsilon^{\prime}M\varepsilon\\
\mathcal{E}(\widehat{\sigma_{0}^{2}}) & = & \frac{1}{n-K}E(Tr\varepsilon^{\prime}M\varepsilon)\\
 & = & \frac{1}{n-K}E(TrM\varepsilon\varepsilon^{\prime})\\
 & = & \frac{1}{n-K}TrE(M\varepsilon\varepsilon^{\prime})\\
 & = & \frac{1}{n-K}\sigma_{0}^{2}TrM\\
 & = & \frac{1}{n-K}\sigma_{0}^{2}\left(n-k\right)\\
 & = & \sigma_{0}^{2}
\end{eqnarray*}

\end_inset

where we use the fact that 
\begin_inset Formula $Tr(AB)=Tr(BA)$
\end_inset

 when both products are conformable.
 Thus, this estimator is also unbiased under these assumptions.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example:-The-Nerlove"

\end_inset

Example: The Nerlove model 
\end_layout

\begin_layout Subsection
Theoretical background
\end_layout

\begin_layout Standard
For a firm that takes input prices 
\begin_inset Formula $w$
\end_inset

 and the output level 
\begin_inset Formula $q$
\end_inset

 as given, the cost minimization problem is to choose the quantities of
 inputs 
\begin_inset Formula $x$
\end_inset

 to solve the problem
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{x}w'x
\]

\end_inset

 subject to the restriction
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x)=q.
\]

\end_inset

 The solution is the vector of factor demands 
\begin_inset Formula $x(w,q)$
\end_inset

.
 The 
\emph on
cost function
\emph default
 is obtained by substituting the factor demands into the criterion function:
 
\begin_inset Formula 
\[
Cw,q)=w'x(w,q).
\]

\end_inset

 
\end_layout

\begin_layout Itemize

\series bold
Monotonicity
\series default
 Increasing factor prices cannot decrease cost, so 
\begin_inset Formula 
\[
\frac{\partial C(w,q)}{\partial w}\geq0
\]

\end_inset

Remember that these derivatives give the conditional factor demands (Shephard's
 Lemma).
\end_layout

\begin_layout Itemize

\series bold
Homogeneity
\series default
 The cost function is homogeneous of degree 1 in input prices: 
\begin_inset Formula $C(tw,q)=tC(w,q)$
\end_inset

 where 
\begin_inset Formula $t$
\end_inset

 is a scalar constant.
 This is because the factor demands are homogeneous of degree zero in factor
 prices - they only depend upon relative prices.
\end_layout

\begin_layout Itemize

\series bold
Returns to scale
\series default
 The 
\emph on
returns to scale
\emph default
 parameter 
\begin_inset Formula $\gamma$
\end_inset

 is defined as the inverse of the elasticity of cost with respect to output:
\begin_inset Formula 
\[
\gamma=\left(\frac{\partial C(w,q)}{\partial q}\frac{q}{C(w,q)}\right)^{-1}
\]

\end_inset


\emph on
Constant returns to scale
\emph default
 is the case where increasing production 
\begin_inset Formula $q$
\end_inset

 implies that cost increases in the proportion 1:1.
 If this is the case, then 
\begin_inset Formula $\gamma=1$
\end_inset

.
\end_layout

\begin_layout Subsection
Cobb-Douglas functional form
\end_layout

\begin_layout Standard
The Cobb-Douglas functional form is linear in the logarithms of the regressors
 and the dependent variable.
 For a cost function, if there are 
\begin_inset Formula $g$
\end_inset

 factors, the Cobb-Douglas cost function has the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C=Aw_{1}^{\beta_{1}}...w_{g}^{\beta_{g}}q^{\beta_{q}}e^{\varepsilon}
\]

\end_inset

What is the elasticity of 
\begin_inset Formula $C$
\end_inset

 with respect to 
\begin_inset Formula $w_{j}$
\end_inset

?
\begin_inset Formula 
\begin{eqnarray*}
e_{w_{j}}^{C} & = & \left(\frac{\partial C}{\partial_{W_{J}}}\right)\left(\frac{w_{j}}{C}\right)\\
 & = & \beta_{j}Aw_{1}^{\beta_{1}}.w_{j}^{\beta_{j}-1}..w_{g}^{\beta_{g}}q^{\beta_{q}}e^{\varepsilon}\frac{w_{j}}{Aw_{1}^{\beta_{1}}...w_{g}^{\beta_{g}}q^{\beta_{q}}e^{\varepsilon}}\\
 & = & \beta_{j}
\end{eqnarray*}

\end_inset

This is one of the reasons the Cobb-Douglas form is popular - the coefficients
 are easy to interpret, since they are the elasticities of the dependent
 variable with respect to the explanatory variable.
 Not that in this case,
\begin_inset Formula 
\begin{eqnarray*}
e_{w_{j}}^{C} & = & \left(\frac{\partial C}{\partial_{W_{J}}}\right)\left(\frac{w_{j}}{C}\right)\\
 & = & x_{j}(w,q)\frac{w_{j}}{C}\\
 & \equiv & s_{j}(w,q)
\end{eqnarray*}

\end_inset

the 
\emph on
cost share
\emph default
 of the 
\begin_inset Formula $j^{th}$
\end_inset

 input.
 So with a Cobb-Douglas cost function, 
\begin_inset Formula $\beta_{j}=s_{j}(w,q)$
\end_inset

.
 The cost shares are constants.
\end_layout

\begin_layout Standard
Note that after a logarithmic transformation we obtain
\begin_inset Formula 
\[
\ln C=\alpha+\beta_{1}\ln w_{1}+...+\beta_{g}\ln w_{g}+\beta_{q}\ln q+\epsilon
\]

\end_inset

where 
\begin_inset Formula $\alpha=\ln A$
\end_inset

 .
 So we see that the transformed model is linear in the logs of the data.
\end_layout

\begin_layout Standard
One can verify that the property of HOD1 implies that 
\begin_inset Formula 
\[
\sum_{i=1}^{g}\beta_{i}=1
\]

\end_inset

In other words, the cost shares add up to 1.
 
\end_layout

\begin_layout Standard
The hypothesis that the technology exhibits CRTS implies that 
\begin_inset Formula 
\[
\gamma=\frac{1}{\beta_{q}}=1
\]

\end_inset

so 
\begin_inset Formula $\beta_{q}=1.$
\end_inset

 Likewise, monotonicity implies that the coefficients 
\begin_inset Formula $\beta_{i}\geq0,i=1,...,g$
\end_inset

.
\end_layout

\begin_layout Subsection
The Nerlove data and OLS
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-Nerlove-data"

\end_inset


\end_layout

\begin_layout Standard
The file 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{nerlove.data}{https://github.com/mcreel/Econometrics/blob/master
/Examples/Data/nerlove.data} 
\end_layout

\end_inset

 contains data on 145 electric utility companies' cost of production, output
 and input prices.
 The data are for the U.S., and were collected by M.
 Nerlove.
 The observations are by row, and the columns are 
\series bold
COMPANY, COST
\series default
 
\begin_inset Formula $(C)$
\end_inset


\series bold
, OUTPUT
\series default
 
\begin_inset Formula $(Q),$
\end_inset

 
\series bold
PRICE
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

OF LABOR
\series default
 
\begin_inset Formula $(P_{L})$
\end_inset

, 
\series bold
PRICE OF
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

FUEL
\series default
 
\begin_inset Formula $(P_{F})$
\end_inset

 and 
\series bold
PRICE OF
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

CAPITAL
\series default
 
\begin_inset Formula $(P_{K}).$
\end_inset

 Note that the data are sorted by output level (the third column).
\end_layout

\begin_layout Standard
We will estimate the Cobb-Douglas model 
\begin_inset Formula 
\begin{equation}
\ln C=\beta_{1}+\beta_{Q}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon\label{simple nerlove model}
\end{equation}

\end_inset

 by OLS, using the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Nerlove.jl}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/OLS/Nerlove.jl} 
\end_layout

\end_inset

, which uses 
\begin_inset CommandInset href
LatexCommand href
name "ols.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/LinearRegression/ols.jl"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\paragraph_spacing single
The results are 
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/OLS/nerlove.out"

\end_inset


\end_layout

\begin_layout Itemize
Do the theoretical restrictions hold?
\end_layout

\begin_layout Itemize
Does the model fit well?
\end_layout

\begin_layout Itemize
What do you think about RTS?
\end_layout

\begin_layout Standard
We will most often use Julia programs that more or less directly implement
 the theory we learn in examples in this document.
 This is because following such transparent programming statements is a
 useful way of learning how theory is put into practice.
 Nevertheless, you may be interested in a more 
\begin_inset Quotes sld
\end_inset

user-friendly
\begin_inset Quotes srd
\end_inset

 environment for doing econometrics, especially after you have mastered
 the theory.
 Julia itself offers packages such as DataFrames.jl and GLM.jl which will
 allow you to avoid some of the nuts and bolts of econometric modeling.
 For example, see 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{NerloveDF.jl}{https://github.com/mcreel/Econometrics/blob/master
/Examples/OLS/NerloveDF.jl} 
\end_layout

\end_inset

 for estimating the Nerlove model using these packages.
 If you run that, you will see that the estimated standard errors differ
 from what Nerlove.jl reports, we will get to the reason for that later.
 For a 
\begin_inset Quotes sld
\end_inset

canned
\begin_inset Quotes srd
\end_inset

 package, apart from what Julia offers, I heartily recommend 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{Gretl}{http://gretl.sourceforge.net}
\end_layout

\end_inset

, the GNU Regression, Econometrics, and Time-Series Library.
 Gretl is free software.
 This is an easy to use program, available in English, French, and Spanish,
 and it comes with a lot of data ready to use.
 It even has an option to save output as \SpecialChar LaTeX
 fragments, so that I can just
 include the results into this document, no muss, no fuss.
 Here is the Nerlove data in the form of a GRETL data set: 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{nerlove.gdt}{https://github.com/mcreel/Econometrics/blob/master/
Examples/Data/nerlove.gdt} 
\end_layout

\end_inset

.
 Here the results of the Nerlove model from GRETL: 
\begin_inset CommandInset include
LatexCommand input
filename "Examples/OLS/NerloveGretl.tex"

\end_inset

 Fortunately, Gretl and my OLS program agree upon the results.
 I recommend using GRETL to repeat the examples that are done using Julia.
 
\end_layout

\begin_layout Standard
The previous properties hold for finite sample sizes.
 Before considering the asymptotic properties of the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator it is useful to review the MLE estimator, since under the assumption
 of normal errors the two estimators coincide.
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Prove that the split sample estimator used to generate figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Gauss-Markov-Result: split sample"

\end_inset

 is unbiased.
\end_layout

\begin_layout Enumerate
Calculate the OLS estimates of the Nerlove model using Julia and GRETL,
 and provide printouts of the results.
 Interpret the results.
 
\end_layout

\begin_layout Enumerate
Do an analysis of whether or not there are influential observations for
 OLS estimation of the Nerlove model.
 Discuss.
\end_layout

\begin_layout Enumerate
Using GRETL, examine the residuals after OLS estimation and tell me whether
 or not you believe that the assumption of independent identically distributed
 normal errors is warranted.
 No need to do formal tests, just look at the plots.
 Print out any that you think are relevant, and interpret them.
\end_layout

\begin_layout Enumerate
For a random vector 
\begin_inset Formula $X\sim N(\mu_{x},\Sigma),$
\end_inset

 what is the distribution of 
\begin_inset Formula $AX+b$
\end_inset

, where 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are conformable matrices of constants?
\end_layout

\begin_layout Enumerate
Using Julia, write a little program that verifies that 
\begin_inset Formula $Tr(AB)=Tr(BA)$
\end_inset

 for 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 4x4 matrices of random numbers.
 Note: there is a Julia function 
\family typewriter
trace().
\end_layout

\begin_layout Enumerate
For the model with a constant and a single regressor, 
\begin_inset Formula $y_{t}=\beta_{1}+\beta_{2}x_{t}+\epsilon_{t}$
\end_inset

, which satisfies the classical assumptions, prove that the variance of
 the OLS estimator declines to zero as the sample size increases.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Asymptotic properties of the least squares estimator
\end_layout

\begin_layout Standard
The OLS estimator under the classical assumptions is BLUE
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
BLUE 
\begin_inset Formula $\equiv$
\end_inset

 best linear unbiased estimator if I haven't defined it before
\end_layout

\end_inset

, for all sample sizes.
 Now let's see what happens when the sample size tends to infinity.
 
\end_layout

\begin_layout Section
Consistency
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & (X^{\prime}X)^{-1}X^{\prime}y\\
 & = & (X^{\prime}X)^{-1}X^{\prime}\left(X\beta+\varepsilon\right)\\
 & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & \beta_{0}+\left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{n}
\end{eqnarray*}

\end_inset

 Consider the last two terms.
 By assumption 
\begin_inset Formula $\lim_{n\rightarrow\infty}\left(\frac{X^{\prime}X}{n}\right)=Q_{X}\Rightarrow\lim_{n\rightarrow\infty}\left(\frac{X^{\prime}X}{n}\right)^{-1}=Q_{X}^{-1},$
\end_inset

 since the inverse of a nonsingular matrix is a continuous function of the
 elements of the matrix.
 Considering 
\begin_inset Formula $\frac{X^{\prime}\varepsilon}{n},$
\end_inset


\begin_inset Formula 
\[
\frac{X^{\prime}\varepsilon}{n}=\frac{1}{n}\sum_{t=1}^{n}x_{t}\varepsilon_{t}
\]

\end_inset

 Each 
\begin_inset Formula $x_{t}\varepsilon_{t}$
\end_inset

 has expectation zero, so 
\begin_inset Formula 
\[
E\left(\frac{X^{\prime}\varepsilon}{n}\right)=0
\]

\end_inset

The variance of each term is
\begin_inset Formula 
\begin{eqnarray*}
V\left(x_{t}\epsilon_{t}\right) & = & x_{t}x_{t}^{\prime}\sigma^{2}.
\end{eqnarray*}

\end_inset

As long as these are finite, and given a technical condition
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
For application of LLN's and CLT's, of which there are very many to choose
 from, I'm going to avoid the technicalities.
 Basically, as long as terms that make up an average have finite variances
 and are not too strongly dependent, one will be able to find a LLN or CLT
 to apply.
 Which one it is doesn't matter, we only need the result.
 When working with particular models, it will be more relevant to consider
 which particular theorems will apply.
\end_layout

\end_inset

, the Kolmogorov SLLN applies, so
\begin_inset Formula 
\[
\frac{1}{n}\sum_{t=1}^{n}x_{t}\varepsilon_{t}\overset{a.s.}{\rightarrow}0.
\]

\end_inset

 This implies that 
\begin_inset Formula 
\[
\hat{\beta}\overset{a.s.}{\rightarrow}\beta_{0}.
\]

\end_inset

 This is the property of 
\emph on
strong consistency:
\emph default
 the estimator converges in almost surely to the true value.
\end_layout

\begin_layout Itemize
The consistency proof does not use the normality assumption.
 
\end_layout

\begin_layout Itemize
Remember that almost sure convergence implies convergence in probability.
\end_layout

\begin_layout Section
Asymptotic normality
\end_layout

\begin_layout Standard
We've seen that the OLS estimator is normally distributed 
\emph on
under the assumption of normal errors.

\emph default
 If the error distribution is unknown, we of course don't know the distribution
 of the estimator.
 However, we can get asymptotic results.
 
\emph on
Assuming the distribution of
\emph default
 
\begin_inset Formula $\varepsilon$
\end_inset

 is unknown, but the the other classical assumptions hold:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
\hat{\beta}-\beta_{0} & = & (X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right) & = & \left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{\sqrt{n}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Now as before, 
\begin_inset Formula $\left(\frac{X^{\prime}X}{n}\right)^{-1}\rightarrow Q_{X}^{-1}.$
\end_inset


\end_layout

\begin_layout Itemize
Considering 
\begin_inset Formula $\frac{X^{\prime}\varepsilon}{\sqrt{n}},$
\end_inset

 the limit of the variance is 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}V\left(\frac{X^{\prime}\varepsilon}{\sqrt{n}}\right) & = & \lim_{n\rightarrow\infty}E\left(\frac{X^{\prime}\epsilon\epsilon^{\prime}X}{n}\right)\\
 & = & \sigma_{0}^{2}Q_{X}
\end{eqnarray*}

\end_inset

 The mean is of course zero.
 To get asymptotic normality, we need to apply a CLT.
 The best known CLTs are for averages of IID terms, but CLTs exist for averages
 of dependent, non-identically distributed terms, too.
 The basic requirement is that variances of the terms in the average must
 not explode, and the terms in the average can not be too highly dependent.
 Without getting into the technical details, which are appropriate to discuss
 when working with some particular type of data, we assume one holds, so
\begin_inset Formula 
\[
\frac{X^{\prime}\varepsilon}{\sqrt{n}}\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}Q_{X}\right)
\]

\end_inset

 Therefore, 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}Q_{X}^{-1}\right)\label{eq:asymp normality OLS}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
In summary, the OLS estimator is normally distributed in small and large
 samples if 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed.
 If 
\begin_inset Formula $\varepsilon$
\end_inset

 is not normally distributed, 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is asymptotically normally distributed when a CLT can be applied.
\end_layout

\begin_layout Section
Asymptotic efficiency
\begin_inset CommandInset label
LatexCommand label
name "sec:Asymptotic-efficiency"

\end_inset


\end_layout

\begin_layout Standard
The least squares objective function is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
s(\beta) & = & \sum_{t=1}^{n}\left(y_{t}-x_{t}^{\prime}\beta\right)^{2}
\end{eqnarray*}

\end_inset

 Supposing that 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed, the model is 
\begin_inset Formula 
\[
y=X\beta_{0}+\varepsilon,
\]

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\varepsilon & \sim & N(0,\sigma_{0}^{2}I_{n}),\textrm{ so}\\
f(\varepsilon) & = & \prod_{t=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{\varepsilon_{t}^{2}}{2\sigma^{2}}\right)
\end{eqnarray*}

\end_inset

 The joint density for 
\begin_inset Formula $y$
\end_inset

 can be constructed using a change of variables.
 We have 
\begin_inset Formula $\varepsilon=y-X\beta,$
\end_inset

 so 
\begin_inset Formula $\frac{\partial\varepsilon}{\partial y^{\prime}}=I_{n}$
\end_inset

 and 
\begin_inset Formula $|\frac{\partial\varepsilon}{\partial y^{\prime}}|=1,$
\end_inset

 so 
\begin_inset Formula 
\[
f(y)=\prod_{t=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(y_{t}-x_{t}^{\prime}\beta)^{2}}{2\sigma^{2}}\right).
\]

\end_inset

 Taking logs, 
\begin_inset Formula 
\[
\ln L(\beta,\sigma)=-n\ln\sqrt{2\pi}-n\ln\sigma-\sum_{t=1}^{n}\frac{\left(y_{t}-x_{t}'\beta\right)^{2}}{2\sigma^{2}}.
\]

\end_inset

 Maximizing this function with respect to 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 gives what is known as the maximum likelihood (ML) estimator.
 It turns out that ML estimators are asymptotically efficient, a concept
 that will be explained in detail later.
 It's clear that the first order conditions for the MLE of 
\begin_inset Formula $\beta_{0}$
\end_inset

 are the same as the first order conditions that define the OLS estimator
 (up to multiplication by a constant), so the OLS estimator of 
\begin_inset Formula $\beta$
\end_inset

 is also the ML estimator.
 
\emph on
The estimators are the same, under the present assumptions.

\emph default
 Therefore, their properties are the same.
 
\emph on
In particular, under the classical assumptions with normality, the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator
\emph default
 
\begin_inset Formula $\hat{\beta}$
\end_inset

 
\emph on
is asymptotically efficient.
 
\emph default
Note that one needs to make an assumption about the distribution of the
 errors to compute the ML estimator.
 If the errors had a distribution other than the normal, then the OLS estimator
 and the ML estimator would not coincide.
\end_layout

\begin_layout Standard
As we'll see later, it will be possible to use (iterated) linear estimation
 methods and still achieve asymptotic efficiency even if the assumption
 that 
\begin_inset Formula $Var(\varepsilon)\neq\sigma^{2}I_{n},$
\end_inset

 as long as 
\begin_inset Formula $\varepsilon$
\end_inset

 is still normally distributed.
 This is 
\series bold
not
\series default
 the case if 
\begin_inset Formula $\varepsilon$
\end_inset

 is nonnormal.
 In general with nonnormal errors it will be necessary to use nonlinear
 estimation methods to achieve asymptotically efficient estimation.
 
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Write an Octave program that generates a histogram for 
\begin_inset Formula $R$
\end_inset

 Monte Carlo replications of 
\begin_inset Formula $\sqrt{n}\left(\hat{\beta}_{j}-\beta_{j}\right)$
\end_inset

, where 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is the OLS estimator and 
\begin_inset Formula $\beta_{j}$
\end_inset

 is one of the 
\begin_inset Formula $k$
\end_inset

 slope parameters.
 
\begin_inset Formula $R$
\end_inset

 should be a large number, at least 1000.
 The model used to generate data should follow the classical assumptions,
 except that the errors should not be normally distributed (try 
\begin_inset Formula $U(-a,a)$
\end_inset

, 
\begin_inset Formula $t(p)$
\end_inset

, 
\begin_inset Formula $\chi^{2}(p)-p,$
\end_inset

 etc).
 Generate histograms for 
\begin_inset Formula $n\in\left\{ 20,50,100,1000\right\} $
\end_inset

.
 Do you observe evidence of asymptotic normality? Comment.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

Consider the following regression model:
\end_layout

\begin_layout Plain Layout


\backslash
begin{equation*} y_{i}=
\backslash
beta _{1}x_{1i}+
\backslash
beta _{2}x_{2i}+u_{i} 
\backslash
end{equation*}
\end_layout

\begin_layout Plain Layout

where $E(x_{1i})=0,E(x_{2i})=0,Var(x_{1i})=
\backslash
sigma _{1}^{2},Var(x_{2i})=
\backslash
sigma _{2}^{2}$ 
\backslash
 and $Cov(x_{1i},x_{2i}) $=$
\backslash
sigma _{12}.$ The model satisfies the basic OLS
\backslash
 assumptions.
 However, a loose econometrician estimates the following model:
\end_layout

\begin_layout Plain Layout


\backslash
begin{equation*} y_{i}=
\backslash
beta _{1}x_{1i}+v_{i} 
\backslash
end{equation*}
\end_layout

\begin_layout Plain Layout

where $
\backslash
tilde{
\backslash
beta}_{1}$ is the estimator of the parameter $
\backslash
beta _{1}.$
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

a) Compute $plim(
\backslash
tilde{
\backslash
beta}_{1})$.
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

b) Compute the asymptotic bias of $
\backslash
tilde{
\backslash
beta}_{1}$ (i.e.
  $plim(
\backslash
tilde{
\backslash
beta}_{1})-
\backslash
beta _{1}$).
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

c) Under which conditions is $
\backslash
tilde{
\backslash
beta}_{1}$ a consistent estimate of $
\backslash
beta _{1}$?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Restrictions and hypothesis tests
\end_layout

\begin_layout Section
Exact linear restrictions
\end_layout

\begin_layout Standard
In many cases, economic theory suggests restrictions on the parameters of
 a model.
 For example, a demand function is supposed to be homogeneous of degree
 zero in prices and income.
 If we have a Cobb-Douglas (log-linear) model, 
\begin_inset Formula 
\[
\ln q=\beta_{0}+\beta_{1}\ln p_{1}+\beta_{2}\ln p_{2}+\beta_{3}\ln m+\varepsilon,
\]

\end_inset

 then we need that 
\begin_inset Formula 
\[
k^{0}\ln q=\beta_{0}+\beta_{1}\ln kp_{1}+\beta_{2}\ln kp_{2}+\beta_{3}\ln km+\varepsilon,
\]

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\beta_{1}\ln p_{1}+\beta_{2}\ln p_{2}+\beta_{3}\ln m & = & \beta_{1}\ln kp_{1}+\beta_{2}\ln kp_{2}+\beta_{3}\ln km\\
 & = & \left(\ln k\right)(\beta_{1}+\beta_{2}+\beta_{3})+\beta_{1}\ln p_{1}+\beta_{2}\ln p_{2}+\beta_{3}\ln m.
\end{eqnarray*}

\end_inset

 The only way to guarantee this for arbitrary 
\begin_inset Formula $k$
\end_inset

 is to set 
\begin_inset Formula 
\[
\beta_{1}+\beta_{2}+\beta_{3}=0,
\]

\end_inset

 which is a 
\emph on
parameter restriction.

\emph default
 In particular, this is a linear equality restriction, which is probably
 the most commonly encountered case.
\end_layout

\begin_layout Subsection
Imposition
\end_layout

\begin_layout Standard
The general formulation of linear equality restrictions is the model 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
R\beta & = & r
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $R$
\end_inset

 is a 
\begin_inset Formula $Q\times K$
\end_inset

 matrix, 
\begin_inset Formula $Q<K$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 is a 
\begin_inset Formula $Q\times1$
\end_inset

 vector of constants.
\end_layout

\begin_layout Itemize
We assume 
\begin_inset Formula $R$
\end_inset

 is of rank 
\begin_inset Formula $Q,$
\end_inset

 so that there are no redundant restrictions.
\end_layout

\begin_layout Itemize
We also assume that 
\begin_inset Formula $\exists\beta$
\end_inset

 that satisfies the restrictions: they aren't infeasible.
 
\end_layout

\begin_layout Standard
Let's consider how to estimate 
\begin_inset Formula $\beta$
\end_inset

 subject to the restrictions 
\begin_inset Formula $R\beta=r.$
\end_inset

 The most obvious approach is to set up the Lagrangean 
\begin_inset Formula 
\[
\min_{\beta,\lambda}s(\beta,\lambda)=\frac{1}{n}\left(y-X\beta\right)^{\prime}\left(y-X\beta\right)+2\lambda^{\prime}(R\beta-r).
\]

\end_inset

 The Lagrange multipliers are scaled by 2, which makes things less messy.
 The fonc are 
\begin_inset Formula 
\begin{eqnarray*}
D_{\beta}s(\hat{\beta},\hat{\lambda}) & = & -2X^{\prime}y+2X^{\prime}X\hat{\beta}_{R}+2R^{\prime}\hat{\lambda}\equiv0\\
D_{\lambda}s(\hat{\beta},\hat{\lambda}) & = & R\hat{\beta}_{R}-r\equiv0,
\end{eqnarray*}

\end_inset

 which can be written as 
\begin_inset Formula 
\[
\left[\begin{array}{cc}
X^{\prime}X & R^{\prime}\\
R & 0
\end{array}\right]\left[\begin{array}{c}
\hat{\beta}_{R}\\
\hat{\lambda}
\end{array}\right]=\left[\begin{array}{c}
X^{\prime}y\\
r
\end{array}\right].
\]

\end_inset

 We get 
\begin_inset Formula 
\[
\left[\begin{array}{c}
\hat{\beta}_{R}\\
\hat{\lambda}
\end{array}\right]=\left[\begin{array}{cc}
X^{\prime}X & R^{\prime}\\
R & 0
\end{array}\right]^{-1}\left[\begin{array}{c}
X^{\prime}y\\
r
\end{array}\right].
\]

\end_inset

 Maybe you're curious about how to invert a partitioned matrix? I can help
 you with that:
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1} & 0\\
-R\left(X^{\prime}X\right)^{-1} & I_{Q}
\end{array}\right]\left[\begin{array}{cc}
X^{\prime}X & R^{\prime}\\
R & 0
\end{array}\right] & \equiv & AB\\
 & = & \left[\begin{array}{cc}
I_{K} & \left(X^{\prime}X\right)^{-1}R^{\prime}\\
0 & -R\left(X^{\prime}X\right)^{-1}R^{\prime}
\end{array}\right]\\
 & \equiv & \left[\begin{array}{cc}
I_{K} & \left(X^{\prime}X\right)^{-1}R^{\prime}\\
0 & -P
\end{array}\right]\\
 & \equiv & C,
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{cc}
I_{K} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
0 & -P^{-1}
\end{array}\right]\left[\begin{array}{cc}
I_{K} & \left(X^{\prime}X\right)^{-1}R^{\prime}\\
0 & -P
\end{array}\right] & \equiv & DC\\
 & = & I_{K+Q},
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
DAB & = & I_{K+Q}\\
DA & = & B^{-1}\\
B^{-1} & = & \left[\begin{array}{cc}
I_{K} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
0 & -P^{-1}
\end{array}\right]\left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1} & 0\\
-R\left(X^{\prime}X\right)^{-1} & I_{Q}
\end{array}\right]\\
 & = & \left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R\left(X^{\prime}X\right)^{-1} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
P^{-1}R\left(X^{\prime}X\right)^{-1} & -P^{-1}
\end{array}\right],
\end{eqnarray*}

\end_inset

If you weren't curious about that, please start paying attention again.
 Also, note that we have made the definition 
\begin_inset Formula $P=R\left(X^{\prime}X\right)^{-1}R^{\prime}$
\end_inset

)
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{c}
\hat{\beta}_{R}\\
\hat{\lambda}
\end{array}\right] & = & \left[\begin{array}{cc}
\left(X^{\prime}X\right)^{-1}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R\left(X^{\prime}X\right)^{-1} & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\\
P^{-1}R\left(X^{\prime}X\right)^{-1} & -P^{-1}
\end{array}\right]\left[\begin{array}{c}
X^{\prime}y\\
r
\end{array}\right]\\
 & = & \left[\begin{array}{c}
\hat{\beta}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}\left(R\hat{\beta}-r\right)\\
P^{-1}\left(R\hat{\beta}-r\right)
\end{array}\right]\\
 & = & \left[\begin{array}{c}
\left(I_{K}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R\right)\\
P^{-1}R
\end{array}\right]\hat{\beta}+\left[\begin{array}{c}
(X^{\prime}X)^{-1}R^{\prime}P^{-1}r\\
-P^{-1}r
\end{array}\right]
\end{eqnarray*}

\end_inset

 The fact that 
\begin_inset Formula $\hat{\beta}_{R}$
\end_inset

 and 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 are linear functions of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 makes it easy to determine their distributions, since the distribution
 of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is already known.
 Recall that for 
\begin_inset Formula $x$
\end_inset

 a random vector, and for 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 a matrix and vector of constants, respectively, 
\begin_inset Formula $Var\left(Ax+b\right)=AVar(x)A^{\prime}.$
\end_inset


\end_layout

\begin_layout Standard
Though this is the obvious way to go about finding the restricted estimator,
 an easier way, if the number of restrictions is small, is to impose them
 by substitution.
 Write 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X_{1}\beta_{1}+X_{2}\beta_{2}+\varepsilon\\
\left[\begin{array}{cc}
R_{1} & R_{2}\end{array}\right]\left[\begin{array}{c}
\beta_{1}\\
\beta_{2}
\end{array}\right] & = & r
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $R_{1}$
\end_inset

 is 
\begin_inset Formula $Q\times Q$
\end_inset

 nonsingular.
 Supposing the 
\begin_inset Formula $Q$
\end_inset

 restrictions are linearly independent, one can always make 
\begin_inset Formula $R_{1}$
\end_inset

 nonsingular by reorganizing the columns of 
\begin_inset Formula $X.$
\end_inset

 Then 
\begin_inset Formula 
\[
\beta_{1}=R_{1}^{-1}r-R_{1}^{-1}R_{2}\beta_{2}.
\]

\end_inset

 Substitute this into the model 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X_{1}R_{1}^{-1}r-X_{1}R_{1}^{-1}R_{2}\beta_{2}+X_{2}\beta_{2}+\varepsilon\\
y-X_{1}R_{1}^{-1}r & = & \left[X_{2}-X_{1}R_{1}^{-1}R_{2}\right]\beta_{2}+\varepsilon
\end{eqnarray*}

\end_inset

 or with the appropriate definitions, 
\begin_inset Formula 
\[
y_{R}=X_{R}\beta_{2}+\varepsilon.
\]

\end_inset

 This model satisfies the classical assumptions, 
\emph on
supposing the restriction is true
\emph default
.
 One can estimate by OLS.
 The variance of 
\begin_inset Formula $\hat{\beta}_{2}$
\end_inset

 is as before 
\begin_inset Formula 
\[
V(\hat{\beta}_{2})=\left(X_{R}^{\prime}X_{R}\right)^{-1}\sigma_{0}^{2}
\]

\end_inset

 and the estimator is 
\begin_inset Formula 
\[
\hat{V}(\hat{\beta}_{2})=\left(X_{R}^{\prime}X_{R}\right)^{-1}\hat{\sigma}^{2}
\]

\end_inset

 where one estimates 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 in the normal way, using the restricted model, 
\emph on
i.e.,
\emph default

\begin_inset Formula 
\[
\widehat{\sigma_{0}^{2}}=\frac{\left(y_{R}-X_{R}\widehat{\beta_{2}}\right)^{\prime}\left(y_{R}-X_{R}\widehat{\beta_{2}}\right)}{n-\left(K-Q\right)}
\]

\end_inset

 To recover 
\begin_inset Formula $\hat{\beta}_{1},$
\end_inset

 use the restriction.
 To find the variance of 
\begin_inset Formula $\hat{\beta}_{1},$
\end_inset

 use the fact that it is a linear function of 
\begin_inset Formula $\hat{\beta}_{2},$
\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
V(\hat{\beta}_{1}) & = & R_{1}^{-1}R_{2}V(\hat{\beta}_{2})R_{2}^{\prime}\left(R_{1}^{-1}\right)^{\prime}\\
 & = & R_{1}^{-1}R_{2}\left(X_{2}^{\prime}X_{2}\right)^{-1}R_{2}^{\prime}\left(R_{1}^{-1}\right)^{\prime}\sigma_{0}^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Properties of the restricted estimator
\end_layout

\begin_layout Standard
We have that 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{R} & = & \hat{\beta}-(X^{\prime}X)^{-1}R^{\prime}P^{-1}\left(R\hat{\beta}-r\right)\\
 & = & \hat{\beta}+(X^{\prime}X)^{-1}R^{\prime}P^{-1}r-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}X^{\prime}y\\
 & = & \beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon+(X^{\prime}X)^{-1}R^{\prime}P^{-1}\left[r-R\beta\right]-(X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
\hat{\beta}_{R}-\beta & = & (X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & + & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\left[r-R\beta\right]\\
 & - & (X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Mean squared error is 
\begin_inset Formula 
\[
MSE(\hat{\beta}_{R})=\mathcal{E}(\hat{\beta}_{R}-\beta)(\hat{\beta}_{R}-\beta)^{\prime}
\]

\end_inset

 Noting that the crosses between the second term and the other terms expect
 to zero, and that the cross of the first and third has a cancellation with
 the square of the third, we obtain 
\begin_inset Formula 
\begin{eqnarray*}
MSE(\hat{\beta}_{R}) & = & (X^{\prime}X)^{-1}\sigma^{2}\\
 & + & (X^{\prime}X)^{-1}R^{\prime}P^{-1}\left[r-R\beta\right]\left[r-R\beta\right]^{\prime}P^{-1}R(X^{\prime}X)^{-1}\\
 & - & (X^{\prime}X)^{-1}R^{\prime}P^{-1}R(X^{\prime}X)^{-1}\sigma^{2}
\end{eqnarray*}

\end_inset

 So, the first term is the OLS covariance.
 The second term is PSD, and the third term is NSD.
\end_layout

\begin_layout Itemize
If the restriction is true, the second term is 0, so we are better off.
 
\emph on
True restrictions improve efficiency of estimation.
\end_layout

\begin_layout Itemize
If the restriction is false, we may be better or worse off, in terms of
 MSE, depending on the magnitudes of 
\begin_inset Formula $r-R\beta$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}.$
\end_inset


\end_layout

\begin_layout Section
Testing
\end_layout

\begin_layout Standard
In many cases, one wishes to test economic theories.
 If theory suggests parameter restrictions, as in the above homogeneity
 example, one can test theory by testing parameter restrictions.
 A number of tests are available.
 The first two (t and F) have a known small sample distributions, when the
 errors are normally distributed.
 The third and fourth (Wald and score) do not require normality of the errors,
 but their distributions are known only approximately, so that they are
 not exactly valid with finite samples.
\end_layout

\begin_layout Subsection
t-test
\end_layout

\begin_layout Standard
Suppose one has the model
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

 and one wishes to test the 
\emph on
single restriction
\emph default
 
\begin_inset Formula $H_{0}:$
\end_inset


\begin_inset Formula $R\beta=r$
\end_inset

 vs.
 
\begin_inset Formula $H_{A}:$
\end_inset


\begin_inset Formula $R\beta\neq r$
\end_inset

 .
 Under 
\begin_inset Formula $H_{0},$
\end_inset

 with normality of the errors, 
\begin_inset Formula 
\[
R\hat{\beta}-r\sim N\left(0,R(X^{\prime}X)^{-1}R^{\prime}\sigma_{0}^{2}\right)
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{R\hat{\beta}-r}{\sqrt{R(X^{\prime}X)^{-1}R^{\prime}\sigma_{0}^{2}}}=\frac{R\hat{\beta}-r}{\sigma_{0}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}\sim N\left(0,1\right).
\]

\end_inset

 The problem is that 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 is unknown.
 One could use the consistent estimator 
\begin_inset Formula $\widehat{\sigma_{0}^{2}}$
\end_inset

 in place of 
\begin_inset Formula $\sigma_{0}^{2},$
\end_inset

 but the test would only be valid asymptotically in this case.
\end_layout

\begin_layout Proposition
\begin_inset Formula $\frac{N(0,1)}{\sqrt{\frac{\chi^{2}(q)}{q}}}\sim t(q)$
\end_inset


\end_layout

\begin_layout Proposition
as long as the 
\begin_inset Formula $N(0,1)$
\end_inset

 and the 
\begin_inset Formula $\chi^{2}(q)$
\end_inset

 are independent.
 
\end_layout

\begin_layout Standard
We need a few results on the 
\begin_inset Formula $\chi{}^{2}$
\end_inset

 distribution.
\end_layout

\begin_layout Proposition
If 
\begin_inset Formula $x\sim N(\mu,I_{n})$
\end_inset

 is a vector of 
\begin_inset Formula $n$
\end_inset

 independent r.v.'s., then 
\begin_inset Formula $x^{\prime}x\sim\chi^{2}(n,\lambda)$
\end_inset

 where 
\begin_inset Formula $\lambda=\sum_{i}\mu_{i}^{2}=\mu^{\prime}\mu$
\end_inset

 is the 
\emph on
noncentrality parameter
\emph default
.
\end_layout

\begin_layout Standard
When a 
\begin_inset Formula $\chi^{2}$
\end_inset

 r.v.
 has the noncentrality parameter equal to zero, it is referred to as a central
 
\begin_inset Formula $\chi^{2}$
\end_inset

 r.v., and it's distribution is written as 
\begin_inset Formula $\chi^{2}(n),$
\end_inset

 suppressing the noncentrality parameter.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "quadratic form in inverse variance"

\end_inset

If the 
\begin_inset Formula $n$
\end_inset

 dimensional random vector 
\begin_inset Formula $x\sim N(0,V),$
\end_inset

 then 
\begin_inset Formula $x^{\prime}V^{-1}x\sim\chi^{2}(n).$
\end_inset


\end_layout

\begin_layout Standard
We'll prove this one as an indication of how the following unproven propositions
 could be proved.
\end_layout

\begin_layout Standard
Proof: 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

Factor 
\end_layout

\end_inset


\begin_inset Formula $V^{-1}$
\end_inset

 as 
\begin_inset Formula $P'P$
\end_inset

 (this is the Cholesky factorization, where 
\begin_inset Formula $P$
\end_inset

 is defined to be upper triangular).
 Then consider 
\begin_inset Formula $y=Px.$
\end_inset

 We have 
\begin_inset Formula 
\[
y\sim N(0,PVP')
\]

\end_inset

 but 
\begin_inset Formula 
\begin{eqnarray*}
VP'P & = & I_{n}\\
PVP'P & = & P
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula $PVP^{\prime}=I_{n}$
\end_inset

 and thus 
\begin_inset Formula $y\sim N(0,I_{n})$
\end_inset

.
 Thus 
\begin_inset Formula $y^{\prime}y\sim\chi^{2}(n)$
\end_inset

 but
\begin_inset Formula 
\[
y^{\prime}y=x^{\prime}P'Px=xV^{-1}x
\]

\end_inset

 and we get the result we wanted.
\end_layout

\begin_layout Standard
A more general proposition which implies this result is
\end_layout

\begin_layout Proposition
If the 
\begin_inset Formula $n$
\end_inset

 dimensional random vector 
\begin_inset Formula $x\sim N(0,V),$
\end_inset

 then 
\begin_inset Formula $x^{\prime}Bx\sim\chi^{2}(\rho(B))$
\end_inset

 if and only if 
\begin_inset Formula $BV$
\end_inset

 is idempotent.
\end_layout

\begin_layout Standard
An immediate consequence is
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "quadratic form in idempotent matrix"

\end_inset

If the random vector (of dimension 
\begin_inset Formula $n$
\end_inset

) 
\begin_inset Formula $x\sim N(0,I),$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is idempotent with rank 
\begin_inset Formula $r,$
\end_inset

 then 
\begin_inset Formula $x^{\prime}Bx\sim\chi^{2}(r)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Consider the random variable 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{\sigma_{0}^{2}} & = & \frac{\varepsilon^{\prime}M_{X}\varepsilon}{\sigma_{0}^{2}}\\
 & = & \left(\frac{\varepsilon}{\sigma_{0}}\right)^{\prime}M_{X}\left(\frac{\varepsilon}{\sigma_{0}}\right)\\
 & \sim & \chi^{2}(n-K)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proposition
If the random vector (of dimension 
\begin_inset Formula $n$
\end_inset

) 
\begin_inset Formula $x\sim N(0,I),$
\end_inset

 then 
\begin_inset Formula $Ax$
\end_inset

 and 
\begin_inset Formula $x^{\prime}Bx$
\end_inset

 are independent if 
\begin_inset Formula $AB=0.$
\end_inset


\end_layout

\begin_layout Standard
Now consider (remember that we have only one restriction in this case)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\frac{R\hat{\beta}-r}{\sigma_{0}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}}{\sqrt{\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{(n-K)\sigma_{0}^{2}}}}=\frac{R\hat{\beta}-r}{\widehat{\sigma_{0}}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}
\]

\end_inset

 This will have the 
\begin_inset Formula $t(n-K)$
\end_inset

 distribution if 
\begin_inset Formula $\hat{\beta}$
\end_inset

 and 
\begin_inset Formula $\hat{\varepsilon}^{\prime}\hat{\varepsilon}$
\end_inset

 are independent.
 But 
\begin_inset Formula $\hat{\beta}=\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon$
\end_inset

 and 
\begin_inset Formula 
\[
(X^{\prime}X)^{-1}X^{\prime}M_{X}=0,
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{R\hat{\beta}-r}{\widehat{\sigma_{0}}\sqrt{R(X^{\prime}X)^{-1}R^{\prime}}}=\frac{R\hat{\beta}-r}{\hat{\sigma}_{R\hat{\beta}}}\sim t(n-K)
\]

\end_inset

 In particular, for the commonly encountered 
\emph on
test of significance
\emph default
 of an individual coefficient, for which 
\begin_inset Formula $H_{0}:\beta_{i}=0$
\end_inset

 vs.
 
\begin_inset Formula $H_{0}:\beta_{i}\neq0$
\end_inset

 , the test statistic is 
\begin_inset Formula 
\[
\frac{\hat{\beta}_{i}}{\hat{\sigma}_{\hat{\beta}i}}\sim t(n-K)
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Note
\series default
: the 
\begin_inset Formula $t-$
\end_inset

 test is strictly valid only if the errors are actually normally distributed.
 If one has nonnormal errors, one could use the above asymptotic result
 to justify taking critical values from the 
\begin_inset Formula $N(0,1)$
\end_inset

 distribution, since 
\begin_inset Formula $t(n-K)\overset{d}{\rightarrow}N(0,1)$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 In practice, a conservative procedure is to take critical values from the
 
\begin_inset Formula $t$
\end_inset

 distribution if nonnormality is suspected.
 This will reject 
\begin_inset Formula $H_{0}$
\end_inset

 less often since the 
\begin_inset Formula $t$
\end_inset

 distribution is fatter-tailed than is the normal.
\end_layout

\begin_layout Subsection
\begin_inset Formula $F$
\end_inset

 test
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $F$
\end_inset

 test allows testing multiple restrictions jointly.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "F statistic"

\end_inset

If 
\begin_inset Formula $x\sim\chi^{2}(r)$
\end_inset

 and 
\begin_inset Formula $y\sim\chi^{2}(s),$
\end_inset

 then 
\begin_inset Formula $\frac{x/r}{y/s}\sim F(r,s)$
\end_inset

, provided that 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are independent.
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Proposition
If the random vector (of dimension 
\begin_inset Formula $n$
\end_inset

) 
\begin_inset Formula $x\sim N(0,I),$
\end_inset

 then 
\begin_inset Formula $x^{\prime}Ax$
\end_inset


\end_layout

\begin_layout Proposition
and 
\begin_inset Formula $x^{\prime}Bx$
\end_inset

 are independent if 
\begin_inset Formula $AB=0.$
\end_inset


\end_layout

\begin_layout Standard
Using these results, and previous results on the 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution, it is simple to show that the following statistic has the
 
\begin_inset Formula $F$
\end_inset

 distribution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F=\frac{\left(R\hat{\beta}-r\right)^{\prime}\left(R\left(X^{\prime}X\right)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)}{q\hat{\sigma}^{2}}\sim F(q,n-K).
\]

\end_inset

 A numerically equivalent expression is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\left(ESS_{R}-ESS_{U}\right)/q}{ESS_{U}/(n-K)}\sim F(q,n-K).
\]

\end_inset

 
\end_layout

\begin_layout Itemize

\series bold
Note:
\series default
 The 
\begin_inset Formula $F$
\end_inset

 test is strictly valid only if the errors are truly normally distributed.
 The following tests will be appropriate when one cannot assume normally
 distributed errors.
\end_layout

\begin_layout Subsection
Wald-type tests
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $F$
\end_inset

 tests require normality of the errors.
 The Wald test does not, but it is an asymptotic test - it is only approximately
 valid in finite samples.
\end_layout

\begin_layout Standard
The Wald principle is based on the idea that if a restriction is true, the
 unrestricted model should 
\begin_inset Quotes eld
\end_inset

approximately
\begin_inset Quotes erd
\end_inset

 satisfy the restriction.
 Given that the least squares estimator is asymptotically normally distributed:
 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}Q_{X}^{-1}\right)
\]

\end_inset

 then under 
\begin_inset Formula $H_{0}:R\beta_{0}=r,$
\end_inset

 we have 
\begin_inset Formula 
\[
\sqrt{n}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)
\]

\end_inset

 so by Proposition [
\begin_inset CommandInset ref
LatexCommand ref
reference "quadratic form in inverse variance"

\end_inset

] 
\begin_inset Formula 
\[
n\left(R\hat{\beta}-r\right)^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 Note that 
\begin_inset Formula $Q_{X}^{-1}$
\end_inset

 or 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 are not observable.
 The test statistic we use substitutes the consistent estimators.
 Use 
\begin_inset Formula $(X^{\prime}X/n)^{-1}$
\end_inset

 as the consistent estimator of 
\begin_inset Formula $Q_{X}^{-1}.$
\end_inset

 With this, there is a cancellation of 
\begin_inset Formula $n^{\prime}s,$
\end_inset

 and the statistic to use is 
\begin_inset Formula 
\[
\left(R\hat{\beta}-r\right)^{\prime}\left(\widehat{\sigma_{0}^{2}}R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset


\end_layout

\begin_layout Itemize
The Wald test is a simple way to test restrictions without having to estimate
 the restricted model.
\end_layout

\begin_layout Itemize
Note that this formula is similar to one of the formulae provided for the
 
\begin_inset Formula $F$
\end_inset

 test.
 
\end_layout

\begin_layout Subsection
Score-type tests (Rao tests, Lagrange multiplier tests)
\end_layout

\begin_layout Standard
The score test is another asymptotically valid test that does not require
 normality of the errors.
\end_layout

\begin_layout Standard
In some cases, an unrestricted model may be nonlinear in the parameters,
 but the model is linear in the parameters under the null hypothesis.
 For example, the model 
\begin_inset Formula 
\[
y=\left(X\beta\right)^{\gamma}+\varepsilon
\]

\end_inset

 is nonlinear in 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\gamma,$
\end_inset

 but is linear in 
\begin_inset Formula $\beta$
\end_inset

 under 
\begin_inset Formula $H_{0}:\gamma=1.$
\end_inset

 Estimation of nonlinear models is a bit more complicated, so one might
 prefer to have a test based upon the restricted, linear model.
 The score test is useful in this situation.
\end_layout

\begin_layout Itemize
Score-type tests are based upon the general principle that the gradient
 vector of the unrestricted model, evaluated at the restricted estimate,
 should be asymptotically normally distributed with mean zero, if the restrictio
ns are true.
 The original development was for ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimation, but the principle is valid for a wide variety of estimation
 methods.
 
\end_layout

\begin_layout Standard
We have seen that 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\lambda} & = & \left(R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\\
 & = & P^{-1}\left(R\hat{\beta}-r\right)
\end{eqnarray*}

\end_inset

so
\begin_inset Formula 
\[
\sqrt{n}\hat{P\lambda}=\sqrt{n}\left(R\hat{\beta}-r\right)
\]

\end_inset

Given that 
\begin_inset Formula 
\[
\sqrt{n}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)
\]

\end_inset

 under the null hypothesis, we obtain 
\begin_inset Formula 
\[
\sqrt{n}\hat{P\lambda}\overset{d}{\rightarrow}N\left(0,\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)
\]

\end_inset

 So
\begin_inset Formula 
\[
\left(\sqrt{n}\hat{P\lambda}\right)^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}\left(\sqrt{n}\hat{P\lambda}\right)\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

Noting that 
\begin_inset Formula $\lim nP=RQ_{X}^{-1}R^{\prime},$
\end_inset

 we obtain, 
\begin_inset Formula 
\[
\hat{\lambda}^{\prime}\left(\frac{R(X^{\prime}X)^{-1}R^{\prime}}{\sigma_{0}^{2}}\right)\hat{\lambda}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 since the powers of 
\begin_inset Formula $n$
\end_inset

 cancel.
 To get a usable test statistic substitute a consistent estimator of 
\begin_inset Formula $\sigma_{0}^{2}.$
\end_inset


\end_layout

\begin_layout Itemize
This makes it clear why the test is sometimes referred to as a Lagrange
 multiplier test.
 It may seem that one needs the actual Lagrange multipliers to calculate
 this.
 If we impose the restrictions by substitution, these are not available.
 Note that the test can be written as 
\begin_inset Formula 
\[
\frac{\left(R^{\prime}\hat{\lambda}\right)^{\prime}(X^{\prime}X)^{-1}R^{\prime}\hat{\lambda}}{\sigma_{0}^{2}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 However, we can use the fonc for the restricted estimator: 
\begin_inset Formula 
\[
-X^{\prime}y+X^{\prime}X\hat{\beta}_{R}+R^{\prime}\hat{\lambda}
\]

\end_inset

 to get that 
\begin_inset Formula 
\begin{eqnarray*}
R^{\prime}\hat{\lambda} & = & X^{\prime}(y-X\hat{\beta}_{R})\\
 & = & X^{\prime}\hat{\varepsilon}_{R}
\end{eqnarray*}

\end_inset

 Substituting this into the above, we get 
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}_{R}^{\prime}X(X^{\prime}X)^{-1}X^{\prime}\hat{\varepsilon}_{R}}{\sigma_{0}^{2}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 but this is simply 
\begin_inset Formula 
\[
\hat{\varepsilon}_{R}^{\prime}\frac{P_{X}}{\sigma_{0}^{2}}\hat{\varepsilon}_{R}\overset{d}{\rightarrow}\chi^{2}(q).
\]

\end_inset


\end_layout

\begin_layout Standard
To see why the test is also known as a score test, note that the fonc for
 restricted least squares 
\begin_inset Formula 
\[
-X^{\prime}y+X^{\prime}X\hat{\beta}_{R}+R^{\prime}\hat{\lambda}
\]

\end_inset

 give us 
\begin_inset Formula 
\[
R^{\prime}\hat{\lambda}=X^{\prime}y-X^{\prime}X\hat{\beta}_{R}
\]

\end_inset

 and the rhs is simply the gradient (score)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

of the unrestricted model, evaluated at the restricted estimator.
 The scores evaluated at the unrestricted estimate are identically zero.
 The logic behind the score test is that the scores evaluated at the restricted
 estimate should be approximately zero, if the restriction is true.
 The test is also known as a Rao test, since P.
 Rao first proposed it in 1948.
\end_layout

\begin_layout Section
The asymptotic equivalence of the LR, Wald and score tests
\end_layout

\begin_layout Standard
Note: the discussion of the LR test has been moved forward in these notes.
 I no longer teach the material in this section, but I'm leaving it here
 for reference.
 
\end_layout

\begin_layout Standard
We have seen that the three tests all converge to 
\begin_inset Formula $\chi^{2}$
\end_inset

 random variables.
 In fact, they all converge to the 
\emph on
same
\emph default
 
\begin_inset Formula $\chi^{2}$
\end_inset

 rv, under the null hypothesis.
 We'll show that the Wald and LR tests are asymptotically equivalent.
 We have seen that the Wald test is asymptotically equivalent to 
\begin_inset Formula 
\begin{equation}
W\overset{a}{=}n\left(R\hat{\beta}-r\right)^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{d}{\rightarrow}\chi^{2}(q)\label{eq:Wald}
\end{equation}

\end_inset

 Using 
\begin_inset Formula 
\[
\hat{\beta}-\beta_{0}=(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\]

\end_inset

 and 
\begin_inset Formula 
\[
R\hat{\beta}-r=R(\hat{\beta}-\beta_{0})
\]

\end_inset

 we get 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}R(\hat{\beta}-\beta_{0}) & = & \sqrt{n}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & R\left(\frac{X^{\prime}X}{n}\right)^{-1}n^{-1/2}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Substitute this into [
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Wald"

\end_inset

] to get 
\begin_inset Formula 
\begin{eqnarray*}
W & \overset{a}{=} & n^{-1}\varepsilon^{\prime}XQ_{X}^{-1}R^{\prime}\left(\sigma_{0}^{2}RQ_{X}^{-1}R^{\prime}\right)^{-1}RQ_{X}^{-1}X^{\prime}\varepsilon\\
 & \overset{a}{=} & \varepsilon^{\prime}X(X^{\prime}X)^{-1}R^{\prime}\left(\sigma_{0}^{2}R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & \overset{a}{=} & \frac{\varepsilon^{\prime}A(A^{\prime}A)^{-1}A^{\prime}\varepsilon}{\sigma_{0}^{2}}\\
 & \overset{a}{=} & \frac{\varepsilon^{\prime}P_{R}\varepsilon}{\sigma_{0}^{2}}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $P_{R}$
\end_inset

 is the projection matrix formed by the matrix 
\begin_inset Formula $X(X^{\prime}X)^{-1}R^{\prime}$
\end_inset

.
\end_layout

\begin_layout Itemize
Note that this matrix is idempotent and has 
\begin_inset Formula $q$
\end_inset

 columns, so the projection matrix has rank 
\begin_inset Formula $q.$
\end_inset


\end_layout

\begin_layout Standard
Now consider the likelihood ratio statistic 
\begin_inset Formula 
\begin{equation}
LR\overset{a}{=}n^{1/2}g(\theta_{0})^{\prime}\mathcal{I}(\theta_{0})^{-1}R^{\prime}\left(R\mathcal{I}(\theta_{0})^{-1}R^{\prime}\right)^{-1}R\mathcal{I}(\theta_{0})^{-1}n^{1/2}g(\theta_{0})\label{eq:LR}
\end{equation}

\end_inset

 Under normality, we have seen that the likelihood function is 
\begin_inset Formula 
\[
\ln L(\beta,\sigma)=-n\ln\sqrt{2\pi}-n\ln\sigma-\frac{1}{2}\frac{\left(y-X\beta\right)^{\prime}\left(y-X\beta\right)}{\sigma^{2}}.
\]

\end_inset

 Using this, 
\begin_inset Formula 
\begin{eqnarray*}
g(\beta_{0}) & \equiv & D_{\beta}\frac{1}{n}\ln L(\beta,\sigma)\\
 & = & \frac{X^{\prime}(y-X\beta_{0})}{n\sigma^{2}}\\
 & = & \frac{X^{\prime}\varepsilon}{n\sigma^{2}}
\end{eqnarray*}

\end_inset

 Also, by the information matrix equality: 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}(\theta_{0}) & = & -H_{\infty}(\theta_{0})\\
 & = & \lim-D_{\beta^{\prime}}g(\beta_{0})\\
 & = & \lim-D_{\beta^{\prime}}\frac{X^{\prime}(y-X\beta_{0})}{n\sigma^{2}}\\
 & = & \lim\frac{X^{\prime}X}{n\sigma^{2}}\\
 & = & \frac{Q_{X}}{\sigma^{2}}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
\mathcal{I}(\theta_{0})^{-1}=\sigma^{2}Q_{X}^{-1}
\]

\end_inset

 Substituting these last expressions into [
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LR"

\end_inset

], we get 
\begin_inset Formula 
\begin{eqnarray*}
LR & \overset{a}{=} & \varepsilon^{\prime}X^{\prime}(X^{\prime}X)^{-1}R^{\prime}\left(\sigma_{0}^{2}R(X^{\prime}X)^{-1}R^{\prime}\right)^{-1}R(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & \overset{a}{=} & \frac{\varepsilon^{\prime}P_{R}\varepsilon}{\sigma_{0}^{2}}\\
 & \overset{a}{=} & W
\end{eqnarray*}

\end_inset

 This completes the proof that the Wald and LR tests are asymptotically
 equivalent.
 Similarly, one can show that, 
\emph on
under the null hypothesis
\emph default
, 
\begin_inset Formula 
\[
qF\overset{a}{=}W\overset{a}{=}LM\overset{a}{=}LR
\]

\end_inset


\end_layout

\begin_layout Itemize
The proof for the statistics except for 
\begin_inset Formula $LR$
\end_inset

 does not depend upon normality of the errors, as can be verified by examining
 the expressions for the statistics.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $LR$
\end_inset

 statistic 
\emph on
is
\emph default
 based upon distributional assumptions, since one can't write the likelihood
 function without them.
\end_layout

\begin_layout Itemize
However, due to the close relationship between the statistics 
\begin_inset Formula $qF$
\end_inset

 and 
\begin_inset Formula $LR,$
\end_inset

 supposing normality, the 
\begin_inset Formula $qF$
\end_inset

 statistic can be thought of as a 
\emph on
pseudo-LR statistic,
\emph default
 in that it's like a LR statistic in that it uses the value of the objective
 functions of the restricted and unrestricted models, but it doesn't require
 distributional assumptions.
\end_layout

\begin_layout Itemize
The presentation of the score and Wald tests has been done in the context
 of the linear model.
 This is readily generalizable to nonlinear models and/or other estimation
 methods.
 
\end_layout

\begin_layout Standard
Though the four statistics 
\emph on
are
\emph default
 asymptotically equivalent, they are numerically different in small samples.
 The numeric values of the tests also depend upon how 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is estimated, and we've already seen than there are several ways to do
 this.
 For example all of the following are consistent for 
\begin_inset Formula $\sigma^{2}$
\end_inset

 under 
\begin_inset Formula $H_{0}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
 & \frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{n-k}\\
 & \frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{n}\\
 & \frac{\hat{\varepsilon}_{R}^{\prime}\hat{\varepsilon}_{R}}{n-k+q}\\
 & \frac{\hat{\varepsilon}_{R}^{\prime}\hat{\varepsilon}_{R}}{n}
\end{eqnarray*}

\end_inset

 and in general the denominator call be replaced with any quantity 
\begin_inset Formula $a$
\end_inset

 such that 
\begin_inset Formula $\lim a/n=1.$
\end_inset


\end_layout

\begin_layout Standard
It can be shown, for linear regression models subject to linear restrictions,
 and if 
\begin_inset Formula $\frac{\hat{\varepsilon}^{\prime}\hat{\varepsilon}}{n}$
\end_inset

 is used to calculate the Wald test and 
\begin_inset Formula $\frac{\hat{\varepsilon}_{R}^{\prime}\hat{\varepsilon}_{R}}{n}$
\end_inset

 is used for the score test, that 
\begin_inset Formula 
\[
W>LR>LM.
\]

\end_inset

 For this reason, the Wald test will always reject if the LR test rejects,
 and in turn the LR test rejects if the LM test rejects.
 This is a bit problematic:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

there is the possibility that by careful choice of the statistic used, one
 can manipulate reported results to favor or disfavor a hypothesis.
 A conservative/honest approach would be to report all three test statistics
 when they are available.
 In the case of linear models with normal errors the 
\begin_inset Formula $F\;$
\end_inset

test is to be preferred, since asymptotic approximations are not an issue.
\end_layout

\begin_layout Standard
The small sample behavior of the tests can be quite different.
 The true size (probability of rejection of the null when the null is true)
 of the Wald test is often dramatically higher than the nominal size associated
 with the asymptotic distribution.
 Likewise, the true size of the score test is often smaller than the nominal
 size.
\end_layout

\begin_layout Section
Interpretation of test statistics
\end_layout

\begin_layout Standard
Now that we have a menu of test statistics, we need to know how to use them.
\end_layout

\begin_layout Section
Confidence intervals
\end_layout

\begin_layout Standard
Confidence intervals for single coefficients are generated in the normal
 manner.
 Given the 
\begin_inset Formula $t$
\end_inset

 statistic 
\begin_inset Formula 
\[
t(\beta)=\frac{\hat{\beta}-\beta}{\widehat{\sigma_{\hat{\beta}}}}
\]

\end_inset

 a 
\begin_inset Formula $100\left(1-\alpha\right)\%$
\end_inset

 confidence interval for 
\begin_inset Formula $\beta_{0}$
\end_inset

 is defined by the bounds of the set of 
\begin_inset Formula $\beta$
\end_inset

 such that 
\begin_inset Formula $t(\beta)$
\end_inset

 does not reject 
\begin_inset Formula $H_{0}:\beta_{0}=\beta,$
\end_inset

 using a 
\begin_inset Formula $\alpha$
\end_inset

 significance level: 
\begin_inset Formula 
\[
C(\alpha)=\{\beta:-c_{\alpha/2}<\frac{\hat{\beta}-\beta}{\widehat{\sigma_{\hat{\beta}}}}<c_{\alpha/2}\}
\]

\end_inset

 The set of such 
\begin_inset Formula $\beta$
\end_inset

 is the interval 
\begin_inset Formula 
\[
\hat{\beta}\pm\widehat{\sigma_{\hat{\beta}}}c_{\alpha/2}
\]

\end_inset


\end_layout

\begin_layout Standard
A confidence ellipse for two coefficients jointly would be, analogously,
 the set of {
\begin_inset Formula $\beta_{1},\beta_{2}\}$
\end_inset

 such that the 
\begin_inset Formula $F$
\end_inset

 (or some other test statistic) doesn't reject at the specified critical
 value.
 This generates an ellipse, if the estimators are correlated.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Joint and Individual Confidence Regions
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/Figures/JointConfidenceRegion.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The region is an ellipse, since the CI for an individual coefficient defines
 a (infinitely long) rectangle with total prob.
 mass 
\begin_inset Formula $1-\alpha,$
\end_inset

 since the other coefficient is marginalized (e.g., can take on any value).
 Since the ellipse is bounded in both dimensions but also contains mass
 
\begin_inset Formula $1-\alpha,$
\end_inset

 it must extend beyond the bounds of the individual CI.
\end_layout

\begin_layout Itemize
From the picture we can see that:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Rejection of hypotheses individually does not imply that the joint test
 will reject.
\end_layout

\begin_layout Itemize
Joint rejection does not imply individual tests will reject.
 
\end_layout

\end_deeper
\begin_layout Section
Bootstrapping
\end_layout

\begin_layout Standard
When we rely on asymptotic theory to use the normal distribution-based tests
 and confidence intervals, we're often at serious risk of making important
 errors.
 If the sample size is small and errors are highly nonnormal, the small
 sample distribution of 
\begin_inset Formula $\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)$
\end_inset

 may be very different from its large sample distribution.
 Also, the distributions of test statistics may not resemble their limiting
 distributions at all.
 A means of trying to gain information on the small sample distribution
 of test statistics and estimators is the 
\emph on
bootstrap.

\emph default
 We'll consider a simple example, just to get the main idea.
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta_{0}+\varepsilon\\
\varepsilon & \sim & IID(0,\sigma_{0}^{2})\\
 & X\textrm{ is nonstochastic}
\end{eqnarray*}

\end_inset

 Given that the distribution of 
\begin_inset Formula $\varepsilon$
\end_inset

 is unknown, the distribution of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 will be unknown in small samples.
 However, since we have random sampling, we could generate 
\emph on
artificial data.

\emph default
 The steps are:
\end_layout

\begin_layout Enumerate
Draw 
\begin_inset Formula $n$
\end_inset

 observations from 
\begin_inset Formula $\hat{\varepsilon}$
\end_inset

 
\series bold
with replacement
\series default
.
 Call this vector 
\begin_inset Formula $\tilde{\varepsilon}^{j}$
\end_inset

 (it's a 
\begin_inset Formula $n\times1).$
\end_inset


\end_layout

\begin_layout Enumerate
Then generate the data by 
\begin_inset Formula $\tilde{y}^{j}=X\hat{\beta}+\tilde{\varepsilon}^{j}$
\end_inset


\end_layout

\begin_layout Enumerate
Now take this and estimate 
\begin_inset Formula 
\[
\tilde{\beta}^{j}=(X^{\prime}X)^{-1}X^{\prime}\tilde{y}^{j}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
Save 
\begin_inset Formula $\tilde{\beta}^{j}$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat steps 1-4, until we have a large number, 
\begin_inset Formula $J,$
\end_inset

 of 
\begin_inset Formula $\tilde{\beta}^{j}.$
\end_inset


\end_layout

\begin_layout Standard
With this, we can use the replications to calculate the 
\emph on
empirical distribution of
\emph default
 
\begin_inset Formula $\tilde{\beta}_{j}.$
\end_inset

 One way to form a 100(1-
\begin_inset Formula $\alpha)\%$
\end_inset

 confidence interval for 
\begin_inset Formula $\beta_{0}$
\end_inset

 would be to order the 
\begin_inset Formula $\tilde{\beta}^{j}$
\end_inset

 from smallest to largest, and drop the first and last 
\begin_inset Formula $J\alpha/2$
\end_inset

 of the replications, and use the remaining endpoints as the limits of the
 CI.
 Note that this will not give the shortest CI if the empirical distribution
 is skewed.
\end_layout

\begin_layout Itemize
Suppose one was interested in the distribution of some function of 
\begin_inset Formula $\hat{\beta},$
\end_inset

 for example a test statistic.
 Simple: just calculate the transformation for each 
\begin_inset Formula $j,$
\end_inset

 and work with the empirical distribution of the transformation.
\end_layout

\begin_layout Itemize
If the assumption of iid errors is too strong (for example if there is heterosce
dasticity or autocorrelation, see below) one can work with a bootstrap defined
 by sampling from 
\begin_inset Formula $(y,x)$
\end_inset

 with replacement.
\end_layout

\begin_layout Itemize
How to choose 
\begin_inset Formula $J$
\end_inset

: 
\begin_inset Formula $J$
\end_inset

 should be large enough that the results don't change with repetition of
 the entire bootstrap.
 This is easy to check.
 If you find the results change a lot, increase 
\begin_inset Formula $J$
\end_inset

 and try again.
\end_layout

\begin_layout Itemize
The bootstrap is based fundamentally on the idea that the empirical distribution
 of the sample data converges to the actual sampling distribution as 
\begin_inset Formula $n$
\end_inset

 becomes large, so statistics based on sampling from the empirical distribution
 should converge in distribution to statistics based on sampling from the
 actual sampling distribution.
\end_layout

\begin_layout Itemize
In finite samples, this doesn't hold.
 At a minimum, the bootstrap is a good way to check if asymptotic theory
 results offer a decent approximation to the small sample distribution.
\end_layout

\begin_layout Itemize
Bootstrapping can be used to test hypotheses.
 Basically, use the bootstrap to get an approximation to the empirical distribut
ion of the test statistic under the alternative hypothesis, and use this
 to get critical values.
 Compare the test statistic calculated using the real data, under the null,
 to the bootstrap critical values.
 There are many variations on this theme, which we won't go into here.
 
\end_layout

\begin_layout Section
Wald test for nonlinear restrictions: the delta method
\end_layout

\begin_layout Standard
Testing nonlinear restrictions of a linear model is not much more difficult,
 at least when the model is linear.
 Since estimation subject to nonlinear restrictions requires nonlinear estimatio
n methods, which are beyond the score of this course, we'll just consider
 the Wald test for nonlinear restrictions on a linear model.
\end_layout

\begin_layout Standard
Consider the 
\begin_inset Formula $q$
\end_inset

 nonlinear restrictions 
\begin_inset Formula 
\[
r(\beta_{0})=0.
\]

\end_inset

 where 
\begin_inset Formula $r(\cdot)\;$
\end_inset

 is a 
\begin_inset Formula $q$
\end_inset

-vector valued function.
 Write the derivative of the restriction evaluated at 
\begin_inset Formula $\beta$
\end_inset

 as 
\begin_inset Formula 
\[
\left.D_{\beta^{\prime}}r(\beta)\right|_{\beta}=R(\beta)
\]

\end_inset

 We suppose that the restrictions are not redundant in a neighborhood of
 
\begin_inset Formula $\beta_{0}$
\end_inset

, so that 
\begin_inset Formula 
\[
\rho(R(\beta))=q
\]

\end_inset

 in a neighborhood of 
\begin_inset Formula $\beta_{0}.$
\end_inset

 Take a first order Taylor's series expansion of 
\begin_inset Formula $r(\hat{\beta})$
\end_inset

 about 
\begin_inset Formula $\beta_{0}$
\end_inset

: 
\begin_inset Formula 
\[
r(\hat{\beta})=r(\beta_{0})+R(\beta^{*})(\hat{\beta}-\beta_{0})
\]

\end_inset

 where 
\begin_inset Formula $\beta^{*}$
\end_inset

 is a convex combination of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 and 
\begin_inset Formula $\beta_{0}.$
\end_inset

 Under the null hypothesis we have 
\begin_inset Formula 
\[
r(\hat{\beta})=R(\beta^{*})(\hat{\beta}-\beta_{0})
\]

\end_inset

 Due to consistency of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 we can replace 
\begin_inset Formula $\beta^{*}$
\end_inset

 by 
\begin_inset Formula $\beta_{0}$
\end_inset

, asymptotically, so 
\begin_inset Formula 
\[
\sqrt{n}r(\hat{\beta})\overset{a}{=}\sqrt{n}R(\beta_{0})(\hat{\beta}-\beta_{0})
\]

\end_inset

 We've already seen the distribution of 
\begin_inset Formula $\sqrt{n}(\hat{\beta}-\beta_{0}).$
\end_inset

 Using this we get 
\begin_inset Formula 
\[
\sqrt{n}r(\hat{\beta})\overset{d}{\rightarrow}N\left(0,R(\beta_{0})Q_{X}^{-1}R(\beta_{0})^{\prime}\sigma_{0}^{2}\right).
\]

\end_inset

 Considering the quadratic form 
\begin_inset Formula 
\[
\frac{nr(\hat{\beta})^{\prime}\left(R(\beta_{0})Q_{X}^{-1}R(\beta_{0})^{\prime}\right)^{-1}r(\hat{\beta})}{\sigma_{0}^{2}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 under the null hypothesis.
 Substituting consistent estimators for 
\begin_inset Formula $\beta_{0,}Q_{X}$
\end_inset

 and 
\begin_inset Formula $\sigma_{0}^{2},$
\end_inset

 the resulting statistic is 
\begin_inset Formula 
\[
\frac{r(\hat{\beta})^{\prime}\left(R(\hat{\beta})(X^{\prime}X)^{-1}R(\hat{\beta})^{\prime}\right)^{-1}r(\hat{\beta})}{\widehat{\sigma^{2}}}\overset{d}{\rightarrow}\chi^{2}(q)
\]

\end_inset

 under the null hypothesis.
\end_layout

\begin_layout Itemize
This is known in the literature as the 
\emph on
delta method
\emph default
, or as 
\emph on
Klein's approximation
\emph default
.
\end_layout

\begin_layout Itemize
Since this is a Wald test, it will tend to over-reject in finite samples.
 The score and LR tests are also possibilities, but they require estimation
 methods for nonlinear models, which aren't in the scope of this course.
 
\end_layout

\begin_layout Standard
Note that this also gives a convenient way to estimate nonlinear functions
 and associated asymptotic confidence intervals.
 If the nonlinear function 
\begin_inset Formula $r(\beta_{0})$
\end_inset

 is not hypothesized to be zero, we just have 
\begin_inset Formula 
\[
\sqrt{n}\left(r(\hat{\beta})-r(\beta_{0})\right)\overset{d}{\rightarrow}N\left(0,R(\beta_{0})Q_{X}^{-1}R(\beta_{0})^{\prime}\sigma_{0}^{2}\right)
\]

\end_inset

 so an approximation to the distribution of the function of the estimator
 is 
\begin_inset Formula 
\[
r(\hat{\beta})\approx N(r(\beta_{0}),R(\beta_{0})(X^{\prime}X)^{-1}R(\beta_{0})^{\prime}\sigma_{0}^{2})
\]

\end_inset

 For example, the vector of elasticities of a function 
\begin_inset Formula $f(x)$
\end_inset

 is 
\begin_inset Formula 
\[
\eta(x)=\frac{\partial f(x)}{\partial x}\odot\frac{x}{f(x)}
\]

\end_inset

where 
\begin_inset Formula $\odot$
\end_inset

 means element-by-element multiplication.
 Suppose we estimate a linear function 
\begin_inset Formula 
\[
y=x^{\prime}\beta+\varepsilon.
\]

\end_inset

 The elasticities of 
\begin_inset Formula $y$
\end_inset

 w.r.t.
 
\begin_inset Formula $x$
\end_inset

 are 
\begin_inset Formula 
\[
\eta(x)=\frac{\beta}{x^{\prime}\beta}\odot x
\]

\end_inset

(note that this is the entire vector of elasticities).
 The estimated elasticities are 
\begin_inset Formula 
\[
\widehat{\eta}(x)=\frac{\hat{\beta}}{x^{\prime}\hat{\beta}}\odot x
\]

\end_inset

 To calculate the estimated standard errors of all five elasticities, use
 
\begin_inset Formula 
\begin{eqnarray*}
R(\beta) & = & \frac{\partial\eta(x)}{\partial\beta^{\prime}}\\
 & = & \frac{\left[\begin{array}{cccc}
x_{1} & 0 & \cdots & 0\\
0 & x_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & x_{k}
\end{array}\right]x'\beta-\left[\begin{array}{cccc}
\beta_{1}x_{1}^{2} & 0 & \cdots & 0\\
0 & \beta_{2}x_{2}^{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \beta_{k}x_{k}^{2}
\end{array}\right]}{(x'\beta)^{2}}.
\end{eqnarray*}

\end_inset

To get a consistent estimator just substitute in 
\begin_inset Formula $\hat{\beta}$
\end_inset

.
 Note that the elasticity and the standard error are functions of 
\begin_inset Formula $x.$
\end_inset

 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{ExampleDeltaMethod.jl}{https://github.com/mcreel/Econometrics/bl
ob/master/Examples/Restrictions/ExampleDeltaMethod.jl}
\end_layout

\end_inset

 shows how this can be done.
 
\end_layout

\begin_layout Standard
In many cases, nonlinear restrictions can also involve the data, not just
 the parameters.
 For example, consider a model of expenditure shares.
 Let 
\begin_inset Formula $x(p,m)$
\end_inset

 be a demand function, where 
\begin_inset Formula $p$
\end_inset

 is prices and 
\begin_inset Formula $m$
\end_inset

 is income.
 An expenditure share system for 
\begin_inset Formula $G$
\end_inset

 goods is 
\begin_inset Formula 
\[
s_{i}(p,m)=\frac{p_{i}x_{i}(p,m)}{m},i=1,2,...,G.
\]

\end_inset

 Now demand must be positive, and we assume that expenditures sum to income,
 so we have the restrictions 
\begin_inset Formula 
\begin{eqnarray*}
0 & \leq s_{i}(p,m)\leq1, & \forall i\\
\sum_{i=1}^{G}s_{i}(p,m) & = & 1
\end{eqnarray*}

\end_inset

 Suppose we postulate a linear model for the expenditure shares: 
\begin_inset Formula 
\[
s_{i}(p,m)=\beta_{1}^{i}+p^{\prime}\beta_{p}^{i}+m\beta_{m}^{i}+\varepsilon^{i}
\]

\end_inset

 It is fairly easy to write restrictions such that the shares sum to one,
 but the restriction that the shares lie in the 
\begin_inset Formula $[0,1]$
\end_inset

 interval depends on both parameters and the values of 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $m.$
\end_inset

 It is impossible to impose the restriction that 
\begin_inset Formula $0\leq s_{i}(p,m)\leq1$
\end_inset

 for all possible 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $m.$
\end_inset

 In such cases, one might consider whether or not a linear model is a reasonable
 specification.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example: Nerlove restrictions"

\end_inset

Example: the Nerlove data
\end_layout

\begin_layout Standard
Remember that we in a previous example (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"

\end_inset

) that the OLS results for the Nerlove model are 
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/OLS/nerlove.out"

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $s_{K}=\beta_{K}<0$
\end_inset

, and that 
\begin_inset Formula $\beta_{L}+\beta_{F}+\beta_{K}\neq1$
\end_inset

.
\end_layout

\begin_layout Standard
Remember that if we have constant returns to scale, then 
\begin_inset Formula $\beta_{Q}=1,$
\end_inset

 and if there is homogeneity of degree 1 then 
\begin_inset Formula $\beta_{L}+\beta_{F}+\beta_{K}=1$
\end_inset

.
 We can test these hypotheses either separately or jointly.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{NerloveRestrictions.jl}{https://github.com/mcreel/Econometrics/b
lob/master/Examples/Restrictions/NerloveRestrictions.jl} 
\end_layout

\end_inset

 imposes and tests CRTS and then HOD1.
 From it we obtain the results that follow: 
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/Restrictions/NerloveRestrictions.out"

\end_inset


\end_layout

\begin_layout Standard
Notice that the input price coefficients in fact sum to 1 when HOD1 is imposed.
 HOD1 is not rejected at usual significance levels (
\emph on
e.g., 
\begin_inset Formula $\alpha=0.10$
\end_inset


\emph default
).
 Also, 
\begin_inset Formula $R^{2}$
\end_inset

 does not drop much when the restriction is imposed, compared to the unrestricte
d results.
 For CRTS, you should note that 
\begin_inset Formula $\beta_{Q}=1$
\end_inset

, so the restriction is satisfied.
 Also note that the hypothesis that 
\begin_inset Formula $\beta_{Q}=1$
\end_inset

 is rejected by the test statistics at all reasonable significance levels.
 Note that 
\begin_inset Formula $R^{2}$
\end_inset

 drops quite a bit when imposing CRTS.
 If you look at the unrestricted estimation results, you can see that a
 t-test for 
\begin_inset Formula $\beta_{Q}=1$
\end_inset

 also rejects, and that a confidence interval for 
\begin_inset Formula $\beta_{Q}$
\end_inset

 does not overlap 1.
 
\end_layout

\begin_layout Standard
From the point of view of neoclassical economic theory, these results are
 not anomalous: HOD1 is an implication of the theory, but CRTS is not.
\end_layout

\begin_layout Exercise
Modify the NerloveRestrictions.jl program to impose and test the restrictions
 jointly.
\end_layout

\begin_layout Paragraph
\begin_inset CommandInset label
LatexCommand label
name "Chow test"

\end_inset

The Chow test
\end_layout

\begin_layout Standard
Since CRTS is rejected, let's examine the possibilities more carefully.
 Recall that the data is sorted by output (the third column).
 Define 5 subsamples of firms, with the first group being the 29 firms with
 the lowest output levels, then the next 29 firms, etc.
 The five subsamples can be indexed by 
\begin_inset Formula $j=1,2,...,5,$
\end_inset

 where 
\begin_inset Formula $j=1$
\end_inset

 for 
\begin_inset Formula $t=1,2,...29$
\end_inset

, 
\begin_inset Formula $j=2$
\end_inset

 for 
\begin_inset Formula $t=30,31,...58$
\end_inset

, etc.
 Define 
\emph on
dummy variables
\emph default
 
\begin_inset Formula $D_{1},D_{2},...,D_{5}$
\end_inset

 where
\begin_inset Formula 
\begin{align*}
D_{1} & =\begin{cases}
1 & t\in\{1,2,...29\}\\
0 & t\notin\{1,2,...29\}
\end{cases}\\
D_{2} & =\begin{cases}
1 & t\in\{30,31,...58\}\\
0 & t\notin\{30,31,...58\}
\end{cases}\\
\vdots\\
D_{5} & =\begin{cases}
1 & t\in\{117,118,...,145\}\\
0 & t\notin\{117,118,...,145\}
\end{cases}
\end{align*}

\end_inset

 Define the model
\begin_inset Formula 
\begin{equation}
\ln C_{t}=\sum_{j=1}^{5}\alpha_{1}D_{j}+\sum_{j=1}^{5}\gamma_{j}D_{j}\ln Q_{t}+\sum_{j=1}^{5}\beta_{Lj}D_{j}\ln P_{Lt}+\sum_{j=1}^{5}\beta_{Fj}D_{j}\ln P_{Ft}+\sum_{j=1}^{5}\beta_{Kj}D_{j}\ln P_{Kt}+\epsilon_{t}
\end{equation}

\end_inset

Note that the first column of nerlove.data indicates this way of breaking
 up the sample, and provides and easy way of defining the dummy variables.
 The new model may be written as
\begin_inset Formula 
\begin{equation}
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
\\
y_{5}
\end{array}\right]=\left[\begin{array}{lllll}
X_{1} & 0 & \cdots &  & 0\\
0 & X_{2}\\
\vdots &  & X_{3}\\
 &  &  & X_{4} & 0\\
0 &  &  &  & X_{5}
\end{array}\right]\left[\begin{array}{l}
\beta^{1}\\
\beta^{2}\\
\\
\\
\beta^{5}
\end{array}\right]+\left[\begin{array}{l}
\epsilon^{1}\\
\epsilon^{2}\\
\vdots\\
\\
\epsilon^{5}
\end{array}\right]\label{nerlove - all coefficients vary}
\end{equation}

\end_inset

 where 
\begin_inset Formula $y_{1}$
\end_inset

 is 29
\begin_inset Formula $\times1,$
\end_inset

 
\begin_inset Formula $X_{1}$
\end_inset

 is 29
\begin_inset Formula $\times5,$
\end_inset

 
\begin_inset Formula $\beta^{j}$
\end_inset

 is the 
\begin_inset Formula $5\times1$
\end_inset

 vector of coefficients for the 
\begin_inset Formula $j^{th}$
\end_inset

 subsample (e.g., 
\begin_inset Formula $\beta^{1}=\mbox{\left(\alpha_{1},\gamma_{1},\beta_{L1},\beta_{F1},\beta_{K1}\right)\ensuremath{^{\prime}}}$
\end_inset

), and 
\begin_inset Formula $\epsilon^{j}$
\end_inset

 is the 
\begin_inset Formula $29\times1$
\end_inset

 vector of errors for the 
\begin_inset Formula $j^{th}$
\end_inset

 subsample.
\end_layout

\begin_layout Standard
The Julia program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Restrictions/ChowTest.jl}{https://github.com/mcreel/Econometrics
/blob/master/Examples/Restrictions/ChowTest.jl}
\end_layout

\end_inset

 estimates the above model.
 It also tests the hypothesis that the five subsamples share the same parameter
 vector, or in other words, that there is coefficient stability across the
 five subsamples.
 The null to test is that the parameter vectors for the separate groups
 are all the same, that is,
\begin_inset Formula 
\[
\beta^{1}=\beta^{2}=\beta^{3}=\beta^{4}=\beta^{5}
\]

\end_inset

 This type of test, that parameters are constant across different sets of
 data, is sometimes referred to as a 
\emph on
Chow
\emph default
 
\emph on
test.
\end_layout

\begin_layout Itemize
There are 20 restrictions.
 If that's not clear to you, look at the Julia program.
\end_layout

\begin_layout Itemize
The restrictions are rejected at all conventional significance levels.
\end_layout

\begin_layout Standard
Since the restrictions are rejected, we should probably use the unrestricted
 model for analysis.
 What is the pattern of RTS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

as a function of the output group (small to large)? Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Nerlove RTS"

\end_inset

 plots RTS.
 We can see that there is increasing RTS for small firms, but that RTS is
 approximately constant for large firms.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Nerlove RTS"

\end_inset

RTS as a function of firm size
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Restrictions/rts.svg

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Using the Chow test on the Nerlove model, we reject that there is coefficient
 stability across the 5 groups.
 But perhaps we could restrict the input price coefficients to be the same
 but let the constant and output coefficients vary by group size.
 This new model is
\begin_inset Formula 
\begin{equation}
\ln C=\sum_{j=1}^{5}\alpha_{j}D_{j}+\sum_{j=1}^{5}\gamma_{j}D_{j}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon\label{Nerlove, favorite model}
\end{equation}

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
estimate this model by OLS, giving 
\begin_inset Formula $R^{2}$
\end_inset

, estimated standard errors for coefficients, t-statistics for tests of
 significance, and the associated p-values.
 Interpret the results in detail.
\end_layout

\begin_layout Enumerate
Test the restrictions implied by this model (relative to the model that
 lets all coefficients vary across groups) using the F, qF, Wald, score
 and likelihood ratio tests.
 Comment on the results.
\end_layout

\begin_layout Enumerate
Estimate this model but imposing the HOD1 restriction, 
\emph on
using an OLS
\emph default
 estimation program.
 Give estimated standard errors for all coefficients.
\end_layout

\begin_layout Enumerate
Plot the estimated RTS parameters as a function of firm size.
 Compare the plot to that given in the notes for the unrestricted model.
 Comment on the results.
\end_layout

\end_deeper
\begin_layout Enumerate
For the model of the above question, compute 95% confidence intervals for
 RTS for each of the 5 groups of firms, using the delta method to compute
 standard errors.
 Comment on the results.
\end_layout

\begin_layout Enumerate
Perform a Monte Carlo study that generates data from the model 
\begin_inset Formula 
\[
y=-2+1x_{2}+1x_{3}+\epsilon
\]

\end_inset

where the sample size is 30, 
\begin_inset Formula $x_{2}$
\end_inset

 and 
\begin_inset Formula $x_{3}$
\end_inset

 are independently uniformly distributed on 
\begin_inset Formula $[0,1]$
\end_inset

 and 
\begin_inset Formula $\epsilon\sim IIN(0,1)$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Compare the means and standard errors of the estimated coefficients using
 OLS and restricted OLS, imposing the restriction that 
\begin_inset Formula $\beta_{2}+\beta_{3}=2.$
\end_inset


\end_layout

\begin_layout Enumerate
Compare the means and standard errors of the estimated coefficients using
 OLS and restricted OLS, imposing the restriction that 
\begin_inset Formula $\beta_{2}+\beta_{3}=1.$
\end_inset


\end_layout

\begin_layout Enumerate
Discuss the results.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Stochastic-regressors"

\end_inset

Stochastic regressors
\end_layout

\begin_layout Standard
Up to now we have treated the regressors as fixed, which is clearly unrealistic.
 Now we will assume they are random.
 There are several ways to think of the problem.
 First, if we are interested in an analysis 
\emph on
conditional
\emph default
 on the explanatory variables, then it is irrelevant if they are stochastic
 or not, since conditional on the values of they regressors take on, they
 are nonstochastic, which is the case already considered.
\end_layout

\begin_layout Itemize
In cross-sectional analysis it is usually reasonable to make the analysis
 conditional on the regressors.
\end_layout

\begin_layout Itemize
In dynamic models, where 
\begin_inset Formula $y_{t}$
\end_inset

 may depend on 
\begin_inset Formula $y_{t-1},$
\end_inset

 a conditional analysis is not sufficiently general, since we may want to
 predict into the future many periods out, so we need to consider the behavior
 of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 and the relevant test statistics unconditional on 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Standard
The model we'll deal will involve a combination of the following assumptions
\end_layout

\begin_layout Assumption

\series bold
Linearity
\series default
: the model is a linear function of the parameter vector 
\begin_inset Formula $\beta_{0}:$
\end_inset


\begin_inset Formula 
\[
y_{t}=x_{t}^{\prime}\beta_{0}+\varepsilon_{t},
\]

\end_inset

 or in matrix form, 
\begin_inset Formula 
\[
y=X\beta_{0}+\varepsilon,
\]

\end_inset

 where 
\begin_inset Formula $y$
\end_inset

 is 
\begin_inset Formula $n\times1,$
\end_inset

 
\begin_inset Formula $X=\left(\begin{array}{cccc}
x_{1} & x_{2} & \cdots & x_{n}\end{array}\right)^{\prime},$
\end_inset

 where 
\begin_inset Formula $x_{t}$
\end_inset

 is 
\begin_inset Formula $K\times1,$
\end_inset

 and 
\begin_inset Formula $\beta_{0}$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are conformable.
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Stochastic, linearly independent regressors
\end_layout

\begin_layout Assumption
\begin_inset Formula $X$
\end_inset

 has rank 
\begin_inset Formula $K$
\end_inset

 with probability 1
\end_layout

\begin_layout Assumption
\begin_inset Formula $X$
\end_inset

 is stochastic
\end_layout

\begin_layout Assumption
\begin_inset Formula $\lim_{n\rightarrow\infty}\Pr\left(\frac{1}{n}X^{\prime}X=Q_{X}\right)=1,$
\end_inset

 where 
\begin_inset Formula $Q_{X}$
\end_inset

 is a finite positive definite matrix.
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Central limit theorem
\end_layout

\begin_layout Assumption
\begin_inset Formula $n^{-1/2}X^{\prime}\varepsilon\overset{d}{\rightarrow}N(0,Q_{X}\sigma_{0}^{2})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Normality (Optional)
\series default
: 
\begin_inset Formula $\varepsilon|X\sim N(0,\sigma^{2}I_{n})$
\end_inset

: 
\begin_inset Formula $\epsilon$
\end_inset

 is normally distributed 
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
Strongly exogenous
\series default
 
\series bold
regressors
\series default
.
 The regressors 
\begin_inset Formula $\mathbf{X}$
\end_inset

 are strongly exogenous if 
\begin_inset Formula 
\begin{eqnarray}
\mathcal{E}(\varepsilon_{t}|\mathbf{X}) & = & 0,\forall t\label{assumption: strong exogeneity}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Assumption

\series bold
\begin_inset CommandInset label
LatexCommand label
name "ass:Weakly-exogenous-regressors:"

\end_inset

Weakly exogenous regressors
\series default
: The regressors are weakly exogenous if 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\varepsilon_{t}|\mathbf{x}_{t}) & = & 0,\forall t
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In both cases, 
\begin_inset Formula $\mathbf{x}_{t}^{\prime}\beta$
\end_inset

 is the conditional mean of 
\begin_inset Formula $y_{t}$
\end_inset

 given 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

: 
\begin_inset Formula $E(y_{t}|\mathbf{x}_{t})=\mathbf{x}_{t}^{\prime}\beta$
\end_inset


\end_layout

\begin_layout Section
Case 1
\end_layout

\begin_layout Standard

\emph on
Normality of
\emph default
 
\begin_inset Formula $\varepsilon,$
\end_inset

 strongly exogenous regressors
\end_layout

\begin_layout Standard
In this case, 
\begin_inset Formula 
\[
\hat{\beta}=\beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\]

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\hat{\beta}|X) & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\mathcal{E}(\varepsilon|X)\\
 & = & \beta_{0}
\end{eqnarray*}

\end_inset

 and since this holds for all 
\begin_inset Formula $X,$
\end_inset

 
\begin_inset Formula $E(\hat{\beta})=\beta$
\end_inset

, unconditional on 
\begin_inset Formula $X.$
\end_inset

 Likewise,
\begin_inset Formula 
\[
\hat{\beta}|X\sim N\left(\beta,(X^{\prime}X)^{-1}\sigma_{0}^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
If the density of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula $d\mu(X),$
\end_inset

 the marginal density of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is obtained by multiplying the conditional density by 
\begin_inset Formula $d\mu(X)$
\end_inset

 and integrating over 
\begin_inset Formula $X.$
\end_inset

 Doing this leads to a nonnormal density for 
\begin_inset Formula $\hat{\beta},$
\end_inset

 in small samples.
\end_layout

\begin_layout Itemize
However, conditional on 
\begin_inset Formula $X,$
\end_inset

 the usual test statistics have the 
\begin_inset Formula $t,$
\end_inset

 
\begin_inset Formula $F$
\end_inset

 and 
\begin_inset Formula $\chi^{2}$
\end_inset

 distributions.
 
\emph on
Importantly,
\emph default
 these distributions don't depend on 
\begin_inset Formula $X,$
\end_inset

 so when marginalizing to obtain the unconditional distribution, nothing
 changes.
 The tests are valid in small samples.
\end_layout

\begin_layout Itemize
Summary: When 
\begin_inset Formula $X$
\end_inset

 is stochastic but strongly exogenous and 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\hat{\beta}$
\end_inset

 is unbiased
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\hat{\beta}$
\end_inset

 is nonnormally distributed
\end_layout

\begin_layout Enumerate
The usual test statistics have the same distribution as with nonstochastic
 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Enumerate
The Gauss-Markov theorem still holds, since it holds conditionally on 
\begin_inset Formula $X,$
\end_inset

 and this is true for all 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Enumerate
Asymptotic properties are treated in the next section.
 
\end_layout

\end_deeper
\begin_layout Section
Case 2
\end_layout

\begin_layout Standard
\begin_inset Formula $\varepsilon$
\end_inset


\emph on

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

nonnormally distributed, strongly exogenous regressors
\end_layout

\begin_layout Standard
The unbiasedness of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 carries through as before.
 However, the argument regarding test statistics doesn't hold, due to nonnormali
ty of 
\begin_inset Formula $\varepsilon.$
\end_inset

 Still, we have 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \beta_{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & \beta_{0}+\left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{n}
\end{eqnarray*}

\end_inset

 Now 
\begin_inset Formula 
\[
\left(\frac{X^{\prime}X}{n}\right)^{-1}\overset{p}{\rightarrow}Q_{X}^{-1}
\]

\end_inset

 by assumption, and 
\begin_inset Formula 
\[
\frac{X^{\prime}\varepsilon}{n}=\frac{n^{-1/2}X^{\prime}\varepsilon}{\sqrt{n}}\overset{p}{\rightarrow}0
\]

\end_inset

 since the numerator converges to a 
\begin_inset Formula $N(0,Q_{X}\sigma^{2})$
\end_inset

 r.v.
 and the denominator still goes to infinity.
 We have unbiasedness and the variance disappearing, so, 
\emph on
the estimator is consistent
\emph default
: 
\begin_inset Formula 
\[
\hat{\beta}\overset{p}{\rightarrow}\beta_{0}.
\]

\end_inset


\end_layout

\begin_layout Standard
Considering the asymptotic distribution 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right) & = & \sqrt{n}\left(\frac{X^{\prime}X}{n}\right)^{-1}\frac{X^{\prime}\varepsilon}{n}\\
 & = & \left(\frac{X^{\prime}X}{n}\right)^{-1}n^{-1/2}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta_{0}\right)\overset{d}{\rightarrow}N(0,Q_{X}^{-1}\sigma_{0}^{2})
\]

\end_inset

 directly following the assumptions.
 
\emph on
Asymptotic normality of the estimator still holds.

\emph default
 Since the asymptotic results on all test statistics only require this,
 all the previous asymptotic results on test statistics are also valid in
 this case.
\end_layout

\begin_layout Itemize
Summary: Under strongly exogenous regressors, with 
\begin_inset Formula $\varepsilon$
\end_inset

 normal or nonnormal, 
\begin_inset Formula $\hat{\beta}$
\end_inset

 has the properties:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Unbiasedness
\end_layout

\begin_layout Enumerate
Consistency
\end_layout

\begin_layout Enumerate
Gauss-Markov theorem holds, since it holds in the previous case and doesn't
 depend on normality.
\end_layout

\begin_layout Enumerate
Asymptotic normality
\end_layout

\begin_layout Enumerate
Tests are asymptotically valid
\end_layout

\begin_layout Enumerate
Tests are not valid in small samples if the error is normally distributed
\end_layout

\end_deeper
\begin_layout Section
Case 3
\end_layout

\begin_layout Standard

\emph on
Weakly exogenous regressors
\end_layout

\begin_layout Standard
An important class of models are 
\emph on
dynamic models
\emph default
, where lagged dependent variables have an impact on the current value.
 A simple version of these models that captures the important points is
 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & z_{t}^{\prime}\alpha+\sum_{s=1}^{p}\gamma_{s}y_{t-s}+\varepsilon_{t}\\
 & = & x_{t}^{\prime}\beta+\varepsilon_{t}
\end{eqnarray*}

\end_inset

 where now 
\begin_inset Formula $x_{t}$
\end_inset

 contains lagged dependent variables.
 Clearly, even with 
\begin_inset Formula $E(\epsilon_{t}|\mathbf{x}_{t})=0,$
\end_inset

 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are not uncorrelated, so one can't show unbiasedness.
 For example, 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon_{t-1}x_{t})\neq0
\]

\end_inset

 since 
\begin_inset Formula $x_{t}$
\end_inset

 contains 
\begin_inset Formula $y_{t-1}$
\end_inset

 (which is a function of 
\begin_inset Formula $\varepsilon_{t-1})$
\end_inset

 as an element.
\end_layout

\begin_layout Itemize
This fact implies that all of the small sample properties such as unbiasedness,
 Gauss-Markov theorem, and small sample validity of test statistics 
\emph on
do not hold
\emph default
 in this case.
 Recall Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-biasedness"

\end_inset

.
 This is a case of weakly exogenous regressors, and we see that the OLS
 estimator is biased in this case.
\end_layout

\begin_layout Itemize
Nevertheless, under the above assumptions, all asymptotic properties continue
 to hold, using the same arguments as before.
\end_layout

\begin_layout Section
When are the assumptions reasonable?
\end_layout

\begin_layout Standard
The two assumptions we've added are
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\lim_{n\rightarrow\infty}\Pr\left(\frac{1}{n}X^{\prime}X=Q_{X}\right)=1,$
\end_inset

 a 
\begin_inset Formula $Q_{X}$
\end_inset

 finite positive definite matrix.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $n^{-1/2}X^{\prime}\varepsilon\overset{d}{\rightarrow}N(0,Q_{X}\sigma_{0}^{2})$
\end_inset


\end_layout

\begin_layout Standard
The most complicated case is that of dynamic models, since the other cases
 can be treated as nested in this case.
 There exist a number of central limit theorems for dependent processes,
 many of which are fairly technical.
 We won't enter into details (see Hamilton, Chapter 7 if you're interested).
 A main requirement for use of standard asymptotics for a dependent sequence
 
\begin_inset Formula 
\[
\{s_{t}\}=\{\frac{1}{n}\sum_{t=1}^{n}z_{t}\}
\]

\end_inset

 to converge in probability to a finite limit is that 
\begin_inset Formula $z_{t}$
\end_inset

 be 
\emph on
stationary
\emph default
, in some sense.
\end_layout

\begin_layout Itemize
Strong stationarity requires that the joint distribution of the set 
\begin_inset Formula 
\[
\{z_{t},z_{t+s},z_{t-q},...\}
\]

\end_inset

 not depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Itemize
Covariance (weak) stationarity requires that the first and second moments
 of this set not depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Itemize
An example of a sequence that doesn't satisfy this is an AR(1) process with
 a unit root (a 
\emph on
random walk)
\emph default
: 
\begin_inset Formula 
\begin{eqnarray*}
x_{t} & = & x_{t-1}+\varepsilon_{t}\\
\varepsilon_{t} & \sim & IIN(0,\sigma^{2})
\end{eqnarray*}

\end_inset

 One can show that the variance of 
\begin_inset Formula $x_{t}$
\end_inset

 depends upon 
\begin_inset Formula $t$
\end_inset

 in this case, so it's not weakly stationary.
\end_layout

\begin_layout Itemize
The series 
\begin_inset Formula $\sin t+\epsilon_{t}$
\end_inset

 has a first moment that depends upon 
\begin_inset Formula $t$
\end_inset

, so it's not weakly stationary either.
\end_layout

\begin_layout Standard
Stationarity prevents the process from trending off to plus or minus infinity,
 and prevents cyclical behavior which would allow correlations between far
 removed 
\begin_inset Formula $z_{t}$
\end_inset

 znd 
\begin_inset Formula $z_{s}$
\end_inset

 to be high.
 
\emph on
Draw a picture here.
\end_layout

\begin_layout Itemize
In summary, the assumptions are reasonable when the stochastic conditioning
 variables have variances that are finite, and are not too strongly dependent.
 The AR(1) model with unit root is an example of a case where the dependence
 is too strong for standard asymptotics to apply.
\end_layout

\begin_layout Itemize
The study of nonstationary processes is an important part of econometrics,
 but it isn't in the scope of this course.
 
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Show that for two random variables 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B,$
\end_inset

 if 
\begin_inset Formula $E(A|B)=0,$
\end_inset

 then 
\begin_inset Formula $E\left(Af(B)\right)=0$
\end_inset

.
 How is this used in the proof of the Gauss-Markov theorem?
\end_layout

\begin_layout Enumerate
Is it possible for an AR(1) model for time series data, 
\emph on
e.g.,
\emph default
 
\begin_inset Formula $y_{t}=0+0.9y_{t-1}+\varepsilon_{t}$
\end_inset

 satisfy weak exogeneity? Strong exogeneity? Discuss.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Data problems
\end_layout

\begin_layout Standard
In this section we'll consider problems associated with the regressor matrix:
 collinearity, missing observations and measurement error.
\end_layout

\begin_layout Section
Collinearity
\end_layout

\begin_layout Subsection
Motivation: Data on Mortality and Related Factors
\begin_inset CommandInset label
LatexCommand label
name "sec:Motivation:-Data-on"

\end_inset


\end_layout

\begin_layout Standard
The data set 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{mortality.data}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/Data/mortality.data} 
\end_layout

\end_inset

 contains annual data from 1947 - 1980 on death rates in the U.S., along with
 data on factors like smoking and consumption of alcohol.
 The data description is:
\end_layout

\begin_layout Standard
DATA4-7: Death rates in the U.S.
 due to coronary heart disease and their
\end_layout

\begin_layout Standard
determinants.
 Data compiled by Jennifer Whisenand
\end_layout

\begin_layout Itemize
chd = death rate per 100,000 population (Range 321.2 - 375.4)
\end_layout

\begin_layout Itemize
cal = Per capita consumption of calcium per day in grams (Range 0.9 - 1.06)
\end_layout

\begin_layout Itemize
unemp = Percent of civilian labor force unemployed in 1,000 of persons 16
 years and older (Range 2.9 - 8.5)
\end_layout

\begin_layout Itemize
cig = Per capita consumption of cigarettes in pounds of tobacco by persons
 18 years and olderapprox.
 339 cigarettes per pound of tobacco (Range 6.75 - 10.46)
\end_layout

\begin_layout Itemize
edfat = Per capita intake of edible fats and oil in poundsincludes lard,
 margarine and butter (Range 42 - 56.5)
\end_layout

\begin_layout Itemize
meat = Per capita intake of meat in poundsincludes beef, veal, pork, lamb
 and mutton (Range 138 - 194.8)
\end_layout

\begin_layout Itemize
spirits = Per capita consumption of distilled spirits in taxed gallons for
 individuals 18 and older (Range 1 - 2.9)
\end_layout

\begin_layout Itemize
beer = Per capita consumption of malted liquor in taxed gallons for individuals
 18 and older (Range 15.04 - 34.9)
\end_layout

\begin_layout Itemize
wine = Per capita consumption of wine measured in taxed gallons for individuals
 18 and older (Range 0.77 - 2.65)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Consider estimation results for several models:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%% the following needs the amsmath LaTeX package
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{gather}
\end_layout

\begin_layout Plain Layout


\backslash
begin{split}
\end_layout

\begin_layout Plain Layout


\backslash
widehat{
\backslash
rm chd} &= 
\end_layout

\begin_layout Plain Layout


\backslash
underset{(58.939)}{334.914}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(5.156)}{5.41216}
\backslash
,
\backslash
mbox{cig}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(7.373)}{36.8783}
\backslash
,
\backslash
mbox{spirits}
\end_layout

\begin_layout Plain Layout

-
\backslash
underset{(1.2513)}{5.10365}
\backslash
,
\backslash
mbox{beer}
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

& +
\backslash
underset{(12.735)}{13.9764}
\backslash
,
\backslash
mbox{wine}
\end_layout

\begin_layout Plain Layout


\backslash
end{split}
\end_layout

\begin_layout Plain Layout

 
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

T = 34 
\backslash
quad 
\backslash
bar{R}^2 = 0.5528 
\backslash
quad F(4,29) = 11.2 
\backslash
quad 
\backslash
hat{
\backslash
sigma} = 9.9945
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
centerline{(standard errors in parentheses)} 
\backslash
notag
\end_layout

\begin_layout Plain Layout


\backslash
end{gather}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%% the following needs the amsmath LaTeX package
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{gather}
\end_layout

\begin_layout Plain Layout


\backslash
widehat{
\backslash
rm chd} = 
\end_layout

\begin_layout Plain Layout


\backslash
underset{(56.624)}{353.581}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(4.7523)}{3.17560}
\backslash
,
\backslash
mbox{cig}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(7.275)}{38.3481}
\backslash
,
\backslash
mbox{spirits}
\end_layout

\begin_layout Plain Layout

-
\backslash
underset{(1.0102)}{4.28816}
\backslash
,
\backslash
mbox{beer}
\end_layout

\begin_layout Plain Layout

 
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

T = 34 
\backslash
quad 
\backslash
bar{R}^2 = 0.5498 
\backslash
quad F(3,30) = 14.433 
\backslash
quad 
\backslash
hat{
\backslash
sigma} = 10.028
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
centerline{(standard errors in parentheses)} 
\backslash
notag
\end_layout

\begin_layout Plain Layout


\backslash
end{gather}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%% the following needs the amsmath LaTeX package
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{gather}
\end_layout

\begin_layout Plain Layout


\backslash
widehat{
\backslash
rm chd} = 
\end_layout

\begin_layout Plain Layout


\backslash
underset{(67.21)}{243.310}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(6.1508)}{10.7535}
\backslash
,
\backslash
mbox{cig}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(8.0359)}{22.8012}
\backslash
,
\backslash
mbox{spirits}
\end_layout

\begin_layout Plain Layout

-
\backslash
underset{(12.638)}{16.8689}
\backslash
,
\backslash
mbox{wine}
\end_layout

\begin_layout Plain Layout

 
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

T = 34 
\backslash
quad 
\backslash
bar{R}^2 = 0.3198 
\backslash
quad F(3,30) = 6.1709 
\backslash
quad 
\backslash
hat{
\backslash
sigma} = 12.327
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
centerline{(standard errors in parentheses)} 
\backslash
notag
\end_layout

\begin_layout Plain Layout


\backslash
end{gather}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%% the following needs the amsmath LaTeX package
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{gather}
\end_layout

\begin_layout Plain Layout


\backslash
widehat{
\backslash
rm chd} = 
\end_layout

\begin_layout Plain Layout


\backslash
underset{(49.119)}{181.219}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(4.4371)}{16.5146}
\backslash
,
\backslash
mbox{cig}
\end_layout

\begin_layout Plain Layout

+
\backslash
underset{(6.2079)}{15.8672}
\backslash
,
\backslash
mbox{spirits}
\end_layout

\begin_layout Plain Layout

 
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

T = 34 
\backslash
quad 
\backslash
bar{R}^2 = 0.3026 
\backslash
quad F(2,31) = 8.1598 
\backslash
quad 
\backslash
hat{
\backslash
sigma} = 12.481
\backslash
notag 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
centerline{(standard errors in parentheses)} 
\backslash
notag
\end_layout

\begin_layout Plain Layout


\backslash
end{gather}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note how the signs of the coefficients change depending on the model, and
 that the magnitudes of the parameter estimates vary a lot, too.
 The parameter estimates are highly sensitive to the particular model we
 estimate.
 Why? We'll see that the problem is that the data exhibit 
\emph on
collinearity
\emph default
.
\end_layout

\begin_layout Subsection
Collinearity: definition
\end_layout

\begin_layout Standard
Collinearity is the existence of linear relationships amongst the regressors.
 We can always write 
\begin_inset Formula 
\[
\lambda_{1}\mathbf{x}_{1}+\lambda_{2}\mathbf{x}_{2}+\cdots+\lambda_{K}\mathbf{x}_{K}+v=0
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 column of the regressor matrix 
\begin_inset Formula $X,$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector.
 In the case that there exists collinearity, the variation in 
\begin_inset Formula $v$
\end_inset

 is relatively small, so that there is an approximately exact linear relation
 between the regressors.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

relative
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

approximate
\begin_inset Quotes erd
\end_inset

 are imprecise, so it's difficult to define when collinearilty exists.
 
\end_layout

\begin_layout Standard
In the extreme, if there are exact linear relationships (every element of
 
\begin_inset Formula $v$
\end_inset

 equal) then 
\begin_inset Formula $\rho(X)<K,$
\end_inset

 so 
\begin_inset Formula $\rho(X^{\prime}X)<K,$
\end_inset

 so 
\begin_inset Formula $X^{\prime}X$
\end_inset

 is not invertible and the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is not uniquely defined.
 For example, if the model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \beta_{1}+\beta_{2}x_{2t}+\beta_{3}x_{3t}+\varepsilon_{t}\\
x_{2t} & = & \alpha_{1}+\alpha_{2}x_{3t}
\end{eqnarray*}

\end_inset

 then we can write 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \beta_{1}+\beta_{2}\left(\alpha_{1}+\alpha_{2}x_{3t}\right)+\beta_{3}x_{3t}+\varepsilon_{t}\\
 & = & \beta_{1}+\beta_{2}\alpha_{1}+\beta_{2}\alpha_{2}x_{3t}+\beta_{3}x_{3t}+\varepsilon_{t}\\
 & = & \left(\beta_{1}+\beta_{2}\alpha_{1}\right)+\left(\beta_{2}\alpha_{2}+\beta_{3}\right)x_{3t}\\
 & = & \gamma_{1}+\gamma_{2}x_{3t}+\varepsilon_{t}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\gamma^{\prime}s$
\end_inset

 can be consistently estimated, but since the 
\begin_inset Formula $\gamma^{\prime}$
\end_inset

s define two equations in three 
\begin_inset Formula $\beta^{\prime}s,$
\end_inset

 the 
\begin_inset Formula $\beta^{\prime}s$
\end_inset

 can't be consistently estimated (there are multiple values of 
\begin_inset Formula $\beta$
\end_inset

 that solve the first order conditions).
 The 
\begin_inset Formula $\beta^{\prime}s$
\end_inset

 are 
\emph on
unidentified
\emph default
 in the case of perfect collinearity.
\end_layout

\begin_layout Itemize
Perfect collinearity is unusual, except in the case of an error in construction
 of the regressor matrix, such as including the same regressor twice.
 
\end_layout

\begin_layout Standard
Another case where perfect collinearity may be encountered is with models
 with dummy variables, if one is not careful.
 Consider a model of rental price 
\begin_inset Formula $(y_{i})$
\end_inset

 of an apartment.
 This could depend factors such as size, quality etc., collected in 
\begin_inset Formula $x_{i},$
\end_inset

 as well as on the location of the apartment.
 Let 
\begin_inset Formula $B_{i}=1$
\end_inset

 if the 
\begin_inset Formula $i^{th}$
\end_inset

 apartment is in Barcelona, 
\begin_inset Formula $B_{i}=0$
\end_inset

 otherwise.
 Similarly, define 
\begin_inset Formula $G_{i},$
\end_inset

 
\begin_inset Formula $T_{i}$
\end_inset

 and 
\begin_inset Formula $L_{i}$
\end_inset

 for Girona, Tarragona and Lleida.
 One could use a model such as 
\begin_inset Formula 
\[
y_{i}=\beta_{1}+\beta_{2}B_{i}+\beta_{3}G_{i}+\beta_{4}T_{i}+\beta_{5}L_{i}+x_{i}^{\prime}\gamma+\varepsilon_{i}
\]

\end_inset

 In this model, 
\begin_inset Formula $B_{i}+G_{i}+T_{i}+L_{i}=1,$
\end_inset

 
\begin_inset Formula $\forall i,$
\end_inset

 so there is an exact relationship between these variables and the column
 of ones corresponding to the constant.
 One must either drop the constant, or one of the qualitative variables.
\end_layout

\begin_layout Subsection
A brief aside on dummy variables
\end_layout

\begin_layout Standard

\series bold
Dummy variable
\series default
: A dummy variable is a binary-valued variable that indicates whether or
 not some condition is true.
 It is customary to assign the value 1 if the condition is true, and 0 if
 the condition is false.
\end_layout

\begin_layout Standard
Dummy variables are used essentially like any other regressor.
 Use 
\begin_inset Formula $d$
\end_inset

 to indicate that a variable is a dummy, so that variables like 
\begin_inset Formula $d_{t}$
\end_inset

 and 
\begin_inset Formula $d_{t2}$
\end_inset

 are understood to be dummy variables.
 Variables like 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $x_{t3}$
\end_inset

 are ordinary continuous regressors.
 You know how to interpret the following models:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}d_{t}+\beta_{2}(1-d_{t})+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\beta_{3}x_{t}+\epsilon_{t}
\]

\end_inset


\series bold
Interaction terms:
\series default
 an interaction term is the product of two variables, so that the effect
 of one variable on the dependent variable depends on the value of the other.
 The following model has an interaction term.
 Note that 
\begin_inset Formula $\frac{\partial E(y|x)}{\partial x}=\beta_{3}+\beta_{4}d_{t}$
\end_inset

.
 The slope depends on the value of 
\begin_inset Formula $d_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\beta_{3}x_{t}+\beta_{4}d_{t}x_{t}+\epsilon_{t}
\]

\end_inset


\series bold
Multiple dummy variables: 
\series default
we can use more than one dummy variable in a model.
 We will study models of the form
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t1}+\beta_{3}d_{t2}+\beta_{4}x_{t}+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t1}+\beta_{3}d_{t2}+\beta_{4}d_{t1}d_{t2}+\beta_{5}x_{t}+\epsilon_{t}
\]

\end_inset


\series bold
Incorrect usage: 
\series default
You should understand why the following models are not correct usages of
 dummy variables:
\end_layout

\begin_layout Enumerate
overparameterization:
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d_{t}+\beta_{3}(1-d_{t})+\epsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Enumerate
multiple values assigned to multiple categories.
 Suppose that we a condition that defines 4 possible categories, and we
 create a variable 
\begin_inset Formula $d=1$
\end_inset

 if the observation is in the first category, 
\begin_inset Formula $d=2$
\end_inset

 if in the second, etc.
 (This is not strictly speaking a dummy variable, according to our definition).
 Why is the following model not a good one?
\begin_inset Formula 
\[
y_{t}=\beta_{1}+\beta_{2}d+\epsilon
\]

\end_inset

What is the correct way to deal with this situation?
\end_layout

\begin_layout Standard

\series bold
Multiple parameterizations.
 
\series default
To formulate a model that conditions on a given set of categorical information,
 there are multiple ways to use dummy variables.
 For example, the two models 
\begin_inset Formula 
\[
y_{t}=\beta_{1}d_{t}+\beta_{2}(1-d_{t})+\beta_{3}x_{t}+\beta_{4}d_{t}x_{t}+\epsilon_{t}
\]

\end_inset

and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=\alpha_{1}+\alpha_{2}d_{t}+\alpha_{3}x_{t}d_{t}+\alpha_{4}x_{t}(1-d_{t})+\epsilon_{t}
\]

\end_inset

are equivalent.
 You should know what are the 4 equations that relate the 
\begin_inset Formula $\beta_{j}$
\end_inset

 parameters to the 
\begin_inset Formula $\alpha_{j}$
\end_inset

 parameters, 
\begin_inset Formula $j=1,2,3,4.$
\end_inset

 You should know how to interpret the parameters of both models.
\end_layout

\begin_layout Subsection
Back to collinearity
\end_layout

\begin_layout Standard
The more common case, if one doesn't make mistakes such as these, is the
 existence of inexact linear relationships, 
\emph on
i.e.
\emph default
, correlations between the regressors that are less than one in absolute
 value, but not zero.
 The basic problem is that when two (or more) variables move together, it
 is difficult to determine their separate influences.
\end_layout

\begin_layout Example
Two children are in a room, along with a broken lamp.
 Both say 
\begin_inset Quotes sld
\end_inset

I didn't do it!
\begin_inset Quotes srd
\end_inset

.
 How can we tell who broke the lamp?
\end_layout

\begin_layout Standard
Lack of knowledge about the separate influences of variables is reflected
 in imprecise estimates, 
\emph on
i.e
\emph default
., estimates with high variances.
 
\emph on
With economic data, collinearity is commonly encountered, and is often a
 severe problem.
\end_layout

\begin_layout Standard
When there is collinearity, the minimizing point of the objective function
 that defines the OLS estimator (
\begin_inset Formula $s(\beta)$
\end_inset

, the sum of squared errors) is relatively poorly defined.
 This is seen in Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "nocollin"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "collin"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "nocollin"

\end_inset


\begin_inset Formula $s(\beta)$
\end_inset

 when there is no collinearity
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/nocollin.pdf

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "collin"

\end_inset


\begin_inset Formula $s(\beta)$
\end_inset

 when there is collinearity
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/collin.pdf

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To see the effect of collinearity on variances, partition the regressor
 matrix as 
\begin_inset Formula 
\[
X=\left[\begin{array}{cc}
\mathbf{x} & W\end{array}\right]
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the first column of 
\begin_inset Formula $X$
\end_inset

 (note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

we can interchange the columns of 
\begin_inset Formula $X$
\end_inset

 isf we like, so there's no loss of generality in considering the first
 column).
 Now, the variance of 
\begin_inset Formula $\hat{\beta},$
\end_inset

 under the classical assumptions, is 
\begin_inset Formula 
\[
V(\hat{\beta})=\left(X^{\prime}X\right)^{-1}\sigma^{2}
\]

\end_inset

 Using the partition, 
\begin_inset Formula 
\[
X^{\prime}X=\left[\begin{array}{cc}
\mathbf{x}^{\prime}\mathbf{x} & \mathbf{x}^{\prime}W\\
W^{\prime}\mathbf{x} & W^{\prime}W
\end{array}\right]
\]

\end_inset

 and following a rule for partitioned inversion, 
\begin_inset Formula 
\begin{eqnarray*}
\left(X^{\prime}X\right)_{1,1}^{-1} & = & \left(\mathbf{x}^{\prime}\mathbf{x}-\mathbf{x}^{\prime}W(W^{\prime}W)^{-1}W^{\prime}\mathbf{x}\right)^{-1}\\
 & = & \left(\mathbf{x}^{\prime}\left(I_{n}-W(W^{\prime}W)^{^{\prime}1}W^{\prime}\right)\mathbf{x}\right)^{-1}\\
 & = & \left(ESS_{\mathbf{x}|W}\right)^{-1}
\end{eqnarray*}

\end_inset

 where by 
\begin_inset Formula $ESS_{\mathbf{x}|W}$
\end_inset

 we mean the error sum of squares obtained from the regression 
\begin_inset Formula 
\[
\mathbf{x}=W\lambda+v.
\]

\end_inset

 Since 
\begin_inset Formula 
\[
R^{2}=1-ESS/TSS,
\]

\end_inset

 we have 
\begin_inset Formula 
\[
ESS=TSS(1-R^{2})
\]

\end_inset

 so the variance of the coefficient corresponding to 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
V(\hat{\beta}_{\mathbf{x}})=\frac{\sigma^{2}}{TSS_{\mathbf{x}}(1-R_{\mathbf{x}|W}^{2})}\label{eq:variance factors}
\end{equation}

\end_inset

 We see three factors influence the variance of this coefficient.
 It will be high if
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sigma^{2}$
\end_inset

 is large
\end_layout

\begin_layout Enumerate
There is little variation in 
\begin_inset Formula $\mathbf{x}.$
\end_inset

 
\emph on
Draw a picture here
\emph default
.
\end_layout

\begin_layout Enumerate
There is a strong linear relationship between 
\begin_inset Formula $x$
\end_inset

 and the other regressors, so that 
\begin_inset Formula $W$
\end_inset

 can explain the movement in 
\begin_inset Formula $\mathbf{x}$
\end_inset

 well.
 In this case, 
\begin_inset Formula $R_{\mathbf{x}|W}^{2}$
\end_inset

 will be close to 1.
 As 
\begin_inset Formula $R_{\mathbf{x}|W}^{2}\rightarrow1,V(\hat{\beta}_{\mathbf{x}})\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Standard
The last of these cases is collinearity.
\end_layout

\begin_layout Standard
Intuitively, when there are strong linear relations between the regressors,
 it is difficult to determine the separate influence of the regressors on
 the dependent variable.
 This can be seen by comparing the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

objective function in the case of no correlation between regressors with
 the objective function with correlation between the regressors.
 See the figures nocollin.ps (no correlation) and collin.ps (correlation),
 available on the web site.
\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{DataProblems/collinearity.jl}{https://github.com/mcreel/Economet
rics/blob/master/Examples/DataProblems/collinearity.jl} 
\end_layout

\end_inset

 performs a Monte Carlo study with correlated regressors.
 The model is 
\begin_inset Formula $y=1+x_{2}+x_{3}+\epsilon$
\end_inset

, where the correlation between 
\begin_inset Formula $x_{2}$
\end_inset

 and 
\begin_inset Formula $x_{3}$
\end_inset

can be set.
 Three estimators are used: OLS, OLS dropping 
\begin_inset Formula $x_{3}$
\end_inset

 (a false restriction), and restricted LS using 
\begin_inset Formula $\beta_{2}=\beta_{3}$
\end_inset

 (a true restriction).
 The output when the correlation between the two regressors is 0.9 is
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/DataProblems/collinearity.out"

\end_inset


\end_layout

\begin_layout Example
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Collinearity:-Monte-Carlo"

\end_inset

 shows histograms for the estimated 
\begin_inset Formula $\beta_{2},$
\end_inset

 for each of the three estimators.
 
\end_layout

\begin_layout Itemize
Check the biases and variances.
\end_layout

\begin_layout Itemize
repeat the experiment with a lower value of rho, and note how the standard
 errors of the OLS estimator change.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Collinearity:-Monte-Carlo"

\end_inset

Collinearity: Monte Carlo results
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/collin.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Detection of collinearity
\end_layout

\begin_layout Standard
The best way is simply to regress each explanatory variable in turn on the
 remaining regressors.
 If any of these auxiliary regressions has a high 
\begin_inset Formula $R^{2},$
\end_inset

 there is a problem of collinearity.
 Furthermore, this procedure identifies which parameters are affected.
\end_layout

\begin_layout Itemize
Sometimes, we're only interested in certain parameters.
 Collinearity isn't a problem if it doesn't affect what we're interested
 in estimating.
 
\end_layout

\begin_layout Standard
An alternative is to examine the matrix of correlations between the regressors.
 High correlations are sufficient but not necessary for severe collinearity.
\end_layout

\begin_layout Standard
Also indicative of collinearity is that the model fits well (high 
\begin_inset Formula $R^{2}),$
\end_inset

 but none of the variables is significantly different from zero (e.g., their
 separate influences aren't well determined).
\end_layout

\begin_layout Standard
In summary, the artificial regressions are the best approach if one wants
 to be careful.
\end_layout

\begin_layout Example
Nerlove data and collinearity.
 The simple Nerlove model is 
\begin_inset Formula 
\[
\ln C=\beta_{1}+\beta_{2}\ln Q+\beta_{3}\ln P_{L}+\beta_{4}\ln P_{F}+\beta_{5}\ln P_{K}+\epsilon
\]

\end_inset

When this model is estimated by OLS, some coefficients are not significant
 (see subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"

\end_inset

).
 Maybe this is due to collinearity? The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{DataProblems/NerloveCollinearity.jl}{https://github.com/mcreel/E
conometrics/blob/master/Examples/DataProblems/NerloveCollinearity.jl} 
\end_layout

\end_inset

 checks the regressors for collinearity.
 If you run this, you will see that collinearity is not a problem with this
 data.
 Why is the coefficient of 
\begin_inset Formula $\ln P_{K}$
\end_inset

 not significantly different from zero?
\end_layout

\begin_layout Subsection
Dealing with collinearity
\end_layout

\begin_layout Subsubsection
More information
\end_layout

\begin_layout Standard
Collinearity is a problem of an uninformative sample.
 The first question is: is all the available information being used? Is
 more data available? Are there coefficient restrictions that have been
 neglected? 
\emph on
Picture illustrating how a restriction can solve problem of perfect collinearity.
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Stochastic-restrictions-and"

\end_inset

Stochastic restrictions and ridge regression
\end_layout

\begin_layout Standard
Note: here's a nice introduction to ridge regression: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Supposing that there is no more data or neglected restrictions, one possibility
 is to change perspectives, to Bayesian econometrics.
 One can express prior beliefs regarding the coefficients using stochastic
 restrictions.
 A stochastic linear restriction would be something of the form 
\begin_inset Formula 
\[
R\beta=r+v
\]

\end_inset

 where 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 are as in the case of exact linear restrictions, but 
\begin_inset Formula $v$
\end_inset

 is a random vector.
 For example, the model could be 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
R\beta & = & r+v\\
\left(\begin{array}{c}
\varepsilon\\
v
\end{array}\right) & \sim & N\left(\begin{array}{c}
0\\
0
\end{array}\right),\left(\begin{array}{cc}
\sigma_{\varepsilon}^{2}I_{n} & 0_{n\times q}\\
0_{q\times n} & \sigma_{v}^{2}I_{q}
\end{array}\right)
\end{eqnarray*}

\end_inset

 This sort of model isn't in line with the classical interpretation of parameter
s as constants: according to this interpretation the left hand side of 
\begin_inset Formula $R\beta=r+v$
\end_inset

 is constant but the right is random.
 This model does fit the Bayesian perspective: we combine information coming
 from the model and the data, summarized in 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
\varepsilon & \sim & N(0,\sigma_{\varepsilon}^{2}I_{n})
\end{eqnarray*}

\end_inset

 with prior beliefs regarding the distribution of the parameter, summarized
 in
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
R\beta\sim N(r,\sigma_{v}^{2}I_{q})
\]

\end_inset

 Since the sample is random it is reasonable to suppose that 
\begin_inset Formula $\mathcal{E}(\varepsilon v^{\prime})=0,$
\end_inset

 which is the last piece of information in the specification.
 How can you estimate using this model? The solution is to treat the restriction
s as artificial data.
 Write 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y\\
r
\end{array}\right]=\left[\begin{array}{c}
X\\
R
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon\\
v
\end{array}\right]
\]

\end_inset

 This model is heteroscedastic, since 
\begin_inset Formula $\sigma_{\varepsilon}^{2}\neq\sigma_{v}^{2}.$
\end_inset

 Define the 
\emph on
prior precision
\emph default
 
\begin_inset Formula $k=\sigma_{\varepsilon}/\sigma_{v}.$
\end_inset

 This expresses the degree of belief in the restriction relative to the
 variability of the data.
 Supposing that we specify 
\begin_inset Formula $k,$
\end_inset

 then the model 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y\\
kr
\end{array}\right]=\left[\begin{array}{c}
X\\
kR
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon\\
kv
\end{array}\right]
\]

\end_inset

 is homoscedastic and can be estimated by OLS.
 Note that this estimator is biased.
 It is consistent, however, given that 
\begin_inset Formula $k$
\end_inset

 is a fixed constant, even if the restriction is false (this is in contrast
 to the case of false exact restrictions).
 To see this, note that there are 
\begin_inset Formula $Q$
\end_inset

 restrictions, where 
\begin_inset Formula $Q$
\end_inset

 is the number of rows of 
\begin_inset Formula $R.$
\end_inset

 As 
\begin_inset Formula $n\rightarrow\infty,$
\end_inset

 these 
\begin_inset Formula $Q$
\end_inset

 artificial observations have no weight in the objective function, so the
 estimator has the same limiting objective function as the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator, and is therefore consistent.
\end_layout

\begin_layout Standard
To motivate the use of stochastic restrictions, consider the expectation
 of the squared length of 
\begin_inset Formula $\hat{\beta}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\hat{\beta}^{\prime}\hat{\beta}) & = & \mathcal{E}\left\{ \left(\beta+\left(X^{\prime}X\right)^{-1}X^{\prime}\varepsilon\right)^{\prime}\left(\beta+\left(X^{\prime}X\right)^{-1}X^{\prime}\varepsilon\right)\right\} \\
 & = & \beta^{\prime}\beta+\mathcal{E}\left(\varepsilon^{\prime}X(X^{\prime}X)^{-1}(X^{\prime}X)^{-1}X^{\prime}\varepsilon\right)\\
 & = & \beta^{\prime}\beta+Tr\left(X^{\prime}X\right)^{-1}\sigma^{2}\\
 & = & \beta^{\prime}\beta+\sigma^{2}\sum_{i=1}^{K}\lambda_{i}\text{(the\:\ trace\:\ is\:\ the\:\ sum\:\ of\:\ eigenvalues)}\\
 & > & \beta^{\prime}\beta+\lambda_{\max\left(X^{\prime}X^{-1}\right)}\sigma^{2}\text{(the\:\ eigenvalues\:\ are\:\ all\:\ positive,\:\ since}X^{\prime}X\text{\:\ is\:\ p.d.}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
\mathcal{E}(\hat{\beta}^{\prime}\hat{\beta})>\beta^{\prime}\beta+\frac{\sigma^{2}}{\lambda_{\min\left(X^{\prime}X\right)}}
\]

\end_inset

 where 
\begin_inset Formula $\lambda_{\min\left(X^{\prime}X\right)}$
\end_inset

 is the minimum eigenvalue of 
\begin_inset Formula $X^{\prime}X$
\end_inset

 (which is the inverse of the maximum eigenvalue of 
\begin_inset Formula $(X^{\prime}X)^{-1}).$
\end_inset

 As collinearity becomes worse and worse, 
\begin_inset Formula $X^{\prime}X$
\end_inset

 becomes more nearly singular, so 
\begin_inset Formula $\lambda_{\min\left(X^{\prime}X\right)}$
\end_inset

 tends to zero (recall that the determinant is the product of the eigenvalues)
 and 
\begin_inset Formula $\mathcal{E}(\hat{\beta}^{\prime}\hat{\beta})$
\end_inset

 tends to infinite.
 On the other hand, 
\begin_inset Formula $\beta^{\prime}\beta$
\end_inset

 is finite.
\end_layout

\begin_layout Standard
Now considering the restriction 
\begin_inset Formula $I_{K}\beta=0+v.$
\end_inset

 With this restriction the model becomes 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y\\
0
\end{array}\right]=\left[\begin{array}{c}
X\\
kI_{K}
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon\\
kv
\end{array}\right]
\]

\end_inset

 and the estimator is 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{ridge} & = & \left(\left[\begin{array}{cc}
X^{\prime} & kI_{K}\end{array}\right]\left[\begin{array}{c}
X\\
kI_{K}
\end{array}\right]\right)^{-1}\left[\begin{array}{cc}
X^{\prime} & I_{K}\end{array}\right]\left[\begin{array}{c}
y\\
0
\end{array}\right]\\
 & = & \left(X^{\prime}X+k^{2}I_{K}\right)^{-1}X^{\prime}y
\end{eqnarray*}

\end_inset

 This is the ordinary 
\emph on
ridge regression
\emph default
 estimator.
 The ridge regression estimator can be seen to add 
\begin_inset Formula $k^{2}I_{K},$
\end_inset

 which is nonsingular, to 
\begin_inset Formula $X^{\prime}X,$
\end_inset

 which is more and more nearly singular as collinearity becomes worse and
 worse.
 As 
\begin_inset Formula $k\rightarrow\infty,$
\end_inset

 the restrictions tend to 
\begin_inset Formula $\beta=0,$
\end_inset

 that is, the coefficients are shrunken toward zero.
 Also, the estimator tends to 
\begin_inset Formula 
\[
\hat{\beta}_{ridge}=\left(X^{\prime}X+k^{2}I_{K}\right)^{-1}X^{\prime}y\rightarrow\left(k^{2}I_{K}\right)^{-1}X^{\prime}y=\frac{X^{\prime}y}{k^{2}}\rightarrow0
\]

\end_inset

 so 
\begin_inset Formula $\hat{\beta}_{ridge}^{\prime}\hat{\beta}_{ridge}\rightarrow0.$
\end_inset

 This is clearly a false restriction in the limit, if our original model
 is at all sensible.
\end_layout

\begin_layout Standard
There should be some amount of shrinkage that is in fact a true restriction.
 The problem is to determine the 
\begin_inset Formula $k$
\end_inset

 such that the restriction is correct.
 The interest in ridge regression centers on the fact that it can be shown
 that there exists a 
\begin_inset Formula $k$
\end_inset

 such that 
\begin_inset Formula $MSE(\hat{\beta}_{ridge})<\hat{\beta}_{OLS}.$
\end_inset

 The problem is that this 
\begin_inset Formula $k$
\end_inset

 depends on 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\sigma^{2},$
\end_inset

 which are unknown.
\end_layout

\begin_layout Standard
The ridge trace method plots 
\begin_inset Formula $\hat{\beta}_{ridge}^{\prime}\hat{\beta}_{ridge}$
\end_inset

 as a function of 
\begin_inset Formula $k,$
\end_inset

 and chooses the value of 
\begin_inset Formula $k$
\end_inset

 that 
\begin_inset Quotes eld
\end_inset

artistically
\begin_inset Quotes erd
\end_inset

 seems appropriate (e.g., where the effect of increasing 
\begin_inset Formula $k$
\end_inset

 dies off).
 
\emph on
Draw picture here.

\emph default
 This means of choosing 
\begin_inset Formula $k$
\end_inset

 is obviously subjective.
 This is not a problem from the Bayesian perspective: the choice of 
\begin_inset Formula $k$
\end_inset

 reflects prior beliefs about the length of 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Standard
In summary, the ridge estimator offers some hope, but it is impossible to
 guarantee that it will outperform the OLS estimator.
 Collinearity is a fact of life in econometrics, and there is no clear solution
 to the problem.
\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{DataProblems/RidgeRegression.jl}{https://github.com/mcreel/Econo
metrics/blob/master/Examples/DataProblems/RidgeRegression.jl} 
\end_layout

\end_inset

 does a Monte Carlo study that shows that ridge regression can help to deal
 with collinearity.
 This script generates the following figures, which show the Monte Carlo
 sampling frequency of the OLS and ridge estimators, after subtracting the
 true parameter values.
 You can see that the ridge estimator has much lower RMSE: both histograms
 are centered close to zero, but the ridge histogram is much tighter.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
OLS and Ridge regression
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/RidgeExample.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Measurement error
\end_layout

\begin_layout Standard
Measurement error is exactly what it says, either the dependent variable
 or the regressors are measured with error.
 Thinking about the way economic data are reported, measurement error is
 probably quite prevalent.
 For example, estimates of growth of GDP, inflation, etc.
 are commonly revised several times.
 Why should the last revision necessarily be correct?
\end_layout

\begin_layout Subsection
Error of measurement of the dependent variable
\end_layout

\begin_layout Standard
Measurement errors in the dependent variable and the regressors have important
 differences.
 First consider error in measurement of the dependent variable.
 The data generating process is presumed to be 
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & X\beta+\varepsilon\\
y & = & y^{*}+v\\
v_{t} & \sim & iid(0,\sigma_{v}^{2})
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $y^{*}=y+v$
\end_inset

 is the unobservable true dependent variable, and 
\begin_inset Formula $y$
\end_inset

 is what is observed.
 We assume that 
\begin_inset Formula $\varepsilon$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

 are independent and that 
\begin_inset Formula $y^{*}=X\beta+\varepsilon$
\end_inset

 satisfies the classical assumptions.
 Given this, we have 
\begin_inset Formula 
\[
y+v=X\beta+\varepsilon
\]

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon-v\\
 & = & X\beta+\omega\\
\omega_{t} & \sim & iid(0,\sigma_{\varepsilon}^{2}+\sigma_{v}^{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
As long as 
\begin_inset Formula $v$
\end_inset

 is uncorrelated with 
\begin_inset Formula $X,$
\end_inset

 this model satisfies the classical assumptions and can be estimated by
 OLS.
 This type of measurement error isn't a problem, then, except in that the
 increased variability of the error term causes an increase in the variance
 of the OLS estimator (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:variance factors"

\end_inset

).
 
\end_layout

\begin_layout Subsection
Error of measurement of the regressors
\end_layout

\begin_layout Standard
The situation isn't so good in this case.
 The DGP is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{*\prime}\beta+\varepsilon_{t}\\
x_{t} & = & x_{t}^{*}+v_{t}\\
v_{t} & \sim & iid(0,\Sigma_{v})
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\Sigma_{v}$
\end_inset

 is a 
\begin_inset Formula $K\times K$
\end_inset

 matrix.
 Now 
\begin_inset Formula $X^{*}$
\end_inset

 contains the true, unobserved regressors, and 
\begin_inset Formula $X$
\end_inset

 is what is observed.
 Again assume that 
\begin_inset Formula $v$
\end_inset

 is independent of 
\begin_inset Formula $\varepsilon,$
\end_inset

 and that the model 
\begin_inset Formula $y=X^{*}\beta+\varepsilon$
\end_inset

 satisfies the classical assumptions.
 Now we have 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \left(x_{t}-v_{t}\right)^{\prime}\beta+\varepsilon_{t}\\
 & = & x_{t}^{\prime}\beta-v_{t}^{\prime}\beta+\varepsilon_{t}\\
 & = & x_{t}^{\prime}\beta+\omega_{t}
\end{eqnarray*}

\end_inset

 The problem is that now there is a correlation between 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $\omega_{t},$
\end_inset

 since 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(x_{t}\omega_{t}) & = & \mathcal{E}\left(\left(x_{t}^{*}+v_{t}\right)\left(-v_{t}^{\prime}\beta+\varepsilon_{t}\right)\right)\\
 & = & -\Sigma_{v}\beta
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
\Sigma_{v}=\mathcal{E}\left(v_{t}v_{t}^{\prime}\right).
\]

\end_inset

 Because of this correlation, the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is biased and inconsistent, just as in the case of autocorrelated
 errors with lagged dependent variables.
 In matrix notation, write the estimated model as 
\begin_inset Formula 
\[
y=X\beta+\omega
\]

\end_inset

 We have that 
\begin_inset Formula 
\[
\hat{\beta}=\left(\frac{X^{\prime}X}{n}\right)^{-1}\left(\frac{X^{\prime}y}{n}\right)
\]

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
plim\left(\frac{X^{\prime}X}{n}\right)^{-1} & = & plim\frac{\left(X^{*\prime}+V^{\prime}\right)\left(X^{*}+V\right)}{n}\\
 & = & \left(Q_{X^{*}}+\Sigma_{v}\right)^{-1}
\end{eqnarray*}

\end_inset

 since 
\begin_inset Formula $X^{*}$
\end_inset

 and 
\begin_inset Formula $V$
\end_inset

 are independent, and 
\begin_inset Formula 
\begin{eqnarray*}
plim\frac{V^{\prime}V}{n} & = & \lim\mathcal{E}\frac{1}{n}\sum_{t=1}^{n}v_{t}v_{t}^{\prime}\\
 & = & \Sigma_{v}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Likewise, 
\begin_inset Formula 
\begin{eqnarray*}
plim\left(\frac{X^{\prime}y}{n}\right) & = & plim\frac{\left(X^{*\prime}+V^{\prime}\right)\left(X^{*}\beta+\varepsilon\right)}{n}\\
 & = & Q_{X^{*}}\beta
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
plim\hat{\beta}=\left(Q_{X^{*}}+\Sigma_{v}\right)^{-1}Q_{X^{*}}\beta
\]

\end_inset

 So we see that the least squares estimator is inconsistent when the regressors
 are measured with error.
\end_layout

\begin_layout Itemize
A potential solution to this problem is the instrumental variables (IV)
 estimator, which we'll discuss shortly.
 
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:Measurement-error-in"

\end_inset

Measurement error in a dynamic model.
 Consider the model 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}^{*} & = & \alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}\\
y_{t} & = & y_{t}^{*}+\upsilon_{t}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 are independent Gaussian white noise errors.
 Suppose that 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is not observed, and instead we observe 
\begin_inset Formula $y_{t}$
\end_inset

.
 What are the properties of the OLS regression on the equation
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\eta_{t}
\]

\end_inset

? The error is 
\begin_inset Formula 
\begin{align*}
\eta_{t} & =y_{t}-\alpha-\rho y_{t-1}-\beta x_{t}\\
 & =y_{t}^{*}+\upsilon_{t}-\alpha-\rho y_{t-1}^{*}-\rho\upsilon_{t-1}-\beta x_{t}\\
 & =\alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}+\upsilon_{t}-\alpha-\rho y_{t-1}^{*}-\rho\upsilon_{t-1}-\beta x_{t}\\
 & =\epsilon_{t}+\upsilon_{t}-\rho\upsilon_{t-1}
\end{align*}

\end_inset

So the error term is autocorrelated.
 Note that 
\begin_inset Formula 
\[
y_{t-1}=\alpha+\rho y_{t-2}+\beta x_{t-1}+\eta_{t-1}
\]

\end_inset

and 
\begin_inset Formula 
\[
\eta_{t-1}=\epsilon_{t-1}+\upsilon_{t-1}-\rho\upsilon_{t-2},
\]

\end_inset

so the error 
\begin_inset Formula $\eta_{t}$
\end_inset

 and the regressor 
\begin_inset Formula $y_{t-1}$
\end_inset

 are correlated, because they share the common term 
\begin_inset Formula $\upsilon_{t-1}.$
\end_inset

 This means that the equation 
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\eta_{t}
\]

\end_inset

 does not satisfy weak exogeneity, and the OLS estimator will be biased
 and inconsistent.
\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{DataProblems/MeasurementError.jl}{https://github.com/mcreel/Econ
ometrics/blob/master/Examples/DataProblems/MeasurementError.jl} 
\end_layout

\end_inset

 does a Monte Carlo study.
 The sample size is 
\begin_inset Formula $n=100$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:measurement error"

\end_inset

 gives the results.
 The first panel shows a histogram for 1000 replications of 
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

, when 
\begin_inset Formula $\sigma_{\nu}=1$
\end_inset

, so that there is significant measurement error.
 The second panel repeats this with 
\begin_inset Formula $\sigma_{\nu}=0,$
\end_inset

 so that there is not measurement error.
 Note that there is much more bias with measurement error.
 There is also bias without measurement error.
 This is due to the same reason that we saw bias in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "figure-biasedness"

\end_inset

: one of the classical assumptions (nonstochastic regressors) that guarantees
 unbiasedness of OLS does not hold for this model.
 Without measurement error, the OLS estimator 
\emph on
is 
\emph default
consistent.
 By re-running the script with larger 
\begin_inset Formula $n$
\end_inset

, you can verify that the bias disappears when 
\begin_inset Formula $\sigma_{\nu}=0$
\end_inset

, but not when 
\begin_inset Formula $\sigma_{\nu}>0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:measurement error"

\end_inset


\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

 with and without measurement error
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
with measurement error: 
\begin_inset Formula $\sigma_{\nu}=1$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/ylag_n100.svg
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
without measurement error: 
\begin_inset Formula $\sigma_{\nu}=0$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/ylag_n100_no_error.svg
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Missing observations
\end_layout

\begin_layout Standard
Missing observations occur quite frequently:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

time series data may not be gathered in a certain year, or respondents to
 a survey may not answer all questions.
 We'll consider two cases: missing observations on the dependent variable
 and missing observations on the regressors.
\end_layout

\begin_layout Subsection
Missing observations on the dependent variable
\end_layout

\begin_layout Standard
In this case, we have 
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

 or 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y_{1}\\
y_{2}
\end{array}\right]=\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]
\]

\end_inset

 where 
\begin_inset Formula $y_{2}$
\end_inset

 is not observed.
 Otherwise, we assume the classical assumptions hold.
\end_layout

\begin_layout Itemize
A clear alternative is to simply estimate using the compete observations
 
\begin_inset Formula 
\[
y_{1}=X_{1}\beta+\varepsilon_{1}
\]

\end_inset

 Since these observations satisfy the classical assumptions, one could estimate
 by OLS.
\end_layout

\begin_layout Itemize
The question remains whether or not one could somehow replace the unobserved
 
\begin_inset Formula $y_{2}$
\end_inset

 by a predictor, and improve over OLS in some sense.
 Let 
\begin_inset Formula $\hat{y}_{2}$
\end_inset

 be the predictor of 
\begin_inset Formula $y_{2}.$
\end_inset

 Now 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \left\{ \left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]^{\prime}\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]\right\} ^{-1}\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]^{\prime}\left[\begin{array}{c}
y_{1}\\
\hat{y}_{2}
\end{array}\right]\\
 & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}\left[X_{1}^{\prime}y_{1}+X_{2}^{\prime}\hat{y}_{2}\right]
\end{eqnarray*}

\end_inset

 Recall that the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

fonc are 
\begin_inset Formula 
\[
X^{\prime}X\hat{\beta}=X^{\prime}y
\]

\end_inset

 so if we regressed using only the first (complete)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

observations, we would have 
\begin_inset Formula 
\[
X_{1}^{\prime}X_{1}\hat{\beta}_{1}=X_{1}^{\prime}y_{1.}
\]

\end_inset

 Likewise, an OLS regression using only the second (filled in)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

observations would give 
\begin_inset Formula 
\[
X_{2}^{\prime}X_{2}\hat{\beta}_{2}=X_{2}^{\prime}\hat{y}_{2}.
\]

\end_inset

 Substituting these into the equation for the overall combined estimator
 gives 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}\left[X_{1}^{\prime}X_{1}\hat{\beta}_{1}+X_{2}^{\prime}X_{2}\hat{\beta}_{2}\right]\\
 & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{1}^{\prime}X_{1}\hat{\beta}_{1}+\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{2}^{\prime}X_{2}\hat{\beta}_{2}\\
 & \equiv & A\hat{\beta}_{1}+(I_{K}-A)\hat{\beta}_{2}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
A\equiv\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{1}^{\prime}X_{1}
\]

\end_inset

 and we use 
\begin_inset Formula 
\begin{eqnarray*}
\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{2}^{\prime}X_{2} & = & \left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}\left[\left(X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right)-X_{1}^{\prime}X_{1}\right]\\
 & = & I_{K}-\left[X_{1}^{\prime}X_{1}+X_{2}^{\prime}X_{2}\right]^{-1}X_{1}^{\prime}X_{1}\\
 & = & I_{K}-A.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Now, 
\begin_inset Formula 
\[
\mathcal{E}(\hat{\beta})=A\beta+(I_{K}-A)\mathcal{E}\left(\hat{\beta}_{2}\right)
\]

\end_inset

 and this will be unbiased only if 
\begin_inset Formula $\mathcal{E}\left(\hat{\beta}_{2}\right)=\beta.$
\end_inset


\end_layout

\begin_layout Itemize
The conclusion is that the filled in observations alone would need to define
 an unbiased estimator.
 This will be the case only if 
\begin_inset Formula 
\[
\hat{y}_{2}=X_{2}\beta+\hat{\varepsilon}_{2}
\]

\end_inset

 where 
\begin_inset Formula $\hat{\varepsilon}_{2}$
\end_inset

 has mean zero.
 Clearly, it is difficult to satisfy this condition without knowledge of
 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Itemize
Note that putting 
\begin_inset Formula $\hat{y}_{2}=\bar{y}_{1}$
\end_inset

 does not satisfy the condition and therefore leads to a biased estimator.
 
\end_layout

\begin_layout Exercise
Formally prove this last statement.
\end_layout

\begin_layout Subsection
The sample selection problem
\end_layout

\begin_layout Standard
In the above discussion we assumed that the missing observations are random.
 The sample selection problem is a case where the missing observations are
 not random.
 Consider the model 
\begin_inset Formula 
\[
y_{t}^{*}=x_{t}^{\prime}\beta+\varepsilon_{t}
\]

\end_inset

 which is assumed to satisfy the classical assumptions.
 However, 
\begin_inset Formula $y_{t}^{*\;}$
\end_inset

 is not always observed.
 What is observed is 
\begin_inset Formula $y_{t}$
\end_inset

 defined as 
\begin_inset Formula 
\[
y_{t}=y_{t}^{*}\text{\textnormal{ }if }y_{t}^{*}\geq0
\]

\end_inset

 Or, in other words, 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is missing when it is less than zero.
\end_layout

\begin_layout Standard
The difference in this case is that the missing values are not random:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

they are correlated with the 
\begin_inset Formula $x_{t}.$
\end_inset

 Consider the case 
\begin_inset Formula 
\[
y^{*}=x+\varepsilon
\]

\end_inset

 with 
\begin_inset Formula $V(\varepsilon)=25$
\end_inset

, but using only the observations for which 
\begin_inset Formula $y^{*}>0$
\end_inset

 to estimate.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Sample-selection-bias"

\end_inset

 illustrates the bias.
 The Julia program is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{sampsel.jl}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/DataProblems/sampsel.jl} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Sample-selection-bias"

\end_inset

Sample selection bias
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DataProblems/sampsel.svg

\end_inset


\end_layout

\end_inset

There are means of dealing with sample selection bias, but we will not go
 into it here.
 One should at least be aware that nonrandom selection of the sample will
 normally lead to bias and inconsistency if the problem is not taken into
 account.
\end_layout

\begin_layout Subsection
Missing observations on the regressors
\end_layout

\begin_layout Standard
Again the model is 
\begin_inset Formula 
\[
\left[\begin{array}{c}
y_{1}\\
y_{2}
\end{array}\right]=\left[\begin{array}{c}
X_{1}\\
X_{2}
\end{array}\right]\beta+\left[\begin{array}{c}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]
\]

\end_inset

 but we assume now that each row of 
\begin_inset Formula $X_{2}$
\end_inset

 has an unobserved component(s).
 Again, one could just estimate using the complete observations, but it
 may seem frustrating to have to drop observations simply because of a single
 missing variable.
 In general, if the unobserved 
\begin_inset Formula $X_{2}$
\end_inset

 is replaced by some prediction, 
\begin_inset Formula $X_{2}^{*},$
\end_inset

 then we are in the case of errors of observation.
 As before, this means that the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is biased when 
\begin_inset Formula $X_{2}^{*}$
\end_inset

 is used instead of 
\begin_inset Formula $X_{2}.$
\end_inset

 Consistency is salvaged, however, as long as the number of missing observations
 doesn't increase with 
\begin_inset Formula $n.$
\end_inset


\end_layout

\begin_layout Itemize
Including observations that have missing values replaced by 
\emph on
ad hoc
\emph default
 values can be interpreted as introducing false stochastic restrictions.
 In general, this introduces bias.
 It is difficult to determine whether MSE increases or decreases.
 Monte Carlo studies suggest that it is dangerous to simply substitute the
 mean, for example.
\end_layout

\begin_layout Itemize
In the case that there is only one regressor other than the constant, substituti
on of 
\begin_inset Formula $\bar{x}$
\end_inset

 for the missing 
\begin_inset Formula $x_{t}$
\end_inset

 
\emph on
does not lead to bias
\emph default
.
 This is a special case that doesn't hold for 
\begin_inset Formula $K>2.$
\end_inset


\end_layout

\begin_layout Exercise
Prove this last statement.
\end_layout

\begin_layout Itemize
In summary, if one is strongly concerned with bias, it is best to drop observati
ons that have missing components.
 There is potential for reduction of MSE through filling in missing elements
 with intelligent guesses, but this could also increase MSE.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Missing-regressors"

\end_inset

Missing regressors
\end_layout

\begin_layout Standard
Suppose that the model 
\begin_inset Formula $y=X\beta+W\gamma+\epsilon$
\end_inset

 satisfies the classical assumptions, so OLS would be a consistent estimator.
 However, let's suppose that the regressors 
\begin_inset Formula $W$
\end_inset

 are not available in the sample.
 What are the properties of the OLS estimator of the model 
\begin_inset Formula $y=X\beta+\omega?$
\end_inset

 We can think of this as a case of imposing false restrictions: 
\begin_inset Formula $\gamma=0$
\end_inset

 when in fact 
\begin_inset Formula $\gamma\ne0$
\end_inset

.
 We know that the restricted least squares estimator is biased and inconsistent,
 in general, when we impose false restrictions.
 Another way of thinking of this is to look to see if 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

 are correlated.
 We have
\begin_inset Formula 
\begin{align*}
E(X_{t}\omega_{t}) & =E\left(X_{t}\left(W_{t}^{\prime}\gamma+\epsilon_{t}\right)\right)\\
 & =E(X_{t}W_{t}^{\prime}\gamma)+E(X_{t}\epsilon_{t})\\
 & =E(X_{t}W_{t}^{\prime}\gamma)
\end{align*}

\end_inset

where the last line follows because 
\begin_inset Formula $E(X_{t}\epsilon_{t})=0$
\end_inset

 by assumption.
 So, there will be correlation between the error and the regressors if there
 is collinearity between the included regressors 
\begin_inset Formula $X_{t}$
\end_inset

 and the missing regressors 
\begin_inset Formula $W_{t}$
\end_inset

.
 If there is not, the OLS estimator will be consistent.
 Because the normal thing is to have collinearity between regressors, we
 expect that missing regressors will lead to bias and inconsistency of the
 OLS estimator.
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Consider the simple Nerlove model 
\begin_inset Formula 
\[
\ln C=\beta_{1}+\beta_{2}\ln Q+\beta_{3}\ln P_{L}+\beta_{4}\ln P_{F}+\beta_{5}\ln P_{K}+\epsilon
\]

\end_inset

When this model is estimated by OLS, some coefficients are not significant.
 We have seen that collinearity is not an important problem.
 Why is 
\begin_inset Formula $\beta_{5}$
\end_inset

 not significantly different from zero? Give an economic explanation.
\end_layout

\begin_layout Enumerate
For the model 
\begin_inset Formula $y=\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon,$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
verify that the level sets of the OLS criterion function (defined in equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:OLS criterion function"

\end_inset

) are straight lines when there is perfect collinearity
\end_layout

\begin_layout Enumerate
For this model with perfect collinearity, the OLS estimator does not exist.
 Depict what this statement means using a drawing.
\end_layout

\begin_layout Enumerate
Show how a restriction 
\begin_inset Formula $R_{1}\beta_{1}+R_{2}\beta_{2}=r$
\end_inset

 causes the restricted least squares estimator to exist, using a drawing.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Functional form and nonnested tests
\end_layout

\begin_layout Standard
Though theory often suggests which conditioning variables should be included,
 and suggests the signs of certain derivatives, it is usually silent regarding
 the functional form of the relationship between the dependent variable
 and the regressors.
 For example, considering a cost function, one could have a Cobb-Douglas
 model 
\begin_inset Formula 
\[
c=Aw_{1}^{\beta_{1}}w_{2}^{\beta_{2}}q^{\beta_{q}}e^{\varepsilon}
\]

\end_inset

 This model, after taking logarithms, gives 
\begin_inset Formula 
\[
\ln c=\beta_{0}+\beta_{1}\ln w_{1}+\beta_{2}\ln w_{2}+\beta_{q}\ln q+\varepsilon
\]

\end_inset

 where 
\begin_inset Formula $\beta_{0}=\ln A.$
\end_inset

 Theory suggests that 
\begin_inset Formula $A>0,\beta_{1}>0,\beta_{2}>0,\beta_{3}>0.$
\end_inset

 This model isn't compatible with a fixed cost of production since 
\begin_inset Formula $c=0$
\end_inset

 when 
\begin_inset Formula $q=0.$
\end_inset

 Homogeneity of degree one in input prices suggests that 
\begin_inset Formula $\beta_{1}+\beta_{2}=1,$
\end_inset

 while constant returns to scale implies 
\begin_inset Formula $\beta_{q}=1.$
\end_inset


\end_layout

\begin_layout Standard
While this model may be reasonable in some cases, an alternative 
\begin_inset Formula 
\[
\sqrt{c}=\beta_{0}+\beta_{1}\sqrt{w_{1}}+\beta_{2}\sqrt{w_{2}}+\beta_{q}\sqrt{q}+\varepsilon
\]

\end_inset

 may be just as plausible.
 Note that 
\begin_inset Formula $\sqrt{x}$
\end_inset

 and 
\begin_inset Formula $\ln(x)$
\end_inset

 look quite alike, for certain values of the regressors, and up to a linear
 transformation, so it may be difficult to choose between these models.
\end_layout

\begin_layout Standard
The basic point is that many functional forms are compatible with the linear-in-
parameters model, since this model can incorporate a wide variety of nonlinear
 transformations of the dependent variable and the regressors.
 For example, suppose that 
\begin_inset Formula $g(\cdot)$
\end_inset

 is a real valued function and that 
\begin_inset Formula $x(\cdot)$
\end_inset

 is a 
\begin_inset Formula $K-$
\end_inset

 vector-valued function.
 The following model is linear in the parameters but nonlinear in the variables:
 
\begin_inset Formula 
\begin{eqnarray*}
x_{t} & = & x(z_{t})\\
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}
\end{eqnarray*}

\end_inset

 There may be 
\begin_inset Formula $P$
\end_inset

 fundamental conditioning variables 
\begin_inset Formula $z_{t}$
\end_inset

, but there may be 
\begin_inset Formula $K$
\end_inset

 regressors, where 
\begin_inset Formula $K$
\end_inset

 may be smaller than, equal to or larger than 
\begin_inset Formula $P.$
\end_inset

 For example, 
\begin_inset Formula $x_{t}$
\end_inset

 could include squares and cross products of the conditioning variables
 in 
\begin_inset Formula $z_{t}.$
\end_inset


\end_layout

\begin_layout Section
Flexible functional forms
\end_layout

\begin_layout Standard
Given that the functional form of the relationship between the dependent
 variable and the regressors is in general unknown, one might wonder if
 there exist parametric models that can closely approximate a wide variety
 of functional relationships.
 A 
\begin_inset Quotes eld
\end_inset

Diewert-Flexible
\begin_inset Quotes erd
\end_inset

 functional form is defined as one such that the function, the vector of
 first derivatives and the matrix of second derivatives can take on an arbitrary
 value 
\emph on
at a single data point.

\emph default
 Flexibility in this sense clearly requires that there be at least 
\begin_inset Formula 
\[
K=1+P+\left(P^{2}-P\right)/2+P
\]

\end_inset

 free parameters: one for each independent effect that we wish to model.
\end_layout

\begin_layout Standard
Suppose that the model is 
\begin_inset Formula 
\[
y=g(x)+\varepsilon
\]

\end_inset

 A second-order Taylor's series expansion (with remainder term) of the function
 
\begin_inset Formula $g(x)$
\end_inset

 about the point 
\begin_inset Formula $x=0$
\end_inset

 is 
\begin_inset Formula 
\[
g(x)=g(0)+x^{\prime}D_{x}g(0)+\frac{x^{\prime}D_{x}^{2}g(0)x}{2}+R
\]

\end_inset

 Use the approximation, which simply drops the remainder term, as an approximati
on to 
\begin_inset Formula $g(x):$
\end_inset


\begin_inset Formula 
\[
g(x)\simeq g_{K}(x)=g(0)+x^{\prime}D_{x}g(0)+\frac{x^{\prime}D_{x}^{2}g(0)x}{2}
\]

\end_inset

 As 
\begin_inset Formula $x\rightarrow0,$
\end_inset

 the approximation becomes more and more exact, in the sense that 
\begin_inset Formula $g_{K}(x)\rightarrow g(x),$
\end_inset

 
\begin_inset Formula $D_{x}g_{K}(x)\rightarrow D_{x}g(x)$
\end_inset

 and 
\begin_inset Formula $D_{x}^{2}g_{K}(x)\rightarrow D_{x}^{2}g(x).$
\end_inset

 For 
\begin_inset Formula $x=0,$
\end_inset

 the approximation is exact, up to the second order.
 The idea behind many flexible functional forms is to note that 
\begin_inset Formula $g(0),$
\end_inset

 
\begin_inset Formula $D_{x}g(0)$
\end_inset

 and 
\begin_inset Formula $D_{x}^{2}g(0)$
\end_inset

 are all constants.
 If we treat them as parameters, the approximation will have exactly enough
 free parameters to approximate the function 
\begin_inset Formula $g(x),$
\end_inset

 which is of unknown form, exactly, up to second order, at the point 
\begin_inset Formula $x=0.$
\end_inset

 The model is 
\begin_inset Formula 
\[
g_{K}(x)=\alpha+x^{\prime}\beta+1/2x^{\prime}\Gamma x
\]

\end_inset

 so the regression model to fit is 
\begin_inset Formula 
\[
y=\alpha+x^{\prime}\beta+1/2x^{\prime}\Gamma x+\varepsilon
\]

\end_inset


\end_layout

\begin_layout Itemize
While the regression model has enough free parameters to be Diewert-flexible,
 the question remains: is 
\begin_inset Formula $plim\hat{\alpha}=g(0)?$
\end_inset

 Is 
\begin_inset Formula $plim\hat{\beta}=D_{x}g(0)?$
\end_inset

 Is 
\begin_inset Formula $plim\hat{\Gamma}=D_{x}^{2}g(0)?$
\end_inset


\end_layout

\begin_layout Itemize
The answer is no, in general.
 The reason is that if we treat the true values of the parameters as these
 derivatives, then 
\begin_inset Formula $\varepsilon$
\end_inset

 is forced to play the part of the remainder term, which is a function of
 
\begin_inset Formula $x,$
\end_inset

 so that 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are correlated in this case.
 As before, the estimator is biased in this case.
\end_layout

\begin_layout Itemize
A simpler example would be to consider a first-order T.S.
 approximation to a quadratic function.
 
\emph on
Draw picture.
\end_layout

\begin_layout Itemize
The conclusion is that 
\begin_inset Quotes eld
\end_inset

flexible functional forms
\begin_inset Quotes erd
\end_inset

 aren't really flexible in a useful statistical sense, in that neither the
 function itself nor its derivatives are consistently estimated, unless
 the function belongs to the parametric family of the specified functional
 form.
 In order to lead to consistent inferences, the regression model must be
 correctly specified.
 
\end_layout

\begin_layout Subsection
The translog form
\end_layout

\begin_layout Standard
In spite of the fact that FFF's aren't really flexible for the purposes
 of econometric estimation and inference, they are useful, and they are
 certainly subject to less bias due to misspecification of the functional
 form than are many popular forms, such as the Cobb-Douglas or the simple
 linear in the variables model.
 The translog model is probably the most widely used FFF.
 This model is as above, except that the variables are subjected to a logarithmi
c tranformation.
 Also, the expansion point is usually taken to be the sample mean of the
 data, after the logarithmic transformation.
 The model is defined by 
\begin_inset Formula 
\begin{eqnarray*}
y & = & \ln(c)\\
x & = & \ln\left(\frac{z}{\bar{z}}\right)\\
 & = & \ln(z)-\ln(\bar{z})\\
y & = & \alpha+x^{\prime}\beta+1/2x^{\prime}\Gamma x+\varepsilon
\end{eqnarray*}

\end_inset

 In this presentation, the 
\begin_inset Formula $t$
\end_inset

 subscript that distinguishes observations is suppressed for simplicity.
 Note that 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial y}{\partial x} & = & \beta+\Gamma x\\
 & = & \frac{\partial\ln(c)}{\partial\ln(z)}\text{\:(the\:\ other\:\ part\:\ of\:}x\,\text{is\:\ constant)}\\
 & = & \frac{\partial c}{\partial z}\frac{z}{c}
\end{eqnarray*}

\end_inset

 which is the elasticity of 
\begin_inset Formula $c$
\end_inset

 with respect to 
\begin_inset Formula $z.$
\end_inset

 This is a convenient feature of the translog model.
 Note that at the means of the conditioning variables, 
\begin_inset Formula $\bar{z}$
\end_inset

, 
\begin_inset Formula $x=0,$
\end_inset

 so 
\begin_inset Formula 
\[
\left.\frac{\partial y}{\partial x}\right|_{z=\bar{z}}=\beta
\]

\end_inset

 so the 
\begin_inset Formula $\beta$
\end_inset

 are the first-order elasticities, at the means of the data.
\end_layout

\begin_layout Standard
To illustrate, consider that 
\begin_inset Formula $y$
\end_inset

 is cost of production: 
\begin_inset Formula 
\[
y=c(w,q)
\]

\end_inset

 where 
\begin_inset Formula $w$
\end_inset

 is a vector of input prices and 
\begin_inset Formula $q$
\end_inset

 is output.
 We could add other variables by extending 
\begin_inset Formula $q$
\end_inset

 in the obvious manner, but this is supressed for simplicity.
 By Shephard's lemma, the conditional factor demands are 
\begin_inset Formula 
\[
x=\frac{\partial c(w,q)}{\partial w}
\]

\end_inset

 and the cost shares of the factors are therefore 
\begin_inset Formula 
\[
s=\frac{wx}{c}=\frac{\partial c(w,q)}{\partial w}\frac{w}{c}
\]

\end_inset

 which is simply the vector of elasticities of cost with respect to input
 prices.
 If the cost function is modeled using a translog function, we have 
\begin_inset Formula 
\begin{eqnarray*}
\ln(c) & = & \alpha+x^{\prime}\beta+z^{\prime}\delta+1/2\left[\begin{array}{cc}
x^{\prime} & z\end{array}\right]\left[\begin{array}{cc}
\Gamma_{11} & \Gamma_{12}\\
\Gamma_{12}^{\prime} & \Gamma_{22}
\end{array}\right]\left[\begin{array}{c}
x\\
z
\end{array}\right]\\
 & = & \alpha+x^{\prime}\beta+z^{\prime}\delta+1/2x^{\prime}\Gamma_{11}x+x^{\prime}\Gamma_{12}z+1/2z^{2}\gamma_{22}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $x=\ln(w/\bar{w})$
\end_inset

 (element-by-element division) and 
\begin_inset Formula $z=\ln(q/\bar{q}),$
\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
\Gamma_{11} & = & \left[\begin{array}{ll}
\gamma_{11} & \gamma_{12}\\
\gamma_{12} & \gamma_{22}
\end{array}\right]\\
\Gamma_{12} & = & \left[\begin{array}{l}
\gamma_{13}\\
\gamma_{23}
\end{array}\right]\\
\Gamma_{22} & = & \gamma_{33}.
\end{eqnarray*}

\end_inset

 Note that symmetry of the second derivatives has been imposed.
\end_layout

\begin_layout Standard
Then the share equations are just 
\begin_inset Formula 
\[
s=\beta+\left[\begin{array}{cc}
\Gamma_{11} & \Gamma_{12}\end{array}\right]\left[\begin{array}{c}
x\\
z
\end{array}\right]
\]

\end_inset

 Therefore, the share equations and the cost equation have parameters in
 common.
 By pooling the equations together and imposing the (true) restriction that
 the parameters of the equations be the same, we can gain efficiency.
\end_layout

\begin_layout Standard
To illustrate in more detail, consider the case of two inputs, so 
\begin_inset Formula 
\[
x=\left[\begin{array}{l}
x_{1}\\
x_{2}
\end{array}\right].
\]

\end_inset

 In this case the translog model of the logarithmic cost function is 
\begin_inset Formula 
\[
\ln c=\alpha+\beta_{1}x_{1}+\beta_{2}x_{2}+\delta z+\frac{\gamma_{11}}{2}x_{1}^{2}+\frac{\gamma_{22}}{2}x_{2}^{2}+\frac{\gamma_{33}}{2}z^{2}+\gamma_{12}x_{1}x_{2}+\gamma_{13}x_{1}z+\gamma_{23}x_{2}z
\]

\end_inset

 The two cost shares of the inputs are the derivatives of 
\begin_inset Formula $\ln c$
\end_inset

 with respect to 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
s_{1} & = & \beta_{1}+\gamma_{11}x_{1}+\gamma_{12}x_{2}+\gamma_{13}z\\
s_{2} & = & \beta_{2}+\gamma_{12}x_{1}+\gamma_{22}x_{2}+\gamma_{13}z
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that the share equations and the cost equation have parameters in common.
 One can do a pooled estimation of the three equations at once, imposing
 that the parameters are the same.
 In this way we're using more observations and therefore more information,
 which will lead to imporved efficiency.
 Note that this does assume that the cost equation is correctly specified
 (
\emph on
i.e.,
\emph default
 not an approximation), since otherwise the derivatives would not be the
 true derivatives of the log cost function, and would then be misspecified
 for the shares.
 To pool the equations, write the model in matrix form (adding in error
 terms) 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\ln c\\
s_{1}\\
s_{2}
\end{array}\right]=\left[\begin{array}{llllllllll}
1 & x_{1} & x_{2} & z & \frac{x_{1}^{2}}{2} & \frac{x_{2}^{2}}{2} & \frac{z^{2}}{2} & x_{1}x_{2} & x_{1}z & x_{2}z\\
0 & 1 & 0 & 0 & x_{1} & 0 & 0 & x_{2} & z & 0\\
0 & 0 & 1 & 0 & 0 & x_{2} & 0 & x_{1} & 0 & z
\end{array}\right]\left[\begin{array}{l}
\alpha\\
\beta_{1}\\
\beta_{2}\\
\delta\\
\gamma_{11}\\
\gamma_{22}\\
\gamma_{33}\\
\gamma_{12}\\
\gamma_{13}\\
\gamma_{23}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\varepsilon_{3}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
This is 
\emph on
one
\emph default
 observation on the three equations.
 With the appropriate notation, a single observation can be written as 
\begin_inset Formula 
\[
y_{t}={X_{t}\theta}+\varepsilon_{t}
\]

\end_inset

 The overall model would stack 
\begin_inset Formula $n$
\end_inset

 observations on the three equations for a total of 
\begin_inset Formula $3n$
\end_inset

 observations: 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right]=\left[\begin{array}{l}
X_{1}\\
X_{2}\\
\vdots\\
X_{n}
\end{array}\right]\theta+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{n}
\end{array}\right]
\]

\end_inset

 Next we need to consider the errors.
 For observation 
\begin_inset Formula $t$
\end_inset

 the errors can be placed in a vector 
\begin_inset Formula 
\[
\varepsilon_{t}=\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}\\
\varepsilon_{3t}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
First consider the covariance matrix of this vector:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the shares are certainly correlated since they must sum to one.
 (In fact, with 2 shares the variances are equal and the covariance is -1
 times the variance.
 General notation is used to allow easy extension to the case of more than
 2 inputs).
 Also, it's likely that the shares and the cost equation have different
 variances.
 Supposing that the model is covariance stationary, the variance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 won
\begin_inset Formula $^{\prime}$
\end_inset

t depend upon 
\begin_inset Formula $t$
\end_inset

: 
\begin_inset Formula 
\[
Var\varepsilon_{t}=\Sigma_{0}=\left[\begin{array}{lll}
\sigma_{11} & \sigma_{12} & \sigma_{13}\\
\cdot & \sigma_{22} & \sigma_{23}\\
\cdot & \cdot & \sigma_{33}
\end{array}\right]
\]

\end_inset

 Note that this matrix is singular, since the shares sum to 1.
 Assuming that there is no autocorrelation, the overall covariance matrix
 has the 
\emph on
seemingly unrelated regressions
\emph default
 (SUR) structure.
 
\begin_inset Formula 
\begin{eqnarray*}
Var\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{n}
\end{array}\right] & = & \Sigma\\
 & = & \left[\begin{array}{llll}
\Sigma_{0} & 0 & \cdots & 0\\
0 & \Sigma_{0} & \ddots & \vdots\\
\vdots & \ddots &  & 0\\
0 & \cdots & 0 & \Sigma_{0}
\end{array}\right]\\
 & = & I_{n}\otimes\Sigma_{0}
\end{eqnarray*}

\end_inset

 where the symbol 
\begin_inset Formula $\otimes$
\end_inset

 indicates the 
\emph on
Kronecker product
\emph default
.
 The Kronecker product of two matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is 
\begin_inset Formula 
\[
A\otimes B=\left[\begin{array}{llll}
a_{11}B & a_{12}B & \cdots & a_{1q}B\\
a_{21}B & \ddots &  & \vdots\\
\vdots\\
a_{pq}B & \cdots &  & a_{pq}B
\end{array}\right].
\]

\end_inset

 
\end_layout

\begin_layout Subsection
FGLS estimation of a translog model
\end_layout

\begin_layout Standard
So, this model has heteroscedasticity and autocorrelation, so OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

won't be efficient.
 The next question is: how do we estimate efficiently using FGLS? FGLS is
 based upon inverting the estimated error covariance 
\begin_inset Formula $\hat{\Sigma}.$
\end_inset

 So we need to estimate 
\begin_inset Formula $\Sigma.$
\end_inset


\end_layout

\begin_layout Standard
An asymptotically efficient procedure is (supposing normality of the errors)
\end_layout

\begin_layout Enumerate
Estimate each equation by OLS
\end_layout

\begin_layout Enumerate
Estimate 
\begin_inset Formula $\Sigma_{0}$
\end_inset

 using 
\begin_inset Formula 
\[
\hat{\Sigma}_{0}=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}^{\prime}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Next we need to account for the singularity of 
\begin_inset Formula $\Sigma_{0}.$
\end_inset

 It can be shown that 
\begin_inset Formula $\hat{\Sigma}_{0}$
\end_inset

 will be singular when the shares sum to one, so FGLS won't work.
 The solution is to drop one of the share equations, for example the second.
 The model becomes 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\ln c\\
s_{1}
\end{array}\right]=\left[\begin{array}{llllllllll}
1 & x_{1} & x_{2} & z & \frac{x_{1}^{2}}{2} & \frac{x_{2}^{2}}{2} & \frac{z^{2}}{2} & x_{1}x_{2} & x_{1}z & x_{2}z\\
0 & 1 & 0 & 0 & x_{1} & 0 & 0 & x_{2} & z & 0
\end{array}\right]\left[\begin{array}{l}
\alpha\\
\beta_{1}\\
\beta_{2}\\
\delta\\
\gamma_{11}\\
\gamma_{22}\\
\gamma_{33}\\
\gamma_{12}\\
\gamma_{13}\\
\gamma_{23}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]
\]

\end_inset

 or in matrix notation for the observation: 
\begin_inset Formula 
\[
y_{t}^{\ast}=X_{t}^{\ast}\theta+\varepsilon_{t}^{\ast}
\]

\end_inset

 and in stacked notation for all observations we have the 
\begin_inset Formula $2n$
\end_inset

 observations: 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}^{\ast}\\
y_{2}^{\ast}\\
\vdots\\
y_{n}^{\ast}
\end{array}\right]=\left[\begin{array}{l}
X_{1}^{\ast}\\
X_{2}^{\ast}\\
\vdots\\
X_{n}^{\ast}
\end{array}\right]\theta+\left[\begin{array}{l}
\varepsilon_{1}^{\ast}\\
\varepsilon_{2}^{\ast}\\
\vdots\\
\varepsilon_{n}^{\ast}
\end{array}\right]
\]

\end_inset

 or, finally in matrix notation for all observations: 
\begin_inset Formula 
\[
y^{\ast}=X^{\ast}\theta+\varepsilon^{\ast}
\]

\end_inset

 Considering the error covariance, we can define 
\begin_inset Formula 
\begin{eqnarray*}
\Sigma_{0}^{\ast} & = & Var\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}
\end{array}\right]\\
\Sigma^{\ast} & = & I_{n}\otimes\Sigma_{0}^{\ast}
\end{eqnarray*}

\end_inset

 Define 
\begin_inset Formula $\hat{\Sigma}_{0}^{\ast}$
\end_inset

 as the leading 
\begin_inset Formula $2\times2$
\end_inset

 block of 
\begin_inset Formula $\hat{\Sigma}_{0}$
\end_inset

 , and form 
\begin_inset Formula 
\[
\hat{\Sigma}^{\ast}=I_{n}\otimes\hat{\Sigma}_{0}^{\ast}.
\]

\end_inset

 This is a consistent estimator, following the consistency of OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

and applying a LLN.
\end_layout

\begin_layout Enumerate
Next compute the Cholesky factorization 
\begin_inset Formula 
\[
\hat{P}_{0}=Chol\left(\hat{\Sigma}_{0}^{\ast}\right)^{-1}
\]

\end_inset

 (I am assuming this is defined as an upper triangular matrix, which is
 consistent with the way Octave does it) and the Cholesky factorization
 of the overall covariance matrix of the 2 equation model, which can be
 calculated as 
\begin_inset Formula 
\[
\hat{P}=Chol\hat{\Sigma}^{\ast}=I_{n}\otimes\hat{P}_{0}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Finally the FGLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator can be calculated by applying OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

to the transformed model 
\begin_inset Formula 
\[
\hat{P}^{\prime}y^{\ast}=\hat{P}^{\prime}X^{\ast}\theta+\hat{\hat{P}^{\prime}}\varepsilon^{\ast}
\]

\end_inset

 or by directly using the GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

formula 
\begin_inset Formula 
\[
\hat{\theta}_{FGLS}=\left(X^{\ast\prime}\left(\hat{\Sigma}_{0}^{\ast}\right)^{-1}X^{\ast}\right)^{-1}X^{\ast\prime}\left(\hat{\Sigma}_{0}^{\ast}\right)^{-1}y^{\ast}
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
It is equivalent to transform each observation individually: 
\begin_inset Formula 
\[
\hat{P}_{0}^{\prime}y_{y}^{\ast}=\hat{P}_{0}^{\prime}X_{t}^{\ast}\theta+\hat{P}_{0}^{\prime}\varepsilon^{\ast}
\]

\end_inset

 and then apply OLS.
 This is probably the simplest approach.
\end_layout

\end_deeper
\begin_layout Standard
A few last comments.
\end_layout

\begin_layout Enumerate
We have assumed no autocorrelation across time.
 This is clearly restrictive.
 It is relatively simple to relax this, but we won't go into it here.
\end_layout

\begin_layout Enumerate
Also, we have only imposed symmetry of the second derivatives.
 Another restriction that the model should satisfy is that the estimated
 shares should sum to 1.
 This can be accomplished by imposing 
\begin_inset Formula 
\begin{eqnarray*}
\beta_{1}+\beta_{2} & = & 1\\
\sum_{i=1}^{3}\gamma_{ij} & = & 0,\textnormal{ }j=1,2,3.
\end{eqnarray*}

\end_inset

 These are linear parameter restrictions, so they are easy to impose and
 will improve efficiency if they are true.
\end_layout

\begin_layout Enumerate
The estimation procedure outlined above can be 
\emph on
iterated.

\emph default
 That is, estimate 
\begin_inset Formula $\hat{\theta}_{FGLS}$
\end_inset

 as above, then re-estimate 
\begin_inset Formula $\Sigma_{0}^{\ast}$
\end_inset

 using errors calculated as 
\begin_inset Formula 
\[
\hat{\varepsilon}=y-X\hat{\theta}_{FGLS}
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
These might be expected to lead to a better estimate than the estimator
 based on 
\begin_inset Formula $\hat{\theta}_{OLS},$
\end_inset

 since FGLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

is asymptotically more efficient.
 Then re-estimate 
\begin_inset Formula $\theta$
\end_inset

 using the new estimated error covariance.
 It can be shown that if this is repeated until the estimates don't change
 (
\emph on
i.e.,
\emph default
 iterated to convergence) then the resulting estimator is the MLE.
 At any rate, the asymptotic properties of the iterated and uniterated estimator
s are the same, since both are based upon a consistent estimator of the
 error covariance.
 
\end_layout

\end_deeper
\begin_layout Section
Testing nonnested hypotheses
\end_layout

\begin_layout Standard
Given that the choice of functional form isn't perfectly clear, in that
 many possibilities exist, how can one choose between forms? When one form
 is a parametric restriction of another, the previously studied tests such
 as Wald, LR, score or 
\begin_inset Formula $qF$
\end_inset

 are all possibilities.
 For example, the Cobb-Douglas model is a parametric restriction of the
 translog: The translog is 
\begin_inset Formula 
\[
y_{t}=\alpha+x_{t}^{\prime}\beta+1/2x_{t}^{\prime}\Gamma x_{t}+\varepsilon
\]

\end_inset

 where the variables are in logarithms, while the Cobb-Douglas is 
\begin_inset Formula 
\[
y_{t}=\alpha+x_{t}^{\prime}\beta+\varepsilon
\]

\end_inset

 so a test of the Cobb-Douglas versus the translog is simply a test that
 
\begin_inset Formula $\Gamma=0.$
\end_inset


\end_layout

\begin_layout Standard
The situation is more complicated when we want to test 
\emph on
non-nested hypotheses.

\emph default
 If the two functional forms are linear in the parameters, and use the same
 transformation of the dependent variable, then they may be written as 
\begin_inset Formula 
\begin{eqnarray*}
M_{1}:y & = & X\beta+\varepsilon\\
\varepsilon_{t} & \sim & iid(0,\sigma_{\varepsilon}^{2})\\
M_{2}:y & = & Z\gamma+\eta\\
\eta & \sim & iid(0,\sigma_{\eta}^{2})
\end{eqnarray*}

\end_inset

 We wish to test hypotheses of the form: 
\begin_inset Formula $H_{0}:M_{i}$
\end_inset

 
\emph on
is correctly specified
\emph default
 versus 
\begin_inset Formula $H_{A}:M_{i}$
\end_inset

 
\emph on
is misspecified
\emph default
, for 
\begin_inset Formula $i=1,2.$
\end_inset


\end_layout

\begin_layout Itemize
One could account for non-iid errors, but we'll suppress this for simplicity.
\end_layout

\begin_layout Itemize
There are a number of ways to proceed.
 We'll consider the 
\begin_inset Formula $J$
\end_inset

 test, proposed by Davidson and MacKinnon, 
\emph on
Econometrica
\emph default
 (1981).
 The idea is to artificially nest the two models, e.g., 
\begin_inset Formula 
\[
y=(1-\alpha)X\beta+\alpha(Z\gamma)+\omega
\]

\end_inset

 If the first model is correctly specified, then the true value of 
\begin_inset Formula $\alpha$
\end_inset

 is zero.
 On the other hand, if the second model is correctly specified then 
\begin_inset Formula $\alpha=1.$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The problem is that this model is not identified in general.
 For example, if the models share some regressors, as in 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
M_{1}:y_{t} & = & \beta_{1}+\beta_{2}x_{2t}+\beta_{3}x_{3t}+\varepsilon_{t}\\
M_{2}:y_{t} & = & \gamma_{1}+\gamma_{2}x_{2t}+\gamma_{3}x_{4t}+\eta_{t}
\end{eqnarray*}

\end_inset

 then the composite model is 
\begin_inset Formula 
\[
y_{t}=(1-\alpha)\beta_{1}+(1-\alpha)\beta_{2}x_{2t}+(1-\alpha)\beta_{3}x_{3t}+\alpha\gamma_{1}+\alpha\gamma_{2}x_{2t}+\alpha\gamma_{3}x_{4t}+\omega_{t}
\]

\end_inset

 Combining terms we get 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \left((1-\alpha)\beta_{1}+\alpha\gamma_{1}\right)+\left((1-\alpha)\beta_{2}+\alpha\gamma_{2}\right)x_{2t}+(1-\alpha)\beta_{3}x_{3t}+\alpha\gamma_{3}x_{4t}+\omega_{t}\\
 & = & \delta_{1}+\delta_{2}x_{2t}+\delta_{3}x_{3t}+\delta_{4}x_{4t}+\omega_{t}
\end{eqnarray*}

\end_inset

 The four 
\begin_inset Formula $\delta^{\prime}s$
\end_inset

 are consistently estimable, but 
\begin_inset Formula $\alpha$
\end_inset

 is not, since we have four equations in 7 unknowns, so one can't test the
 hypothesis that 
\begin_inset Formula $\alpha=0.$
\end_inset


\end_layout

\begin_layout Standard
The idea of the 
\begin_inset Formula $J$
\end_inset

 test is to substitute 
\begin_inset Formula $\hat{\gamma}$
\end_inset

 in place of 
\begin_inset Formula $\gamma.$
\end_inset

 This is a consistent estimator supposing that the second model is correctly
 specified.
 It will tend to a finite probability limit even if the second model is
 misspecified.
 Then estimate the model 
\begin_inset Formula 
\begin{eqnarray*}
y & = & (1-\alpha)X\beta+\alpha(Z\hat{\gamma})+\omega\\
 & = & X\theta+\alpha\hat{y}+\omega
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\hat{y}=Z(Z^{\prime}Z)^{-1}Z^{\prime}y=P_{Z}y.$
\end_inset

 In this model, 
\begin_inset Formula $\alpha$
\end_inset

 is consistently estimable, and one can show that, under the hypothesis
 that the first model is correct, 
\begin_inset Formula $\alpha\overset{p}{\rightarrow}0$
\end_inset

 and that the ordinary 
\begin_inset Formula $t$
\end_inset

 -statistic for 
\begin_inset Formula $\alpha=0$
\end_inset

 is asymptotically normal: 
\begin_inset Formula 
\[
t=\frac{\hat{\alpha}}{\hat{\sigma}_{\hat{\alpha}}}\overset{a}{\sim}N(0,1)
\]

\end_inset


\end_layout

\begin_layout Itemize
If the second model is correctly specified, then 
\begin_inset Formula $t\overset{p}{\rightarrow}\infty,$
\end_inset

 since 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 tends in probability to 1, while it's estimated standard error tends to
 zero.
 Thus the test will always reject the false null model, asymptotically,
 since the statistic will eventually exceed any critical value with probability
 one.
\end_layout

\begin_layout Itemize
We can reverse the roles of the models, testing the second against the first.
\end_layout

\begin_layout Itemize
It may be the case that 
\emph on
neither
\emph default
 model is correctly specified.
 In this case, the test will still reject the null hypothesis, asymptotically,
 if we use critical values from the 
\begin_inset Formula $N(0,1)$
\end_inset

 distribution, since as long as 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 tends to something different from zero, 
\begin_inset Formula $|t|\overset{p}{\rightarrow}\infty.$
\end_inset

 Of course, when we switch the roles of the models the other will also be
 rejected asymptotically.
\end_layout

\begin_layout Itemize
In summary, there are 4 possible outcomes when we test two models, each
 against the other.
 Both may be rejected, neither may be rejected, or one of the two may be
 rejected.
\end_layout

\begin_layout Itemize
There are other tests available for non-nested models.
 The 
\begin_inset Formula $J-$
\end_inset

 test is simple to apply when both models are linear in the parameters.
 The 
\begin_inset Formula $P$
\end_inset

-test is similar, but easier to apply when 
\begin_inset Formula $M_{1}$
\end_inset

 is nonlinear.
\end_layout

\begin_layout Itemize
The above presentation assumes that the same transformation of the dependent
 variable is used by both models.
 MacKinnon, White and Davidson, 
\emph on
Journal of Econometrics
\emph default
, (1983) shows how to deal with the case of different transformations.
 
\end_layout

\begin_layout Itemize
Monte-Carlo evidence shows that these tests often over-reject a correctly
 specified model.
 Can use bootstrap critical values to get better-performing tests.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Generalized least squares
\end_layout

\begin_layout Standard
Recall the assumptions of the classical linear regression model, in Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-classical-linear"

\end_inset

.
 One of the assumptions we've made up to now is that 
\begin_inset Formula 
\[
\varepsilon_{t}\sim IID(0,\sigma^{2})
\]

\end_inset

or occasionally 
\begin_inset Formula 
\[
\varepsilon_{t}\sim IIN(0,\sigma^{2}).
\]

\end_inset

Now we'll investigate the consequences of non-identically and/or dependently
 distributed errors.
 We'll assume fixed regressors for now, to keep the presentation simple,
 and later we'll look at the consequences of relaxing this admittedly unrealisti
c assumption.
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y & = & X\beta+\varepsilon\\
\mathcal{E}(\varepsilon) & = & 0\\
V(\varepsilon) & = & \Sigma
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $\Sigma$
\end_inset

 is a general symmetric positive definite matrix (we'll write 
\begin_inset Formula $\beta$
\end_inset

 in place of 
\begin_inset Formula $\beta_{0}$
\end_inset

 to simplify the typing of these notes).
\end_layout

\begin_layout Itemize
The case where 
\begin_inset Formula $\Sigma$
\end_inset

 is a diagonal matrix gives uncorrelated, non-identically distributed errors.
 This is known as 
\emph on
heteroscedasticity
\emph default
: 
\begin_inset Formula $\exists i,j\,s.t.\,V(\epsilon_{i})\ne V(\epsilon_{j})$
\end_inset


\end_layout

\begin_layout Itemize
The case where 
\begin_inset Formula $\Sigma$
\end_inset

 has the same number on the main diagonal but nonzero elements off the main
 diagonal gives identically (assuming higher moments are also the same)
 dependently distributed errors.
 This is known as 
\emph on
autocorrelation
\emph default
: 
\begin_inset Formula $\exists i\ne j\,s.t.\,E(\epsilon_{i}\epsilon_{j})\ne0)$
\end_inset


\end_layout

\begin_layout Itemize
The general case combines heteroscedasticity and autocorrelation.
 This is known as 
\begin_inset Quotes eld
\end_inset

non-spherical
\begin_inset Quotes erd
\end_inset

 disturbances, though why this term is used, I have no idea.
 Perhaps it's because under the classical assumptions, a joint confidence
 region for 
\begin_inset Formula $\varepsilon$
\end_inset

 would be an 
\begin_inset Formula $n-$
\end_inset

 dimensional hypersphere.
 
\end_layout

\begin_layout Section
Effects of non-spherical disturbances on the OLS estimator
\end_layout

\begin_layout Standard
The least square estimator is 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta} & = & (X^{\prime}X)^{-1}X^{\prime}y\\
 & = & \beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
We have unbiasedness, as before.
\end_layout

\begin_layout Itemize
The variance of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray}
\mathcal{E}\left[(\hat{\beta}-\beta)(\hat{\beta}-\beta)^{\prime}\right] & = & \mathcal{E}\left[(X^{\prime}X)^{-1}X^{\prime}\varepsilon\varepsilon^{\prime}X(X^{\prime}X)^{-1}\right]\nonumber \\
 & = & (X^{\prime}X)^{-1}X^{\prime}\Sigma X(X^{\prime}X)^{-1}\label{OLS covariance with nonspaerical}
\end{eqnarray}

\end_inset

 Due to this, any test statistic that is based upon an estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is invalid, since there 
\emph on
isn't
\emph default
 any 
\begin_inset Formula $\sigma^{2}$
\end_inset

, it doesn't exist as a feature of the true d.g.p.
 In particular, the formulas for the 
\begin_inset Formula $t,$
\end_inset

 
\begin_inset Formula $F,\chi^{2}$
\end_inset

 based tests given above do not lead to statistics with these distributions.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}$
\end_inset

 is still consistent, following exactly the same argument given before.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\varepsilon$
\end_inset

 is normally distributed, then
\begin_inset Formula 
\[
\hat{\beta}\sim N\left(\beta,(X^{\prime}X)^{-1}X^{\prime}\Sigma X(X^{\prime}X)^{-1}\right)
\]

\end_inset

 The problem is that 
\begin_inset Formula $\Sigma$
\end_inset

 is unknown in general, so this distribution won't be useful for testing
 hypotheses.
\end_layout

\begin_layout Itemize
Without normality, and with stochastic 
\begin_inset Formula $X$
\end_inset

 (e.g., weakly exogenous regressors) we still have 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}\left(\hat{\beta}-\beta\right) & = & \sqrt{n}(X^{\prime}X)^{-1}X^{\prime}\varepsilon\\
 & = & \left(\frac{X^{\prime}X}{n}\right)^{-1}n^{-1/2}X^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Define the limiting variance of 
\begin_inset Formula $n^{-1/2}X^{\prime}\varepsilon$
\end_inset

 (supposing a CLT applies) as 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)=\Omega,\,\textrm{a.s.}
\]

\end_inset

 so we obtain 
\begin_inset Formula $\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right)$
\end_inset

.
 Note that the true asymptotic distribution of the OLS has changed with
 respect to the results under the classical assumptions.
 If we neglect to take this into account, the Wald and score tests will
 not be asymptotically valid.
 So we need to figure out 
\emph on
how 
\emph default
to take it into account.
\end_layout

\begin_layout Standard
To see the invalidity of test procedures that are correct under the classical
 assumptions, when we have non-spherical errors, consider the Julia script
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GLS/EffectsOLS.jl}{https://github.com/mcreel/Econometrics/blob/m
aster/Examples/GLS/EffectsOLS.jl}
\end_layout

\end_inset

.
 This script does a Monte Carlo study, generating data that are either heterosce
dastic or homoscedastic, and then computes the empirical rejection frequency
 of a nominally 10% t-test.
 When the data are heteroscedastic, we obtain something like what we see
 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rejection-frequency-of"

\end_inset

.
 This sort of heteroscedasticity causes us to reject a true null hypothesis
 regarding the slope parameter much too often.
 You can experiment with the script to look at the effects of other sorts
 of HET, and to vary the sample size.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Rejection-frequency-of"

\end_inset

Rejection frequency of 10% t-test, H0 is true.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/EffectsOLS.svg
	lyxscale 50
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
\begin_inset Newpage newpage
\end_inset

Summary
\series default
: OLS with heteroscedasticity and/or autocorrelation is:
\end_layout

\begin_layout Itemize
unbiased with fixed or strongly exogenous regressors
\end_layout

\begin_layout Itemize
biased with weakly exogenous regressors
\end_layout

\begin_layout Itemize
has a different variance than before, so the previous test statistics aren't
 valid 
\end_layout

\begin_layout Itemize
is consistent
\end_layout

\begin_layout Itemize
is asymptotically normally distributed, but with a different limiting covariance
 matrix.
 Previous test statistics aren't valid in this case for this reason.
\end_layout

\begin_layout Itemize
is inefficient, as is shown below.
 
\end_layout

\begin_layout Section
The GLS estimator
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $\Sigma$
\end_inset

 were known.
 Then one could form the Cholesky decomposition
\begin_inset Formula 
\[
P^{\prime}P=\Sigma^{-1}
\]

\end_inset

 Here, 
\begin_inset Formula $P$
\end_inset

 is an upper triangular matrix.
 We have 
\begin_inset Formula 
\[
P^{\prime}P\Sigma=I_{n}
\]

\end_inset

 so 
\begin_inset Formula 
\[
P^{\prime}P\Sigma P^{\prime}=P^{\prime},
\]

\end_inset

 which implies that 
\begin_inset Formula 
\[
P\Sigma P^{\prime}=I_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
Let's take some time to play with the Cholesky decomposition.
 Try out the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GLS/cholesky.jl}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/GLS/cholesky.jl}
\end_layout

\end_inset

 Julia script to see that the above claims are true, and also to see how
 one can generate data from a 
\begin_inset Formula $N(0,V)$
\end_inset

 distribution.
\end_layout

\begin_layout Standard
Consider the model 
\begin_inset Formula 
\[
Py=PX\beta+P\varepsilon,
\]

\end_inset

 or, making the obvious definitions, 
\begin_inset Formula 
\[
y^{*}=X^{*}\beta+\varepsilon^{*}.
\]

\end_inset

 This variance of 
\begin_inset Formula $\varepsilon^{*}=P\varepsilon$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(P\varepsilon\varepsilon^{\prime}P^{\prime}) & = & P\Sigma P^{\prime}\\
 & = & I_{n}
\end{eqnarray*}

\end_inset

 Therefore, the model 
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & X^{*}\beta+\varepsilon^{*}\\
\mathcal{E}(\varepsilon^{*}) & = & 0\\
V(\varepsilon^{*}) & = & I_{n}
\end{eqnarray*}

\end_inset

 satisfies the classical assumptions.
 The GLS estimator is simply OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

applied to the transformed model: 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{GLS} & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}y^{*}\\
 & = & (X^{\prime}P'PX)^{-1}X^{\prime}P'Py\\
 & = & (X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is unbiased in the same circumstances under which the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is unbiased.
 For example, assuming 
\begin_inset Formula $X$
\end_inset

 is nonstochastic 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\hat{\beta}_{GLS}) & = & \mathcal{E}\left\{ (X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}y\right\} \\
 & = & \mathcal{E}\left\{ (X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}(X\beta+\varepsilon\right\} \\
 & = & \beta.
\end{eqnarray*}

\end_inset

To get the variance of the estimator, we have 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{GLS} & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}y^{*}\\
 & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}\left(X^{*}\beta+\varepsilon^{*}\right)\\
 & = & \beta+(X^{*\prime}X^{*})^{-1}X^{*\prime}\varepsilon^{*}
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}\left\{ \left(\hat{\beta}_{GLS}-\beta\right)\left(\hat{\beta}_{GLS}-\beta\right)^{\prime}\right\}  & = & \mathcal{E}\left\{ (X^{*\prime}X^{*})^{-1}X^{*\prime}\varepsilon^{*}\varepsilon^{*\prime}X^{*}(X^{*\prime}X^{*})^{-1}\right\} \\
 & = & (X^{*\prime}X^{*})^{-1}X^{*\prime}X^{*}(X^{*\prime}X^{*})^{-1}\\
 & = & (X^{*\prime}X^{*})^{-1}\\
 & = & (X^{\prime}\Sigma^{-1}X)^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Either of these last formulas can be used.
\end_layout

\begin_layout Itemize
All the previous results regarding the desirable properties of the least
 squares estimator hold, when dealing with the transformed model, since
 the transformed model satisfies the classical assumptions..
\end_layout

\begin_layout Itemize
Tests are valid, using the previous formulas, as long as we substitute 
\begin_inset Formula $X^{\ast}$
\end_inset

 in place of 
\begin_inset Formula $X.$
\end_inset

 Furthermore, any test that involves 
\begin_inset Formula $\sigma^{2}$
\end_inset

 can set it to 
\begin_inset Formula $1.$
\end_inset

 This is preferable to re-deriving the appropriate formulas.
\end_layout

\begin_layout Itemize
The GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is more efficient than the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator.
 This is a consequence of the Gauss-Markov theorem, since the GLS estimator
 is based on a model that satisfies the classical assumptions but the OLS
 estimator is not.
 To see this directly, note that
\begin_inset Formula 
\begin{eqnarray*}
Var(\hat{\beta})-Var(\hat{\beta}_{GLS}) & = & (X'X)^{-1}X'\Sigma X(X'X)^{-1}-(X'\Sigma^{-1}X)^{-1}\\
 & = & A\Sigma A^{'}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $A=\left[\left(X^{\prime}X\right)^{-1}X^{\prime}-(X'\Sigma^{-1}X)^{-1}X'\Sigma^{-1}\right].$
\end_inset

 This may not seem obvious, but it is true, as you can verify for yourself.
 Then noting that 
\begin_inset Formula $A\Sigma A^{'}$
\end_inset

 is a quadratic form in a positive definite matrix, we conclude that 
\begin_inset Formula $A\Sigma A^{'}$
\end_inset

 is positive semi-definite, and that GLS is efficient relative to OLS.
\end_layout

\begin_layout Itemize
As one can verify by calculating first order conditions, the GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is the solution to the minimization problem 
\begin_inset Formula 
\[
\hat{\beta}_{GLS}=\arg\min(y-X\beta)^{\prime}\Sigma^{-1}(y-X\beta)
\]

\end_inset

 so the 
\emph on
metric
\emph default
 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 is used to weight the residuals.
 
\end_layout

\begin_layout Section
Feasible GLS
\end_layout

\begin_layout Standard
The problem is that 
\begin_inset Formula $\Sigma$
\end_inset

 ordinarily isn't known, so this estimator isn't available.
\end_layout

\begin_layout Itemize
Consider the dimension of 
\begin_inset Formula $\Sigma$
\end_inset

 : it's an 
\begin_inset Formula $n\times n$
\end_inset

 matrix with 
\begin_inset Formula $\left(n^{2}-n\right)/2+n=\left(n^{2}+n\right)/2$
\end_inset

 unique elements (remember - it is symmetric, because it's a covariance
 matrix).
\end_layout

\begin_layout Itemize
The number of parameters to estimate is larger than 
\begin_inset Formula $n$
\end_inset

 and increases faster than 
\begin_inset Formula $n.$
\end_inset

 There's no way to devise an estimator that satisfies a LLN without adding
 restrictions.
\end_layout

\begin_layout Itemize
The 
\emph on
feasible GLS estimator
\emph default
 is based upon making sufficient assumptions regarding the form of 
\begin_inset Formula $\Sigma$
\end_inset

 so that a consistent estimator can be devised.
 
\end_layout

\begin_layout Standard
Suppose that we 
\emph on
parameterize
\emph default
 
\begin_inset Formula $\Sigma$
\end_inset

 as a function of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

, where 
\begin_inset Formula $\theta$
\end_inset

 may include 
\begin_inset Formula $\beta$
\end_inset

 as well as other parameters, so that 
\begin_inset Formula 
\[
\Sigma=\Sigma(X,\theta)
\]

\end_inset

 where 
\begin_inset Formula $\theta$
\end_inset

 is of fixed dimension.
 If we can consistently estimate 
\begin_inset Formula $\theta,$
\end_inset

 we can consistently estimate 
\begin_inset Formula $\Sigma,$
\end_inset

 as long as the elements of 
\begin_inset Formula $\Sigma(X,\theta)$
\end_inset

 are continuous functions of 
\begin_inset Formula $\theta$
\end_inset

 (by the Slutsky theorem).
 In this case, 
\begin_inset Formula 
\[
\widehat{\Sigma}=\Sigma(X,\hat{\theta})\overset{p}{\rightarrow}\Sigma(X,\theta)
\]

\end_inset

 If we replace 
\begin_inset Formula $\Sigma$
\end_inset

 in the formulas for the GLS estimator with 
\begin_inset Formula $\widehat{\Sigma},$
\end_inset

 we obtain the FGLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator.
 
\series bold
The FGLS estimator shares the same asymptotic properties as GLS.
 These are
\end_layout

\begin_layout Enumerate
Consistency
\end_layout

\begin_layout Enumerate
Asymptotic normality
\end_layout

\begin_layout Enumerate
Asymptotic efficiency 
\emph on
if
\emph default
 the errors are normally distributed.
 (Cramr-Rao).
\end_layout

\begin_layout Enumerate
Test procedures are asymptotically valid.
 
\end_layout

\begin_layout Standard

\series bold
In practice, the usual way to proceed is
\end_layout

\begin_layout Enumerate
Define a consistent estimator of 
\begin_inset Formula $\theta.$
\end_inset

 This is a case-by-case proposition, depending on the parameterization 
\begin_inset Formula $\Sigma(\theta).$
\end_inset

 We'll see examples below.
\end_layout

\begin_layout Enumerate
Form 
\begin_inset Formula $\widehat{\Sigma}=\Sigma(X,\hat{\theta})$
\end_inset


\end_layout

\begin_layout Enumerate
Calculate the Cholesky factorization 
\begin_inset Formula $\widehat{P}=Chol(\hat{\Sigma}^{-1})$
\end_inset

.
\end_layout

\begin_layout Enumerate
Transform the model using 
\begin_inset Formula 
\[
\hat{P}y=\hat{P}X\beta+\hat{P}\varepsilon
\]

\end_inset


\end_layout

\begin_layout Enumerate
Estimate using OLS on the transformed model.
 
\end_layout

\begin_layout Section
Heteroscedasticity
\end_layout

\begin_layout Standard
Heteroscedasticity is the case where 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon\varepsilon^{\prime})=\Sigma
\]

\end_inset

 is a diagonal matrix, so that the errors are uncorrelated, but have different
 variances.
 Heteroscedasticity is usually thought of as associated with cross sectional
 data, though there is absolutely no reason why time series data cannot
 also be heteroscedastic.
 Actually, the popular ARCH (autoregressive conditionally heteroscedastic)
 models explicitly assume that a time series is conditionally heteroscedastic.
\end_layout

\begin_layout Standard
Consider a supply function 
\begin_inset Formula 
\[
q_{i}=\beta_{1}+\beta_{p}P_{i}+\beta_{s}S_{i}+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula $P_{i}$
\end_inset

 is price and 
\begin_inset Formula $S_{i}$
\end_inset

 is some measure of size of the 
\begin_inset Formula $i^{th}$
\end_inset

 firm.
 One might suppose that unobservable factors (e.g., talent of managers, degree
 of coordination between production units, 
\emph on
etc.
\emph default
) account for the error term 
\begin_inset Formula $\varepsilon_{i}.$
\end_inset

 If there is more variability in these factors for large firms than for
 small firms, then 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 may have a higher variance when 
\begin_inset Formula $S_{i}$
\end_inset

 is high than when it is low.
\end_layout

\begin_layout Standard
Another example, individual demand.
 
\begin_inset Formula 
\[
q_{i}=\beta_{1}+\beta_{p}P_{i}+\beta_{m}M_{i}+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula $P$
\end_inset

 is price and 
\begin_inset Formula $M$
\end_inset

 is income.
 In this case, 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 can reflect variations in preferences.
 There are more possibilities for expression of preferences when one is
 rich, so it is possible that the variance of 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 could be higher when 
\begin_inset Formula $M$
\end_inset

 is high.
\end_layout

\begin_layout Standard

\emph on
Add example of group means.
\end_layout

\begin_layout Subsection
OLS with heteroscedastic consistent varcov estimation
\end_layout

\begin_layout Standard
Eicker (1967) and White (1980) showed how to modify test statistics to account
 for heteroscedasticity of unknown form.
 The OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator has asymptotic distribution 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right)
\]

\end_inset

 as we've already seen.
 Recall that we defined 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)=\Omega
\]

\end_inset

 This matrix has dimension 
\begin_inset Formula $K\times K$
\end_inset

 and can be consistently estimated, even if we can't estimate 
\begin_inset Formula $\Sigma$
\end_inset

 consistently.
 The consistent estimator, under heteroscedasticity but no autocorrelation
 is 
\begin_inset Formula 
\[
\widehat{\Omega}=\frac{1}{n}\sum_{t=1}^{n}x_{t}x_{t}^{\prime}\hat{\varepsilon}_{t}^{2}
\]

\end_inset

 One can then modify the previous test statistics to obtain tests that are
 valid when there is heteroscedasticity of unknown form.
 For example, the Wald test for 
\begin_inset Formula $H_{0}:R\beta-r=0$
\end_inset

 would be 
\begin_inset Formula 
\[
n\left(R\hat{\beta}-r\right)^{\prime}\left(R\left(\frac{X^{\prime}X}{n}\right)^{-1}\hat{\Omega}\left(\frac{X^{\prime}X}{n}\right)^{-1}R^{\prime}\right)^{-1}\left(R\hat{\beta}-r\right)\overset{a}{\sim}\chi^{2}(q)
\]

\end_inset


\end_layout

\begin_layout Subsection
Detection
\end_layout

\begin_layout Standard
There exist many tests for the presence of heteroscedasticity.
 We'll discuss three methods.
\end_layout

\begin_layout Paragraph
Goldfeld-Quandt
\end_layout

\begin_layout Standard
The sample is divided in to three parts, with 
\begin_inset Formula $n_{1},n_{2}$
\end_inset

 and 
\begin_inset Formula $n_{3}$
\end_inset

 observations, where 
\begin_inset Formula $n_{1}+n_{2}+n_{3}=n$
\end_inset

.
 The model is estimated using the first and third parts of the sample, separatel
y, so that 
\begin_inset Formula $\hat{\beta}^{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}^{3}$
\end_inset

 will be independent.
 Then we have 
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}^{1\prime}\hat{\varepsilon}^{1}}{\sigma^{2}}=\frac{\varepsilon^{1^{\prime}}M^{1}\varepsilon^{1}}{\sigma^{2}}\overset{d}{\rightarrow}\chi^{2}(n_{1}-K)
\]

\end_inset

 and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}^{3\prime}\hat{\varepsilon}^{3}}{\sigma^{2}}=\frac{\varepsilon^{3^{\prime}}M^{3}\varepsilon^{3}}{\sigma^{2}}\overset{d}{\rightarrow}\chi^{2}(n_{3}-K)
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{\hat{\varepsilon}^{1\prime}\hat{\varepsilon}^{1}/(n_{1}-K)}{\hat{\varepsilon}^{3\prime}\hat{\varepsilon}^{3}/(n_{3}-K)}\overset{d}{\rightarrow}F(n_{1}-K,n_{3}-K).
\]

\end_inset

 The distributional result is exact if the errors are normally distributed.
 This test is a two-tailed test.
 Alternatively, and probably more conventionally, if one has prior ideas
 about the possible magnitudes of the variances of the observations, one
 could order the observations accordingly, from largest to smallest.
 In this case, one would use a conventional one-tailed F-test.
 
\emph on
Draw picture.
\end_layout

\begin_layout Itemize
Ordering the observations is an important step if the test is to have any
 power.
\end_layout

\begin_layout Itemize
The motive for dropping the middle observations is to increase the difference
 between the average variance in the subsamples, supposing that there exists
 heteroscedasticity.
 This can increase the power of the test.
 On the other hand, dropping too many observations will substantially increase
 the variance of the statistics 
\begin_inset Formula $\hat{\varepsilon}^{1\prime}\hat{\varepsilon}^{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\varepsilon}^{3\prime}\hat{\varepsilon}^{3}.$
\end_inset

 A rule of thumb, based on Monte Carlo experiments is to drop around 25%
 of the observations.
\end_layout

\begin_layout Itemize
If one doesn't have any ideas about the form of the het.
 the test will probably have low power since a sensible data ordering isn't
 available.
 
\end_layout

\begin_layout Paragraph
White's test
\end_layout

\begin_layout Standard
When one has little idea if there exists heteroscedasticity, and no idea
 of its potential form, the White test is a possibility.
 The idea is that if there is homoscedasticity, then 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon_{t}^{2}|x_{t})=\sigma^{2},\forall t
\]

\end_inset

 so that 
\begin_inset Formula $x_{t}$
\end_inset

 or functions of 
\begin_inset Formula $x_{t}$
\end_inset

 shouldn't help to explain 
\begin_inset Formula $\mathcal{E}(\varepsilon_{t}^{2}).$
\end_inset

 The test works as follows:
\end_layout

\begin_layout Enumerate
Since 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 isn't available, use the consistent estimator 
\begin_inset Formula $\hat{\varepsilon}_{t}$
\end_inset

 instead.
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}^{2}=\sigma^{2}+z_{t}^{\prime}\gamma+v_{t}
\]

\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 is a 
\begin_inset Formula $P$
\end_inset

-vector.
 
\begin_inset Formula $z_{t}$
\end_inset

 may include some or all of the variables in 
\begin_inset Formula $x_{t},$
\end_inset

 as well as other variables.
 White's original suggestion was to use 
\begin_inset Formula $x_{t}$
\end_inset

, plus the set of all unique squares and cross products of variables in
 
\begin_inset Formula $x_{t}.$
\end_inset


\end_layout

\begin_layout Enumerate
Test the hypothesis that 
\begin_inset Formula $\gamma=0.$
\end_inset

 The 
\begin_inset Formula $qF$
\end_inset

 statistic in this case is 
\begin_inset Formula 
\[
qF=\frac{P\left(ESS_{R}-ESS_{U}\right)/P}{ESS_{U}/\left(n-P-1\right)}
\]

\end_inset

 Note that 
\begin_inset Formula $ESS_{R}=TSS_{U},$
\end_inset

 so dividing both numerator and denominator by this we get 
\begin_inset Formula 
\[
qF=\left(n-P-1\right)\frac{R^{2}}{1-R^{2}}
\]

\end_inset

 Note that this is the 
\begin_inset Formula $R^{2}$
\end_inset

 of the artificial regression used to test for heteroscedasticity, not the
 
\begin_inset Formula $R^{2}$
\end_inset

 of the original model.
 
\end_layout

\begin_layout Standard
An asymptotically equivalent statistic, under the null of no heteroscedasticity
 (so that 
\begin_inset Formula $R^{2}$
\end_inset

 should tend to zero), is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
nR^{2}\overset{a}{\sim}\chi^{2}(P).
\]

\end_inset

 This doesn't require normality of the errors, though it does assume that
 the fourth moment of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is constant, under the null.
 
\series bold
Question
\series default
: why is this necessary?
\end_layout

\begin_layout Itemize
The White test has the disadvantage that it may not be very powerful unless
 the 
\begin_inset Formula $z_{t}$
\end_inset

 vector is chosen well, and this is hard to do without knowledge of the
 form of heteroscedasticity.
\end_layout

\begin_layout Itemize
It also has the problem that specification errors other than heteroscedasticity
 may lead to rejection.
\end_layout

\begin_layout Itemize
Note: the null hypothesis of this test may be interpreted as 
\begin_inset Formula $\theta=0$
\end_inset

 for the variance model 
\begin_inset Formula $V(\varepsilon_{t}^{2})=h(\alpha+z_{t}^{\prime}\theta),$
\end_inset

 where 
\begin_inset Formula $h(\cdot)$
\end_inset

 is an arbitrary function of unknown form.
 The test is more general than is may appear from the regression that is
 used.
\end_layout

\begin_layout Paragraph
Plotting the residuals
\end_layout

\begin_layout Standard
A very simple method is to simply plot the residuals (or their squares).
 
\emph on
Draw pictures here
\emph default
.
 Like the Goldfeld-Quandt test, this will be more informative if the observation
s are ordered according to the suspected form of the heteroscedasticity.
\end_layout

\begin_layout Subsection
Correction
\end_layout

\begin_layout Standard
Correcting for heteroscedasticity requires that a parametric form for 
\begin_inset Formula $\Sigma(\theta)$
\end_inset

 be supplied, and that a means for estimating 
\begin_inset Formula $\theta$
\end_inset

 consistently be determined.
 The estimation method will be specific to the assumed form 
\begin_inset Formula $\Sigma(\theta).$
\end_inset

 In recent years, there has been a trend toward simply estimating by OLS,
 and using robust standard errors.
 This may be somewhat unfortunate, as the weighted least squares estimator
 (GLS when there is only HET) is still consistent even if the specification
 of 
\begin_inset Formula $\Sigma(\theta)$
\end_inset

 is incorrect, and it may be a good deal more efficient than OLS.
 Also, robust standard errors don't always work so well.
 
\end_layout

\begin_layout Example
The GRETL script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GLS/Heteroscedasticity.inp}{https://github.com/mcreel/Econometri
cs/blob/master/Examples/GLS/Heteroscedasticity.inp}
\end_layout

\end_inset

 illustrates these points.
 
\end_layout

\begin_layout Standard
Perhaps a middle ground is to attempt to use GLS when severe HET is detected,
 but to continue to use robust standard errors, to account for misspecifications
 in the modeling of 
\begin_inset Formula $\Sigma(\theta).$
\end_inset


\end_layout

\begin_layout Standard
We'll consider two examples.
 Before this, let's consider the general nature of GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

when there is heteroscedasticity.
 When we have HET but no AUT, 
\begin_inset Formula $\Sigma$
\end_inset

 is a diagonal matrix:
\begin_inset Formula 
\[
\Sigma=\left[\begin{array}{cccc}
\sigma_{1}^{2} & 0 & \ldots & 0\\
\vdots & \sigma_{2}^{2} &  & \vdots\\
 &  & \ddots & 0\\
0 & \cdots & 0 & \sigma_{n}^{2}
\end{array}\right]
\]

\end_inset

Likewise, 
\begin_inset Formula $\Sigma^{-1}$
\end_inset

 is diagonal
\begin_inset Formula 
\[
\Sigma^{-1}=\left[\begin{array}{cccc}
\frac{1}{\sigma_{1}^{2}} & 0 & \ldots & 0\\
\vdots & \frac{1}{\sigma_{2}^{2}} &  & \vdots\\
 &  & \ddots & 0\\
0 & \cdots & 0 & \frac{1}{\sigma_{n}^{2}}
\end{array}\right]
\]

\end_inset

and so is the Cholesky decomposition 
\begin_inset Formula $P=chol(\Sigma^{-1}$
\end_inset

)
\begin_inset Formula 
\[
P=\left[\begin{array}{cccc}
\frac{1}{\sigma_{1}} & 0 & \ldots & 0\\
\vdots & \frac{1}{\sigma_{2}} &  & \vdots\\
 &  & \ddots & 0\\
0 & \cdots & 0 & \frac{1}{\sigma_{n}}
\end{array}\right]
\]

\end_inset

 We need to transform the model, just as before, in the general case: 
\begin_inset Formula 
\[
Py=PX\beta+P\varepsilon,
\]

\end_inset

 or, making the obvious definitions, 
\begin_inset Formula 
\[
y^{*}=X^{*}\beta+\varepsilon^{*}.
\]

\end_inset

 Note that multiplying by 
\begin_inset Formula $P$
\end_inset

 just divides the data for each observation (
\begin_inset Formula $y_{i},x_{i})$
\end_inset

 by the corresponding standard error of the error term, 
\begin_inset Formula $\sigma_{i}$
\end_inset

.
 That is, 
\begin_inset Formula $y_{i}^{*}=y_{i}/\sigma_{i}$
\end_inset

 and 
\begin_inset Formula $x_{i}^{*}=x_{i}/\sigma_{i}$
\end_inset

 (note that 
\begin_inset Formula $x_{i}$
\end_inset

 is a 
\begin_inset Formula $K$
\end_inset

-vector: we divided each element, including the 1 corresponding to the constant).
\end_layout

\begin_layout Standard
This makes sense.
 Consider Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Motivation-for-GLS"

\end_inset

, which shows a true regression line with heteroscedastic errors.
 Which sample is more informative about the location of the line? The ones
 with observations with smaller variances.
 So, the GLS solution is equivalent to OLS on the transformed data.
 By the transformed data is the original data, weighted by the inverse of
 the standard error of the observation's error term.
 When the standard error is small, the weight is high, and vice versa.
 The GLS correction for the case of HET is also known as weighted least
 squares, for this reason.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Motivation-for-GLS"

\end_inset

Motivation for GLS correction when there is HET
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/wls.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Multiplicative heteroscedasticity
\end_layout

\begin_layout Standard
Suppose the model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\sigma_{t}^{2} & = & \mathcal{E}(\varepsilon_{t}^{2})=\left(z_{t}^{\prime}\gamma\right)^{\delta}
\end{eqnarray*}

\end_inset

 but the other classical assumptions hold.
 In this case 
\begin_inset Formula 
\[
\varepsilon_{t}^{2}=\left(z_{t}^{\prime}\gamma\right)^{\delta}+v_{t}
\]

\end_inset

 and 
\begin_inset Formula $v_{t}$
\end_inset

 has mean zero.
 Nonlinear least squares could be used to estimate 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\delta$
\end_inset

 consistently, were 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 observable.
 The solution is to substitute the squared OLS residuals 
\begin_inset Formula $\hat{\varepsilon}_{t}^{2}$
\end_inset

 in place of 
\begin_inset Formula $\varepsilon_{t}^{2},$
\end_inset

 since it is consistent by the Slutsky theorem.
 Once we have 
\begin_inset Formula $\hat{\gamma}$
\end_inset

 and 
\begin_inset Formula $\hat{\delta},$
\end_inset

 we can estimate 
\begin_inset Formula $\sigma_{t}^{2}$
\end_inset

 consistently using 
\begin_inset Formula 
\[
\hat{\sigma}_{t}^{2}=\left(z_{t}^{\prime}\hat{\gamma}\right)^{\hat{\delta}}\overset{p}{\rightarrow\sigma_{t}^{2}}.
\]

\end_inset

 In the second step, we transform the model by dividing by the standard
 deviation: 
\begin_inset Formula 
\[
\frac{y_{t}}{\hat{\sigma}_{t}}=\frac{x_{t}^{\prime}\beta}{\hat{\sigma}_{t}}+\frac{\varepsilon_{t}}{\hat{\sigma}_{t}}
\]

\end_inset

 or 
\begin_inset Formula 
\[
y_{t}^{*}=x_{t}^{*\prime}\beta+\varepsilon_{t}^{*}.
\]

\end_inset

 Asymptotically, this model satisfies the classical assumptions.
\end_layout

\begin_layout Itemize
This model is a bit complex in that NLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

is required to estimate the model of the variance.
 A simpler version would be 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\sigma_{t}^{2} & =\mathcal{E}(\varepsilon_{t}^{2})= & \sigma^{2}z_{t}^{\delta}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 is a single variable.
 There are still two parameters to be estimated, and the model of the variance
 is still nonlinear in the parameters.
 However, the 
\emph on
search method
\emph default
 can be used in this case to reduce the estimation problem to repeated applicati
ons of OLS.
\end_layout

\begin_layout Itemize
First, we define an interval of reasonable values for 
\begin_inset Formula $\delta,$
\end_inset

 e.g., 
\begin_inset Formula $\delta\in[0,3].$
\end_inset


\end_layout

\begin_layout Itemize
Partition this interval into 
\begin_inset Formula $M$
\end_inset

 equally spaced values, e.g., 
\begin_inset Formula $\{0,.1,.2,...,2.9,3\}.$
\end_inset


\end_layout

\begin_layout Itemize
For each of these values, calculate the variable 
\begin_inset Formula $z_{t}^{\delta_{m}}.$
\end_inset


\end_layout

\begin_layout Itemize
The regression 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}^{2}=\sigma^{2}z_{t}^{\delta_{m}}+v_{t}
\]

\end_inset

 is linear in the parameters, conditional on 
\begin_inset Formula $\delta_{m},$
\end_inset

 so one can estimate 
\begin_inset Formula $\sigma^{2}$
\end_inset

 by OLS.
\end_layout

\begin_layout Itemize
Save the pairs (
\begin_inset Formula $\sigma_{m}^{2},\delta_{m}),$
\end_inset

 and the corresponding 
\begin_inset Formula $ESS_{m}.$
\end_inset

 Choose the pair with the minimum 
\begin_inset Formula $ESS_{m}$
\end_inset

 as the estimate.
 
\end_layout

\begin_layout Itemize
Next, divide the model by the estimated standard deviations.
\end_layout

\begin_layout Itemize
Can refine.
 
\emph on
Draw picture.
 
\end_layout

\begin_layout Itemize
Works well when the parameter to be searched over is low dimensional, as
 in this case.
 
\end_layout

\begin_layout Subsubsection
Groupwise heteroscedasticity
\end_layout

\begin_layout Standard
A common case is where we have repeated observations on each of a number
 of economic agents: e.g., 10 years of macroeconomic data on each of a set
 of countries or regions, or daily observations of transactions of 200 banks.
 This sort of data is a 
\emph on
pooled cross-section time-series model.

\emph default
 It may be reasonable to presume that the variance is constant over time
 within the cross-sectional units, but that it differs across them (e.g.,
 firms or countries of different sizes...).
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{it} & = & x_{it}^{\prime}\beta+\varepsilon_{it}\\
\mathcal{E}(\varepsilon_{it}^{2}) & = & \sigma_{i}^{2},\forall t
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $i=1,2,...,G$
\end_inset

 are the agents, and 
\begin_inset Formula $t=1,2,...,n$
\end_inset

 are the observations on each agent.
\end_layout

\begin_layout Itemize
The other classical assumptions are presumed to hold.
\end_layout

\begin_layout Itemize
In this case, the variance 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 is specific to each agent, but constant over the 
\begin_inset Formula $n$
\end_inset

 observations for that agent.
\end_layout

\begin_layout Itemize
In this model, we assume that 
\begin_inset Formula $\mathcal{E}(\varepsilon_{it}\varepsilon_{is})=0.$
\end_inset

 This is a strong assumption that we'll relax later.
 
\end_layout

\begin_layout Standard
To correct for heteroscedasticity, just estimate each 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 using the natural estimator: 
\begin_inset Formula 
\[
\hat{\sigma}_{i}^{2}=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{it}^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that we use 
\begin_inset Formula $1/n$
\end_inset

 here since it's possible that there are more than 
\begin_inset Formula $n$
\end_inset

 regressors, so 
\begin_inset Formula $n-K$
\end_inset

 could be negative.
 Asymptotically the difference is unimportant.
\end_layout

\begin_layout Itemize
With each of these, transform the model as usual: 
\begin_inset Formula 
\[
\frac{y_{it}}{\hat{\sigma}_{i}}=\frac{x_{it}^{\prime}\beta}{\hat{\sigma}_{i}}+\frac{\varepsilon_{it}}{\hat{\sigma}_{i}}
\]

\end_inset

 Do this for each cross-sectional group.
 This transformed model satisfies the classical assumptions, asymptotically.
 
\end_layout

\begin_layout Subsection
Example: the Nerlove model
\end_layout

\begin_layout Itemize
Here's the data in Gretl format: 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{nerlove.gdt}{https://github.com/mcreel/Econometrics/blob/master/
Examples/Data/nerlove.gdt} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
estimate the basic Nerlove model by OLS, using Gretl, and plot the residuals:
 evidence of HET and AUT
\end_layout

\begin_layout Itemize
include square of 
\begin_inset Formula $\ln Q$
\end_inset

, now there's no AUT, but still HET.
 Conclusion: apparent AUT may be evidence of misspecification, rather than
 true autocorrelation.
 
\end_layout

\begin_layout Itemize
estimate using HET correction, and compare standard error estimates.
\end_layout

\begin_layout Section
Autocorrelation
\end_layout

\begin_layout Standard
Autocorrelation, which is the serial correlation of the error term, is a
 problem that is usually associated with time series data, but also can
 affect cross-sectional data.
 For example, a shock to oil prices will simultaneously affect all countries,
 so one could expect contemporaneous correlation of macroeconomic variables
 across countries.
\end_layout

\begin_layout Subsection
Example
\end_layout

\begin_layout Standard
Consider the Keeling-Whorf data on atmospheric CO2 concentrations an Mauna
 Loa, Hawaii (see 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Keeling_Curve
\end_layout

\end_inset

 and 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://cdiac.ornl.gov/ftp/ndp001/maunaloa.txt
\end_layout

\end_inset

).
\end_layout

\begin_layout Standard
From the file maunaloa.txt: 
\begin_inset Quotes sld
\end_inset

THE DATA FILE PRESENTED IN THIS SUBDIRECTORY CONTAINS MONTHLY AND ANNUAL
 ATMOSPHERIC CO2 CONCENTRATIONS DERIVED FROM THE SCRIPPS INSTITUTION OF
 OCEANOGRAPHY'S (SIO's) CONTINUOUS MONITORING PROGRAM AT MAUNA LOA OBSERVATORY,
 HAWAII.
 THIS RECORD CONSTITUTES THE LONGEST CONTINUOUS RECORD OF ATMOSPHERIC CO2
 CONCENTRATIONS AVAILABLE IN THE WORLD.
 MONTHLY AND ANNUAL AVERAGE MOLE FRACTIONS OF CO2 IN WATER-VAPOR-FREE AIR
 ARE GIVEN FROM MARCH 1958 THROUGH DECEMBER 2003, EXCEPT FOR A FEW INTERRUPTIONS.
\begin_inset Quotes srd
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The data is available in Octave format at 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{CO2.data}{https://github.com/mcreel/Econometrics/blob/master/Exa
mples/Data/CO2.data} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
If we fit the model 
\begin_inset Formula $CO2_{t}=\beta_{1}+\beta_{2}t+\epsilon_{t}$
\end_inset

, we get the results
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GLS/CO2Example.out"

\end_inset

It seems pretty clear that CO2 concentrations have been going up in the
 last 50 years, surprise, surprise.
 Let's look at a residual plot for the last 3 years of the data, see Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Residuals-from-time"

\end_inset

.
 Note that there is a very predictable pattern.
 This is pretty strong evidence that the errors of the model are not independent
 of one another, which means there seems to be autocorrelation.
 
\end_layout

\begin_layout Itemize
this data is clearly nonstationary.
 The very large t-statistics that you get from OLS are suspicious, no?
\end_layout

\begin_layout Itemize
What is the limit of 
\begin_inset Formula $X^{\prime}X/n$
\end_inset

 when there is a time trend in the regressor matrix?
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Residuals-from-time"

\end_inset

Residuals from time trend for CO2 data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/CO2Residuals.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Causes
\end_layout

\begin_layout Standard
Autocorrelation is the existence of correlation across the error term: 
\begin_inset Formula 
\[
\mathcal{E}(\varepsilon_{t}\varepsilon_{s})\neq0,t\neq s.
\]

\end_inset

 Why might this occur? Plausible explanations include
\end_layout

\begin_layout Enumerate
Lags in adjustment to shocks.
 In a model such as 
\begin_inset Formula 
\[
y_{t}=x_{t}^{\prime}\beta+\varepsilon_{t},
\]

\end_inset

 one could interpret 
\begin_inset Formula $x_{t}^{\prime}\beta$
\end_inset

 as the equilibrium value.
 Suppose 
\begin_inset Formula $x_{t}$
\end_inset

 is constant over a number of observations.
 One can interpret 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 as a shock that moves the system away from equilibrium.
 If the time needed to return to equilibrium is long with respect to the
 observation frequency, one could expect 
\begin_inset Formula $\varepsilon_{t+1}$
\end_inset

 to be positive, conditional on 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 positive, which induces a correlation.
 
\end_layout

\begin_layout Enumerate
Unobserved factors that are correlated over time.
 The error term is often assumed to correspond to unobservable factors.
 If these factors are correlated over time, there will be autocorrelation.
\end_layout

\begin_layout Enumerate
Misspecification of the model.
 Suppose that the DGP is 
\begin_inset Formula 
\[
y_{t}=\beta_{0}+\beta_{1}x_{t}+\beta_{2}x_{t}^{2}+\varepsilon_{t}
\]

\end_inset

 but we estimate 
\begin_inset Formula 
\[
y_{t}=\beta_{0}+\beta_{1}x_{t}+\varepsilon_{t}
\]

\end_inset

 The effects are illustrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Autocorrelation-induced-by"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Autocorrelation-induced-by"

\end_inset

Autocorrelation induced by misspecification
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/Figures/MisspecCausesAutcorrelation.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Neglecting to include dynamics in a model.
 Lags of the dependent variable may be relevant regressors, and if they
 are omitted, their effects go into the error term, which will be autocorrelated.
 
\end_layout

\begin_layout Subsection
Effects on the OLS estimator
\end_layout

\begin_layout Standard
The variance of the OLS estimator is the same as in the case of heteroscedastici
ty - the standard formula does not apply.
 The correct formula is given in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "OLS covariance with nonspaerical"

\end_inset

.
 Next we discuss two GLS corrections for OLS.
 This sort of solution may lead to inconsistent estimation of betas in some
 cases, and it has definitely gone completely out of style.
 The standard procedure is to include enough lags of the dependent variable
 so that detectable AUT disappears, and then to use robust covariance estimation
 to take care of residual effects (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Asymptotically-valid-inferences"

\end_inset

).
 For reference, a couple of examples of the old-fashioned GLS corrections
 follow, but I will not discuss this in class.
\end_layout

\begin_layout Subsection
AR(1)
\end_layout

\begin_layout Standard
There are many types of autocorrelation.
 We'll consider two examples.
 The first is the most commonly encountered case: autoregressive order 1
 (AR(1) errors.
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\varepsilon_{t} & = & \rho\varepsilon_{t-1}+u_{t}\\
u_{t} & \sim & iid(0,\sigma_{u}^{2})\\
\mathcal{E}(\varepsilon_{t}u_{s}) & = & 0,t<s
\end{eqnarray*}

\end_inset

 We assume that the model satisfies the other classical assumptions.
\end_layout

\begin_layout Itemize
We need a stationarity assumption: 
\begin_inset Formula $|\rho|<1.$
\end_inset

 Otherwise the variance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 explodes as 
\begin_inset Formula $t$
\end_inset

 increases, so standard asymptotics will not apply.
\end_layout

\begin_layout Itemize
By recursive substitution we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\varepsilon_{t} & = & \rho\varepsilon_{t-1}+u_{t}\\
 & = & \rho\left(\rho\varepsilon_{t-2}+u_{t-1}\right)+u_{t}\\
 & = & \rho^{2}\varepsilon_{t-2}+\rho u_{t-1}+u_{t}\\
 & = & \rho^{2}\left(\rho\varepsilon_{t-3}+u_{t-2}\right)+\rho u_{t-1}+u_{t}
\end{eqnarray*}

\end_inset

 In the limit the lagged 
\begin_inset Formula $\varepsilon$
\end_inset

 drops out, since 
\begin_inset Formula $\rho^{m}\rightarrow0$
\end_inset

 as 
\begin_inset Formula $m\rightarrow\infty,$
\end_inset

 so we obtain 
\begin_inset Formula 
\[
\varepsilon_{t}=\sum_{m=0}^{\infty}\rho^{m}u_{t-m}
\]

\end_inset

 With this, the variance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is found as 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\varepsilon_{t}^{2}) & = & \sigma_{u}^{2}\sum_{m=0}^{\infty}\rho^{2m}\\
 & = & \frac{\sigma_{u}^{2}}{1-\rho^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
If we had directly assumed that 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 were covariance stationary, we could obtain this using 
\begin_inset Formula 
\begin{eqnarray*}
V(\varepsilon_{t}) & = & \rho^{2}\mathcal{E}(\varepsilon_{t-1}^{2})+2\rho\mathcal{E}(\varepsilon_{t-1}u_{t})+\mathcal{E}(u_{t}^{2})\\
 & = & \rho^{2}V(\varepsilon_{t})+\sigma_{u}^{2},
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
V(\varepsilon_{t})=\frac{\sigma_{u}^{2}}{1-\rho^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
The variance is the 
\begin_inset Formula $0^{th}$
\end_inset

 order autocovariance: 
\begin_inset Formula $\gamma_{0}=V(\varepsilon_{t})$
\end_inset


\end_layout

\begin_layout Itemize
Note that the variance does not depend on 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Standard
Likewise, the first order autocovariance 
\begin_inset Formula $\gamma_{1}$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
Cov(\varepsilon_{t},\varepsilon_{t-1}) & =\gamma_{s}= & \mathcal{E}(\left(\rho\varepsilon_{t-1}+u_{t}\right)\varepsilon_{t-1})\\
 & = & \rho V(\varepsilon_{t})\\
 & = & \frac{\rho\sigma_{u}^{2}}{1-\rho^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Using the same method, we find that for 
\begin_inset Formula $s<t$
\end_inset


\begin_inset Formula 
\[
Cov(\varepsilon_{t},\varepsilon_{t-s})=\gamma_{s}=\frac{\rho^{s}\sigma_{u}^{2}}{1-\rho^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
The autocovariances don't depend on 
\begin_inset Formula $t$
\end_inset

: the process 
\begin_inset Formula $\{\varepsilon_{t}\}$
\end_inset

 is 
\emph on
covariance stationary
\end_layout

\begin_layout Standard
The 
\emph on
correlation (
\emph default
in general, for r.v.'s 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

) is defined as 
\begin_inset Formula 
\[
\text{corr}(x,y)=\frac{\text{cov}(x,y)}{\text{se}(x)\text{se}(y)}
\]

\end_inset

 but in this case, the two standard errors are the same, so the 
\begin_inset Formula $s$
\end_inset

-order autocorrelation 
\begin_inset Formula $\rho_{s}$
\end_inset

 is 
\begin_inset Formula 
\[
\rho_{s}=\rho^{s}
\]

\end_inset


\end_layout

\begin_layout Itemize
All this means that the overall matrix 
\begin_inset Formula $\Sigma$
\end_inset

 has the form 
\begin_inset Formula 
\[
\Sigma=\underbrace{\frac{\sigma_{u}^{2}}{1-\rho^{2}}}_{\textrm{this is the variance}}\underbrace{\left[\begin{array}{ccccc}
1 & \rho & \rho^{2} & \cdots & \rho^{n-1}\\
\rho & 1 & \rho & \cdots & \rho^{n-2}\\
\vdots &  & \ddots &  & \vdots\\
 &  &  & \ddots & \rho\\
\rho^{n-1} & \cdots &  &  & 1
\end{array}\right]}_{\textrm{this is the correlation matrix}}
\]

\end_inset

 So we have homoscedasticity, but elements off the main diagonal are not
 zero.
 All of this depends only on two parameters, 
\begin_inset Formula $\rho$
\end_inset

 and 
\begin_inset Formula $\sigma_{u}^{2}.$
\end_inset

 If we can estimate these consistently, we can apply FGLS.
 
\end_layout

\begin_layout Standard
It turns out that it's easy to estimate these consistently.
 The steps are
\end_layout

\begin_layout Enumerate
Estimate the model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\varepsilon_{t}$
\end_inset

 by OLS.
\end_layout

\begin_layout Enumerate
Take the residuals, and estimate the model 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}=\rho\hat{\varepsilon}_{t-1}+u_{t}^{*}
\]

\end_inset

 Since 
\begin_inset Formula $\hat{\varepsilon}_{t}\overset{p}{\rightarrow}\varepsilon_{t},$
\end_inset

 this regression is asymptotically equivalent to the regression 
\begin_inset Formula 
\[
\varepsilon_{t}=\rho\varepsilon_{t-1}+u_{t}
\]

\end_inset

 which satisfies the classical assumptions.
 Therefore, 
\begin_inset Formula $\hat{\rho}$
\end_inset

 obtained by applying OLS to 
\begin_inset Formula $\hat{\varepsilon}_{t}=\rho\hat{\varepsilon}_{t-1}+u_{t}^{*}$
\end_inset

 is consistent.
 Also, since 
\begin_inset Formula $u_{t}^{*}\overset{p}{\rightarrow}u_{t}$
\end_inset

, the estimator 
\begin_inset Formula 
\[
\hat{\sigma}_{u}^{2}=\frac{1}{n}\sum_{t=2}^{n}\left(\hat{u}_{t}^{*}\right)^{2}\overset{p}{\rightarrow}\sigma_{u}^{2}
\]

\end_inset


\end_layout

\begin_layout Enumerate
With the consistent estimators 
\begin_inset Formula $\hat{\sigma}_{u}^{2}$
\end_inset

 and 
\begin_inset Formula $\hat{\rho},$
\end_inset

 form 
\begin_inset Formula $\hat{\Sigma}=\Sigma(\hat{\sigma}_{u}^{2},\hat{\rho})$
\end_inset

 using the previous structure of 
\begin_inset Formula $\Sigma,$
\end_inset

 and estimate by FGLS.
 Actually, one can omit the factor 
\begin_inset Formula $\hat{\sigma}_{u}^{2}/(1-\rho^{2}),$
\end_inset

 since it cancels out in the formula 
\begin_inset Formula 
\[
\hat{\beta}_{FGLS}=\left(X^{\prime}\hat{\Sigma}^{-1}X\right)^{-1}(X^{\prime}\hat{\Sigma}^{-1}y).
\]

\end_inset


\end_layout

\begin_layout Itemize
One can iterate the process, by taking the first FGLS estimator of 
\begin_inset Formula $\beta,$
\end_inset

 re-estimating 
\begin_inset Formula $\rho$
\end_inset

 and 
\begin_inset Formula $\sigma_{u}^{2},$
\end_inset

 etc.
 If one iterates to convergences it's equivalent to MLE (supposing normal
 errors).
\end_layout

\begin_layout Itemize
An asymptotically equivalent approach is to simply estimate the transformed
 model 
\begin_inset Formula 
\[
y_{t}-\hat{\rho}y_{t-1}=(x_{t}-\hat{\rho}x_{t-1})^{\prime}\beta+u_{t}^{*}
\]

\end_inset

 using 
\begin_inset Formula $n-1$
\end_inset

 observations (since 
\begin_inset Formula $y_{0}$
\end_inset

 and 
\begin_inset Formula $x_{0}$
\end_inset

 aren't available).
 This is the method of Cochrane and Orcutt.
 Dropping the first observation is asymptotically irrelevant, but 
\emph on
it can be very important in small samples.

\emph default
 One can recuperate the first observation by putting 
\begin_inset Formula 
\begin{eqnarray*}
y_{1}^{*} & =y_{1} & \sqrt{1-\hat{\rho}^{2}}\\
x_{1}^{*} & =x_{1} & \sqrt{1-\hat{\rho}^{2}}
\end{eqnarray*}

\end_inset

 This somewhat odd-looking result is related to the Cholesky factorization
 of 
\begin_inset Formula $\Sigma^{-1}.$
\end_inset

 See Davidson and MacKinnon, pg.
 348-49 for more discussion.
 Note that the variance of 
\begin_inset Formula $y_{1}^{*}$
\end_inset

 is 
\begin_inset Formula $\sigma_{u}^{2},$
\end_inset

 asymptotically, so we see that the transformed model will be homoscedastic
 (and nonautocorrelated, since the 
\begin_inset Formula $u^{\prime}s$
\end_inset

 are uncorrelated with the 
\begin_inset Formula $y^{\prime}s,$
\end_inset

 in different time periods.
 
\end_layout

\begin_layout Subsection
MA(1)
\end_layout

\begin_layout Standard
The linear regression model with moving average order 1 errors is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\beta+\varepsilon_{t}\\
\varepsilon_{t} & = & u_{t}+\phi u_{t-1}\\
u_{t} & \sim & iid(0,\sigma_{u}^{2})\\
\mathcal{E}(\varepsilon_{t}u_{s}) & = & 0,t<s
\end{eqnarray*}

\end_inset

 In this case, 
\begin_inset Formula 
\begin{eqnarray*}
V(\varepsilon_{t}) & =\gamma_{0}= & \mathcal{E}\left[\left(u_{t}+\phi u_{t-1}\right)^{2}\right]\\
 & = & \sigma_{u}^{2}+\phi^{2}\sigma_{u}^{2}\\
 & = & \sigma_{u}^{2}(1+\phi^{2})
\end{eqnarray*}

\end_inset

 Similarly 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{1} & = & \mathcal{E}\left[\left(u_{t}+\phi u_{t-1}\right)\left(u_{t-1}+\phi u_{t-2}\right)\right]\\
 & = & \phi\sigma_{u}^{2}
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{2} & = & \left[\left(u_{t}+\phi u_{t-1}\right)\left(u_{t-2}+\phi u_{t-3}\right)\right]\\
 & = & 0
\end{eqnarray*}

\end_inset

 so in this case 
\begin_inset Formula 
\[
\Sigma=\sigma_{u}^{2}\left[\begin{array}{ccccc}
1+\phi^{2} & \phi & 0 & \cdots & 0\\
\phi & 1+\phi^{2} & \phi\\
0 & \phi & \ddots &  & \vdots\\
\vdots &  &  & \ddots & \phi\\
0 & \cdots &  & \phi & 1+\phi^{2}
\end{array}\right]
\]

\end_inset

 Note that the first order autocorrelation is 
\begin_inset Formula 
\begin{eqnarray*}
\rho_{1} & =\frac{\phi\sigma_{u}^{2}}{\sigma_{u}^{2}(1+\phi^{2})} & =\frac{\gamma_{1}}{\gamma_{0}}\\
 & = & \frac{\phi}{(1+\phi^{2})}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
This achieves a maximum at 
\begin_inset Formula $\phi=1$
\end_inset

 and a minimum at 
\begin_inset Formula $\phi=-1,$
\end_inset

 and the maximal and minimal autocorrelations are 1/2 and -1/2.
 Therefore, series that are more strongly autocorrelated can't be MA(1)
 processes.
 
\end_layout

\begin_layout Standard
Again the covariance matrix has a simple structure that depends on only
 two parameters.
 The problem in this case is that one can't estimate 
\begin_inset Formula $\phi$
\end_inset

 using OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

on 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}=u_{t}+\phi u_{t-1}
\]

\end_inset

 because the 
\begin_inset Formula $u_{t}$
\end_inset

 are unobservable and they can't be estimated consistently.
 However, there is a simple way to estimate the parameters.
\end_layout

\begin_layout Itemize
Since the model is homoscedastic, we can estimate 
\begin_inset Formula 
\[
V(\varepsilon_{t})=\sigma_{\varepsilon}^{2}=\sigma_{u}^{2}(1+\phi^{2})
\]

\end_inset

 using the typical estimator: 
\begin_inset Formula 
\[
\widehat{\sigma_{\varepsilon}^{2}}=\widehat{\sigma_{u}^{2}(1+\phi^{2})}=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
By the Slutsky theorem, we can interpret this as defining an (unidentified)
 estimator of both 
\begin_inset Formula $\sigma_{u}^{2}$
\end_inset

 and 
\begin_inset Formula $\phi,$
\end_inset

 e.g., use this as 
\begin_inset Formula 
\[
\widehat{\sigma_{u}^{2}}(1+\widehat{\phi}^{2})=\frac{1}{n}\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}
\]

\end_inset

 However, this isn't sufficient to define consistent estimators of the parameter
s, since it's unidentified - two unknowns, one equation.
\end_layout

\begin_layout Itemize
To solve this problem, estimate the covariance of 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{t-1}$
\end_inset

 using 
\begin_inset Formula 
\[
\widehat{Cov}(\varepsilon_{t},\varepsilon_{t-1})=\widehat{\phi\sigma_{u}^{2}}=\frac{1}{n}\sum_{t=2}^{n}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t-1}
\]

\end_inset

 This is a consistent estimator, following a LLN (and given that the epsilon
 hats are consistent for the epsilons).
 As above, this can be interpreted as defining an unidentified estimator
 of the two parameters: 
\begin_inset Formula 
\[
\hat{\phi}\widehat{\sigma_{u}^{2}}=\frac{1}{n}\sum_{t=2}^{n}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t-1}
\]

\end_inset


\end_layout

\begin_layout Itemize
Now solve these two equations to obtain identified (and therefore consistent)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimators of both 
\begin_inset Formula $\phi$
\end_inset

 and 
\begin_inset Formula $\sigma_{u}^{2}.$
\end_inset

 Define the consistent estimator 
\begin_inset Formula 
\[
\hat{\Sigma}=\Sigma(\hat{\phi},\widehat{\sigma_{u}^{2}})
\]

\end_inset

 following the form we've seen above, and transform the model using the
 Cholesky decomposition.
 The transformed model satisfies the classical assumptions asymptotically.
\end_layout

\begin_layout Itemize
Note: there is no guarantee that 
\begin_inset Formula $\Sigma$
\end_inset

 estimated using the above method will be positive definite, which may pose
 a problem.
 Another method would be to use ML estimation, if one is willing to make
 distributional assumptions regarding the white noise errors.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Monte-Carlo-example:"

\end_inset

Monte Carlo example: AR1
\end_layout

\begin_layout Standard
(too lazy to convert the code to Julia)
\end_layout

\begin_layout Standard
Let's look at a Monte Carlo study that compares OLS and GLS when we have
 AR1 errors.
 The model is 
\begin_inset Formula 
\begin{align*}
y_{t} & =1+x_{t}+\epsilon_{t}\\
\epsilon_{t} & =\rho\epsilon_{t-1}+u_{t}
\end{align*}

\end_inset

with 
\begin_inset Formula $\rho=0.9$
\end_inset

.
 The sample size is 
\begin_inset Formula $n=30,$
\end_inset

 and 1000 Monte Carlo replications are done.
 The Octave script is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GLS/AR1Errors.m}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/GLS/AR1Errors.m}
\end_layout

\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Efficiency-of-OLS"

\end_inset

 shows histograms of the estimated coefficient of 
\begin_inset Formula $x$
\end_inset

 minus the true value.
 We can see that the GLS histogram is much more concentrated about 0, which
 is indicative of the efficiency of GLS relative to OLS.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Efficiency-of-OLS"

\end_inset

Efficiency of OLS and FGLS, AR1 errors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/AR1ErrorsOLS.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
OLS
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/AR1ErrorsGLS.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
GLS
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Asymptotically-valid-inferences"

\end_inset

Asymptotically valid inferences with autocorrelation of unknown form
\end_layout

\begin_layout Standard
See Hamilton Ch.
 10, pp.
 261-2 and 280-84.
\end_layout

\begin_layout Standard
When the form of autocorrelation is unknown, one may decide to use the OLS
 estimator, without correction.
 We've seen that this estimator has the limiting distribution 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right)
\]

\end_inset

 where, as before, 
\begin_inset Formula $\Omega$
\end_inset

 is 
\begin_inset Formula 
\[
\Omega=\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)
\]

\end_inset

 We need a consistent estimate of 
\begin_inset Formula $\Omega$
\end_inset

.
 Define 
\begin_inset Formula $m_{t}=x_{t}\varepsilon_{t}$
\end_inset

 (recall that 
\begin_inset Formula $x_{t}$
\end_inset

 is defined as a 
\begin_inset Formula $K\times1$
\end_inset

 vector).
 Note that 
\begin_inset Formula 
\begin{eqnarray*}
X^{\prime}\varepsilon & = & \left[\begin{array}{cccc}
x_{1} & x_{2} & \cdots & x_{n}\end{array}\right]\left[\begin{array}{c}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{n}
\end{array}\right]\\
 & = & \sum_{t=1}^{n}x_{t}\varepsilon_{t}\\
 & = & \sum_{t=1}^{n}m_{t}
\end{eqnarray*}

\end_inset

 so that 
\begin_inset Formula 
\[
\Omega=\lim_{n\rightarrow\infty}\frac{1}{n}\mathcal{E}\left[\left(\sum_{t=1}^{n}m_{t}\right)\left(\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]
\]

\end_inset

 We assume that 
\begin_inset Formula $m_{t}$
\end_inset

 is covariance stationary (so that the covariance between 
\begin_inset Formula $m_{t}$
\end_inset

 and 
\begin_inset Formula $m_{t-s}$
\end_inset

 does not depend on 
\begin_inset Formula $t).$
\end_inset


\end_layout

\begin_layout Standard
Define the 
\begin_inset Formula $v-th$
\end_inset

 autocovariance of 
\begin_inset Formula $m_{t}$
\end_inset

 as 
\begin_inset Formula 
\[
\Gamma_{v}=\mathcal{E}(m_{t}m_{t-v}^{\prime}).
\]

\end_inset

 Note that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+v}^{\prime})=\Gamma_{v}^{\prime}.$
\end_inset

 
\emph on
(show this with an example).

\emph default
 In general, we expect that:
\end_layout

\begin_layout Itemize
\begin_inset Formula $m_{t}$
\end_inset

 will be autocorrelated, since 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is potentially autocorrelated: 
\begin_inset Formula 
\[
\Gamma_{v}=\mathcal{E}(m_{t}m_{t-v}^{\prime})\neq0
\]

\end_inset

 Note that this autocovariance does not depend on 
\begin_inset Formula $t,$
\end_inset

 due to covariance stationarity.
\end_layout

\begin_layout Itemize
contemporaneously correlated ( 
\begin_inset Formula $\mathcal{E}(m_{it}m_{jt})\neq0$
\end_inset

 ), since the regressors in 
\begin_inset Formula $x_{t}$
\end_inset

 will in general be correlated (more on this later).
\end_layout

\begin_layout Itemize
and heteroscedastic (
\begin_inset Formula $\mathcal{E}(m_{it}^{2})=\sigma_{i}^{2}$
\end_inset

 , which depends upon 
\begin_inset Formula $i$
\end_inset

 ), again since the regressors will have different variances.
 
\end_layout

\begin_layout Standard
While one could estimate 
\begin_inset Formula $\Omega$
\end_inset

 parametrically, we in general have little information upon which to base
 a parametric specification.
 Recent research has focused on consistent nonparametric estimators of 
\begin_inset Formula $\Omega.$
\end_inset


\end_layout

\begin_layout Standard
Now define 
\begin_inset Formula 
\[
\Omega_{n}=\mathcal{E}\frac{1}{n}\left[\left(\sum_{t=1}^{n}m_{t}\right)\left(\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]
\]

\end_inset

 We have (
\emph on
show that the following is true, by expanding sum and shifting rows to left)
\emph default

\begin_inset Formula 
\[
\Omega_{n}=\Gamma_{0}+\frac{n-1}{n}\left(\Gamma_{1}+\Gamma_{1}^{\prime}\right)+\frac{n-2}{n}\left(\Gamma_{2}+\Gamma_{2}^{\prime}\right)\cdots+\frac{1}{n}\left(\Gamma_{n-1}+\Gamma_{n-1}^{\prime}\right)
\]

\end_inset

 The natural, consistent estimator of 
\begin_inset Formula $\Gamma_{v}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\Gamma_{v}}=\frac{1}{n}\sum_{t=v+1}^{n}\hat{m}_{t}\hat{m}_{t-v}^{\prime}.
\]

\end_inset

 where 
\begin_inset Formula 
\[
\hat{m}_{t}=x_{t}\hat{\varepsilon}_{t}
\]

\end_inset

 (note: one could put 
\begin_inset Formula $1/(n-v)$
\end_inset

 instead of 
\begin_inset Formula $1/n$
\end_inset

 here).
 So, a natural, but inconsistent, estimator of 
\begin_inset Formula $\Omega_{n}$
\end_inset

 would be 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Omega}_{n} & = & \widehat{\Gamma_{0}}+\frac{n-1}{n}\left(\widehat{\Gamma_{1}}+\widehat{\Gamma_{1}^{\prime}}\right)+\frac{n-2}{n}\left(\widehat{\Gamma_{2}}+\widehat{\Gamma_{2}^{\prime}}\right)+\cdots+\frac{1}{n}\left(\widehat{\Gamma_{n-1}}+\widehat{\Gamma_{n-1}^{\prime}}\right)\\
 & = & \widehat{\Gamma_{0}}+\sum_{v=1}^{n-1}\frac{n-v}{n}\left(\widehat{\Gamma_{v}}+\widehat{\Gamma_{v}^{\prime}}\right).
\end{eqnarray*}

\end_inset

 This estimator is inconsistent in general, since the number of parameters
 to estimate is more than the number of observations, and increases more
 rapidly than 
\begin_inset Formula $n$
\end_inset

, so information does not build up as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Standard
On the other hand, supposing that 
\begin_inset Formula $\Gamma_{v}$
\end_inset

 tends to zero sufficiently rapidly as 
\begin_inset Formula $v$
\end_inset

 tends to 
\begin_inset Formula $\infty,$
\end_inset

 a modified estimator 
\begin_inset Formula 
\[
\hat{\Omega}_{n}=\widehat{\Gamma_{0}}+\sum_{v=1}^{q(n)}\left(\widehat{\Gamma_{v}}+\widehat{\Gamma_{v}^{\prime}}\right),
\]

\end_inset

 where 
\begin_inset Formula $q(n)\overset{p}{\rightarrow}\infty$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 will be consistent, provided 
\begin_inset Formula $q(n)$
\end_inset

 grows sufficiently slowly.
\end_layout

\begin_layout Itemize
The assumption that autocorrelations die off is reasonable in many cases.
 For example, the AR(1) model with 
\begin_inset Formula $|\rho|<1$
\end_inset

 has autocorrelations that die off.
\end_layout

\begin_layout Itemize
The term 
\begin_inset Formula $\frac{n-v}{n}$
\end_inset

 can be dropped because it tends to one for 
\begin_inset Formula $v<q(n)$
\end_inset

, given that 
\begin_inset Formula $q(n)$
\end_inset

 increases slowly relative to 
\begin_inset Formula $n.$
\end_inset


\end_layout

\begin_layout Itemize
A disadvantage of this estimator is that is may not be positive definite.
 This could cause one to calculate a negative 
\begin_inset Formula $\chi^{2}$
\end_inset

 statistic, for example!
\end_layout

\begin_layout Itemize
Newey and West proposed and estimator (
\emph on
Econometrica
\emph default
, 1987) that solves the problem of possible nonpositive definiteness of
 the above estimator.
 Their estimator is 
\begin_inset Formula 
\[
\hat{\Omega}_{n}=\widehat{\Gamma_{0}}+\sum_{v=1}^{q(n)}\left[1-\frac{v}{q+1}\right]\left(\widehat{\Gamma_{v}}+\widehat{\Gamma_{v}^{\prime}}\right).
\]

\end_inset

 This estimator is p.d.
 by construction.
 The condition for consistency is that 
\begin_inset Formula $n^{-1/4}q(n)\rightarrow0.$
\end_inset

 Note that this is a very slow rate of growth for 
\begin_inset Formula $q.$
\end_inset

 This estimator is nonparametric - we've placed no parametric restrictions
 on the form of 
\begin_inset Formula $\Omega.$
\end_inset

 It is an example of a 
\emph on
kernel
\emph default
 estimator.
 
\end_layout

\begin_layout Standard
Finally, since 
\begin_inset Formula $\Omega_{n}$
\end_inset

 has 
\begin_inset Formula $\Omega$
\end_inset

 as its limit, 
\begin_inset Formula $\hat{\Omega}_{n}\overset{p}{\rightarrow}\Omega.$
\end_inset

 We can now use 
\begin_inset Formula $\hat{\Omega}_{n}$
\end_inset

 and 
\begin_inset Formula $\widehat{Q_{X}}=\frac{1}{n}X^{\prime}X$
\end_inset

 to consistently estimate the limiting distribution of the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator under heteroscedasticity and autocorrelation of unknown form.
 With this, asymptotically valid tests are constructed in the usual way.
\end_layout

\begin_layout Subsection
Testing for autocorrelation
\end_layout

\begin_layout Standard

\series bold
Breusch-Godfrey test
\end_layout

\begin_layout Standard
This test uses an auxiliary regression, as does the White test for heteroscedast
icity.
 The regression is 
\begin_inset Formula 
\[
\hat{\varepsilon}_{t}=x_{t}^{\prime}\delta+\gamma_{1}\hat{\varepsilon}_{t-1}+\gamma_{2}\hat{\varepsilon}_{t-2}+\cdots+\gamma_{P}\hat{\varepsilon}_{t-P}+v_{t}
\]

\end_inset

 and the test statistic is the 
\begin_inset Formula $nR^{2}$
\end_inset

 statistic, just as in the White test.
 There are 
\begin_inset Formula $P$
\end_inset

 restrictions, so the test statistic is asymptotically distributed as a
 
\begin_inset Formula $\chi^{2}(P).$
\end_inset


\end_layout

\begin_layout Itemize
The intuition is that the lagged errors shouldn't contribute to explaining
 the current error if there is no autocorrelation.
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{t}$
\end_inset

 is included as a regressor to account for the fact that the 
\begin_inset Formula $\hat{\varepsilon}_{t}$
\end_inset

 are not independent even if the 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 are.
 This is a technicality that we won't go into here.
\end_layout

\begin_layout Itemize
This test is valid even if the regressors are stochastic and contain lagged
 dependent variables, so it is considerably more useful than the DW test
 for typical time series data.
\end_layout

\begin_layout Itemize
The alternative is not that the model is an AR(P), following the argument
 above.
 The alternative is simply that some or all of the first 
\begin_inset Formula $P\;$
\end_inset

autocorrelations are different from zero.
 This is compatible with many specific forms of autocorrelation.
 
\end_layout

\begin_layout Standard

\series bold
Durbin-Watson test
\end_layout

\begin_layout Standard
The Durbin-Watson test is not strictly valid in most situations where we
 would like to use it.
 Nevertheless, it is encountered often enough so that one should know something
 about it (perhaps: I'm no longer teaching this, but I'll leave it in the
 notes for reference).
 The Durbin-Watson test statistic is 
\begin_inset Formula 
\begin{eqnarray*}
DW & = & \frac{\sum_{t=2}^{n}\left(\hat{\varepsilon}_{t}-\hat{\varepsilon}_{t-1}\right)^{2}}{\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}}\\
 & = & \frac{\sum_{t=2}^{n}\left(\hat{\varepsilon}_{t}^{2}-2\hat{\varepsilon}_{t}\hat{\varepsilon}_{t-1}+\hat{\varepsilon}_{t-1}^{2}\right)}{\sum_{t=1}^{n}\hat{\varepsilon}_{t}^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The null hypothesis is that the first order autocorrelation of the errors
 is zero: 
\begin_inset Formula $H_{0}:\rho_{1}=0.$
\end_inset

 The alternative is of course 
\begin_inset Formula $H_{A}:\rho_{1}\neq0.$
\end_inset

 Note that the alternative is not that the errors are AR(1), since many
 general patterns of autocorrelation will have the first order autocorrelation
 different than zero.
 For this reason the test is useful for detecting autocorrelation in general.
 For the same reason, one shouldn't just assume that an AR(1) model is appropria
te when the DW test rejects the null.
\end_layout

\begin_layout Itemize
Under the null, the middle term tends to zero, and the other two tend to
 one, so 
\begin_inset Formula $DW\overset{p}{\rightarrow}2.$
\end_inset


\end_layout

\begin_layout Itemize
Supposing that we had an AR(1) error process with 
\begin_inset Formula $\rho=1.$
\end_inset

 In this case the middle term tends to 
\begin_inset Formula $-2,$
\end_inset

 so 
\begin_inset Formula $DW\overset{p}{\rightarrow}0$
\end_inset


\end_layout

\begin_layout Itemize
Supposing that we had an AR(1) error process with 
\begin_inset Formula $\rho=-1.$
\end_inset

 In this case the middle term tends to 
\begin_inset Formula $2,$
\end_inset

 so 
\begin_inset Formula $DW\overset{p}{\rightarrow}4$
\end_inset


\end_layout

\begin_layout Itemize
These are the extremes: 
\begin_inset Formula $DW$
\end_inset

 always lies between 0 and 4.
\end_layout

\begin_layout Itemize
The distribution of the test statistic depends on the matrix of regressors,
 
\begin_inset Formula $X,$
\end_inset

 so tables can't give exact critical values.
 The give upper and lower bounds, which correspond to the extremes that
 are possible.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Durbin-Watson-critical-values"

\end_inset

.
 There are means of determining exact critical values conditional on 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Itemize
Note that DW
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

can be used to test for nonlinearity (add discussion).
 
\end_layout

\begin_layout Itemize
The DW test is based upon the assumption that the matrix 
\begin_inset Formula $X$
\end_inset

 is fixed in repeated samples.
 This is often unreasonable in the context of economic time series, which
 is precisely the context where the test would have application.
 It is possible to relate the DW test to other test statistics which are
 valid without strict exogeneity.
\end_layout

\begin_layout Verse
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Durbin-Watson-critical-values"

\end_inset

Durbin-Watson critical values
\end_layout

\end_inset


\begin_inset Graphics
	filename Examples/Figures/DurbinWatson.pdf
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Lagged dependent variables and autocorrelation
\end_layout

\begin_layout Standard
We've seen that the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is consistent under autocorrelation, as long as 
\begin_inset Formula $plim\frac{X^{\prime}\varepsilon}{n}=0.$
\end_inset

 This will be the case when 
\begin_inset Formula $\mathcal{E}(X^{\prime}\varepsilon)$
\end_inset

 
\begin_inset Formula $=0,$
\end_inset

 following a LLN.
 An important exception is the case where 
\begin_inset Formula $X$
\end_inset

 contains lagged 
\begin_inset Formula $y^{\prime}s$
\end_inset

 and the errors are autocorrelated.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:-Dynamic-model"

\end_inset

Dynamic model with MA1 errors.
 Consider the model 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \alpha+\rho y_{t-1}+\beta x_{t}+\epsilon_{t}\\
\epsilon_{t} & = & \upsilon_{t}+\phi\upsilon_{t-1}
\end{eqnarray*}

\end_inset

We can easily see that a regressor is not weakly exogenous: 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(y_{t-1}\varepsilon_{t}) & = & \mathcal{E}\left\{ (\alpha+\rho y_{t-2}+\beta x_{t-1}+\upsilon_{t-1}+\phi\upsilon_{t-2})(\upsilon_{t}+\phi\upsilon_{t-1})\right\} \\
 & \neq & 0
\end{eqnarray*}

\end_inset

 since one of the terms is 
\begin_inset Formula $\mathcal{E}(\phi\upsilon_{t-1}^{2})$
\end_inset

 which is clearly nonzero.
 In this case 
\begin_inset Formula $\mathcal{E}(\mathbf{x}_{t}\varepsilon_{t})\neq0,$
\end_inset

 and therefore 
\begin_inset Formula $plim\frac{X^{\prime}\varepsilon}{n}\neq0.$
\end_inset

 Since
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
plim\hat{\beta}=\beta+plim\frac{X^{\prime}\varepsilon}{n}
\]

\end_inset

 the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is inconsistent in this case.
 One needs to estimate by instrumental variables (IV), which we'll get to
 later
\end_layout

\begin_layout Standard
The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GLS/DynamicMA.m}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/GLS/DynamicMA.m} 
\end_layout

\end_inset

 does a Monte Carlo study.
 The sample size is 
\begin_inset Formula $n=100$
\end_inset

.
 The true coefficients are 
\begin_inset Formula $\alpha=1$
\end_inset

 
\begin_inset Formula $\rho=0.9$
\end_inset

 and 
\begin_inset Formula $\beta=1$
\end_inset

.
 The MA parameter is 
\begin_inset Formula $\phi=-0.95$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dynamic-model-with"

\end_inset

 gives the results.
 You can see that the constant and the autoregressive parameter have a lot
 of bias.
 By re-running the script with 
\begin_inset Formula $\phi=0$
\end_inset

, you will see that much of the bias disappears (not all - why?).
\begin_inset Newpage newpage
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dynamic-model-with"

\end_inset

Dynamic model with MA(1) errors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $\hat{\alpha}-\alpha$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/constant_n100.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/ylag_n100.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $\hat{\beta}-\beta$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GLS/x_n100.png
	lyxscale 25
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

Consider the following model with $n$ observations and only one explanatory
 variable:
\end_layout

\begin_layout Plain Layout


\backslash
begin{equation*} y_{i}=
\backslash
beta x_{i}+u_{i}
\backslash
text{ (1)} 
\backslash
end{equation*}
\end_layout

\begin_layout Plain Layout

which satisfies the basic OLS
\backslash
 assumptions except for the homoskedasticity assumption.
 More precisely, the variance of the error term for the first $n/2$ observations
 is $Var(u_{i})=2$, and for the remaining observations $Var(u_{i})=
\backslash
sigma ^{2}$, where $
\backslash
sigma ^{2}$ is unknown.
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

a) How can you get an asymptotically efficient estimator of $
\backslash
beta$? Provide an expression for the estimator.
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

b) How can you test the hypothesis that the explanatory variable ($x_{i})$
 is relevant in explaining $y_{i}$ in the model in equation (1)? Provide
 the statistic to be used in the test as well as its distribution.
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Comparing the variances of the OLS and GLS estimators, I claimed that the
 following holds:
\begin_inset Formula 
\begin{eqnarray*}
Var(\hat{\beta})-Var(\hat{\beta}_{GLS}) & = & A\Sigma A^{'}
\end{eqnarray*}

\end_inset

Verify that this is true.
\end_layout

\begin_layout Enumerate
Show that the GLS estimator can be defined as
\begin_inset Formula 
\[
\hat{\beta}_{GLS}=\arg\min(y-X\beta)^{\prime}\Sigma^{-1}(y-X\beta)
\]

\end_inset


\end_layout

\begin_layout Enumerate
The limiting distribution of the OLS estimator with heteroscedasticity of
 unknown form is
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\overset{d}{\rightarrow}N\left(0,Q_{X}^{-1}\Omega Q_{X}^{-1}\right),
\]

\end_inset

 where
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}\left(\frac{X^{\prime}\varepsilon\varepsilon^{\prime}X}{n}\right)=\Omega
\]

\end_inset

 Explain why
\begin_inset Formula 
\[
\widehat{\Omega}=\frac{1}{n}\sum_{t=1}^{n}x_{t}x_{t}^{\prime}\hat{\varepsilon}_{t}^{2}
\]

\end_inset

 is a consistent estimator of this matrix.
\end_layout

\begin_layout Enumerate
Define the 
\begin_inset Formula $v-th$
\end_inset

 autocovariance of a covariance stationary process 
\begin_inset Formula $m_{t}$
\end_inset

, where 
\begin_inset Formula $E(m_{t})=0$
\end_inset

 as 
\begin_inset Formula 
\[
\Gamma_{v}=\mathcal{E}(m_{t}m_{t-v}^{\prime}).
\]

\end_inset

 Show that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+v}^{\prime})=\Gamma_{v}^{\prime}.$
\end_inset


\end_layout

\begin_layout Enumerate
Perhaps we can be a little more parsimonious with the Nerlove data (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{nerlove.data}{https://github.com/mcreel/Econometrics/blob/master
/Examples/Data/nerlove.data} 
\end_layout

\end_inset

), rather than using so many parameters to account for non-constant returns
 to scale, and to account for heteroscedasticity.
 Consider the original model 
\begin_inset Formula 
\[
\ln C=\beta+\beta_{Q}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Estimate by OLS, plot the residuals, and test for autocorrelation and heterosced
asticity.
 Explain your findings.
\end_layout

\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\[
\ln C=\beta+\beta_{Q}\ln Q+\gamma_{Q}\left(\ln Q\right)^{2}+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Explain how this model can account for non-constant returns to scale.
 
\end_layout

\begin_layout Enumerate
estimate this model, and test for autocorrelation and heteroscedasticity.
 You should find that there is HET, but no strong evidence of AUT.
 Why is this the case?
\end_layout

\begin_layout Enumerate
Do a GLS correction where it is assumed that 
\begin_inset Formula $V(\epsilon_{i})=\frac{\sigma^{2}}{\left(\ln Q_{i}\right)^{2}}$
\end_inset

.
 In GRETL, there is a weighted least squares option that you can use.
 Why does this assumed form of HET make sense?
\end_layout

\begin_layout Enumerate
plot the weighted residuals versus output.
 Is there evidence of HET, or has the correction eliminated the problem?
\end_layout

\begin_layout Enumerate
plot the fitted values for returns to scale, for all of the firms.
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
The 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{hall.csv}{https://github.com/mcreel/Econometrics/blob/master/Exa
mples/Data/hall.csv} 
\end_layout

\end_inset

 or 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{hall.gdt}{https://github.com/mcreel/Econometrics/blob/master/Exa
mples/Data/hall.gdt} 
\end_layout

\end_inset

 dataset contains monthly observation on 3 variables: the consumption ratio
 
\begin_inset Formula $c_{t}/c_{t-1}$
\end_inset

; the gross return of an equally weighted index of assets 
\begin_inset Formula $ewr_{t}$
\end_inset

; and the gross return of the same index, but weighted by value, 
\begin_inset Formula $vwr_{t}$
\end_inset

.
 The idea is that a representative consumer may finance consumption by investing
 in assets.
 Present wealth is used for two things: consumption and investment.
 The return on investment defines wealth in the next period, and the process
 repeats.
 For the moment, explore the properties of the variables.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Are the variances constant over time?
\end_layout

\begin_layout Enumerate
Do the variables appear to be autocorrelated? Hint: regress a variable on
 its own lags.
\end_layout

\begin_layout Enumerate
Do the variable seem to be normally distributed?
\end_layout

\begin_layout Enumerate
Look at the properties of the growth rates of the variables: repeat a-c
 for growth rates.
 The growth rate of a variable 
\begin_inset Formula $x_{t}$
\end_inset

 is given by 
\begin_inset Formula $\log\left(x_{t}/x_{t-1}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\begin{align*}
y_{t} & =C+A_{1}y_{t-1}+\epsilon_{t}\\
E(\epsilon_{t}\epsilon_{t}^{\prime}) & =\Sigma\\
E(\epsilon_{t}\epsilon_{s}^{\prime}) & =0,t\ne s
\end{align*}

\end_inset

where 
\begin_inset Formula $y_{t}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 are 
\begin_inset Formula $G\times1$
\end_inset

 vectors, 
\begin_inset Formula $C$
\end_inset

 is a 
\begin_inset Formula $G\times1$
\end_inset

 of constants, and 
\begin_inset Formula $A_{1}$
\end_inset

is a 
\begin_inset Formula $G\times G$
\end_inset

 matrix of parameters.
 The matrix 
\begin_inset Formula $\Sigma$
\end_inset

 is a 
\begin_inset Formula $G\times G$
\end_inset

 covariance matrix.
 Assume that we have 
\begin_inset Formula $n$
\end_inset

 observations.
 This is a 
\emph on
vector autoregressive
\emph default
 model, of order 1 - commonly referred to as a VAR(1) model.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Show how the model can be written in the form 
\begin_inset Formula $Y=X\beta+\nu$
\end_inset

, where 
\begin_inset Formula $Y$
\end_inset

 is a 
\begin_inset Formula $Gn\times1$
\end_inset

 vector, 
\begin_inset Formula $\beta$
\end_inset

 is a 
\begin_inset Formula $(G+G^{2})\times$
\end_inset

1 parameter vector, and the other items are conformable.
 What is the structure of 
\begin_inset Formula $X$
\end_inset

? What is the structure of the covariance matrix of 
\begin_inset Formula $\nu$
\end_inset

?
\end_layout

\begin_layout Enumerate
This model has HET and AUT.
 Verify this statement.
\end_layout

\begin_layout Enumerate
Set 
\begin_inset Formula $G=2,$
\end_inset


\begin_inset Formula $C=(0\,0)^{\prime}$
\end_inset


\begin_inset Formula $A=\left[\begin{array}{cc}
0.8 & -0.1\\
0.2 & 0.5
\end{array}\right]$
\end_inset

, 
\begin_inset Formula $\Sigma=\left[\begin{array}{cc}
1 & 0.5\\
0.5 & 1
\end{array}\right]$
\end_inset

.
 Simulate data from this model, then estimate the model using OLS and feasible
 GLS.
 You should find that the two estimators are identical, which might seem
 surprising, given that there is HET and AUT.
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:(advanced).-Prove-analytically"

\end_inset

(optional, and advanced).
 Prove analytically that the OLS and GLS estimators are identical.
 Hint: this model is of the form of 
\emph on
seemingly unrelated regressions
\emph default
.
 
\end_layout

\end_deeper
\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\begin{align*}
y_{t} & =\alpha+\rho_{1}y_{t-1}+\rho_{2}y_{t-2}+\epsilon_{t}
\end{align*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is a 
\begin_inset Formula $N(0,1)$
\end_inset

 white noise error.
 This is an autogressive model of order 2 (AR2) model.
 Suppose that data is generated from the AR2 model, but the econometrician
 mistakenly decides to estimate an AR1 model (
\begin_inset Formula $y_{t}=\alpha+\rho_{1}y_{t-1}+\epsilon_{t}$
\end_inset

).
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
simulate data from the AR2 model, setting 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset

 and 
\begin_inset Formula $\rho_{2}=0.4,$
\end_inset

 using a sample size of 
\begin_inset Formula $n=30.$
\end_inset

 
\end_layout

\begin_layout Enumerate
Estimate the AR1 model by OLS, using the simulated data
\end_layout

\begin_layout Enumerate
test the hypothesis that 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset


\end_layout

\begin_layout Enumerate
test for autocorrelation using the test of your choice
\end_layout

\begin_layout Enumerate
repeat the above steps 10000 times.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
What percentage of the time does a t-test reject the hypothesis that 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset

?
\end_layout

\begin_layout Enumerate
What percentage of the time is the hypothesis of no autocorrelation rejected?
\end_layout

\end_deeper
\begin_layout Enumerate
discuss your findings.
 Include a residual plot for a representative sample.
\end_layout

\end_deeper
\begin_layout Enumerate
Modify the script given in Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Monte-Carlo-example:"

\end_inset

 so that the first observation is dropped, rather than given special treatment.
 This corresponds to using the Cochrane-Orcutt method, whereas the script
 as provided implements the Prais-Winsten method.
 Check if there is an efficiency loss when the first observation is dropped.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Endogeneity and simultaneity
\end_layout

\begin_layout Standard
Several times we've encountered cases where correlation between regressors
 and the error term lead to biasedness and inconsistency of the OLS estimator.
 Cases include autocorrelation with lagged dependent variables (Example
 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:-Dynamic-model"

\end_inset

), measurement error in the regressors (Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"

\end_inset

) and missing regressors (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Missing-regressors"

\end_inset

).
 Another important case we have not seen yet is that of simultaneous equations.
 The cause is different, but the effect is the same: bias and inconsistency
 when OLS is applied to a single equation.
 The basic idea is presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Exogeneity-and-Endogeneity"

\end_inset

.
 A simple regression will estimate the overall effect of x on y.
 If we're interested in the direct effect, 
\begin_inset Formula $\beta$
\end_inset

, then we have a problem when the overall effect and the direct effect differ.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Exogeneity-and-Endogeneity"

\end_inset

Exogeneity and Endogeneity (adapted from Cameron and Trivedi)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/EndogExog.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Simultaneous equations
\end_layout

\begin_layout Standard
Up until now our model is 
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

where we assume weak exogeneity of the regressors, so that 
\begin_inset Formula $E(x_{t}\epsilon_{t})=0$
\end_inset

.
 With weak exogeneity, the OLS estimator has desirable large sample properties
 (consistency, asymptotic normality).
\end_layout

\begin_layout Standard
Simultaneous equations is a different prospect.
 An example of a simultaneous equation system is a simple supply-demand
 system: 
\begin_inset Formula 
\begin{eqnarray}
\text{Demand:\;\ }q_{t} & = & \alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}y_{t}+\varepsilon_{1t}\label{eq:demand function}\\
\text{Supply:\;\ }q_{t} & = & \beta_{1}+\beta_{2}p_{t}+\varepsilon_{2t}\nonumber \\
\mathcal{E}\left(\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}
\end{array}\right]\left[\begin{array}{ll}
\varepsilon_{1t} & \varepsilon_{2t}\end{array}\right]\right) & = & \left[\begin{array}{ll}
\sigma_{11} & \sigma_{12}\\
\cdot & \sigma_{22}
\end{array}\right]\nonumber \\
 & \equiv & \Sigma,\forall t\nonumber 
\end{eqnarray}

\end_inset

 The presumption is that 
\begin_inset Formula $q_{t}$
\end_inset

 and 
\begin_inset Formula $p_{t}$
\end_inset

 are jointly determined at the same time by the intersection of these equations.
 We'll assume that 
\begin_inset Formula $y_{t}$
\end_inset

 is determined by some unrelated process.
 It's easy to see that we have correlation between regressors and errors.
 Solving for 
\begin_inset Formula $p_{t}$
\end_inset

 : 
\begin_inset Formula 
\begin{eqnarray*}
\alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}y_{t}+\varepsilon_{1t} & = & \beta_{1}+\beta_{2}p_{t}+\varepsilon_{2t}\\
\beta_{2}p_{t}-\alpha_{2}p_{t} & = & \alpha_{1}-\beta_{1}+\alpha_{3}y_{t}+\varepsilon_{1t}-\varepsilon_{2t}\\
p_{t} & = & \frac{\alpha_{1}-\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Now consider whether 
\begin_inset Formula $p_{t}$
\end_inset

 is uncorrelated with 
\begin_inset Formula $\varepsilon_{1t}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(p_{t}\varepsilon_{1t}) & = & \mathcal{E}\left\{ \left(\frac{\alpha_{1}-\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\varepsilon_{1t}\right\} \\
 & = & \frac{\sigma_{11}-\sigma_{12}}{\beta_{2}-\alpha_{2}}
\end{eqnarray*}

\end_inset

 Because of this correlation, weak exogeneity does not hold, and OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimation of the demand equation will be biased and inconsistent.
 The same applies to the supply equation, for the same reason.
\end_layout

\begin_layout Standard
A GRETL script which generates data according to this simple supply-demand
 system is here: 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Simeq/simeq.inp}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/Simeq/simeq.inp}
\end_layout

\end_inset

.
 It does a Monte Carlo study which verifies that the OLS estimator is inconsiste
nt, and that the IV estimator is consistent.
\end_layout

\begin_layout Standard
In this model, 
\begin_inset Formula $q_{t}$
\end_inset

 and 
\begin_inset Formula $p_{t}$
\end_inset

 are the 
\emph on
endogenous
\emph default
 variables (endogs), that are determined within the system.
 
\begin_inset Formula $y_{t}$
\end_inset

 is an 
\emph on
exogenous
\emph default
 variable (exogs).
 These concepts are a bit tricky, and we'll return to it in a minute.
 First, some notation.
 Suppose we group together current endogs in the vector 
\begin_inset Formula $Y_{t}.$
\end_inset

 If there are 
\begin_inset Formula $G$
\end_inset

 endogs, 
\begin_inset Formula $Y_{t}$
\end_inset

 is 
\begin_inset Formula $G\times1.$
\end_inset

 Group current and lagged exogs, as well as lagged endogs in the vector
 
\begin_inset Formula $X_{t}$
\end_inset

 , which is 
\begin_inset Formula $K\times1.$
\end_inset

 Stack the errors of the 
\begin_inset Formula $G$
\end_inset

 equations into the error vector 
\begin_inset Formula $E_{t}.$
\end_inset

 The model, with additional assumptions, can be written as 
\begin_inset Formula 
\begin{eqnarray}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}^{\prime}\nonumber \\
E_{t} & \sim & N(0,\Sigma),\forall t\label{eq:SIMEQ structural form}\\
\mathcal{E}(E_{t}E_{s}^{\prime}) & = & 0,t\neq s\nonumber 
\end{eqnarray}

\end_inset

 There are 
\begin_inset Formula $G$
\end_inset

 equations here, and the parameters that enter into each equation are contained
 in the 
\emph on
columns
\emph default
 of the matrices 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

.
 We can stack all 
\begin_inset Formula $n$
\end_inset

 observations and write the model as 
\begin_inset Formula 
\begin{eqnarray*}
Y\Gamma & = & XB+E\\
\mathcal{E}(X^{\prime}E) & = & 0_{(K\times G)}\\
vec(E) & \sim & N(0,\Psi)
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
Y=\left[\begin{array}{l}
Y_{1}^{\prime}\\
Y_{2}^{\prime}\\
\vdots\\
Y_{n}^{\prime}
\end{array}\right],X=\left[\begin{array}{l}
X_{1}^{\prime}\\
X_{2}^{\prime}\\
\vdots\\
X_{n}^{\prime}
\end{array}\right],E=\left[\begin{array}{l}
E_{1}^{\prime}\\
E_{2}^{\prime}\\
\vdots\\
E_{n}^{\prime}
\end{array}\right]
\]

\end_inset

 
\begin_inset Formula $Y$
\end_inset

 is 
\begin_inset Formula $n\times G,$
\end_inset

 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula $n\times K,$
\end_inset

 and 
\begin_inset Formula $E$
\end_inset

 is 
\begin_inset Formula $n\times G.$
\end_inset


\end_layout

\begin_layout Itemize
This system is 
\emph on
complete
\emph default
, in that there are as many equations as endogs.
\end_layout

\begin_layout Itemize
There is a normality assumption.
 This isn't necessary, but allows us to consider the relationship between
 least squares and ML estimators.
\end_layout

\begin_layout Itemize
Since there is no autocorrelation of the 
\begin_inset Formula $E_{t}$
\end_inset

 's, and since the columns of 
\begin_inset Formula $E$
\end_inset

 are individually homoscedastic, then 
\begin_inset Formula 
\begin{eqnarray*}
\Psi & = & \left[\begin{array}{llll}
\sigma_{11}I_{n} & \sigma_{12}I_{n} & \cdots & \sigma_{1G}I_{n}\\
 & \sigma_{22}I_{n} &  & \vdots\\
 &  & \ddots & \vdots\\
\cdot &  &  & \sigma_{GG}I_{n}
\end{array}\right]\\
 & = & I_{n}\otimes\Sigma
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $X$
\end_inset

 may contain lagged endogenous and exogenous variables.
 These variables are 
\emph on
predetermined.
 
\end_layout

\begin_layout Itemize
We need to define what is meant by 
\begin_inset Quotes eld
\end_inset

endogenous
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

exogenous
\begin_inset Quotes erd
\end_inset

 when classifying the current period variables.
 Remember the definition of weak exogeneity Assumption 
\begin_inset CommandInset ref
LatexCommand ref
reference "ass:Weakly-exogenous-regressors:"

\end_inset

, the regressors are weakly exogenous if 
\begin_inset Formula $E(E_{t}|X_{t})=0.$
\end_inset

 Endogenous regressors are those for which this assumption does not hold.
 As long as there is no autocorrelation, lagged endogenous variables are
 weakly exogenous.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Reduced-form"

\end_inset

Reduced form
\end_layout

\begin_layout Standard
Recall that the model is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}^{\prime}\\
V(E_{t}) & = & \Sigma
\end{eqnarray*}

\end_inset

 This is the model in 
\emph on
structural form.
\end_layout

\begin_layout Definition
[Structural form] 
\begin_inset CommandInset label
LatexCommand label
name "Structural form"

\end_inset

An equation is in structural form when more than one current period endogenous
 variable is included.
\end_layout

\begin_layout Standard
The solution for the current period endogs is easy to find.
 It is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}^{\prime}
\end{eqnarray*}

\end_inset

 Now only one current period endog appears in each equation.
 This is the 
\emph on
reduced form.
\end_layout

\begin_layout Definition
[Reduced form] 
\begin_inset CommandInset label
LatexCommand label
name "Reduced form"

\end_inset

An equation is in reduced form if only one current period endog is included.
\end_layout

\begin_layout Standard
An example is our supply/demand system.
 The reduced form for quantity is obtained by solving the supply equation
 for price and substituting into demand:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
q_{t} & = & \alpha_{1}+\alpha_{2}\left(\frac{q_{t}-\beta_{1}-\varepsilon_{2t}}{\beta_{2}}\right)+\alpha_{3}y_{t}+\varepsilon_{1t}\\
\beta_{2}q_{t}-\alpha_{2}q_{t} & = & \beta_{2}\alpha_{1}-\alpha_{2}\left(\beta_{1}+\varepsilon_{2t}\right)+\beta_{2}\alpha_{3}y_{t}+\beta_{2}\varepsilon_{1t}\\
q_{t} & = & \frac{\beta_{2}\alpha_{1}-\alpha_{2}\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\beta_{2}\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\\
 & = & \pi_{11}+\pi_{21}y_{t}+V_{1t}
\end{eqnarray*}

\end_inset

 Similarly, the rf for price, as we've seen above, is 
\begin_inset Formula 
\begin{eqnarray*}
p_{t} & = & \frac{\alpha_{1}-\beta_{1}}{\beta_{2}-\alpha_{2}}+\frac{\alpha_{3}y_{t}}{\beta_{2}-\alpha_{2}}+\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\\
 & = & \pi_{12}+\pi_{22}y_{t}+V_{2t}
\end{eqnarray*}

\end_inset

 The interesting thing about the rf is that the equations individually satisfy
 the classical assumptions, since 
\begin_inset Formula $y_{t}$
\end_inset

 is uncorrelated with 
\begin_inset Formula $\varepsilon_{1t}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{2t}$
\end_inset

 by assumption, and therefore 
\begin_inset Formula $\mathcal{E}(y_{t}V_{it})=0,$
\end_inset

 i=1,2, 
\begin_inset Formula $\forall t.$
\end_inset

 The errors of the rf are 
\begin_inset Formula 
\[
\left[\begin{array}{l}
V_{1t}\\
V_{2t}
\end{array}\right]=\left[\begin{array}{l}
\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\\
\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The variance of 
\begin_inset Formula $V_{1t}$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
V(V_{1t}) & = & \mathcal{E}\left[\left(\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\left(\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\right]\\
 & = & \frac{\beta_{2}^{2}\sigma_{11}-2\beta_{2}\alpha_{2}\sigma_{12}+\alpha_{2}\sigma_{22}}{\left(\beta_{2}-\alpha_{2}\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
This is constant over time, so the first rf equation is homoscedastic.
\end_layout

\begin_layout Itemize
Likewise, since the 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 are independent over time, so are the 
\begin_inset Formula $V_{t}.$
\end_inset


\end_layout

\begin_layout Standard
The variance of the second rf error is 
\begin_inset Formula 
\begin{eqnarray*}
V(V_{2t}) & = & \mathcal{E}\left[\left(\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\left(\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\right]\\
 & = & \frac{\sigma_{11}-2\sigma_{12}+\sigma_{22}}{\left(\beta_{2}-\alpha_{2}\right)^{2}}
\end{eqnarray*}

\end_inset

 and the contemporaneous covariance of the errors across equations is 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(V_{1t}V_{2t}) & = & \mathcal{E}\left[\left(\frac{\beta_{2}\varepsilon_{1t}-\alpha_{2}\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\left(\frac{\varepsilon_{1t}-\varepsilon_{2t}}{\beta_{2}-\alpha_{2}}\right)\right]\\
 & = & \frac{\beta_{2}\sigma_{11}-\left(\beta_{2}+\alpha_{2}\right)\sigma_{12}+\sigma_{22}}{\left(\beta_{2}-\alpha_{2}\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
In summary the rf equations individually satisfy the classical assumptions,
 under the assumptions we've made, but they are contemporaneously correlated.
 
\end_layout

\begin_layout Standard
The general form of the rf is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}^{\prime}
\end{eqnarray*}

\end_inset

 so we have that 
\begin_inset Formula 
\[
V_{t}=\left(\Gamma^{-1}\right)^{\prime}E_{t}\sim N\left(0,\left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}\right),\forall t
\]

\end_inset

 and that the 
\begin_inset Formula $V_{t}$
\end_inset

 are timewise independent (note that this wouldn't be the case if the 
\begin_inset Formula $E_{t}$
\end_inset

 were autocorrelated).
\end_layout

\begin_layout Standard
From the reduced form, we can easily see that the endogenous variables are
 correlated with the structural errors:
\begin_inset Formula 
\begin{align}
E(E_{t}Y_{t}^{\prime}) & =E\left(E_{t}\left(X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\right)\right)\nonumber \\
 & =E\left(E_{t}X_{t}^{\prime}B\Gamma^{-1}+E_{t}E_{t}^{\prime}\Gamma^{-1}\right)\nonumber \\
 & =\Sigma\Gamma^{-1}\label{eq:correlation between endogs and structural error}
\end{align}

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:EstimationRF"

\end_inset

Estimation of the reduced form equations
\end_layout

\begin_layout Standard
From above, the RF equations are 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}^{\prime}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}^{\prime}
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\[
V_{t}\sim N\left(0,\Xi\right),\forall t
\]

\end_inset

where we define 
\begin_inset Formula $\Xi\equiv\left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}$
\end_inset

.
 The rf parameter estimator 
\begin_inset Formula $\hat{\Pi},$
\end_inset

 is simply OLS applied to this model, equation by equation:: 
\begin_inset Formula 
\[
\hat{\Pi}=(X^{\prime}X)^{-1}X^{\prime}Y
\]

\end_inset

 which is simply 
\begin_inset Formula 
\[
\hat{\Pi}=(X^{\prime}X)^{-1}X^{\prime}\left[\begin{array}{llll}
y_{1} & y_{2} & \cdots & y_{G}\end{array}\right]
\]

\end_inset

 that is, OLS equation by equation using 
\emph on
all
\emph default
 the exogs in the estimation of each column of 
\begin_inset Formula $\Pi.$
\end_inset


\end_layout

\begin_layout Standard
It may seem odd that we use OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

on the reduced form, since the rf equations are correlated, because 
\begin_inset Formula $\Xi\equiv\left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}$
\end_inset

 is a full matrix.
 Why don't we do GLS to improve efficiency of estimation of the RF parameters?
\end_layout

\begin_layout Standard
OLS equation by equation to get the rf is equivalent to 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{G}
\end{array}\right]=\left[\begin{array}{llll}
X & 0 & \cdots & 0\\
0 & X &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & X
\end{array}\right]\left[\begin{array}{l}
\pi_{1}\\
\pi_{2}\\
\vdots\\
\pi_{G}
\end{array}\right]+\left[\begin{array}{l}
v_{1}\\
v_{2}\\
\vdots\\
v_{G}
\end{array}\right]
\]

\end_inset

 where 
\begin_inset Formula $y_{i}$
\end_inset

 is the 
\begin_inset Formula $n\times1$
\end_inset

 vector of observations of the 
\begin_inset Formula $i^{th}$
\end_inset

 endog, 
\begin_inset Formula $X$
\end_inset

 is the entire 
\begin_inset Formula $n\times K$
\end_inset

 matrix of exogs, 
\begin_inset Formula $\pi_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 column of 
\begin_inset Formula $\Pi,$
\end_inset

 and 
\begin_inset Formula $v_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 column of 
\begin_inset Formula $V.$
\end_inset

 Use the notation 
\begin_inset Formula 
\[
y=\mathbf{X}\pi+v
\]

\end_inset

 to indicate the pooled model.
 Following this notation, the error covariance matrix is 
\begin_inset Formula 
\[
V(v)=\Xi\otimes I_{n}
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a special case of a type of model known as a set of 
\emph on
seemingly unrelated equations (SUR)
\emph default
 since the parameter vector of each equation is different.
 The important feature of this special case is that 
\emph on
the regressors are the same in each equation
\emph default
.
 The equations are contemporanously correlated, because of the non-zero
 off diagonal elements in 
\begin_inset Formula $\Xi$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that each equation of the system individually satisfies the classical
 assumptions.
\end_layout

\begin_layout Itemize
Normally when doing SUR, one simply does GLS on the whole system 
\begin_inset Formula $y=\mathbf{X}\pi+v$
\end_inset

, where 
\begin_inset Formula $V(v)=\Xi\otimes I_{n}$
\end_inset

, which is in general more efficient than OLS on each equation.
\end_layout

\begin_layout Itemize
However, when the regressors are the same in all equations, as is true in
 the present case of estimation of the RF parameters, SUR 
\begin_inset Formula $\equiv$
\end_inset

OLS.
 To show this note that in this case 
\begin_inset Formula $\mathbf{X}=I_{n}\otimes X.$
\end_inset

 Using the rules
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $(A\otimes B)^{-1}=(A^{-1}\otimes B^{-1})$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $(A\otimes B)^{\prime}=(A^{\prime}\otimes B^{\prime})$
\end_inset

 and
\end_layout

\begin_layout Enumerate
\begin_inset Formula $(A\otimes B)(C\otimes D)=(AC\otimes BD),$
\end_inset

 we get 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\pi}_{SUR} & = & \left(\left(I_{n}\otimes X\right)^{\prime}\left(\Xi\otimes I_{n}\right)^{-1}\left(I_{n}\otimes X\right)\right)^{-1}\left(I_{n}\otimes X\right)^{\prime}\left(\Xi\otimes I_{n}\right)^{-1}y\\
 & = & \left(\left(\Xi^{-1}\otimes X^{\prime}\right)\left(I_{n}\otimes X\right)\right)^{-1}\left(\Xi^{-1}\otimes X^{\prime}\right)y\\
 & = & \left(\Xi\otimes(X^{\prime}X)^{-1}\right)\left(\Xi^{-1}\otimes X^{\prime}\right)y\\
 & = & \left[I_{G}\otimes(X^{\prime}X)^{-1}X^{\prime}\right]y\\
 & = & \left[\begin{array}{l}
\hat{\pi}_{1}\\
\hat{\pi}_{2}\\
\vdots\\
\hat{\pi}_{G}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Note that this provides the answer to the exercise 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:(advanced).-Prove-analytically"

\end_inset

 in the chapter on GLS.
\end_layout

\begin_layout Itemize
So the unrestricted rf coefficients can be estimated efficiently (assuming
 normality) by OLS, even if the equations are correlated.
\end_layout

\begin_layout Itemize
We have ignored any potential zeros in the matrix 
\begin_inset Formula $\Pi,$
\end_inset

 which if they exist could potentially increase the efficiency of estimation
 of the rf.
\end_layout

\begin_layout Itemize
Another example where SUR
\begin_inset Formula $\equiv$
\end_inset

OLS is in estimation of vector autoregressions which is discussed in Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:VAR-models"

\end_inset

.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Bias and inconsistency of OLS estimation of a structural equation
\end_layout

\begin_layout Standard
Considering the first equation (this is without loss of generality, since
 we can always reorder the equations) we can partition the 
\begin_inset Formula $Y$
\end_inset

 matrix as 
\begin_inset Formula 
\[
Y=\left[\begin{array}{lll}
y & Y_{1} & Y_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $y$
\end_inset

 is the first column
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y_{1}$
\end_inset

 are the other endogenous variables that enter the first equation
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y_{2}$
\end_inset

 are endogs that are excluded from this equation 
\end_layout

\begin_layout Standard
Similarly, partition 
\begin_inset Formula $X$
\end_inset

 as 
\begin_inset Formula 
\[
X=\left[\begin{array}{ll}
X_{1} & X_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $X_{1}$
\end_inset

 are the included exogs, and 
\begin_inset Formula $X_{2}$
\end_inset

 are the excluded exogs.
 
\end_layout

\begin_layout Standard
Finally, partition the error matrix as 
\begin_inset Formula 
\[
E=\left[\begin{array}{ll}
\varepsilon & E_{12}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Assume that 
\begin_inset Formula $\Gamma$
\end_inset

 has ones on the main diagonal.
 These are normalization restrictions that simply scale the remaining coefficien
ts on each equation, and which scale the variances of the error terms.
\end_layout

\begin_layout Standard
Given this scaling and our partitioning, the coefficient matrices can be
 written as 
\begin_inset Formula 
\begin{eqnarray*}
\Gamma & = & \left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}
\end{array}\right]\\
B & = & \left[\begin{array}{ll}
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
With this, the first equation can be written as 
\begin_inset Formula 
\begin{eqnarray}
y & = & Y_{1}\gamma_{1}+X_{1}\beta_{1}+\varepsilon\label{eq:single equation from system}\\
 & = & Z\delta+\varepsilon\nonumber 
\end{eqnarray}

\end_inset

 The problem, as we've seen, is that the columns of 
\begin_inset Formula $Z$
\end_inset

 corresponding to 
\begin_inset Formula $Y_{1}$
\end_inset

 are correlated with 
\begin_inset Formula $\varepsilon,$
\end_inset

 because these are endogenous variables, and as we saw in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:correlation between endogs and structural error"

\end_inset

, the endogenous variables are correlated with the structural errors, so
 they don't satisfy weak exogeneity.
 So, 
\begin_inset Formula $E(Z^{\prime}\epsilon)\ne$
\end_inset

0.
 What are the properties of the OLS estimator in this situation?
\begin_inset Formula 
\begin{eqnarray*}
\hat{\delta} & = & \left(Z^{\prime}Z\right)^{-1}Z^{\prime}y\\
 & = & \left(Z^{\prime}Z\right)^{-1}Z^{\prime}\left(Z\delta^{0}+\varepsilon\right)\\
 & = & \delta^{0}+\left(Z^{\prime}Z\right)^{-1}Z^{\prime}\epsilon
\end{eqnarray*}

\end_inset

It's clear that the OLS estimator is biased in general.
 Also, 
\begin_inset Formula 
\[
\hat{\delta}-\delta^{0}=\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\frac{Z^{\prime}\epsilon}{n}
\]

\end_inset

Say that 
\begin_inset Formula $\lim\frac{Z^{\prime}\epsilon}{n}=A,$
\end_inset

a.s., and 
\begin_inset Formula $\lim\frac{Z^{\prime}Z}{n}=Q_{Z},\,a.s.$
\end_inset

 Then
\begin_inset Formula 
\[
\lim\left(\hat{\delta}-\delta^{0}\right)=Q_{Z}^{-1}A\ne0,\,a.s.
\]

\end_inset

So the OLS estimator of a structural equation is inconsistent.
 In general, correlation between regressors and errors leads to this problem,
 whether due to measurement error, simultaneity, or omitted regressors.
\end_layout

\begin_layout Standard
A GRETL script which generates data according to a simple supply-demand
 system is here: 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Simeq/simeq.inp}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/Simeq/simeq.inp}
\end_layout

\end_inset

.
 It does a Monte Carlo study which verifies that the OLS estimator is inconsiste
nt, and that the IV estimator is consistent.
\end_layout

\begin_layout Section
Note about the rest of this chapter
\end_layout

\begin_layout Standard
In class, I will not teach the material in the rest of this chapter at this
 time, but instead we will go on to GMM.
 The material that follows is easier to understand in the context of GMM,
 where we get a nice unified theory.
\end_layout

\begin_layout Section
Identification by exclusion restrictions
\end_layout

\begin_layout Standard
The material in the rest of this chapter is no longer used in classes, but
 I'm leaving it in the notes for reference.
\end_layout

\begin_layout Standard
The identification problem in simultaneous equations is in fact of the same
 nature as the identification problem in any estimation setting: does the
 limiting objective function have the proper curvature so that there is
 a unique global minimum or maximum at the true parameter value? In the
 context of IV estimation, this is the case if the limiting covariance of
 the IV estimator is positive definite and 
\begin_inset Formula $plim\frac{1}{n}W^{\prime}\varepsilon=0$
\end_inset

.
 This matrix is 
\begin_inset Formula 
\[
V_{\infty}(\hat{\beta}_{IV})=(Q_{XW}Q_{WW}^{-1}Q_{XW}^{\prime})^{-1}\sigma^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
The necessary and sufficient condition for identification is simply that
 this matrix be positive definite, and that the instruments be (asymptotically)
 uncorrelated with 
\begin_inset Formula $\varepsilon$
\end_inset

.
\end_layout

\begin_layout Itemize
For this matrix to be positive definite, we need that the conditions noted
 above hold: 
\begin_inset Formula $Q_{WW}$
\end_inset

 must be positive definite and 
\begin_inset Formula $Q_{XW}$
\end_inset

 must be of full rank ( 
\begin_inset Formula $K$
\end_inset

 ).
\end_layout

\begin_layout Itemize
These identification conditions are not that intuitive nor is it very obvious
 how to check them.
 
\end_layout

\begin_layout Subsection
Necessary conditions
\end_layout

\begin_layout Standard
If we use IV estimation for a single equation of the system, the equation
 can be written as 
\begin_inset Formula 
\[
y=Z\delta+\varepsilon
\]

\end_inset

 where 
\begin_inset Formula 
\[
Z=\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Notation:
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $K$
\end_inset

 be the total numer of weakly exogenous variables.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $K^{\ast}=cols(X_{1})\;$
\end_inset

be the number of included exogs, and let 
\begin_inset Formula $K^{\ast\ast}=K-K^{\ast}$
\end_inset

 be the number of excluded exogs (in this equation).
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $G^{\ast}=cols(Y_{1})+1$
\end_inset

 be the total number of included endogs, and let 
\begin_inset Formula $G^{\ast\ast}=G-G^{\ast}$
\end_inset

 be the number of excluded endogs.
 
\end_layout

\begin_layout Standard
Using this notation, consider the selection of instruments.
\end_layout

\begin_layout Itemize
Now the 
\begin_inset Formula $X_{1}$
\end_inset

 are weakly exogenous and can serve as their own instruments.
\end_layout

\begin_layout Itemize
It turns out that 
\begin_inset Formula $X$
\end_inset

 exhausts the set of possible instruments, in that if the variables in 
\begin_inset Formula $X$
\end_inset

 don't lead to an identified model then no other instruments will identify
 the model either.
 Assuming this is true (we'll prove it in a moment), then a necessary condition
 for identification is that 
\begin_inset Formula $cols(X_{2})\geq cols(Y_{1})$
\end_inset

 since if not then at least one instrument must be used twice, so 
\begin_inset Formula $W$
\end_inset

 will not have full column rank: 
\begin_inset Formula 
\[
\rho(W)<K^{\ast}+G^{\ast}-1\Rightarrow\rho(Q_{ZW})<K^{\ast}+G^{\ast}-1
\]

\end_inset

 This is the 
\emph on
order condition
\emph default
 for identification in a set of simultaneous equations.
 When the only identifying information is exclusion restrictions on the
 variables that enter an equation, then the number of excluded exogs must
 be greater than or equal to the number of included endogs, minus 1 (the
 normalized lhs endog), e.g., 
\begin_inset Formula 
\[
K^{\ast\ast}\geq G^{\ast}-1
\]

\end_inset


\end_layout

\begin_layout Itemize
To show that this is in fact a necessary condition consider some arbitrary
 set of instruments 
\begin_inset Formula $W.$
\end_inset

 A necessary condition for identification is that 
\begin_inset Formula 
\[
\rho\left(plim\frac{1}{n}W^{\prime}Z\right)=K^{\ast}+G^{\ast}-1
\]

\end_inset

 where 
\begin_inset Formula 
\[
Z=\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right]
\]

\end_inset

 Recall that we've partitioned the model 
\begin_inset Formula 
\[
Y\Gamma=XB+E
\]

\end_inset

 as 
\begin_inset Formula 
\[
Y=\left[\begin{array}{lll}
y & Y_{1} & Y_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X=\left[\begin{array}{ll}
X_{1} & X_{2}\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Given the reduced form 
\begin_inset Formula 
\[
Y=X\Pi+V
\]

\end_inset

 we can write the reduced form using the same partition 
\begin_inset Formula 
\[
\left[\begin{array}{lll}
y & Y_{1} & Y_{2}\end{array}\right]=\left[\begin{array}{ll}
X_{1} & X_{2}\end{array}\right]\left[\begin{array}{lll}
\pi_{11} & \Pi_{12} & \Pi_{13}\\
\pi_{21} & \Pi_{22} & \Pi_{23}
\end{array}\right]+\left[\begin{array}{lll}
v & V_{1} & V_{2}\end{array}\right]
\]

\end_inset

 so we have 
\begin_inset Formula 
\[
Y_{1}=X_{1}\Pi_{12}+X_{2}\Pi_{22}+V_{1}
\]

\end_inset

 so 
\begin_inset Formula 
\[
\frac{1}{n}W^{\prime}Z=\frac{1}{n}W^{\prime}\left[\begin{array}{ll}
X_{1}\Pi_{12}+X_{2}\Pi_{22}+V_{1} & X_{1}\end{array}\right]
\]

\end_inset

 Because the 
\begin_inset Formula $W$
\end_inset

 's are uncorrelated with the 
\begin_inset Formula $V_{1}$
\end_inset

 's, by assumption, the cross between 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $V_{1}$
\end_inset

 converges in probability to zero, so 
\begin_inset Formula 
\[
plim\frac{1}{n}W^{\prime}Z=plim\frac{1}{n}W^{\prime}\left[\begin{array}{ll}
X_{1}\Pi_{12}+X_{2}\Pi_{22} & X_{1}\end{array}\right]
\]

\end_inset

 Since the far rhs term is formed only of linear combinations of columns
 of 
\begin_inset Formula $X,$
\end_inset

 the rank of this matrix can never be greater than 
\begin_inset Formula $K,$
\end_inset

 regardless of the choice of instruments.
 If 
\begin_inset Formula $Z$
\end_inset

 has more than 
\begin_inset Formula $K$
\end_inset

 columns, then it is not of full column rank.
 When 
\begin_inset Formula $Z$
\end_inset

 has more than 
\begin_inset Formula $K\;$
\end_inset

columns we have 
\begin_inset Formula 
\[
G^{*}-1+K^{*}>K
\]

\end_inset

 or noting that 
\begin_inset Formula $K^{**}=K-K^{*},$
\end_inset


\begin_inset Formula 
\[
G^{*}-1>K^{**}
\]

\end_inset

 In this case, the limiting matrix is not of full column rank, and the identific
ation condition fails.
\end_layout

\begin_layout Subsection
Sufficient conditions
\end_layout

\begin_layout Standard
Identification essentially requires that the structural parameters be recoverabl
e from the data.
 This won't be the case, in general, unless the structural model is subject
 to some restrictions.
 We've already identified necessary conditions.
 Turning to sufficient conditions (again, we're only considering identification
 through zero restricitions on the parameters, for the moment).
\end_layout

\begin_layout Standard
The model is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}\\
V(E_{t}) & = & \Sigma
\end{eqnarray*}

\end_inset

 This leads to the reduced form 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}\\
V(V_{t}) & = & \left(\Gamma^{-1}\right)^{\prime}\Sigma\Gamma^{-1}\\
 & = & \Omega
\end{eqnarray*}

\end_inset

 The reduced form parameters are consistently estimable, but none of them
 are known 
\begin_inset Formula $\emph{a}$
\end_inset

 
\emph on
priori,
\emph default
 and there are no restrictions on their values.
 The problem is that more than one structural form has the same reduced
 form, so knowledge of the reduced form parameters alone isn't enough to
 determine the structural parameters.
 To see this, consider the model 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma F & = & X_{t}^{\prime}BF+E_{t}F\\
V(E_{t}F) & = & F^{\prime}\Sigma F
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $F$
\end_inset

 is some arbirary nonsingular 
\begin_inset Formula $G\times G$
\end_inset

 matrix.
 The rf of this new model is 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime} & = & X_{t}^{\prime}BF\left(\Gamma F\right)^{-1}+E_{t}F\left(\Gamma F\right)^{-1}\\
 & = & X_{t}^{\prime}BFF^{-1}\Gamma^{-1}+E_{t}FF^{-1}\Gamma^{-1}\\
 & = & X_{t}^{\prime}B\Gamma^{-1}+E_{t}\Gamma^{-1}\\
 & = & X_{t}^{\prime}\Pi+V_{t}
\end{eqnarray*}

\end_inset

 Likewise, the covariance of the rf of the transformed model is 
\begin_inset Formula 
\begin{eqnarray*}
V(E_{t}F\left(\Gamma F\right)^{-1}) & = & V(E_{t}\Gamma^{-1})\\
 & = & \Omega
\end{eqnarray*}

\end_inset

 Since the two structural forms lead to the same rf, and the rf is all that
 is directly estimable, the models are said to be 
\emph on
observationally equivalent.

\emph default
 What we need for identification are restrictions on 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 such that the only admissible 
\begin_inset Formula $F$
\end_inset

 is an identity matrix (if all of the equations are to be identified).
 Take the coefficient matrices as partitioned before:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma\\
B
\end{array}\right]=\left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}\\
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]
\]

\end_inset

 The coefficients of the first equation of the transformed model are simply
 these coefficients multiplied by the first column of 
\begin_inset Formula $F$
\end_inset

.
 This gives 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma\\
B
\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=\left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}\\
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]
\]

\end_inset

 For identification of the first equation we need that there be enough restricti
ons so that the only admissible 
\begin_inset Formula 
\[
\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]
\]

\end_inset

 be the leading column of an identity matrix, so that 
\begin_inset Formula 
\[
\left[\begin{array}{ll}
1 & \Gamma_{12}\\
-\gamma_{1} & \Gamma_{22}\\
0 & \Gamma_{32}\\
\beta_{1} & B_{12}\\
0 & B_{22}
\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=\left[\begin{array}{l}
1\\
-\gamma_{1}\\
0\\
\beta_{1}\\
0
\end{array}\right]
\]

\end_inset

 Note that the third and fifth rows are 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]F_{2}=\left[\begin{array}{l}
0\\
0
\end{array}\right]
\]

\end_inset

 Supposing that the leading matrix is of full column rank, e.g., 
\begin_inset Formula 
\[
\rho\left(\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]\right)=cols\left(\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]\right)=G-1
\]

\end_inset

 then the only way this can hold, without additional restrictions on the
 model's parameters, is if 
\begin_inset Formula $F_{2}$
\end_inset

 is a vector of zeros.
 Given that 
\begin_inset Formula $F_{2}$
\end_inset

 is a vector of zeros, then the first equation 
\begin_inset Formula 
\[
\left[\begin{array}{ll}
1 & \Gamma_{12}\end{array}\right]\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=1\Rightarrow f_{11}=1
\]

\end_inset

 Therefore, as long as 
\begin_inset Formula 
\[
\rho\left(\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]\right)=G-1
\]

\end_inset

 then 
\begin_inset Formula 
\[
\left[\begin{array}{l}
f_{11}\\
F_{2}
\end{array}\right]=\left[\begin{array}{l}
1\\
0_{G-1}
\end{array}\right]
\]

\end_inset

The first equation is identified in this case, so the condition is sufficient
 for identification.
 It is also necessary, since the condition implies that this submatrix must
 have at least 
\begin_inset Formula $G-1$
\end_inset

 rows.
 Since this matrix has 
\begin_inset Formula 
\[
G^{\ast\ast}+K^{\ast\ast}=G-G^{\ast}+K^{\ast\ast}
\]

\end_inset

 rows, we obtain 
\begin_inset Formula 
\[
G-G^{\ast}+K^{\ast\ast}\geq G-1
\]

\end_inset

 or 
\begin_inset Formula 
\[
K^{\ast\ast}\geq G^{\ast}-1
\]

\end_inset

 which is the previously derived necessary condition.
\end_layout

\begin_layout Standard
The above result is fairly intuitive (draw picture here).
 The necessary condition ensures that there are enough variables not in
 the equation of interest to potentially move the other equations, so as
 to trace out the equation of interest.
 The sufficient condition ensures that those other equations in fact do
 move around as the variables change their values.
 Some points:
\end_layout

\begin_layout Itemize
When an equation has 
\begin_inset Formula $K^{\ast\ast}=G^{\ast}-1,$
\end_inset

 is is 
\emph on
exactly identified
\emph default
, in that omission of an identifiying restriction is not possible without
 loosing consistency.
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $K^{\ast\ast}>G^{\ast}-1,$
\end_inset

 the equation is 
\emph on
overidentified
\emph default
, since one could drop a restriction and still retain consistency.
 Overidentifying restrictions are therefore testable.
 When an equation is overidentified we have more instruments than are strictly
 necessary for consistent estimation.
 Since estimation by IV with more instruments is more efficient asymptotically,
 one should employ overidentifying restrictions if one is confident that
 they're true.
\end_layout

\begin_layout Itemize
We can repeat this partition for each equation in the system, to see which
 equations are identified and which aren't.
\end_layout

\begin_layout Itemize
These results are valid assuming that the only identifying information comes
 from knowing which variables appear in which equations, e.g., by exclusion
 restrictions, and through the use of a normalization.
 There are other sorts of identifying information that can be used.
 These include
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Cross equation restrictions
\end_layout

\begin_layout Enumerate
Additional restrictions on parameters within equations (as in the Klein
 model discussed below)
\end_layout

\begin_layout Enumerate
Restrictions on the covariance matrix of the errors
\end_layout

\begin_layout Enumerate
Nonlinearities in variables 
\end_layout

\end_deeper
\begin_layout Itemize
When these sorts of information are available, the above conditions aren't
 necessary for identification, though they are of course still sufficient.
 
\end_layout

\begin_layout Standard
To give an example of how other information can be used, consider the model
 
\begin_inset Formula 
\[
Y\Gamma=XB+E
\]

\end_inset

 where 
\begin_inset Formula $\Gamma$
\end_inset

 is an upper triangular matrix with 1's on the main diagonal.
 This is a 
\emph on
triangular system
\emph default
 of equations.
 In this case, the first equation is 
\begin_inset Formula 
\[
y_{1}=XB_{\cdot1}+E_{\cdot1}
\]

\end_inset

 Since only exogs appear on the rhs, this equation is identified.
\end_layout

\begin_layout Standard
The second equation is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{2}=-\gamma_{21}y_{1}+XB_{\cdot2}+E_{\cdot2}
\]

\end_inset

 This equation has 
\begin_inset Formula $K^{**}=0$
\end_inset

 excluded exogs, and 
\begin_inset Formula $G^{*}=2$
\end_inset

 included endogs, so it fails the order (necessary) condition for identification.
\end_layout

\begin_layout Itemize
However, suppose that we have the restriction 
\begin_inset Formula $\Sigma_{21}=0,$
\end_inset

 so that the first and second structural errors are uncorrelated.
 In this case 
\begin_inset Formula 
\[
\mathcal{E}(y_{1t}\varepsilon_{2t})=\mathcal{E}\left\{ (X_{t}^{\prime}B_{\cdot1}+\varepsilon_{1t})\varepsilon_{2t}\right\} =0
\]

\end_inset

 so there's no problem of simultaneity.
 If the entire 
\begin_inset Formula $\Sigma$
\end_inset

 matrix is diagonal, then following the same logic, all of the equations
 are identified.
 This is known as a 
\emph on
fully recursive
\emph default
 model.
 
\end_layout

\begin_layout Section
2SLS
\end_layout

\begin_layout Standard
When we have no information regarding cross-equation restrictions or the
 structure of the error covariance matrix, one can estimate the parameters
 of a single equation of the system without regard to the other equations.
\end_layout

\begin_layout Itemize
This isn't always efficient, as we'll see, but it has the advantage that
 misspecifications in other equations will not affect the consistency of
 the estimator of the parameters of the equation of interest.
\end_layout

\begin_layout Itemize
Also, estimation of the equation won't be affected by identification problems
 in other equations.
 
\end_layout

\begin_layout Standard
The 2SLS estimator is very simple: it is the GIV estimator, using all of
 the weakly exogenous variables as instruments.
 In the first stage, each column of 
\begin_inset Formula $Y_{1}$
\end_inset

 is regressed on 
\emph on
all
\emph default
 the weakly exogenous variables in the system, e.g., the entire 
\begin_inset Formula $X$
\end_inset

 matrix.
 The fitted values are 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Y}_{1} & = & X(X^{\prime}X)^{-1}X^{\prime}Y_{1}\\
 & = & P_{X}Y_{1}\\
 & = & X\hat{\Pi}_{1}
\end{eqnarray*}

\end_inset

 Since these fitted values are the projection of 
\begin_inset Formula $Y_{1}$
\end_inset

 on the space spanned by 
\begin_inset Formula $X,$
\end_inset

 and since any vector in this space is uncorrelated with 
\begin_inset Formula $\varepsilon$
\end_inset

 by assumption, 
\begin_inset Formula $\hat{Y}_{1}$
\end_inset

 is uncorrelated with 
\begin_inset Formula $\varepsilon.$
\end_inset

 Since 
\begin_inset Formula $\hat{Y}_{1}$
\end_inset

 is simply the reduced-form prediction, it is correlated with 
\begin_inset Formula $Y_{1},$
\end_inset

 The only other requirement is that the instruments be linearly independent.
 This should be the case when the order condition is satisfied, since there
 are more columns in 
\begin_inset Formula $X_{2}$
\end_inset

 than in 
\begin_inset Formula $Y_{1}$
\end_inset

 in this case.
\end_layout

\begin_layout Standard
The second stage substitutes 
\begin_inset Formula $\hat{Y}_{1}$
\end_inset

 in place of 
\begin_inset Formula $Y_{1},$
\end_inset

 and estimates by OLS.
 This original model is 
\begin_inset Formula 
\begin{eqnarray*}
y & = & Y_{1}\gamma_{1}+X_{1}\beta_{1}+\varepsilon\\
 & = & Z\delta+\varepsilon
\end{eqnarray*}

\end_inset

 and the second stage model is 
\begin_inset Formula 
\[
y=\hat{Y_{1}}\gamma_{1}+X_{1}\beta_{1}+\varepsilon.
\]

\end_inset

 Since 
\begin_inset Formula $X_{1}$
\end_inset

 is in the space spanned by 
\begin_inset Formula $X,$
\end_inset

 
\begin_inset Formula $P_{X}X_{1}=X_{1},$
\end_inset

 so we can write the second stage model as 
\begin_inset Formula 
\begin{eqnarray*}
y & = & P_{X}Y_{1}\gamma_{1}+P_{X}X_{1}\beta_{1}+\varepsilon\\
 & \equiv & P_{X}Z\delta+\varepsilon
\end{eqnarray*}

\end_inset

 The OLS estimator applied to this model is 
\begin_inset Formula 
\[
\hat{\delta}=(Z^{\prime}P_{X}Z)^{-1}Z^{\prime}P_{X}y
\]

\end_inset

 which is exactly what we get if we estimate using IV, with the reduced
 form predictions of the endogs used as instruments.
 Note that if we define 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Z} & = & P_{X}Z\\
 & = & \left[\begin{array}{cc}
\hat{Y}_{1} & X_{1}\end{array}\right]
\end{eqnarray*}

\end_inset

 so that 
\begin_inset Formula $\hat{Z}$
\end_inset

 are the instruments for 
\begin_inset Formula $Z,$
\end_inset

 then we can write 
\begin_inset Formula 
\[
\hat{\delta}=(\hat{Z}^{\prime}Z)^{-1}\hat{Z}^{\prime}y
\]

\end_inset


\end_layout

\begin_layout Itemize
Important note: OLS on the transformed model can be used to calculate the
 2SLS estimate of 
\begin_inset Formula $\delta,$
\end_inset

 since we see that it's equivalent to IV using a particular set of instruments.
 However 
\emph on
the OLS covariance formula is not valid.

\emph default
 We need to apply the IV
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

covariance formula already seen above.
 
\end_layout

\begin_layout Standard
Actually, there is also a simplification of the general IV variance formula.
 Define 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Z} & = & P_{X}Z\\
 & = & \left[\begin{array}{ll}
\hat{Y} & X\end{array}\right]
\end{eqnarray*}

\end_inset

 The IV
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

covariance estimator would ordinarily be 
\begin_inset Formula 
\[
\hat{V}(\hat{\delta})=\left(\hat{Z}^{\prime}Z\right)^{-1}\left(\hat{Z}^{\prime}\hat{Z}\right)\left(Z^{\prime}\hat{Z}\right)^{-1}\hat{\sigma}_{IV}^{2}
\]

\end_inset

 However, looking at the first term in parentheses
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\hat{Z}^{\prime}Z=\left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]^{\prime}\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right]=\left[\begin{array}{ll}
Y_{1}^{\prime}(P_{X})Y_{1} & Y_{1}^{\prime}(P_{X})X_{1}\\
X_{1}^{\prime}Y_{1} & X_{1}^{\prime}X_{1}
\end{array}\right]
\]

\end_inset

 but since 
\begin_inset Formula $P_{X}$
\end_inset

 is idempotent and since 
\begin_inset Formula $P_{X}X=X,$
\end_inset

 we can write 
\begin_inset Formula 
\begin{eqnarray*}
\left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]^{\prime}\left[\begin{array}{ll}
Y_{1} & X_{1}\end{array}\right] & = & \left[\begin{array}{ll}
Y_{1}^{\prime}P_{X}P_{X}Y_{1} & Y_{1}^{\prime}P_{X}X_{1}\\
X_{1}^{\prime}P_{X}Y_{1} & X_{1}^{\prime}X_{1}
\end{array}\right]\\
 & = & \left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]^{\prime}\left[\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array}\right]\\
 & = & \hat{Z}^{\prime}\hat{Z}
\end{eqnarray*}

\end_inset

 Therefore, the first and second terms in the variance formula cancel, so
 the 2SLS varcov estimator simplifies to 
\begin_inset Formula 
\[
\hat{V}(\hat{\delta})=\left(Z^{\prime}\hat{Z}\right)^{-1}\hat{\sigma}_{IV}^{2}
\]

\end_inset

 which, following some algebra similar to the above, can also be written
 as 
\begin_inset Formula 
\begin{equation}
\hat{V}(\hat{\delta})=\left(\hat{Z}^{\prime}\hat{Z}\right)^{-1}\hat{\sigma}_{IV}^{2}\label{eq:2sls varcov}
\end{equation}

\end_inset

Finally, recall that though this is presented in terms of the first equation,
 it is general, since any equation can be placed first.
\end_layout

\begin_layout Standard

\series bold
Properties of 2SLS:
\end_layout

\begin_layout Enumerate
Consistent
\end_layout

\begin_layout Enumerate
Asymptotically normal
\end_layout

\begin_layout Enumerate
Biased when the mean esists (the existence of moments is a technical issue
 we won't go into here).
\end_layout

\begin_layout Enumerate
Asymptotically inefficient, except in special circumstances (more on this
 later).
 
\end_layout

\begin_layout Section
Testing the overidentifying restrictions
\end_layout

\begin_layout Standard
The selection of which variables are endogs and which are exogs 
\emph on
is part of the specification of the model
\emph default
.
 As such, there is room for error here: one might erroneously classify a
 variable as exog when it is in fact correlated with the error term.
 A general test for the specification on the model can be formulated as
 follows:
\end_layout

\begin_layout Standard
The IV
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator can be calculated by applying OLS to the transformed model, so
 the IV objective function at the minimized value is 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\left(y-X\hat{\beta}_{IV}\right)^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right),
\]

\end_inset

 but 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\varepsilon}_{IV} & = & y-X\hat{\beta}_{IV}\\
 & = & y-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}y\\
 & = & \left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)y\\
 & = & \left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\left(X\beta+\varepsilon\right)\\
 & = & A\left(X\beta+\varepsilon\right)
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula 
\[
A\equiv I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}
\]

\end_inset

 so 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\left(\varepsilon^{\prime}+\beta^{\prime}X^{\prime}\right)A^{\prime}P_{W}A\left(X\beta+\varepsilon\right)
\]

\end_inset

 Moreover, 
\begin_inset Formula $A^{\prime}P_{W}A$
\end_inset

 is idempotent, as can be verified by multiplication: 
\begin_inset Formula 
\begin{eqnarray*}
A^{\prime}P_{W}A & = & \left(I-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}\right)P_{W}\left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\\
 & = & \left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\\
 & = & \left(I-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}\right)P_{W}.
\end{eqnarray*}

\end_inset

 Furthermore, 
\begin_inset Formula $A$
\end_inset

 is orthogonal to 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
AX & = & \left(I-X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)X\\
 & = & X-X\\
 & = & 0
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\varepsilon^{\prime}A^{\prime}P_{W}A\varepsilon
\]

\end_inset

 Supposing the 
\begin_inset Formula $\varepsilon$
\end_inset

 are normally distributed, with variance 
\begin_inset Formula $\sigma^{2},$
\end_inset

 then the random variable 
\begin_inset Formula 
\[
\frac{s(\hat{\beta}_{IV})}{\sigma^{2}}=\frac{\varepsilon^{\prime}A^{\prime}P_{W}A\varepsilon}{\sigma^{2}}
\]

\end_inset

 is a quadratic form of a 
\begin_inset Formula $N(0,1)$
\end_inset

 random variable with an idempotent matrix in the middle, so 
\begin_inset Formula 
\[
\frac{s(\hat{\beta}_{IV})}{\sigma^{2}}\sim\chi^{2}(\rho(A^{\prime}P_{W}A))
\]

\end_inset

 This isn't available, since we need to estimate 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Substituting a consistent estimator, 
\begin_inset Formula 
\[
\frac{s(\hat{\beta}_{IV})}{\widehat{\sigma^{2}}}\overset{a}{\sim}\chi^{2}(\rho(A^{\prime}P_{W}A))
\]

\end_inset


\end_layout

\begin_layout Itemize
Even if the 
\begin_inset Formula $\varepsilon$
\end_inset

 aren't normally distributed, the asymptotic result still holds.
 The last thing we need to determine is the rank of the idempotent matrix.
 We have 
\begin_inset Formula 
\[
A^{\prime}P_{W}A=\left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)
\]

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\rho(A^{\prime}P_{W}A) & = & Tr\left(P_{W}-P_{W}X(X^{\prime}P_{W}X)^{-1}X^{\prime}P_{W}\right)\\
 & = & TrP_{W}-TrX^{\prime}P_{W}P_{W}X(X^{\prime}P_{W}X)^{-1}\\
 & = & TrW(W^{\prime}W)^{-1}W^{\prime}-K_{X}\\
 & = & TrW^{\prime}W(W^{\prime}W)^{-1}-K_{X}\\
 & = & K_{W}-K_{X}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $K_{W}$
\end_inset

 is the number of columns of 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $K_{X}$
\end_inset

 is the number of columns of 
\begin_inset Formula $X.$
\end_inset

 The degrees of freedom of the test is simply the number of overidentifying
 restrictions: the number of instruments we have beyond the number that
 is strictly necessary for consistent estimation.
\end_layout

\begin_layout Itemize
This test is an overall specification test: the joint null hypothesis is
 that the model is correctly specified 
\begin_inset Formula $\emph{and}$
\end_inset

 that the 
\begin_inset Formula $W$
\end_inset

 form valid instruments (e.g., that the variables classified as exogs really
 are uncorrelated with 
\begin_inset Formula $\varepsilon.$
\end_inset

 Rejection can mean that either the model 
\begin_inset Formula $y=Z\delta+\varepsilon$
\end_inset

 is misspecified, or that there is correlation between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\varepsilon.$
\end_inset


\end_layout

\begin_layout Itemize
This is a particular case of the GMM criterion test, which is covered in
 the second half of the course.
 See Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:A-specification-test"

\end_inset

.
\end_layout

\begin_layout Itemize
Note that since 
\begin_inset Formula 
\[
\hat{\varepsilon}_{IV}=A\varepsilon
\]

\end_inset

 and 
\begin_inset Formula 
\[
s(\hat{\beta}_{IV})=\varepsilon^{\prime}A^{\prime}P_{W}A\varepsilon
\]

\end_inset

 we can write 
\begin_inset Formula 
\begin{eqnarray*}
\frac{s(\hat{\beta}_{IV})}{\widehat{\sigma^{2}}} & = & \frac{\left(\hat{\varepsilon}^{\prime}W(W^{\prime}W)^{-1}W^{\prime}\right)\left(W(W^{\prime}W)^{-1}W^{\prime}\hat{\varepsilon}\right)}{\hat{\varepsilon}^{\prime}\hat{\varepsilon}/n}\\
 & = & n(RSS_{\hat{\varepsilon}_{IV}|W}/TSS_{\hat{\varepsilon}_{IV}})\\
 & = & nR_{u}^{2}
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $R_{u}^{2}$
\end_inset

 is the uncentered 
\begin_inset Formula $R^{2}$
\end_inset

 from a regression of the 
\begin_inset Formula $IV$
\end_inset

 residuals on all of the instruments 
\begin_inset Formula $W$
\end_inset

.
 This is a convenient way to calculate the test statistic.
 
\end_layout

\begin_layout Standard
On an aside, consider IV estimation of a just-identified model, using the
 standard notation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=X\beta+\varepsilon
\]

\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 is the matrix of instruments.
 If we have exact identification then 
\begin_inset Formula $cols(W)=cols(X)$
\end_inset

, so 
\begin_inset Formula $W^{'}X$
\end_inset

 is a square matrix.
 The transformed model is 
\begin_inset Formula 
\[
P_{W}y=P_{W}X\beta+P_{W}\varepsilon
\]

\end_inset

 and the fonc are 
\begin_inset Formula 
\[
X^{\prime}P_{W}(y-X\hat{\beta}_{IV})=0
\]

\end_inset

 The IV estimator is 
\begin_inset Formula 
\[
\hat{\beta}_{IV}=\left(X^{\prime}P_{W}X\right)^{-1}X^{\prime}P_{W}y
\]

\end_inset

 Considering the inverse here 
\begin_inset Formula 
\begin{eqnarray*}
\left(X^{\prime}P_{W}X\right)^{-1} & = & \left(X^{\prime}W(W^{\prime}W)^{-1}W^{\prime}X\right)^{-1}\\
 & = & (W^{\prime}X)^{-1}\left(X^{\prime}W(W^{\prime}W)^{-1}\right)^{-1}\\
 & = & (W^{\prime}X)^{-1}(W^{\prime}W)\left(X^{\prime}W\right)^{-1}
\end{eqnarray*}

\end_inset

 Now multiplying this by 
\begin_inset Formula $X^{\prime}P_{W}y,$
\end_inset

 we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\beta}_{IV} & = & (W^{\prime}X)^{-1}(W^{\prime}W)\left(X^{\prime}W\right)^{-1}X^{\prime}P_{W}y\\
 & = & (W^{\prime}X)^{-1}(W^{\prime}W)\left(X^{\prime}W\right)^{-1}X^{\prime}W(W^{\prime}W)^{-1}W^{\prime}y\\
 & = & (W^{\prime}X)^{-1}W^{\prime}y
\end{eqnarray*}

\end_inset

 The objective function for the generalized IV estimator is 
\begin_inset Formula 
\begin{eqnarray*}
s(\hat{\beta}_{IV}) & = & \left(y-X\hat{\beta}_{IV}\right)^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)-\hat{\beta}_{IV}^{\prime}X^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)-\hat{\beta}_{IV}^{\prime}X^{\prime}P_{W}y+\hat{\beta}_{IV}^{\prime}X^{\prime}P_{W}X\hat{\beta}_{IV}\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)-\hat{\beta}_{IV}^{\prime}\left(X^{\prime}P_{W}y+X^{\prime}P_{W}X\hat{\beta}_{IV}\right)\\
 & = & y^{\prime}P_{W}\left(y-X\hat{\beta}_{IV}\right)
\end{eqnarray*}

\end_inset

 by the fonc for generalized IV.
 However, when we're in the just indentified case, this is 
\begin_inset Formula 
\begin{eqnarray*}
s(\hat{\beta}_{IV}) & = & y^{\prime}P_{W}\left(y-X(W^{\prime}X)^{-1}W^{\prime}y\right)\\
 & = & y^{\prime}P_{W}\left(I-X(W^{\prime}X)^{-1}W^{\prime}\right)y\\
 & = & y^{\prime}\left(W(W^{\prime}W)^{-1}W^{\prime}-W(W^{\prime}W)^{-1}W^{\prime}X(W^{\prime}X)^{-1}W^{\prime}\right)y\\
 & = & 0
\end{eqnarray*}

\end_inset

 
\emph on
The value of the objective function of the IV estimator is zero in the just
 identified case.

\emph default
 This makes sense, since we've already shown that the objective function
 after dividing by 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is asymptotically 
\begin_inset Formula $\chi^{2}$
\end_inset

 with degrees of freedom equal to the number of overidentifying restrictions.
 In the present case, there are no overidentifying restrictions, so we have
 a 
\begin_inset Formula $\chi^{2}(0)$
\end_inset

 rv, which has mean 0 and variance 0, e.g., it's simply 0.
 This means we're not able to test the identifying restrictions in the case
 of exact identification.
\end_layout

\begin_layout Section
System methods of estimation
\end_layout

\begin_layout Standard
2SLS is a single equation method of estimation, as noted above.
 The advantage of a single equation method is that it's unaffected by the
 other equations of the system, so they don't need to be specified (except
 for defining what are the exogs, so 2SLS can use the complete set of instrument
s).
 The disadvantage of 2SLS is that it's inefficient, in general.
\end_layout

\begin_layout Itemize
Recall that overidentification improves efficiency of estimation, since
 an overidentified equation can use more instruments than are necessary
 for consistent estimation.
\end_layout

\begin_layout Itemize
Secondly, the assumption is that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
Y\Gamma & = & XB+E\\
\mathcal{E}(X^{\prime}E) & = & 0_{(K\times G)}\\
vec(E) & \sim & N(0,\Psi)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Since there is no autocorrelation of the 
\begin_inset Formula $E_{t}$
\end_inset

 's, and since the columns of 
\begin_inset Formula $E$
\end_inset

 are individually homoscedastic, then 
\begin_inset Formula 
\begin{eqnarray*}
\Psi & = & \left[\begin{array}{llll}
\sigma_{11}I_{n} & \sigma_{12}I_{n} & \cdots & \sigma_{1G}I_{n}\\
 & \sigma_{22}I_{n} &  & \vdots\\
 &  & \ddots & \vdots\\
\cdot &  &  & \sigma_{GG}I_{n}
\end{array}\right]\\
 & = & \Sigma\otimes I_{n}
\end{eqnarray*}

\end_inset

 This means that the structural equations are heteroscedastic and correlated
 with one another
\end_layout

\begin_layout Itemize
In general, ignoring this will lead to inefficient estimation, following
 the section on GLS.
 When equations are correlated with one another estimation should account
 for the correlation in order to obtain efficiency.
\end_layout

\begin_layout Itemize
Also, since the equations are correlated, information about one equation
 is implicitly information about all equations.
 Therefore, overidentification restrictions in any equation improve efficiency
 for 
\emph on
all
\emph default
 equations, even the just identified equations.
\end_layout

\begin_layout Itemize
Single equation methods can't use these types of information, and are therefore
 inefficient (in general).
 
\end_layout

\begin_layout Subsection
3SLS
\end_layout

\begin_layout Standard
Note: It is easier and more practical to treat the 3SLS estimator as a generaliz
ed method of moments estimator (see Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "cha:Generalized-method-of"

\end_inset

).
 I no longer teach the following section, but it is retained for its possible
 historical interest.
 Another alternative is to use FIML (Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:FIML"

\end_inset

), if you are willing to make distributional assumptions on the errors.
 This is computationally feasible with modern computers.
 
\end_layout

\begin_layout Standard
Following our above notation, each structural equation can be written as
 
\begin_inset Formula 
\begin{eqnarray*}
y_{i} & = & Y_{i}\gamma_{1}+X_{i}\beta_{1}+\varepsilon_{i}\\
 & = & Z_{i}\delta_{i}+\varepsilon_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Grouping the 
\begin_inset Formula $G$
\end_inset

 equations together we get 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{G}
\end{array}\right]=\left[\begin{array}{llll}
Z_{1} & 0 & \cdots & 0\\
0 & Z_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & Z_{G}
\end{array}\right]\left[\begin{array}{l}
\delta_{1}\\
\delta_{2}\\
\vdots\\
\delta_{G}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{G}
\end{array}\right]
\]

\end_inset

 or 
\begin_inset Formula 
\[
y=Z\delta+\varepsilon
\]

\end_inset

 where we already have that 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(\varepsilon\varepsilon^{\prime}) & = & \Psi\\
 & = & \Sigma\otimes I_{n}
\end{eqnarray*}

\end_inset

 The 3SLS estimator is just 2SLS combined with a GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

correction that takes advantage of the structure of 
\begin_inset Formula $\Psi.$
\end_inset

 Define 
\begin_inset Formula $\hat{Z}$
\end_inset

 as 
\begin_inset Formula 
\begin{eqnarray*}
\hat{Z} & = & \left[\begin{array}{llll}
X(X^{\prime}X)^{-1}X^{\prime}Z_{1} & 0 & \cdots & 0\\
0 & X(X^{\prime}X)^{-1}X^{\prime}Z_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & X(X^{\prime}X)^{-1}X^{\prime}Z_{G}
\end{array}\right]\\
 & = & \left[\begin{array}{llll}
\begin{array}{ll}
\hat{Y}_{1} & X_{1}\end{array} & 0 & \cdots & 0\\
0 & \begin{array}{ll}
\hat{Y}_{2} & X_{2}\end{array} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \begin{array}{ll}
\hat{Y}_{G} & X_{G}\end{array}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
These instruments are simply the 
\emph on
unrestricted
\emph default
 rf predicitions of the endogs, combined with the exogs.
 The distinction is that if the model is overidentified, then 
\begin_inset Formula 
\[
\Pi=B\Gamma^{-1}
\]

\end_inset

 may be subject to some zero restrictions, depending on the restrictions
 on 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B,$
\end_inset

 and 
\begin_inset Formula $\hat{\Pi}$
\end_inset

 does not impose these restrictions.
 Also, note that 
\begin_inset Formula $\hat{\Pi}$
\end_inset

 is calculated using OLS equation by equation, as was discussed in Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:EstimationRF"

\end_inset

.
\end_layout

\begin_layout Standard
The 2SLS estimator would be 
\begin_inset Formula 
\[
\hat{\delta}=(\hat{Z}^{\prime}Z)^{-1}\hat{Z}^{\prime}y
\]

\end_inset

 as can be verified by simple multiplication, and noting that the inverse
 of a block-diagonal matrix is just the matrix with the inverses of the
 blocks on the main diagonal.
 This IV
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator still ignores the covariance information.
 The natural extension is to add the GLS transformation, putting the inverse
 of the error covariance into the formula, which gives the 3SLS estimator
 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\delta}_{3SLS} & = & \left(\hat{Z}^{\prime}\left(\Sigma\otimes I_{n}\right)^{-1}Z\right)^{-1}\hat{Z}^{\prime}\left(\Sigma\otimes I_{n}\right)^{-1}y\\
 & = & \left(\hat{Z}^{\prime}\left(\Sigma^{-1}\otimes I_{n}\right)Z\right)^{-1}\hat{Z}^{\prime}\left(\Sigma^{-1}\otimes I_{n}\right)y
\end{eqnarray*}

\end_inset

 This estimator requires knowledge of 
\begin_inset Formula $\Sigma.$
\end_inset

 The solution is to define a feasible estimator using a consistent estimator
 of 
\begin_inset Formula $\Sigma.$
\end_inset

 The obvious solution is to use an estimator based on the 2SLS residuals:
 
\begin_inset Formula 
\[
\hat{\varepsilon}_{i}=y_{i}-Z_{i}\hat{\delta}_{i,2SLS}
\]

\end_inset

 
\series bold
(IMPORTANT NOTE
\series default
: this is calculated using 
\begin_inset Formula $Z_{i},$
\end_inset

 not 
\begin_inset Formula $\hat{Z}_{i}).$
\end_inset

 Then the element 
\begin_inset Formula $i,j$
\end_inset

 of 
\begin_inset Formula $\Sigma$
\end_inset

 is estimated by 
\begin_inset Formula 
\[
\hat{\sigma}_{ij}=\frac{\hat{\varepsilon}_{i}^{\prime}\hat{\varepsilon}_{j}}{n}
\]

\end_inset

 Substitute 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 into the formula above to get the feasible 3SLS estimator.
\end_layout

\begin_layout Standard
Analogously to what we did in the case of 2SLS, the asymptotic distribution
 of the 3SLS estimator can be shown to be 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\delta}_{3SLS}-\delta\right)\overset{a}{\sim}N\left(0,\lim_{n\rightarrow\infty}\mathcal{E}\left\{ \left(\frac{\hat{Z}^{\prime}\left(\Sigma\otimes I_{n}\right)^{-1}\hat{Z}}{n}\right)^{-1}\right\} \right)
\]

\end_inset

 A formula for estimating the variance of the 3SLS estimator in finite samples
 (cancelling out the powers of 
\begin_inset Formula $n)$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{V}\left(\hat{\delta}_{3SLS}\right)=\left(\hat{Z}^{\prime}\left(\hat{\Sigma}^{-1}\otimes I_{n}\right)\hat{Z}\right)^{-1}
\]

\end_inset


\end_layout

\begin_layout Itemize
This is analogous to the 2SLS formula in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2sls varcov"

\end_inset

), combined with the GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

correction.
\end_layout

\begin_layout Itemize
In the case that all equations are just identified, 3SLS is numerically
 equivalent to 2SLS.
 Proving this is easiest if we use a GMM interpretation of 2SLS and 3SLS.
 GMM is presented in the next econometrics course.
 For now, take it on faith.
 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Subsection
FIML
\begin_inset CommandInset label
LatexCommand label
name "subsec:FIML"

\end_inset


\end_layout

\begin_layout Standard
Full information maximum likelihood is an alternative estimation method.
 FIML will be asymptotically efficient, since ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimators based on a given information set are asymptotically efficient
 w.r.t.
 all other estimators that use the same information set, and in the case
 of the full-information ML estimator we use the entire information set.
 The 2SLS and 3SLS estimators don't require distributional assumptions,
 while FIML of course does.
 Our model is, recall 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t}^{\prime}\Gamma & = & X_{t}^{\prime}B+E_{t}^{\prime}\\
E_{t} & \sim & N(0,\Sigma),\forall t\\
\mathcal{E}(E_{t}E_{s}^{\prime}) & = & 0,t\neq s
\end{eqnarray*}

\end_inset

 The joint normality of 
\begin_inset Formula $E_{t}$
\end_inset

 means that the density for 
\begin_inset Formula $E_{t}$
\end_inset

 is the multivariate normal, which is 
\begin_inset Formula 
\[
(2\pi)^{-g/2}\left(\det\Sigma^{-1}\right)^{-1/2}\exp\left(-\frac{1}{2}E_{t}^{\prime}\Sigma^{-1}E_{t}\right)
\]

\end_inset

 The transformation from 
\begin_inset Formula $E_{t}$
\end_inset

 to 
\begin_inset Formula $Y_{t}$
\end_inset

 requires the Jacobian 
\begin_inset Formula 
\[
|\det\frac{dE_{t}}{dY_{t}'}|=|\det\Gamma|
\]

\end_inset

 so the density for 
\begin_inset Formula $Y_{t}$
\end_inset

 is 
\begin_inset Formula 
\[
(2\pi)^{-G/2}|\det\Gamma|\left(\det\Sigma^{-1}\right)^{-1/2}\exp\left(-\frac{1}{2}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)\Sigma^{-1}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)^{\prime}\right)
\]

\end_inset

 Given the assumption of independence over time, the joint log-likelihood
 function is 
\begin_inset Formula 
\[
\ln L(B,\Gamma,\Sigma)=-\frac{nG}{2}\ln(2\pi)+n\ln(|\det\Gamma|)-\frac{n}{2}\ln\det\Sigma^{-1}-\frac{1}{2}\sum_{t=1}^{n}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)\Sigma^{-1}\left(Y_{t}^{\prime}\Gamma-X_{t}^{\prime}B\right)^{\prime}
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a nonlinear in the parameters objective function.
 Maximixation of this can be done using iterative numeric methods.
 We'll see how to do this in the next section.
\end_layout

\begin_layout Itemize
It turns out that the asymptotic distribution of 3SLS and FIML are the same,
 
\emph on
assuming normality of the errors
\emph default
.
\end_layout

\begin_layout Itemize
One can calculate the FIML estimator by iterating the 3SLS estimator, thus
 avoiding the use of a nonlinear optimizer.
 The steps are
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Calculate 
\begin_inset Formula $\hat{\Gamma}_{3SLS}$
\end_inset

 and 
\begin_inset Formula $\hat{B}_{3SLS}$
\end_inset

 as normal.
\end_layout

\begin_layout Enumerate
Calculate 
\begin_inset Formula $\hat{\Pi}=\hat{B}_{3SLS}\hat{\Gamma}_{3SLS}^{-1}.$
\end_inset

 This is new, we didn't estimate 
\begin_inset Formula $\Pi$
\end_inset

 in this way before.
 This estimator may have some zeros in it.
 When Greene says iterated 3SLS doesn't lead to FIML, he means this for
 a procedure that doesn't update 
\begin_inset Formula $\hat{\Pi},$
\end_inset

 but only updates 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 and 
\begin_inset Formula $\hat{B}$
\end_inset

 and 
\begin_inset Formula $\hat{\Gamma}.$
\end_inset

 If you update 
\begin_inset Formula $\hat{\Pi}$
\end_inset

 you 
\emph on
do
\emph default
 converge to FIML.
\end_layout

\begin_layout Enumerate
Calculate the instruments 
\begin_inset Formula $\hat{Y}=X\hat{\Pi}$
\end_inset

 and calculate 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 using 
\begin_inset Formula $\hat{\Gamma}$
\end_inset

 and 
\begin_inset Formula $\hat{B}$
\end_inset

 to get the estimated errors, applying the usual estimator.
\end_layout

\begin_layout Enumerate
Apply 3SLS using these new instruments and the estimate of 
\begin_inset Formula $\Sigma.$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat steps 2-4 until there is no change in the parameters.
 
\end_layout

\end_deeper
\begin_layout Itemize
FIML is fully efficient, since it's an ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator that uses all information.
 This implies that 3SLS is fully efficient 
\emph on
when the errors are normally distributed.

\emph default
 Also, if each equation is just identified and the errors are normal, then
 2SLS will be fully efficient, since in this case 2SLS
\begin_inset Formula $\equiv$
\end_inset

3SLS.
\end_layout

\begin_layout Itemize
When the errors aren't normally distributed, the likelihood function is
 of course different than what's written above.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Example:-Klein's-Model"

\end_inset

Example: Klein's Model 1
\end_layout

\begin_layout Standard
To give a practical example, consider the following (old-fashioned, but
 illustrative) macro model (this is the widely known Klein's Model 1) 
\begin_inset Formula 
\begin{eqnarray*}
\text{Consumption:\;\ }C_{t} & = & \alpha_{0}+\alpha_{1}P_{t}+\alpha_{2}P_{t-1}+\alpha_{3}(W_{t}^{p}+W_{t}^{g})+\varepsilon_{1t}\\
\text{Investment:\;\ }I_{t} & = & \beta_{0}+\beta_{1}P_{t}+\beta_{2}P_{t-1}+\beta_{3}K_{t-1}+\varepsilon_{2t}\\
\text{Private\:\ Wages:\;\ }W_{t}^{p} & = & \gamma_{0}+\gamma_{1}X_{t}+\gamma_{2}X_{t-1}+\gamma_{3}A_{t}+\varepsilon_{3t}\\
\text{Output:\;\ }X_{t} & = & C_{t}+I_{t}+G_{t}\\
\text{Profits:\;\ }P_{t} & = & X_{t}-T_{t}-W_{t}^{p}\\
\text{Capital\:\ Stock:\;\ }K_{t} & = & K_{t-1}+I_{t}\\
\left(\begin{array}{c}
\epsilon_{1t}\\
\epsilon_{2t}\\
\epsilon_{3t}
\end{array}\right) & \sim & IID\left(\left(\begin{array}{c}
0\\
0\\
0
\end{array}\right),\left(\begin{array}{ccc}
\sigma_{11} & \sigma_{12} & \sigma_{13}\\
 & \sigma_{22} & \sigma_{23}\\
 &  & \sigma_{33}
\end{array}\right)\right)
\end{eqnarray*}

\end_inset

 The other variables are the government wage bill, 
\begin_inset Formula $W_{t}^{g},$
\end_inset

 taxes, 
\begin_inset Formula $T_{t},$
\end_inset

 government nonwage spending, 
\begin_inset Formula $G_{t},$
\end_inset

and a time trend, 
\begin_inset Formula $A_{t}.$
\end_inset

 The endogenous variables are the lhs variables, 
\begin_inset Formula 
\[
Y_{t}^{\prime}=\left[\begin{array}{cccccc}
C_{t} & I_{t} & W_{t}^{p} & X_{t} & P_{t} & K_{t}\end{array}\right]
\]

\end_inset

 and the predetermined variables are all others: 
\begin_inset Formula 
\[
X_{t}^{\prime}=\left[\begin{array}{cccccccc}
1 & W_{t}^{g} & G_{t} & T_{t} & A_{t} & P_{t-1} & K_{t-1} & X_{t-1}\end{array}\right].
\]

\end_inset

 The model assumes that the errors of the equations are contemporaneously
 correlated, but nonautocorrelated.
 The model written as 
\begin_inset Formula $Y\Gamma=XB+E$
\end_inset

 gives 
\begin_inset Formula 
\[
\Gamma=\left[\begin{array}{llllll}
1 & 0 & 0 & -1 & 0 & 0\\
0 & 1 & 0 & -1 & 0 & -1\\
-\alpha_{3} & 0 & 1 & 0 & 1 & 0\\
0 & 0 & -\gamma_{1} & 1 & -1 & 0\\
-\alpha_{1} & -\beta_{1} & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0 & 1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
B=\left[\begin{array}{llllll}
\alpha_{0} & \beta_{0} & \gamma_{0} & 0 & 0 & 0\\
\alpha_{3} & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & -1 & 0\\
0 & 0 & \gamma_{3} & 0 & 0 & 0\\
\alpha_{2} & \beta_{2} & 0 & 0 & 0 & 0\\
0 & \beta_{3} & 0 & 0 & 0 & 1\\
0 & 0 & \gamma_{2} & 0 & 0 & 0
\end{array}\right]
\]

\end_inset

 To check this identification of the consumption equation, we need to extract
 
\begin_inset Formula $\Gamma_{32}$
\end_inset

 and 
\begin_inset Formula $B_{22},$
\end_inset

 the submatrices of coefficients of endogs and exogs that 
\emph on
don't
\emph default
 appear in this equation.
 These are the rows that have zeros in the first column, and we need to
 drop the first column.
 We get 
\begin_inset Formula 
\[
\left[\begin{array}{l}
\Gamma_{32}\\
B_{22}
\end{array}\right]=\left[\begin{array}{lllll}
1 & 0 & -1 & 0 & -1\\
0 & -\gamma_{1} & 1 & -1 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & \gamma_{3} & 0 & 0 & 0\\
\beta_{3} & 0 & 0 & 0 & 1\\
0 & \gamma_{2} & 0 & 0 & 0
\end{array}\right]
\]

\end_inset

 We need to find a set of 5 rows of this matrix gives a full-rank 5
\begin_inset Formula $\times5$
\end_inset

 matrix.
 For example, selecting rows 3,4,5,6, and 7 we obtain the matrix 
\begin_inset Formula 
\[
A=\left[\begin{array}{lllll}
0 & 0 & 0 & 0 & 1\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & \gamma_{3} & 0 & 0 & 0\\
\beta_{3} & 0 & 0 & 0 & 1
\end{array}\right]
\]

\end_inset

 This matrix is of full rank, so the sufficient condition for identification
 is met.
 Counting included endogs, 
\begin_inset Formula $G^{\ast}=3,$
\end_inset

 and counting excluded exogs, 
\begin_inset Formula $K^{\ast\ast}=5,$
\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
K^{\ast\ast}-L & =G^{\ast}-1\\
5-L & =3-1\\
L & =3
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The equation is over-identified by three restrictions, according to the
 counting rules, which are correct when the only identifying information
 are the exclusion restrictions.
 However, there is additional information in this case.
 Both 
\begin_inset Formula $W_{t}^{p}$
\end_inset

 and 
\begin_inset Formula $W_{t}^{g}$
\end_inset

 enter the consumption equation, and their coefficients are restricted to
 be the same.
 For this reason the consumption equation is in fact overidentified by four
 restrictions.
 
\end_layout

\begin_layout Standard
The Octave program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Simeq/Klein2SLS.m}{https://github.com/mcreel/Econometrics/blob/m
aster/Examples/Simeq/Klein2SLS.m}
\end_layout

\end_inset

 performs 2SLS estimation for the 3 equations of Klein's model 1, assuming
 nonautocorrelated errors, so that lagged endogenous variables can be used
 as instruments.
 The results are:
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/Simeq/Klein.out"

\end_inset


\end_layout

\begin_layout Standard
The above results are not valid (specifically, they are inconsistent) if
 the errors are autocorrelated, since lagged endogenous variables will not
 be valid instruments in that case.
 You might consider eliminating the lagged endogenous variables as instruments,
 and re-estimating by 2SLS, to obtain consistent parameter estimates in
 this more complex case.
 Standard errors will still be estimated inconsistently, unless use a Newey-West
 type covariance estimator.
 Food for thought...
\end_layout

\begin_layout Standard
Here's a Gretl script to estimate Klein's model 1: 
\begin_inset CommandInset href
LatexCommand href
name "http://gretl.sourceforge.net/gretl-help/scripts/klein.inp"
target "http://gretl.sourceforge.net/gretl-help/scripts/klein.inp"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Numeric-optimization-methods"

\end_inset

Numeric optimization methods
\end_layout

\begin_layout Standard

\series bold
Readings:
\series default
 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Ch.
 10; Hamilton, ch.
 5, section 7 (pp.
 133-139)
\begin_inset Formula $^{*};$
\end_inset

 Gourieroux and Monfort, Vol.
 1, ch.
 13, pp.
 443-60
\begin_inset Formula $^{*}$
\end_inset

; Goffe, et.
 al.
 (1994).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The next chapter introduces extremum estimators, which are minimizers or
 maximizers of objective functions.
 If we're going to be applying extremum estimators, we'll need to know how
 to find an extremum.
 This section gives a very brief introduction to what is a large literature
 on numeric optimization methods.
 We'll consider a few well-known techniques, and one fairly new technique
 that may allow one to solve difficult problems.
\end_layout

\begin_layout Standard
The main objectives are 
\end_layout

\begin_layout Itemize
to become familiar with the issues, which should lead you to take a cautious
 attitude
\end_layout

\begin_layout Itemize
to learn how to use gradient-based local minimizers such as 
\family typewriter
fminunc
\family default
 and 
\family typewriter
fmincon
\family default
 at the practical level.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The general problem we consider is how to find the maximizing element 
\begin_inset Formula $\hat{\theta}$
\end_inset

 (a 
\begin_inset Formula $K$
\end_inset

 -vector) of a function 
\begin_inset Formula $s(\theta).$
\end_inset

 This function may not be continuous, and it may not be differentiable.
 Even if it is twice continuously differentiable, it may not be globally
 concave, so 
\begin_inset CommandInset href
LatexCommand href
name "local maxima, minima"
target "https://en.wikipedia.org/wiki/Maxima_and_minima"
literal "false"

\end_inset

 and 
\begin_inset CommandInset href
LatexCommand href
name "saddlepoints"
target "https://en.wikipedia.org/wiki/Saddle_point"
literal "false"

\end_inset

 may all exist.
 Supposing 
\begin_inset Formula $s(\theta)$
\end_inset

 were a quadratic function of 
\begin_inset Formula $\theta,$
\end_inset

 e.g., 
\begin_inset Formula 
\[
s(\theta)=a+b^{\prime}\theta+\frac{1}{2}\theta^{\prime}C\theta,
\]

\end_inset

 the first order conditions would be linear:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{\theta}s(\theta)=b+C\theta
\]

\end_inset

 so the maximizing (minimizing)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

element would be 
\begin_inset Formula $\hat{\theta}=-C^{-1}b.$
\end_inset

 This is the sort of problem we have with linear models estimated by OLS.
 It's also the case for feasible GLS, since conditional on the estimate
 of the varcov matrix, we have a quadratic objective function in the remaining
 parameters.
 
\end_layout

\begin_layout Standard
More general problems will not have linear f.o.c., and we will not be able
 to solve for the maximizer analytically.
 This is when we need a numeric optimization method.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Search
\end_layout

\begin_layout Standard
The idea is to create a grid over the parameter space and evaluate the function
 at each point on the grid.
 Select the best point.
 Then refine the grid in the neighborhood of the best point, and continue
 until the accuracy is 
\begin_inset Quotes sld
\end_inset

good enough
\begin_inset Quotes srd
\end_inset

.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Search-method"

\end_inset

.
 One has to be careful that the grid is fine enough in relationship to the
 irregularity of the function to ensure that sharp peaks are not missed
 entirely.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Search-method"

\end_inset

Search method
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/Search.png
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
To check 
\begin_inset Formula $q$
\end_inset

 values in each dimension of a 
\begin_inset Formula $K$
\end_inset

 dimensional parameter space, we need to check 
\begin_inset Formula $q^{K}$
\end_inset

 points.
 For example, if 
\begin_inset Formula $q=100$
\end_inset

 and 
\begin_inset Formula $K=10,$
\end_inset

 there would be 
\begin_inset Formula $100^{10}$
\end_inset

 points to check.
 If 1000 points can be checked in a second, it would take 
\begin_inset Formula $3.\,171\times10^{9}$
\end_inset

 years to perform the calculations, which is approximately 2/3 the age of
 the earth.
 The search method is a very reasonable choice if 
\begin_inset Formula $K$
\end_inset

 is small, but it quickly becomes infeasible if 
\begin_inset Formula $K$
\end_inset

 is moderate or large.
 
\end_layout

\begin_layout Standard
The Julia function 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{GridExample.jl}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/NonlinearOptimization/GridExample.jl} 
\end_layout

\end_inset

 allows you to play around with a simple one dimensional grid search, selecting
 the number of evenly spaced values to try.
 Try running GridExample(5) and GridExample(10).
 The result of GridExample(10) is in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Grid-search,-one"

\end_inset

.
 In this example, we're in the neighborhood of the minimizer, but still
 not too close to the minimizer.
 However, we're close enough so refinement will lead us to converge to the
 global minimizer.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Grid-search,-one"

\end_inset

Grid search, one dimension
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/gridsearch.svg
	lyxscale 25
	width 15cm

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Derivative-based methods
\end_layout

\begin_layout Standard

\emph on
In the following, the superscript 
\begin_inset Formula $k$
\end_inset

 is used as the index of the iterations of a given method.
 It is not an exponent, and it is not the dimension of the parameter vector.
\end_layout

\begin_layout Standard
We assume that the objective function is at least one time differentiable.
 Otherwise, these methods are not applicable, obviously.
 Derivative-based methods are defined by
\end_layout

\begin_layout Enumerate
the method for choosing the initial value, 
\begin_inset Formula $\theta^{1}$
\end_inset


\end_layout

\begin_layout Enumerate
the iteration method for choosing 
\begin_inset Formula $\theta^{k+1}$
\end_inset

 given that we're at 
\begin_inset Formula $\theta^{k}$
\end_inset

 at iteration 
\begin_inset Formula $k$
\end_inset

 (based upon derivatives)
\end_layout

\begin_layout Enumerate
the stopping criterion.
 
\end_layout

\begin_layout Standard
The iteration method can be broken into two problems: choosing the stepsize
 
\begin_inset Formula $a^{k}$
\end_inset

 (a scalar) and choosing the direction of movement, 
\begin_inset Formula $d^{k},$
\end_inset

 which is of the same dimension of 
\begin_inset Formula $\theta,$
\end_inset

 so that 
\begin_inset Formula 
\[
\theta^{(k+1)}=\theta^{(k)}+a^{k}d^{k}.
\]

\end_inset


\end_layout

\begin_layout Standard

\emph on
\begin_inset Newpage newpage
\end_inset

A locally increasing direction of search
\emph default
 
\begin_inset Formula $d$
\end_inset

 is a direction such that 
\begin_inset Formula 
\[
\frac{\partial s(\theta+ad)}{\partial a}>0.
\]

\end_inset

 That is, if we go in direction 
\begin_inset Formula $d$
\end_inset

, we will improve on the objective function, at least if we don't go too
 far.
\end_layout

\begin_layout Itemize
As long as the gradient at 
\begin_inset Formula $\theta^{k}$
\end_inset

 is not zero, there exist increasing directions, and they can all be represented
 as 
\begin_inset Formula $Q^{k}g(\theta^{k})$
\end_inset

 where 
\begin_inset Formula $Q^{k}$
\end_inset

 is a symmetric pd matrix and 
\begin_inset Formula $g\left(\theta\right)=D_{\theta}s(\theta)$
\end_inset

 is the gradient at 
\begin_inset Formula $\theta$
\end_inset

.
 To see this, take a Taylor's series expansion around 
\begin_inset Formula $a^{0}=0$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
s(\theta+ad) & = & s(\theta+0d)+\left(a-0\right)g(\theta+0d)^{\prime}d+o(1)\\
 & = & s(\theta)+ag(\theta)^{\prime}d+o(1)
\end{eqnarray*}

\end_inset

For small enough 
\begin_inset Formula $a$
\end_inset

 the 
\begin_inset Formula $o(1)$
\end_inset

 term can be ignored.
 If 
\begin_inset Formula $d$
\end_inset

 is to be an increasing direction, we need 
\begin_inset Formula $g(\theta)^{\prime}d>0.$
\end_inset

 Defining 
\begin_inset Formula $d=Qg(\theta),$
\end_inset

 where 
\begin_inset Formula $Q$
\end_inset

 is positive definite, we guarantee that 
\begin_inset Formula 
\[
g(\theta)^{\prime}d=g(\theta)^{\prime}Qg(\theta)>0
\]

\end_inset

 unless 
\begin_inset Formula $g(\theta)=0.$
\end_inset

 Every increasing direction can be represented in this way (p.d.
 matrices are those such that the angle between 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $Qg(\theta)$
\end_inset

 is less than 90 degrees).
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Increasing directions"

\end_inset

.
\begin_inset Newpage newpage
\end_inset

 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Increasing directions"

\end_inset

Increasing directions of search
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/IncreasingDirections.pdf
	width 5in

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
With this, the iteration rule becomes 
\begin_inset Formula 
\[
\theta^{(k+1)}=\theta^{(k)}+a^{k}Q^{k}g(\theta^{k})
\]

\end_inset


\end_layout

\begin_layout Standard
and we keep going until the gradient becomes zero, so that there is no increasin
g direction.
 The problem is now 
\emph on
how to choose 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $Q.$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Conditional on
\series default
 
\begin_inset Formula $Q$
\end_inset

, choosing 
\begin_inset Formula $a$
\end_inset

 is fairly straightforward.
 A simple line (1 dimensional grid) search is an attractive possibility,
 since 
\begin_inset Formula $a$
\end_inset

 is a scalar.
 But there are other methods that may be better (bisection, golden, etc.).
\end_layout

\begin_layout Itemize
The remaining problem is how to choose 
\begin_inset Formula $Q.$
\end_inset


\end_layout

\begin_layout Itemize
Note also that this gives no guarantees to find a global maximum.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Steepest ascent
\end_layout

\begin_layout Standard
Steepest ascent (descent if we're minimizing) just sets 
\begin_inset Formula $Q$
\end_inset

 to an identity matrix, since the gradient provides the direction of maximum
 rate of increase of the objective function.
\end_layout

\begin_layout Itemize
Advantages:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

fast - doesn't require anything more than first derivatives.
\end_layout

\begin_layout Itemize
Disadvantages: This doesn't always work too well however: see the Rosenbrock,
 or 
\begin_inset Quotes sld
\end_inset

banana
\begin_inset Quotes srd
\end_inset

 function: 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Rosenbrock_function
\end_layout

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Newton's method
\end_layout

\begin_layout Standard
Newton's method uses information about the slope and curvature of the objective
 function to determine which direction and how far to move from an initial
 point.
 Supposing we're trying to maximize 
\begin_inset Formula $s_{n}(\theta).$
\end_inset

 Take a second order Taylor's series approximation of 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 about 
\begin_inset Formula $\theta^{k}$
\end_inset

 (an initial guess).
 
\begin_inset Formula 
\[
s_{n}(\theta)\approx s_{n}(\theta^{k})+g(\theta^{k})^{\prime}\left(\theta-\theta^{k}\right)+1/2\left(\theta-\theta^{k}\right)^{\prime}H(\theta^{k})\left(\theta-\theta^{k}\right)
\]

\end_inset

(
\begin_inset Formula $g$
\end_inset

 is the gradient vector and 
\begin_inset Formula $H$
\end_inset

 is the Hessian matrix).
 To attempt to maximize 
\begin_inset Formula $s_{n}(\theta),$
\end_inset

 we can maximize the portion of the right-hand side that depends on 
\begin_inset Formula $\theta,$
\end_inset

 
\emph on
i.e.
\emph default
, we can maximize 
\begin_inset Formula 
\[
\tilde{s}(\theta)=g(\theta^{k})^{\prime}\theta+1/2\left(\theta-\theta^{k}\right)^{\prime}H(\theta^{k})\left(\theta-\theta^{k}\right)
\]

\end_inset

 with respect to 
\begin_inset Formula $\theta.$
\end_inset

 This is a much easier problem, since it is a quadratic function in 
\begin_inset Formula $\theta,$
\end_inset

 so it has linear first order conditions.
 These are
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{\theta}\tilde{s}(\theta)=g(\theta^{k})+H(\theta^{k})\left(\theta-\theta^{k}\right)
\]

\end_inset

 So the solution for the next round estimate is 
\begin_inset Formula 
\[
\theta^{k+1}=\theta^{k}-H(\theta^{k})^{-1}g(\theta^{k})
\]

\end_inset

See 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization
\end_layout

\end_inset

 for more information.
 This is illustrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Newton-iteration"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Newton-iteration"

\end_inset

Newton iteration
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/newton.png
	lyxscale 50
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, it's good to include a stepsize, since the approximation to 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 may be bad far away from the maximizer 
\begin_inset Formula $\hat{\theta},$
\end_inset

 so the actual iteration formula is 
\begin_inset Formula 
\[
\theta^{k+1}=\theta^{k}-a^{k}H(\theta^{k})^{-1}g(\theta^{k})
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
A potential problem is that the Hessian may not be negative definite when
 we're far from the maximizing point.
 So 
\begin_inset Formula $-H(\theta^{k})^{-1}$
\end_inset

 may not be positive definite, and 
\begin_inset Formula $-H(\theta^{k})^{-1}g(\theta^{k})$
\end_inset

 may not define an increasing direction of search.
 This can happen when the objective function has flat regions, in which
 case the Hessian matrix is very ill-conditioned (e.g., is nearly singular),
 or when we're in the vicinity of a local minimum, 
\begin_inset Formula $H(\theta^{k})$
\end_inset

 is positive definite, and our direction is a 
\emph on
decreasing
\emph default
 direction of search.
 Matrix inverses by computers are subject to large errors when the matrix
 is ill-conditioned.
 Also, we certainly don't want to go in the direction of a minimum when
 we're maximizing.
 To solve this problem, 
\emph on
Quasi-Newton
\emph default
 methods simply add a positive definite component to 
\begin_inset Formula $H(\theta)$
\end_inset

 to ensure that the resulting matrix is positive definite, 
\emph on
e.g.,
\emph default
 
\begin_inset Formula $Q=-H(\theta)+b\mathbf{I},$
\end_inset

 where 
\begin_inset Formula $b$
\end_inset

 is chosen large enough so that 
\begin_inset Formula $Q$
\end_inset

 is well-conditioned and positive definite.
 This has the benefit that improvement in the objective function is guaranteed.
 See 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://en.wikipedia.org/wiki/Quasi-Newton_method
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Another variation of quasi-Newton methods is to approximate the Hessian
 by using successive gradient evaluations.
 This avoids actual calculation of the Hessian, which is an order of magnitude
 (in the dimension of the parameter vector) more costly than calculation
 of the gradient.
 They can be done to ensure that the approximation is p.d.
 DFP
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

and BFGS are two well-known examples.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
BFGS minimization: cut and paste the following code into julia to see some
 BFGS minimization of the Rosenbrock function
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

using Optim
\end_layout

\begin_layout Plain Layout

rosenbrock(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
\end_layout

\begin_layout Plain Layout

result = optimize(rosenbrock, zeros(2), BFGS())
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Stopping criteria
\end_layout

\begin_layout Standard
The last thing we need is to decide when to stop.
 A digital computer is subject to limited machine precision and round-off
 errors.
 For these reasons, it is unreasonable to hope that a program can 
\series bold
exactly
\series default
 find the point that maximizes a function.
 We need to define acceptable tolerances.
 Some stopping criteria are:
\end_layout

\begin_layout Itemize
Negligible change in parameters: 
\begin_inset Formula 
\[
|\theta_{j}^{k}-\theta_{j}^{k-1}|<\varepsilon_{1},\forall j
\]

\end_inset


\end_layout

\begin_layout Itemize
Negligible relative change: 
\begin_inset Formula 
\[
|\frac{\theta_{j}^{k}-\theta_{j}^{k-1}}{\theta_{j}^{k-1}}|<\varepsilon_{2},\forall j
\]

\end_inset


\end_layout

\begin_layout Itemize
Negligible change of function: 
\begin_inset Formula 
\[
|s(\theta^{k})-s(\theta^{k-1})|<\varepsilon_{3}
\]

\end_inset


\end_layout

\begin_layout Itemize
Gradient negligibly different from zero: 
\begin_inset Formula 
\[
|g_{j}(\theta^{k})|<\varepsilon_{4},\forall j
\]

\end_inset


\end_layout

\begin_layout Itemize
Or, even better, check all of these.
\end_layout

\begin_layout Itemize
Also, if we're maximizing, it's good to check that the last round (real,
 not approximate) Hessian is negative definite.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Starting values
\end_layout

\begin_layout Standard
The Newton-Raphson and related algorithms work well if the objective function
 is concave (when maximizing), but not so well if there are convex regions
 and local minima or multiple local maxima.
 The algorithm may converge to a local minimum or to a local maximum that
 is not optimal.
 The algorithm may also have difficulties converging at all.
\end_layout

\begin_layout Itemize
The usual way to 
\begin_inset Quotes eld
\end_inset

ensure
\begin_inset Quotes erd
\end_inset

 that a global maximum has been found is to use many different starting
 values, and choose the solution that returns the highest objective function
 value.
 
\series bold
THIS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

IS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

IMPORTANT in practice.

\series default
 More on this later.
\end_layout

\begin_layout Itemize
an alternative is to use a global optimization algorithm, e.g., simulated
 annealing or others, which may or may not be gradient based.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Calculating derivatives
\end_layout

\begin_layout Standard
The Newton-Raphson algorithm requires first and second derivatives.
 It is often difficult to calculate derivatives (especially the Hessian)
 analytically if the function 
\begin_inset Formula $s_{n}(\cdot)$
\end_inset

 is complicated.
 Possible solutions are to calculate derivatives numerically, or to use
 programs such as MuPAD or Mathematica to calculate analytic derivatives.
 For example, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-Sage-to"

\end_inset

 shows Sage 
\begin_inset Foot
status open

\begin_layout Plain Layout
Sage is free software that has both symbolic and numeric computational capabilit
ies.
 See 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.sagemath.org/
\end_layout

\end_inset


\end_layout

\end_inset

 calculating a couple of derivatives.
 The KAIST Sage cell server will let you try Sage online, its address is
 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://aleph.sagemath.org/
\end_layout

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Using-Sage-to"

\end_inset

Using Sage to get analytic derivatives
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/NonlinearOptimization/sage.png
	width 15cm

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Numeric derivatives are less accurate than analytic derivatives, and are
 usually more costly to evaluate.
 Both factors usually cause optimization programs to be less successful
 when numeric derivatives are used.
 
\end_layout

\begin_layout Itemize
One advantage of numeric derivatives is that you don't have to worry about
 having made an error in calculating the analytic derivative.
 When programming analytic derivatives it's a good idea to check that they
 are correct by using numeric derivatives.
 This is a lesson I learned the hard way when writing my thesis.
\end_layout

\begin_layout Itemize
there are also methods for 
\emph on
automatic differentiation
\emph default
.
 This will probably become important in the future, but existing econometric
 software makes little use of it.
\end_layout

\begin_deeper
\begin_layout Itemize
update as of 2017: the Optim.jl and MXNet.jl packages for Julia can use automatic
 differentiation.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Numeric second derivatives are much more accurate if the data are scaled
 so that the elements of the gradient are of the same order of magnitude.
 Example: if the model is 
\begin_inset Formula $y_{t}=h(\alpha x_{t}+\beta z_{t})+\varepsilon_{t},$
\end_inset

 and estimation is by NLS.
 Let 
\begin_inset Formula $g()$
\end_inset

 be the derivative of 
\begin_inset Formula $h().$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
so, 
\begin_inset Formula $s_{n}(\theta)=\frac{1}{n}\sum_{t}\left(y_{t}-h(\alpha x_{t}+\beta z_{t})\right)^{2}$
\end_inset

 and
\end_layout

\begin_layout Itemize
\begin_inset Formula $D_{\alpha}s_{n}(\cdot)=\frac{1}{n}\sum_{t}2\left(y_{t}-h(\alpha x_{t}+\beta z_{t})\right)g(\alpha x_{t}+\beta z_{t})x_{t}$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $D_{\beta}s_{n}(\cdot)=\frac{1}{n}\sum_{t}2\left(y_{t}-h(\alpha x_{t}+\beta z_{t})\right)g(\alpha x_{t}+\beta z_{t})z_{t}$
\end_inset


\end_layout

\begin_layout Itemize
suppose that 
\begin_inset Formula $D_{\alpha}s_{n}(\cdot)=1000$
\end_inset

 and 
\begin_inset Formula $D_{\beta}s_{n}(\cdot)=0.001.$
\end_inset

 One could define 
\begin_inset Formula $\alpha^{\ast}=1000\alpha;$
\end_inset

 
\begin_inset Formula $x_{t}^{\ast}=x_{t}/1000$
\end_inset

;
\begin_inset Formula $\beta^{\ast}=\beta/1000;z_{t}^{\ast}=1000z_{t}.$
\end_inset

 
\end_layout

\begin_layout Itemize
then 
\begin_inset Formula $D_{\alpha^{*}}s_{n}(\cdot)=\frac{1}{n}\sum_{t}2\left(y_{t}-h(\alpha^{*}x_{t}^{*}+\beta^{*}z_{t}^{*})\right)g(\alpha^{*}x_{t}^{*}+\beta^{*}z_{t}^{*})x_{t}^{*}$
\end_inset

.
 Everything is the same as before, except there is an 
\begin_inset Formula $x_{t}^{*}$
\end_inset

 at the end, which causes the derivative to be 1 now.
\end_layout

\begin_layout Itemize
the same is true for the other derivative, it will be 1.
\end_layout

\begin_layout Itemize
this scaling causes the derivatives to be of the same order.
\end_layout

\begin_layout Standard
In general, estimation programs always work better if data is scaled in
 this way, since roundoff errors are less likely to become important.
 
\emph on
This is important in practice.
 
\emph default
In the future, if you start to do empirical work and get results that seem
 meaningless or crazy, try to remember this point.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
There are algorithms (such as BFGS and DFP) that use the sequential gradient
 evaluations to build up an approximation to the Hessian.
 The iterations are faster because the actual Hessian isn't calculated,
 but more iterations usually are required for convergence.
 Versions of BFGS are probably the most widely used optimizers in econometrics.
\end_layout

\begin_layout Itemize
Switching between algorithms during iterations is sometimes useful.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
The Nerlove model via numeric minimization.
 The example code 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{EstimateRestrictedNerlove.jl}{https://github.com/mcreel/Economet
rics/blob/master/Examples/NonlinearOptimization/EstimateRestrictedNerlove.jl}
 
\end_layout

\end_inset

 shows how to use unconstrained and constrained minimization to estimate
 the simple Nerlove model (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "simple nerlove model"

\end_inset

) with and without parameter restrictions.
 You should use the objective function values to compute the 
\begin_inset Formula $qF$
\end_inset

 test.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Simulated Annealing
\end_layout

\begin_layout Standard
Simulated annealing is an algorithm which can find an optimum in the presence
 of nonconcavities, discontinuities and multiple local minima/maxima.
 Basically, the algorithm randomly selects evaluation points, accepts all
 points that yield an increase in the objective function, but also accepts
 some points that decrease the objective function.
 This allows the algorithm to escape from local minima.
 As more and more points are tried, periodically the algorithm focuses on
 the best point so far, and reduces the range over which random points are
 generated.
 Also, the probability that a negative move is accepted reduces.
 The algorithm relies on many evaluations, as in the search method, but
 focuses in on promising areas, which reduces function evaluations with
 respect to the search method.
 It does not require derivatives to be evaluated.
 Run 
\family typewriter
samin()
\family default
 (with the 
\family typewriter
Econometrics.jl
\family default
 package installed) to see an example, and see the 
\begin_inset CommandInset href
LatexCommand href
name "source code for samin.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/Optimization/samin.jl"
literal "false"

\end_inset

, or run 
\family typewriter
edit(samin,())
\family default
 to get an idea of how the algorithm works.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Paste this code into Julia to see an example of simulated annealing.
 Try experimenting by passing rt=0.9 or rt=0.25 as arguments, and see what
 happens.
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

	junk=2.
 
\end_layout

\begin_layout Plain Layout

	# shows use of obj.
 fun.
 as a closure
\end_layout

\begin_layout Plain Layout

	function sse(x) # very simple quadratic objective function
\end_layout

\begin_layout Plain Layout

		objvalue = junk + sum(x.*x)
\end_layout

\begin_layout Plain Layout

	end
\end_layout

\begin_layout Plain Layout

    k = 5
\end_layout

\begin_layout Plain Layout

    x = rand(k,1)
\end_layout

\begin_layout Plain Layout

    lb = -ones(k,1)
\end_layout

\begin_layout Plain Layout

    ub = -lb
\end_layout

\begin_layout Plain Layout

    @time xopt = samin(sse, x, lb, ub, verbosity=2)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An additional example is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{OLSviaSA.jl}{https://github.com/mcreel/Econometrics/blob/master/
Examples/NonlinearOptimization/OLSviaSA.jl} 
\end_layout

\end_inset

, which uses SA to compute the OLS estimator, and plots the trace of the
 improvements as they are found.
 You can see how SA narrows in on the solution, in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Trace-of-SA"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Trace-of-SA"

\end_inset

Trace of SA path to minimize sum of squared errors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/OLSviaSA.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:MEPS data"

\end_inset

A practical example: Maximum likelihood estimation using count data: The
 MEPS data and the Poisson model
\end_layout

\begin_layout Standard
To show optimization methods in practice, using real economic data, this
 section presents maximum likelihood estimation results for a particular
 model using real data.
 The focus at present is simply on numeric optimization.
 Later, after studying maximum likelihood estimation, this section can be
 read again.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Demand for health care is usually thought of a a derived demand: health
 care is an input to a home production function that produces health, and
 health is an argument of the utility function.
 Grossman (1972), for example, models health as a capital stock that is
 subject to depreciation (e.g., the effects of ageing).
 Health care visits restore the stock.
 Under the home production framework, individuals decide when to make health
 care visits to maintain their health stock, or to deal with negative shocks
 to the stock in the form of accidents or illnesses.
 As such, individual demand will be a function of the parameters of the
 individuals' utility functions.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset CommandInset label
LatexCommand label
name "The-,-meps1996.data,"

\end_inset

The 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{MEPS health data file}{https://github.com/mcreel/Econometrics/b
lob/master/Examples/Data/meps1996.data} 
\end_layout

\end_inset

, 
\family typewriter
meps1996.data,
\family default
 contains 4564 observations on six measures of health care usage.
 The data is from the 1996 Medical Expenditure Panel Survey (MEPS).
 There are now more than 20 years of data! You can get more information
 at 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.meps.ahrq.gov/
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Itemize
The six measures of use are are office-based visits (OBDV), outpatient visits
 (OPV), inpatient visits (IPV), emergency room visits (ERV), dental visits
 (VDV), and number of prescription drugs taken (PRESCR).
 These form columns 1 - 6 of 
\family typewriter
meps1996.data
\family default
.
 
\end_layout

\begin_layout Standard
The conditioning variables are public insurance (PUBLIC), private insurance
 (PRIV), sex (SEX), age (AGE), years of education (EDUC), and income (INCOME).
 These form columns 7 - 12 of the file
\family typewriter
,
\family default
 in the order given here.
 PRIV and PUBLIC are 0/1 binary variables, where a 1 indicates that the
 person has access to public or private insurance coverage.
 SEX is also 0/1, where 1 indicates that the person is female.
 This data will be used in several examples in what follows.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The program 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{ExploreMEPS.jl}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/MEPS-I/ExploreMEPS.jl} 
\end_layout

\end_inset

 shows how the data may be read in, and gives some descriptive information
 about variables, which follows:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status collapsed

\begin_layout Plain Layout

MEPS data, 1996, complete data set statistics
\end_layout

\begin_layout Plain Layout

4564 observations
\end_layout

\begin_layout Plain Layout

            mean    st.
 dev.
         min         max  
\end_layout

\begin_layout Plain Layout

OBDV       3.279       6.171       0.000     133.000
\end_layout

\begin_layout Plain Layout

OPV        0.260       1.962       0.000      78.000
\end_layout

\begin_layout Plain Layout

IPV        0.194       0.637       0.000      17.000
\end_layout

\begin_layout Plain Layout

ERV        0.086       0.389       0.000       5.000
\end_layout

\begin_layout Plain Layout

DV         1.054       1.875       0.000      32.000
\end_layout

\begin_layout Plain Layout

RX         8.384      18.852       0.000     316.000
\end_layout

\begin_layout Plain Layout

PUB        0.141       0.334       0.000       1.000
\end_layout

\begin_layout Plain Layout

PRIV       0.674       0.449       0.000       1.000
\end_layout

\begin_layout Plain Layout

SEX        0.517       0.500       0.000       1.000
\end_layout

\begin_layout Plain Layout

AGE       39.354      12.198      18.000      65.000
\end_layout

\begin_layout Plain Layout

EDUC      12.652       2.896       0.000      17.000
\end_layout

\begin_layout Plain Layout

INC    42803.630   34108.362       0.000  250463.330
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
All of the measures of use are count data, which means that they take on
 the values 
\begin_inset Formula ${0,1,2,...}$
\end_inset

.
 It might be reasonable to try to use this information by specifying the
 density as a count data density.
 One of the simplest count data densities is the Poisson density, which
 is
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y) & = & \frac{\exp(-\lambda)\lambda^{y}}{y!}.
\end{eqnarray*}

\end_inset

For this density, 
\begin_inset Formula $E(Y)=V(Y)=\lambda$
\end_inset

.
 The Poisson average log-likelihood function is 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\left(-\lambda_{i}+y_{i}\ln\lambda_{i}-\ln y_{i}!\right)
\]

\end_inset

We will parameterize the model as
\begin_inset Formula 
\begin{eqnarray}
\lambda_{i} & = & \exp(\mathbf{x}_{i}^{\prime}\beta)\nonumber \\
\mathbf{x}_{i} & = & [1\,\,PUBLIC\,\,PRIV\,\,SEX\,\,AGE\,\,EDUC\,\,INC]^{\prime}\label{eq:Poisson model OBDV}
\end{eqnarray}

\end_inset

This ensures that the mean is positive, as is required for the Poisson model,
 and now the mean (and the variance) depend upon explanatory variables.
 Note that for this parameterization
\begin_inset Formula 
\[
\frac{\partial\lambda}{\partial x_{j}}=\lambda\beta_{j}
\]

\end_inset

so
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\beta_{j}x_{j}=\frac{\partial\lambda}{\partial x_{j}}\frac{x_{j}}{\lambda}=\eta_{x_{j}}^{\lambda},
\]

\end_inset

the elasticity of the conditional mean of 
\begin_inset Formula $y$
\end_inset

 with respect to the 
\begin_inset Formula $j^{th}$
\end_inset

 conditioning variable.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimatePoisson.jl}{https://github.com/mcreel/Econometrics/blob/
master/Examples/MEPS-I/EstimatePoisson.jl}
\end_layout

\end_inset

 estimates a Poisson model using the full data set.
 The results of the estimation, using OBDV as the dependent variable are
 here: 
\begin_inset CommandInset label
LatexCommand label
name "OBDV ML results"

\end_inset


\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MEPS-I/PoissonOBDV.out"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "PoissonOBDV_results"

\end_inset


\end_layout

\begin_layout Section
Numeric optimization: pitfalls
\end_layout

\begin_layout Standard
In this section we'll examine two common problems that can be encountered
 when doing numeric optimization of nonlinear models, and some solutions.
\end_layout

\begin_layout Subsection
Poor scaling of the data
\end_layout

\begin_layout Standard
When the data is scaled so that the magnitudes of the first and second derivativ
es are of different orders, problems can easily result.
 If we comment the appropriate line in 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimatePoisson.jl}{https://github.com/mcreel/Econometrics/blob/
master/Examples/MEPS-I/EstimatePoisson.jl}
\end_layout

\end_inset

, the data will not be scaled, and the estimation program will have difficulty
 converging (note that the likelihood value is lower with poor scaling).
\end_layout

\begin_layout Subsection
Multiple optima
\end_layout

\begin_layout Standard
Multiple optima (one global, others local) can complicate life, since we
 have limited means of determining if there is a higher maximum than the
 one we're at.
 Think of climbing a mountain in an unknown range, in a very foggy place.
 A nice picture is Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "foggy mountain-1"

\end_inset

, but try to imagine the scene if the clouds were 2000m thicker.
 A representation is Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "foggy mountain"

\end_inset

).
 You can go up until there's nowhere else to go up, but since you're in
 the fog you don't know if the true summit is across the gap that's at your
 feet.
 Do you claim victory and go home, or do you trudge down the gap and explore
 the other side? (example inspired by H.W.
 Tilman 
\emph on
Snow on the Equator).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "foggy mountain-1"

\end_inset

Mountains with low fog
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/mountain.jpg
	width 6in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "foggy mountain"

\end_inset

A foggy mountain
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/NonlinearOptimization/FoggySurface.png
	width 6in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The best way to avoid stopping at a local maximum is to use many starting
 values, for example on a grid, or randomly generated.
 Or perhaps one might have priors about possible values for the parameters
 (
\emph on
e.g.,
\emph default
 from previous studies of similar data).
\end_layout

\begin_layout Standard
Let's try to find the true minimizer of minus 1 times the foggy mountain
 function (since the algorithms are set up to minimize).
 From the picture, you can see it's close to 
\begin_inset Formula $(0,0)$
\end_inset

, but let's pretend there is fog, and that we don't know that.
 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{FoggyMountain.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/NonlinearOptimization/FoggyMountain.jl}
\end_layout

\end_inset

 shows that poor start values can lead to problems.
 It uses SA, which finds the true global minimum, and it shows that BFGS
 using a battery of random start values can also find the global minimum
 help.
 The output of one run is here: 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/NonlinearOptimization/FoggyMountain.out"

\end_inset

In that run, the single BFGS run with bad start values converged to a point
 far from the true minimizer, while simulated annealing found the true minimizer.
 BFGS using multiple start values also gets the correct solution, and if
 you check, you'll find that it's faster than SA.
 The moral of the story is to be cautious and don't publish your results
 too quickly.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
In Julia (with the 
\family typewriter
Econometrics.jl
\family default
 package installed), type 
\family typewriter
fminunc()
\family default
, to learn a bit more about the 
\family typewriter
fminunc
\family default
 function for unconstrained minimization, and to see a simple example.
\end_layout

\begin_layout Enumerate
In Julia (with the 
\family typewriter
Econometrics.jl
\family default
 package installed), type 
\family typewriter
fmincon()
\family default
, to learn a bit more about the 
\family typewriter
fmincon
\family default
 function for unconstrained minimization, and to see a simple example.
\end_layout

\begin_layout Enumerate
In Julia (with the 
\family typewriter
Econometrics.jl
\family default
 package installed), type 
\family typewriter
samin()
\family default
, to learn a bit more about the 
\family typewriter
samin
\family default
 function for minimization by simulated annealing, and to see a simple example.
\end_layout

\begin_layout Enumerate
Numerically minimize the function 
\begin_inset Formula $\sin(x)+0.01\left(x-a\right)^{2}$
\end_inset

, setting 
\begin_inset Formula $a=0$
\end_inset

, using the software of your choice.
 Plot the function over the interval 
\begin_inset Formula $\left(-2\pi,2\pi\right)$
\end_inset

.
 Does the software find the global minimum? Does this depend on the starting
 value you use? Outline a strategy that would allow you to find the minimum
 reliably, when 
\begin_inset Formula $a$
\end_inset

 can take on any given value in the interval 
\begin_inset Formula $\left(-\pi,\pi\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Numerically compute the OLS estimator of the Nerlove model by using an interativ
e minimization algorithm to minimize the sum of squared residuals.
 Verify that the results coincide with those given in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"

\end_inset

.
 The important part of this problem is to learn how to minimize a function
 that depends on both parameters and data.
 Try to write your function so that it is easy to use it with an arbitrary
 data set.
\end_layout

\begin_layout Enumerate
Numerically compute the OLS estimator of the Nerlove model 
\begin_inset Formula 
\[
\ln C=\beta+\beta_{Q}\ln Q+\beta_{L}\ln P_{L}+\beta_{F}\ln P_{F}+\beta_{K}\ln P_{K}+\epsilon
\]

\end_inset

by using the fminunc function in the 
\begin_inset CommandInset href
LatexCommand href
name "Econometrics.jl"
target "https://github.com/mcreel/Econometrics.jl"
literal "false"

\end_inset

 package to minimize the sum of squared residuals.
 The data is at the link 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{nerlove.data}{https://github.com/mcreel/Econometrics/blob/master
/Examples/Data/nerlove.data} 
\end_layout

\end_inset

 .
 Verify that the results coincide with those given in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"

\end_inset

, or with what you get from GRETL, i.e.: 
\begin_inset ERT
status open

\begin_layout Plain Layout

%%% the following needs the amsmath LaTeX package
\end_layout

\begin_layout Plain Layout


\backslash
begin{gather} 
\backslash
begin{split} 
\backslash
widehat{
\backslash
rm l
\backslash
_cost} &= -
\backslash
underset{(1.7744)}{3.52650} +
\backslash
underset{(0.017466)}{0.720394}
\backslash
,
\backslash
mbox{l
\backslash
_output} +
\backslash
underset{(0.29105)}{0.436341}
\backslash
,
\backslash
mbox{l
\backslash
_labor} +
\backslash
underset{(0.10037)}{0.426517}
\backslash
,
\backslash
mbox{l
\backslash
_fuel}
\backslash

\backslash
 & -
\backslash
underset{(0.33943)}{0.219888}
\backslash
,
\backslash
mbox{l
\backslash
_capital} 
\backslash
end{split} 
\backslash
notag 
\backslash

\backslash
 T = 145 
\backslash
quad 
\backslash
bar{R}^2 = 0.9238 
\backslash
quad F(4,140) = 437.69 
\backslash
quad 
\backslash
hat{
\backslash
sigma} = 0.39236 
\backslash
notag 
\backslash

\backslash
 
\backslash
centerline{(standard errors in parentheses)} 
\backslash
notag 
\backslash
end{gather} 
\end_layout

\end_inset

The important part of this problem is to learn how to minimize a function
 that depends on both parameters and data.
 Try to write your function so that it is re-usable, with a different data
 set.
 
\end_layout

\begin_layout Enumerate
Suppose we have an 
\begin_inset Formula $AR(1)$
\end_inset

 model 
\begin_inset Formula $y_{t}=\rho y_{t-1}+u_{t}$
\end_inset

.
 Suppose that 
\begin_inset Formula $y_{t}$
\end_inset

 is stationary, and that the error 
\begin_inset Formula $u_{t}$
\end_inset

 is white noise.
 Explain how one could compute an estimator of 
\begin_inset Formula $\rho$
\end_inset

 using the grid search method.
 You must define your criterion function and explain how to implement the
 grid search.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Asymptotic-properties-of"

\end_inset

Asymptotic properties of extremum estimators
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
: 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Ch.
 5; Hayashi (2000), Ch.
 7; Gourieroux and Monfort (1995), Vol.
 2, Ch.
 24; Amemiya, Ch.
 4 section 4.1; Davidson and MacKinnon, pp.
 591-96; Gallant, Ch.
 3; 
\begin_inset CommandInset citation
LatexCommand cite
key "NeweyMcfadden"
literal "true"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Extremum estimators
\end_layout

\begin_layout Standard
We'll begin with study of 
\emph on
extremum estimators
\emph default
 in general.
 Let 
\begin_inset Formula $\mathbf{Z}_{n}=\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

 be the available data, arranged in a 
\begin_inset Formula $n\times p$
\end_inset

 matrix, based on a sample of size 
\begin_inset Formula $n$
\end_inset

 (there are 
\begin_inset Formula $p$
\end_inset

 variables).
 Our paradigm is that data are generated as a draw from the joint density
 
\begin_inset Formula $f_{Z_{n}}(z)$
\end_inset

.
 This density may not be known, but it exists in principle.
 The draw from the density may be thought of as the outcome of a random
 experiment that is characterized by the probability space 
\begin_inset Formula $\left\{ \Omega,\mathcal{F},P\right\} $
\end_inset

.
 When the experiment is performed, 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 is the result, and 
\begin_inset Formula $\mathbf{Z}_{n}(\omega)=\left\{ Z_{1}(\omega),Z_{2}(\omega),...,Z_{n}(\omega)\right\} =\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

 is the realized data.
 The probability space is rich enough to allow us to consider events defined
 in terms of an infinite sequence of data 
\begin_inset Formula $\mathbf{Z}=\left\{ z_{1},z_{2},...,\right\} $
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
[Extremum estimator] 
\begin_inset CommandInset label
LatexCommand label
name "Extremum estimator"

\end_inset

An extremum estimator
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
extremum estimator
\end_layout

\end_inset

 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the optimizing element of an objective function 
\begin_inset Formula $s_{n}(\mathbf{Z}_{n},\theta)$
\end_inset

 over a compact set 
\begin_inset Formula $\overline{\Theta}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Because the data 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\mathbf{Z}_{n}(\omega)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 depends on 
\begin_inset Formula $\omega$
\end_inset

, we can emphasize this by writing 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $s_{n}(\omega,\theta)$
\end_inset

.
 I'll be loose with notation and interchange when convenient.
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
OLS.
 Let the d.g.p.
 be 
\begin_inset Formula $y_{t}=\mathbf{x}_{t}^{\prime}\theta^{0}+\varepsilon_{t},\,t=1,2,...,n,\,\theta^{0}\in\Theta.$
\end_inset

 Stacking observations vertically, 
\begin_inset Formula $\mathbf{y}_{n}=\mathbf{X}_{n}\theta^{0}+\varepsilon_{n},$
\end_inset

 where 
\begin_inset Formula $\mathbf{X}_{n}=\left(\begin{array}{llll}
\boldsymbol{x}_{1} & \boldsymbol{x}_{2} & \cdots & \boldsymbol{x}_{n}\end{array}\right)^{\prime}.$
\end_inset

 Let 
\begin_inset Formula $\mathbf{Z}_{n}=[\mathbf{y}_{n}\,\mathbf{X}_{n}]$
\end_inset

.
 The least squares estimator is defined as 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}s_{n}(\mathbf{Z}_{n},\theta)
\]

\end_inset

where 
\begin_inset Formula 
\[
s_{n}(\mathbf{Z}_{n},\theta)=1/n\sum_{t=1}^{n}\left(y_{t}-\boldsymbol{x}_{t}^{\prime}\theta\right)^{2}
\]

\end_inset

 As you already know, 
\begin_inset Formula $\hat{\theta}=(\mathbf{X}^{\prime}\mathbf{X})^{-1}\mathbf{X}^{\prime}\mathbf{y},$
\end_inset

 as this is a case where we can solve the f.o.c.
 analytically.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The contours of the OLS objective function are plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:OLS-objective-function"
plural "false"
caps "false"
noprefix "false"

\end_inset

, based on the Julia script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{OLScontours.jl}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/NonlinearOptimization/OLScontours.jl} 
\end_layout

\end_inset

.
 This illustrates the idea that an extremum estimator minimizes or maximizes
 a function, and that the estimate and parameters that we are trying to
 estimate are distinct points.
 As the sample gets large, the two points get close together, if the estimator
 is consistent.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:OLS-objective-function"

\end_inset

OLS objective function contours
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/NonlinearOptimization/OLScontours.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Maximum likelihood.
 Suppose that the continuous random variables 
\begin_inset Formula $Y_{t}\sim IIN(\mu^{0},\sigma_{0}^{2}),\,t=1,2,...,n$
\end_inset

.
 If 
\begin_inset Formula $\epsilon$
\end_inset

 is a standard normal random variable, its density is 
\begin_inset Formula 
\[
f_{\epsilon}(z)=\left(2\pi\right)^{-1/2}\exp\left(-\frac{z^{2}}{2}\right).
\]

\end_inset

 We have that 
\begin_inset Formula $\epsilon_{t}=(Y_{t}-\mu_{0})/\sigma_{0}$
\end_inset

 is standard normal, and the Jacobian 
\begin_inset Formula $\left|\partial\epsilon_{t}/\partial y_{t}\right|=1/\sigma_{0}$
\end_inset

.
 Thus, doing a change of variable, the density of a single observation on
 
\begin_inset Formula $Y$
\end_inset

 is
\begin_inset Formula 
\[
f_{Y}(y_{t};\theta)=\left(2\pi\right)^{-1/2}(1/\sigma)\exp\left(-\frac{1}{2}\left(\frac{y_{t}-\mu}{\sigma}\right)^{2}\right).
\]

\end_inset

where 
\begin_inset Formula $\theta=(\mu,\sigma$
\end_inset

).
 The maximum likelihood estimator is maximizes the joint density of the
 sample.
 Because the data are i.i.d., the joint density of the sample 
\begin_inset Formula $\left\{ y_{1},y_{2},...,y_{n}\right\} $
\end_inset

 is the product of the densities of each observation, and the ML estimator
 is 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\max_{\Theta}\,\,\mathcal{L}_{n}(\theta)=\prod_{t=1}^{n}f_{Y}(y_{t};\theta)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Because the natural logarithm is strictly increasing on 
\begin_inset Formula $(0,\infty)$
\end_inset

, maximization of the average logarithmic likelihood function is achieved
 at the same 
\begin_inset Formula $\hat{\theta}$
\end_inset

 as for the likelihood function.
 So, the ML estimator 
\begin_inset Formula $\hat{\theta}\equiv\arg\max_{\Theta}s_{n}(\theta)$
\end_inset

 where 
\begin_inset Formula 
\begin{align*}
s_{n}(\theta) & =\left(1/n\right)\ln\mathcal{L}_{n}(\theta)=\left(1/n\right)\sum_{t=1}^{n}\ln f_{Y}(y_{t};\theta)\\
 & =-\ln\sqrt{2\pi}-\text{\log}\sigma-\left(1/n\right)\sum_{t=1}^{n}\left(\frac{y_{t}-\mu}{\sigma}\right)^{2}
\end{align*}

\end_inset

Solution of the f.o.c.
 leads to the familiar result that 
\begin_inset Formula $\hat{\theta}=\bar{\mathbf{y}}.$
\end_inset

 We'll come back to this in more detail later.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Bayesian estimator
\end_layout

\begin_layout Example
(reminder to self in lectures: that squiggle is a 
\begin_inset Quotes sld
\end_inset

zeta
\begin_inset Quotes srd
\end_inset

) Bayesian point estimators such as the posterior mode, median or mean can
 be expressed as extremum estimators.
 For example, the posterior mean 
\begin_inset Formula $E(\theta|Z_{n})$
\end_inset

 is the minimizer (with respect to 
\begin_inset Formula $\zeta$
\end_inset

) of the function
\begin_inset Formula 
\[
s_{n}(\zeta)=\int_{\Theta}\left(\theta-\zeta\right)^{2}f(Z_{n};\theta)\pi(\theta)/f(Z_{n})d\theta
\]

\end_inset

where 
\begin_inset Formula $f(Z_{n};\theta)$
\end_inset

 is the likelihood function, 
\begin_inset Formula $\pi(\theta)$
\end_inset

 is a prior density, and 
\begin_inset Formula $f(Z_{n})$
\end_inset

 is the marginal likelihood of the data.
 These concepts are explained later, for now the point is that Bayesian
 estimators can be thought of as extremum estimators, and the theory for
 extremum estimators will apply.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Note that the objective function 
\begin_inset Formula $s_{n}(\mathbf{Z}_{n},\theta)$
\end_inset

 is a random function, because it depends on 
\begin_inset Formula $\mathbf{Z}_{n}(\omega)=\left\{ Z_{1}(\omega),Z_{2}(\omega),...,Z_{n}(\omega)\right\} =\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

.
 
\end_layout

\begin_layout Itemize
We need to consider what happens as different outcomes 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 occur.
 These different outcomes lead to different data being generated, and the
 different data causes the objective function to change.
 
\end_layout

\begin_layout Itemize
Note, however, that for a fixed 
\begin_inset Formula $\omega\in\Omega$
\end_inset

, the data 
\begin_inset Formula $\mathbf{Z}_{n}(\omega)=\left\{ Z_{1}(\omega),Z_{2}(\omega),...,Z_{n}(\omega)\right\} =\left\{ z_{1},z_{2},...,z_{n}\right\} $
\end_inset

 are a fixed realization, and the objective function 
\begin_inset Formula $s_{n}(\mathbf{Z}_{n},\theta)$
\end_inset

 becomes a non-random function of 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
When actually 
\emph on
computing
\emph default
 an extremum estimator, we condition on the observed data, and treat it
 as fixed.
 Then we compute estimators either by solving the f.o.c., as in the case of
 OLS, or if that is not possible, by employing algorithms for optimization
 of nonstochastic functions.
 How to do this is the topic of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "cha:Numeric-optimization-methods"

\end_inset

.
\end_layout

\begin_layout Itemize
When 
\emph on
analyzing the properties
\emph default
 of an extremum estimator, we need to investigate what happens throughout
 
\begin_inset Formula $\Omega$
\end_inset

: we do not focus only on the 
\begin_inset Formula $\omega$
\end_inset

 that generated the observed data.
 This is because we would like to find estimators that work well on average
 for any data set that can result from 
\begin_inset Formula $\omega\in\Omega$
\end_inset

.
 This is the topic of the remainder of the present Chapter.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
We'll often write the objective function suppressing the dependence on 
\begin_inset Formula $\mathbf{Z}_{n},$
\end_inset

 as 
\begin_inset Formula $s_{n}(\omega,\theta)$
\end_inset

 or simply 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

, depending on context.
 The first of these emphasizes the fact that the objective function is random,
 and the second is more compact.
 However, the data is still in there, and because the data is randomly sampled,
 the objective function is random, too.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Existence
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is continuous in 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 is compact, then a maximizer exists, by the Weierstrass maximum theorem
 (Debreu, 1959).
 In some cases of interest, 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 may not be continuous.
 Nevertheless, it may still converge to a continuous function, in which
 case existence will not be a problem, at least asymptotically.
 Henceforth in this course, we assume that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is continuous.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Consistency
\end_layout

\begin_layout Standard
The following theorem is patterned on a proof in 
\begin_inset CommandInset citation
LatexCommand cite
key "gallant_1987"
literal "true"

\end_inset

.
 It is interesting to compare the following proof with Amemiya's Theorem
 4.1.1, which is done in terms of convergence in probability.
\end_layout

\begin_layout Theorem

\emph on
[Consistency of e.e.]
\emph default
 
\begin_inset CommandInset label
LatexCommand label
name "Consistency of ee"

\end_inset

Suppose that 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is obtained by maximizing 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 over 
\begin_inset Formula $\overline{\Theta}.$
\end_inset


\end_layout

\begin_layout Theorem
Assume
\end_layout

\begin_layout Theorem

\shape italic
(a) Compactness:
\shape default
 The parameter space 
\begin_inset Formula $\Theta$
\end_inset

 is an open bounded subset of Euclidean space 
\begin_inset Formula $\Re^{K}.$
\end_inset

 So, the closure of 
\begin_inset Formula $\Theta,$
\end_inset

 
\begin_inset Formula $\overline{\Theta}$
\end_inset

, is compact.
\end_layout

\begin_layout Theorem

\shape italic
(b) Uniform Convergence:
\shape default
 There is a nonstochastic function 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 that is continuous in 
\begin_inset Formula $\theta$
\end_inset

 on 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{\theta\in\overline{\Theta}}|s_{n}(\omega,\theta)-s_{\infty}(\theta)|=0,\,\text{a.s.}
\]

\end_inset


\end_layout

\begin_layout Theorem

\shape italic
(c) Identification:
\shape default
 
\begin_inset Formula $s_{\infty}(\cdot)$
\end_inset

 has a unique global maximum at 
\begin_inset Formula $\theta^{0}\in\Theta,$
\end_inset

 
\shape italic
i.e.,
\shape default
 
\begin_inset Formula $s_{\infty}(\theta^{0})>s_{\infty}(\theta),$
\end_inset

 
\begin_inset Formula $\forall\theta\neq\theta^{0},\theta\in\overline{\Theta}$
\end_inset


\end_layout

\begin_layout Theorem
Then 
\begin_inset Formula $\hat{\theta}_{n}\stackrel{a.s.}{\rightarrow}\theta^{0}.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Proof:
\series default
 
\end_layout

\begin_layout Itemize
Select a 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 and hold it fixed.
 Then 
\begin_inset Formula $\left\{ s_{n}(\omega,\theta)\right\} $
\end_inset

 is a fixed sequence of functions.
 Suppose that 
\begin_inset Formula $\omega$
\end_inset

 is such that 
\begin_inset Formula $s_{n}(\omega,\theta)$
\end_inset

 converges to 
\begin_inset Formula $s_{\infty}(\theta).$
\end_inset

 
\end_layout

\begin_layout Itemize
The sequence 
\begin_inset Formula $\{\hat{\theta}_{n}\}$
\end_inset

 lies in the compact set 
\begin_inset Formula $\overline{\Theta},$
\end_inset

 by assumption (a) and the fact that maximization is over 
\begin_inset Formula $\overline{\Theta}$
\end_inset

.
 Since every sequence from a compact set has at least one limit point (Bolzano-W
eierstrass), say that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a limit point of 
\begin_inset Formula $\{\hat{\theta}_{n}\}.$
\end_inset

 As such, there is a subsequence 
\begin_inset Formula $\{\hat{\theta}_{n_{m}}\}$
\end_inset

 (
\begin_inset Formula $\{n_{m}\}$
\end_inset

 is simply a sequence of increasing integers) with 
\begin_inset Formula $\lim_{m\rightarrow\infty}\hat{\theta}_{n_{m}}=\hat{\theta}$
\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
By uniform convergence and continuity of the limiting objective function,
 
\begin_inset Formula 
\[
\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{n_{m}})=s_{\infty}(\hat{\theta}).
\]

\end_inset

 To see this, first of all, select an element 
\begin_inset Formula $\hat{\theta}_{t}$
\end_inset

 from the sequence 
\begin_inset Formula $\left\{ \hat{\theta}_{n_{m}}\right\} .$
\end_inset

 Then uniform convergence (assn.
 b) implies 
\begin_inset Formula 
\[
\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{t})=s_{\infty}(\hat{\theta}_{t})
\]

\end_inset

Continuity of 
\begin_inset Formula $s_{\infty}\left(\cdot\right)$
\end_inset

 implies that 
\begin_inset Formula 
\[
\lim_{t\rightarrow\infty}s_{\infty}(\hat{\theta}_{t})=s_{\infty}(\hat{\theta})
\]

\end_inset

 since the limit as 
\begin_inset Formula $t\rightarrow\infty$
\end_inset

 of 
\begin_inset Formula $\left\{ \hat{\theta}_{t}\right\} $
\end_inset

 is 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 So the above claim is true.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Next, by maximization 
\begin_inset Formula 
\[
s_{n_{m}}(\hat{\theta}_{n_{m}})\geq s_{n_{m}}(\theta^{0})
\]

\end_inset

 which holds in the limit, so 
\begin_inset Formula 
\[
\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{n_{m}})\geq\lim_{m\rightarrow\infty}s_{n_{m}}(\theta^{0}).
\]

\end_inset

 
\end_layout

\begin_layout Itemize
However, for the left hand side, the previous slide showed that
\begin_inset Formula 
\[
\lim_{m\rightarrow\infty}s_{n_{m}}(\hat{\theta}_{n_{m}})=s_{\infty}(\hat{\theta}),
\]

\end_inset

 For the right hand side, a similar argument gives
\begin_inset Formula 
\[
\lim_{m\rightarrow\infty}s_{n_{m}}(\theta^{0})=s_{\infty}(\theta^{0})
\]

\end_inset

 by uniform convergence, so the above inequality can be written as
\begin_inset Formula 
\[
s_{\infty}(\hat{\theta})\geq s_{\infty}(\theta^{0}).
\]

\end_inset

 
\end_layout

\begin_layout Itemize
But by assumption (c), there is a unique global maximum of 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 at 
\begin_inset Formula $\theta^{0},$
\end_inset

 so we must have 
\begin_inset Formula $s_{\infty}(\hat{\theta})=s_{\infty}(\theta^{0}),$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}=\theta^{0}$
\end_inset

 , in the limit.
\end_layout

\begin_layout Itemize
Finally, so far we have held 
\begin_inset Formula $\omega$
\end_inset

 fixed, but now we need to consider all 
\begin_inset Formula $\omega\in\Omega$
\end_inset

.
 All of the above limits hold almost surely, by assumption (b).
  Therefore 
\begin_inset Formula $\{\hat{\theta}_{n}\}$
\end_inset

 has only one limit point, 
\begin_inset Formula $\theta^{0},$
\end_inset

 except on a set 
\begin_inset Formula $C\subset\Omega$
\end_inset

 with 
\begin_inset Formula $P(C)=0.$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
Discussion of the proof:
\end_layout

\begin_layout Itemize
We assume that 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is in fact a global maximum of 
\begin_inset Formula $s_{n}\left(\theta\right).$
\end_inset

 It is not required to be unique for 
\begin_inset Formula $n$
\end_inset

 finite, though the identification assumption requires that the limiting
 objective function have a unique maximizing argument.
 The previous section on numeric optimization methods showed that actually
 finding the global maximum of 
\begin_inset Formula $s_{n}\left(\theta\right)$
\end_inset

 may be a non-trivial problem.
\end_layout

\begin_layout Itemize
See Amemiya's Example 4.1.4 for a case where discontinuity leads to breakdown
 of consistency.
\end_layout

\begin_layout Itemize
uniform convergence is needed, so that the maximum of 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is eventually close to 
\begin_inset Formula $\theta_{0}.$
\end_inset

 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Why-uniform-convergence"

\end_inset

.
 If the objective function is not converging at 
\begin_inset Formula $\theta_{*},$
\end_inset

 there's no guarantee that 
\begin_inset Formula $s_{n}(\theta_{*})$
\end_inset

 will be lower than 
\begin_inset Formula $s_{\infty}(\theta_{0})$
\end_inset

 as 
\begin_inset Formula $n$
\end_inset

 gets large.
 
\emph on

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\emph on
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Why-uniform-convergence"

\end_inset

Why uniform convergence of 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is needed
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/UniformConvergence.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The assumption that 
\begin_inset Formula $\theta^{0}$
\end_inset

 is in the interior of 
\begin_inset Formula $\overline{\Theta}$
\end_inset

 (part of the identification assumption) has not been used to prove consistency,
 so we could directly assume that 
\begin_inset Formula $\theta^{0}$
\end_inset

 is simply an element of a compact set 
\begin_inset Formula $\overline{\Theta}.$
\end_inset

 The reason that we assume it's in the interior here is that this is necessary
 for subsequent proof of asymptotic normality, and I'd like to maintain
 a minimal set of simple assumptions, for clarity.
 Parameters on the boundary of the parameter set cause theoretical difficulties
 that we will not deal with in this course.
 Just note that conventional hypothesis testing methods do not apply in
 this case.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Later, when explaining finite mixture models, note
\end_layout

\begin_layout Plain Layout
that the colapse of the mixture is a case of a parameter
\end_layout

\begin_layout Plain Layout
on the boundary.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $s_{n}\left(\theta\right)$
\end_inset

 is not required to be continuous, though 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 is.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection

\series bold
Sufficient conditions for assumption
\series default
 (b)
\end_layout

\begin_layout Standard
We need a uniform strong law of large numbers in order to verify assumption
 (2) of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

.
 To verify the uniform convergence assumption, it is often feasible to employ
 the following set of stronger assumptions:
\end_layout

\begin_layout Itemize
the parameter space is compact, which is given by assumption (b)
\end_layout

\begin_layout Itemize
the objective function 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is continuous and bounded with probability one on the entire parameter
 space
\end_layout

\begin_layout Itemize
a standard SLLN can be shown to apply to some point 
\begin_inset Formula $\theta$
\end_inset

 in the parameter space.
 That is, we can show that 
\begin_inset Formula $s_{n}(\theta)\stackrel{a.s.}{\rightarrow}s_{\infty}(\theta)$
\end_inset

 for some 
\begin_inset Formula $\theta$
\end_inset

.
 Note that in most cases, the objective function will be an average of terms,
 such as 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}s_{t}(\theta)
\]

\end_inset

As long as the 
\begin_inset Formula $s_{t}(\theta)$
\end_inset

 are not too strongly dependent, and have finite variances, we can usually
 find a SLLN that will apply.
\end_layout

\begin_layout Standard
With these assumptions, it can be shown that assumption (b) holds.
\end_layout

\begin_layout Standard
These are reasonable conditions in many cases, and henceforth when dealing
 with specific estimators we'll simply assume that assumption (b) holds.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example: 
\begin_inset CommandInset label
LatexCommand label
name "eeolsexample"

\end_inset

Consistency of Least Squares
\end_layout

\begin_layout Standard
Thus example shows how the above theorem can be used to show that the OLS
 estimator under the classical assumptions is consistent.
 Of course, this is not the easiest way to show that.
 The purpose is to show that the theorem gives us a result that we already
 know to be true.
 This may help you to believe it in the cases where we do not have external
 confirmation.
\end_layout

\begin_layout Standard
We suppose that data is generated by random sampling of 
\begin_inset Formula $(Y,X)$
\end_inset

, where 
\begin_inset Formula $y_{t}=\beta_{0}x_{t}$
\end_inset

 
\begin_inset Formula $+\varepsilon_{t}$
\end_inset

.
 
\begin_inset Formula $\left(X,\varepsilon\right)$
\end_inset

 has the common distribution function 
\begin_inset Formula $F_{Z}=\mu_{x}\mu_{\varepsilon}$
\end_inset

 (
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are independent) with support 
\begin_inset Formula $\mathcal{Z=X}\times\mathcal{E}.$
\end_inset

 Suppose that the variances 
\begin_inset Formula $\sigma_{X}^{2}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\varepsilon}^{2}$
\end_inset

 are finite.
 The sample objective function for a sample size 
\begin_inset Formula $n$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\theta) & = & \frac{1}{n}\sum_{t=1}^{n}\left(y_{t}-\beta x_{t}\right)^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(\beta_{0}x_{t}+\varepsilon_{t}-\beta x_{t}\right)^{2}\\
 & = & \frac{1}{n}\sum_{i=1}^{n}\left((\beta_{0}-\beta)x_{t}+\varepsilon_{t}\right)^{2}\\
 & = & \frac{1}{n}\sum_{t=1}^{n}\left((\beta_{0}-\beta)x_{t}\right)^{2}+\frac{2}{n}\sum_{t=1}^{n}(\beta_{0}-\beta)x_{t}\varepsilon_{t}+\frac{1}{n}\sum_{t=1}^{n}\varepsilon_{t}^{2}
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Itemize
Considering the last term, by the SLLN, 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{t=1}^{n}\varepsilon_{t}^{2}\stackrel{a.s.}{\rightarrow}\int_{\mathcal{X}}\int_{\mathcal{E}}\varepsilon^{2}d\mu_{\mathcal{X}}d\mu_{\mathcal{E}}=\sigma_{\varepsilon}^{2}.
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Considering the second term, since 
\begin_inset Formula $E(\varepsilon)=0$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are independent, the SLLN implies that it converges to zero.
 
\end_layout

\begin_layout Itemize
Finally, for the first term, for a given 
\begin_inset Formula $\beta$
\end_inset

, we assume that a SLLN applies so that
\begin_inset Formula 
\begin{eqnarray}
\frac{1}{n}\sum_{t=1}^{n}\left((\beta_{0}-\beta)x_{t}\right)^{2} & \stackrel{a.s.}{\rightarrow} & \int_{\mathcal{X}}\left((\beta_{0}-\beta)x\right)^{2}d\mu_{\mathcal{X}}\label{olslim}\\
 & = & \left(\beta^{0}-\beta\right)^{2}\int_{\mathcal{X}}x^{2}d\mu_{\mathcal{X}}\nonumber \\
 & = & \left(\beta^{0}-\beta\right)^{2}E\left(X^{2}\right)\nonumber 
\end{eqnarray}

\end_inset

 
\end_layout

\begin_layout Standard
Finally, the objective function is clearly continuous, and the parameter
 space is assumed to be compact, so the convergence is also uniform.
 Thus, 
\begin_inset Formula 
\[
s_{\infty}(\beta)=\left(\beta^{0}-\beta\right)^{2}E\left(X^{2}\right)+\sigma_{\varepsilon}^{2}
\]

\end_inset

 A minimizer of this is clearly 
\begin_inset Formula $\beta=\beta^{0}.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
\begin_inset CommandInset label
LatexCommand label
name "Identification of OLS"

\end_inset

 Show that in order for the above solution to be unique it is necessary
 that 
\begin_inset Formula $E(X^{2})\neq0.$
\end_inset

 Interpret this condition.
\end_layout

\begin_layout Standard
This example shows that Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

 can be used to prove strong consistency of the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator.
 There are easier ways to show this, of course - this is only an example
 of application of the theorem.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
For a more concrete example, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Consistency-of-OLS"

\end_inset

 shows computations that illustrate that the OLS estimator is consistent,
 when the true relationship is 
\begin_inset Formula $y=1+1x+\epsilon,$
\end_inset

 
\begin_inset Formula $\epsilon$
\end_inset

 satisfies the classical assumptions, and 
\begin_inset Formula $x$
\end_inset

 is distributed uniform
\begin_inset Formula $(0,1).$
\end_inset

 The computations show that the true parameter values satisfy the first
 order conditions for minimization of the first term of the limiting objective
 function, eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "olslim"

\end_inset

, above.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Consistency-of-OLS"

\end_inset

Consistency of OLS
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/OLSextremum.png
	width 15cm

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
More on the limiting objective function: correctly and incorrectly specified
 models
\end_layout

\begin_layout Standard
The limiting objective function in assumption (b) is 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

.
 What is the nature of this function and where does it come from?
\end_layout

\begin_layout Itemize
Remember our paradigm - data is presumed to be generated as a draw from
 
\begin_inset Formula $f_{Z_{n}}(z)$
\end_inset

, and the objective function is 
\begin_inset Formula $s_{n}(Z_{n},\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
Usually, 
\begin_inset Formula $s_{n}(Z_{n},\theta)$
\end_inset

 is an average of terms.
\end_layout

\begin_layout Itemize
The limiting objective function is found by applying a strong (weak) law
 of large numbers to 
\begin_inset Formula $s_{n}(Z_{n},\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
A strong (weak) LLN says that an average of terms converges almost surely
 (in probability) to the limit of the expectation of the average.
\end_layout

\begin_layout Standard
Supposing one holds,
\begin_inset Formula 
\[
s_{\infty}(\theta)=\lim_{n\rightarrow\infty}\mathcal{E}s_{n}(Z_{n},\theta)=\lim_{n\rightarrow\infty}\int_{\mathcal{Z}_{n}}s_{n}(z,\theta)f_{Z_{n}}(z)dz
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Now suppose that the density 
\begin_inset Formula $f_{Z_{n}}(z)$
\end_inset

 that characterizes the DGP is parametric: 
\begin_inset Formula $f_{Z_{n}}(z;\rho),\,\rho\in\varrho$
\end_inset

, and the data is generated by 
\begin_inset Formula $\rho^{0}\in\varrho$
\end_inset

.
 Now we have two parameters to worry about, 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\rho$
\end_inset

.
 We are probably interested in learning about the true DGP, which means
 that 
\begin_inset Formula $\rho^{0}$
\end_inset

 is the item of interest.
 When the DGP is parametric, the limiting objective function is 
\begin_inset Formula 
\[
s_{\infty}(\theta)=\lim_{n\rightarrow\infty}\mathcal{E}s_{n}(Z_{n},\theta)=\lim_{n\rightarrow\infty}\int_{\mathcal{Z}_{n}}s_{n}(z,\theta)f_{Z_{n}}(z;\rho^{0})dz
\]

\end_inset

and we can write the limiting objective function as 
\begin_inset Formula $s_{\infty}(\theta,\rho^{0})$
\end_inset

 to emphasize the dependence on the parameter of the DGP.
 From the theorem, we know that 
\begin_inset Formula $\hat{\theta}_{n}\stackrel{a.s.}{\rightarrow}\theta^{0}$
\end_inset

 
\emph on
What is the relationship between 
\begin_inset Formula $\theta^{0}$
\end_inset

 and 
\begin_inset Formula $\rho^{0}$
\end_inset

?
\end_layout

\begin_layout Itemize
\begin_inset Formula $\rho$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 may have different dimensions.
 Often, the statistical model (with parameter 
\begin_inset Formula $\theta)$
\end_inset

 only partially describes the DGP.
 For example, the case of OLS with errors of unknown distribution.
 In some cases, the dimension of 
\begin_inset Formula $\theta$
\end_inset

 may be greater than that of 
\begin_inset Formula $\rho.$
\end_inset

 For example, fitting a polynomial to an unknown nonlinear function.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
case 1: If knowledge of 
\begin_inset Formula $\theta^{0}$
\end_inset

 is sufficient for knowledge of 
\begin_inset Formula $\rho^{0}$
\end_inset

, we have a correctly and fully specified model.
 
\begin_inset Formula $\theta^{0}$
\end_inset

 is referred to as the 
\emph on
true parameter value
\emph default
.
 Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "eeolsexample"

\end_inset

 illustrates this case.
\end_layout

\begin_layout Itemize
case 2: If knowledge of 
\begin_inset Formula $\theta^{0}$
\end_inset

 is sufficient for knowledge of some but not all elements of 
\begin_inset Formula $\rho^{0},$
\end_inset

 we have a correctly specified 
\emph on
semiparametric
\emph default
 model.
 
\begin_inset Formula $\theta^{0}$
\end_inset

 is referred to as the 
\emph on
true parameter value
\emph default
, understanding that not all parameters of the DGP are estimated.
 An example would be OLS with heteroscedasticity of unknown form: we can
 learn about the parameters of the conditional mean, but not about the condition
al variances.
\end_layout

\begin_layout Itemize
case 3: If knowledge of 
\begin_inset Formula $\theta^{0}$
\end_inset

 is not sufficient for knowledge of any elements of 
\begin_inset Formula $\rho^{0},$
\end_inset

 or if it causes us to draw false conclusions regarding at least some of
 the elements of 
\begin_inset Formula $\rho^{0},$
\end_inset

 our model is 
\emph on
misspecified
\emph default
.
 
\begin_inset Formula $\theta^{0}$
\end_inset

 is referred to as the 
\emph on
pseudo-true parameter value
\emph default
.
 The next section provides an example.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example: Inconsistency of Misspecified Least Squares
\end_layout

\begin_layout Standard
You already know that the OLS estimator is inconsistent when relevant variables
 are omitted.
 Let's verify this result in the context of extremum estimators.
 We suppose that data is generated by random sampling of 
\begin_inset Formula $(Y,X)$
\end_inset

, where 
\begin_inset Formula $y_{t}=\beta_{0}x_{t}$
\end_inset

 
\begin_inset Formula $+\varepsilon_{t}$
\end_inset

.
 
\begin_inset Formula $\left(X,\varepsilon\right)$
\end_inset

 has the common distribution function 
\begin_inset Formula $F_{Z}=\mu_{x}\mu_{\varepsilon}$
\end_inset

 (
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are independent) with support 
\begin_inset Formula $\mathcal{Z=X}\times\mathcal{E}.$
\end_inset

 Suppose that the variances 
\begin_inset Formula $\sigma_{X}^{2}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\varepsilon}^{2}$
\end_inset

 are finite.
 However, the econometrician is unaware of the true DGP, and instead proposes
 the misspecified model 
\begin_inset Formula $y_{t}=\gamma_{0}w_{t}$
\end_inset

 
\begin_inset Formula $+\eta_{t}$
\end_inset

.
 Suppose that 
\begin_inset Formula $E(W\epsilon)=0$
\end_inset

 and that 
\begin_inset Formula $E(WX)\ne0.$
\end_inset


\end_layout

\begin_layout Standard
The sample objective function for a sample size 
\begin_inset Formula $n$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\gamma) & = & 1/n\sum_{t=1}^{n}\left(y_{t}-\gamma w_{t}\right)^{2}=1/n\sum_{i=1}^{n}\left(\beta_{0}x_{t}+\varepsilon_{t}-\gamma w_{t}\right)^{2}\\
 & = & 1/n\sum_{t=1}^{n}\left(\beta_{0}x_{t}\right)^{2}+1/n\sum_{t=1}^{n}\left(\gamma w_{t}\right)^{2}+1/n\sum_{t=1}^{n}\varepsilon_{t}^{2}\\
 &  & +2/n\sum_{t=1}^{n}\beta_{0}x_{t}\varepsilon_{t}-2/n\sum_{t=1}^{n}\beta_{0}\gamma x_{t}w_{t}-2/n\sum_{t=1}^{n}\varepsilon_{t}x_{t}w_{t},
\end{eqnarray*}

\end_inset

which one can verify if armed with patience.
 Using arguments similar to the correctly specified case, above, 
\begin_inset Formula 
\[
s_{\infty}(\gamma)=\gamma^{2}E\left(W^{2}\right)-2\beta_{0}\gamma E(WX)+C
\]

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 holds all terms that do not depend on 
\begin_inset Formula $\gamma$
\end_inset

.
 So, finding the minimizer with respect to 
\begin_inset Formula $\gamma$
\end_inset

, we get 
\begin_inset Formula $\gamma_{0}=\frac{\beta_{0}E(WX)}{E(W^{2})}$
\end_inset

, which is the true parameter of the DGP, multiplied by the pseudo-true
 value of a regression of 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $W.$
\end_inset

 The OLS estimator is not consistent for the true parameter, 
\begin_inset Formula $\beta_{0}$
\end_inset

 
\end_layout

\begin_layout Subsubsection
Summary
\end_layout

\begin_layout Standard
The theorem for consistency is really quite intuitive.
 It says that, with probability one, an extremum estimator converges to
 the value that maximizes the limit of the expectation of the objective
 function.
 Because the objective function may or may not make sense, depending on
 how good or poor is the model, we may or may not be estimating parameters
 of the DGP.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example:-Linearization-of"

\end_inset

Example: Linearization of a nonlinear model
\end_layout

\begin_layout Standard
See 
\begin_inset CommandInset citation
LatexCommand citet
key "white1980using"
literal "false"

\end_inset

 and Gourieroux and Monfort, section 8.3.4.
\end_layout

\begin_layout Standard
Suppose we have a nonlinear model 
\begin_inset Formula 
\[
y_{i}=h(x_{i},\theta^{0})+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula 
\[
\varepsilon_{i}\sim iid(0,\sigma^{2})
\]

\end_inset

 The 
\emph on
nonlinear least squares
\emph default
 estimator solves 
\begin_inset Formula 
\[
\hat{\theta}_{n}=\arg\min\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-h(x_{i},\theta)\right)^{2}
\]

\end_inset

 
\begin_inset Newpage newpage
\end_inset

We'll study this more later, but for now it is clear that the foc for minimizati
on will require solving a set of nonlinear equations.
 A common approach to the problem seeks to avoid this difficulty by 
\emph on
linearizing
\emph default
 the model.
 A first order Taylor's series expansion about the point 
\begin_inset Formula $x_{0}$
\end_inset

 with remainder gives 
\begin_inset Formula 
\[
y_{i}=h(x^{0},\theta^{0})+\left(x_{i}-x_{0}\right)^{\prime}\frac{\partial h(x_{0},\theta^{0})}{\partial x}+\nu_{i}
\]

\end_inset

 where 
\begin_inset Formula $\nu_{i}$
\end_inset

 encompasses both 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 and the Taylor's series remainder.
 
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\nu_{i}$
\end_inset

 is no longer a classical error - its mean is not zero.
 We should expect problems.
\end_layout

\begin_layout Itemize
Define 
\begin_inset Formula 
\begin{eqnarray*}
\alpha^{*} & = & h(x_{0},\theta^{0})-x_{0}^{\prime}\frac{\partial h(x^{0},\theta^{0})}{\partial x}\\
\beta^{*} & = & \frac{\partial h(x_{0},\theta^{0})}{\partial x}
\end{eqnarray*}

\end_inset

as the intercept and slope of the Taylor's series tangent line.
 
\end_layout

\begin_layout Itemize
Given this, one might try to estimate 
\begin_inset Formula $\alpha^{*}$
\end_inset

 and 
\begin_inset Formula $\beta^{*}$
\end_inset

 by applying OLS to 
\begin_inset Formula 
\[
y_{i}=\alpha+\beta x_{i}+\nu_{i}
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Question, will 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}$
\end_inset

 be consistent for 
\begin_inset Formula $\alpha^{*}$
\end_inset

 and 
\begin_inset Formula $\beta^{*}$
\end_inset

?
\end_layout

\begin_layout Itemize
The answer is no, as one can see by interpreting 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}$
\end_inset

 as extremum estimators.
 Let 
\begin_inset Formula $\gamma=(\alpha,\beta^{\prime})^{\prime}.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\gamma}=\arg\min s_{n}(\gamma)=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}
\]

\end_inset

 The objective function converges to its expectation 
\begin_inset Formula 
\[
s_{n}(\gamma)\stackrel{u.a.s.}{\rightarrow}s_{\infty}(\gamma)=\mathcal{E}_{X}\mathcal{E}_{Y|X}\left(y-\alpha-\beta x\right)^{2}
\]

\end_inset

 and 
\begin_inset Formula $\hat{\gamma}$
\end_inset

 converges 
\begin_inset Formula $a.s.$
\end_inset

 to the 
\begin_inset Formula $\gamma^{0}$
\end_inset

 that minimizes 
\begin_inset Formula $s_{\infty}(\gamma)$
\end_inset

: 
\begin_inset Formula 
\[
\gamma^{0}=\arg\min\mathcal{E}_{X}\mathcal{E}_{Y|X}\left(y-\alpha-\beta x\right)^{2}
\]

\end_inset

 Noting that 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}_{X}\mathcal{E}_{Y|X}\left(y-\alpha-x^{\prime}\beta\right)^{2} & = & \mathcal{E}_{X}\mathcal{E}_{Y|X}\left(h(x,\theta^{0})+\varepsilon-\alpha-\beta x\right)^{2}\\
 & = & \sigma^{2}+\mathcal{E}_{X}\left(h(x,\theta^{0})-\alpha-\beta x\right)^{2}
\end{eqnarray*}

\end_inset

 since cross products involving 
\begin_inset Formula $\varepsilon$
\end_inset

 drop out.
 
\begin_inset Formula $\alpha^{0}$
\end_inset

 and 
\begin_inset Formula $\beta^{0}$
\end_inset

 correspond to the hyperplane that is closest to the true regression function
 
\begin_inset Formula $h(x,\theta^{0})$
\end_inset

 according to the mean squared error criterion.
 This depends on both the shape of 
\begin_inset Formula $h(\cdot)$
\end_inset

 and the density function of the conditioning variables.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Linear-Approximation"

\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Linear-Approximation"

\end_inset

Linear Approximation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/LinearApproximation.pdf
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
It is clear that the tangent line does not minimize MSE, since, for example,
 if 
\begin_inset Formula $h(x,\theta^{0})$
\end_inset

 is concave, all errors between the tangent line and the true function are
 negative.
\end_layout

\begin_layout Itemize
Note that the true underlying parameter 
\begin_inset Formula $\theta^{0}$
\end_inset

 is not estimated consistently, either (it may be of a different dimension
 than the dimension of the parameter of the approximating model, which is
 2 in this example).
\end_layout

\begin_layout Itemize
see Exercise 
\begin_inset CommandInset ref
LatexCommand ref
reference "ols"

\end_inset

 in the list at the end of the chapter for a practical example.
\end_layout

\begin_layout Itemize
Second order and higher-order approximations suffer from exactly the same
 problem, though to a less severe degree, of course.
 For this reason, translog, Generalized Leontiev and other 
\begin_inset Quotes eld
\end_inset

flexible functional forms
\begin_inset Quotes erd
\end_inset

 based upon second-order approximations in general suffer from bias and
 inconsistency.
 The bias may not be too important for analysis of conditional means, but
 it can be very important for analyzing first and second derivatives.
 In production and consumer analysis, first and second derivatives (
\emph on
e.g.,
\emph default
 elasticities of substitution) are often of interest, so in this case, one
 should be cautious of unthinking application of models that impose stong
 restrictions on second derivatives.
\end_layout

\begin_layout Itemize
This sort of linearization about a long run equilibrium is a common practice
 in working with dynamic macroeconomic models.
 It is justified for the purposes of theoretical analysis of a model 
\emph on
given
\emph default
 the model's parameters, but it will lead to
\emph on
 bias and inconsistency
\emph default
 if it is done before estimation of the parameters of the model using data.
 The section on simulation-based methods offers a means of obtaining consistent
 estimators of the parameters of dynamic macro models that are too complex
 for standard methods of analysis.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Asymptotic Normality
\end_layout

\begin_layout Standard
A consistent estimator is oftentimes not very useful unless we know how
 fast it is likely to be converging to the true value, and the probability
 that it is far away from the true value.
 Establishment of asymptotic normality with a known scaling factor solves
 these two problems.
 The following theorem is similar to Amemiya's Theorem 4.1.3 (pg.
 111).
\end_layout

\begin_layout Theorem

\emph on
[Asymptotic normality of e.e.]
\emph default
 
\begin_inset CommandInset label
LatexCommand label
name "Normality of ee"

\end_inset

In addition to the assumptions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

, assume
\end_layout

\begin_layout Theorem
(a) 
\begin_inset Formula $\mathcal{J}_{n}(\theta)\equiv D_{\theta}^{2}s_{n}(\theta)$
\end_inset

 exists and is continuous in an open, convex neighborhood of 
\begin_inset Formula $\theta^{0}.$
\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Theorem
(b) 
\begin_inset Formula $\mathcal{J}_{n}(\theta_{n})\stackrel{a.s.}{\rightarrow}\mathcal{J}_{\infty}(\theta^{0}),$
\end_inset

 a finite negative definite matrix, for any sequence of 
\begin_inset Formula $\theta_{n}$
\end_inset

 that converges almost surely to 
\begin_inset Formula $\theta^{0}.$
\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Theorem
(c) 
\begin_inset Formula $\sqrt{n}D_{\theta}s_{n}(\theta^{0})\stackrel{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta^{0})\right],$
\end_inset

 where 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta^{0})=\lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta^{0})$
\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Theorem
Then 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta^{0})^{-1}\mathcal{I}_{\infty}(\theta^{0})\mathcal{J}_{\infty}(\theta^{0})^{-1}\right]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Proof:
\series default
 By Taylor expansion: 
\begin_inset Formula 
\[
D_{\theta}s_{n}(\hat{\theta}_{n})=D_{\theta}s_{n}(\theta^{0})+D_{\theta}^{2}s_{n}(\theta^{*})\left(\hat{\theta}-\theta^{0}\right)
\]

\end_inset

 where 
\begin_inset Formula $\theta^{*}=\lambda\hat{\theta}+(1-\lambda)\theta^{0},$
\end_inset

 
\begin_inset Formula $0\leq\lambda\leq1.$
\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 will be in the neighborhood where 
\begin_inset Formula $D_{\theta}^{2}s_{n}(\theta)$
\end_inset

 exists with probability one as 
\begin_inset Formula $n$
\end_inset

 becomes large, by consistency.
\end_layout

\begin_layout Itemize
Now the l.h.s.
 of this equation is zero, at least asymptotically, since 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is a maximizer and the f.o.c.
 must hold exactly since the limiting objective function is strictly concave
 in a neighborhood of 
\begin_inset Formula $\theta^{0}$
\end_inset

 (assns.
 a and b)
\end_layout

\begin_layout Itemize
Also, since 
\begin_inset Formula $\theta^{*}$
\end_inset

 is between 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 and 
\begin_inset Formula $\theta^{0},$
\end_inset

 and since 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 
\begin_inset Formula $\stackrel{a.s.}{\rightarrow}$
\end_inset

 
\begin_inset Formula $\theta^{0}$
\end_inset

 , assumption (b) gives 
\begin_inset Formula 
\[
D_{\theta}^{2}s_{n}(\theta^{*})\stackrel{a.s.}{\rightarrow}\mathcal{J}_{\infty}(\theta^{0})
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
So 
\begin_inset Formula 
\[
0=D_{\theta}s_{n}(\theta^{0})+\left[\mathcal{J}_{\infty}(\theta^{0})+o_{s}(1)\right]\left(\hat{\theta}-\theta^{0}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
And 
\begin_inset Formula 
\[
0=\sqrt{n}D_{\theta}s_{n}(\theta^{0})+\left[\mathcal{J}_{\infty}(\theta^{0})+o_{s}(1)\right]\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)
\]

\end_inset

 Now 
\begin_inset Formula $\sqrt{n}D_{\theta}s_{n}(\theta^{0})\stackrel{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta^{0})\right]$
\end_inset

 by assumption c, so 
\begin_inset Formula 
\[
-\left[\mathcal{J}_{\infty}(\theta^{0})+o_{s}(1)\right]\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta^{0})\right]
\]

\end_inset

 Also, 
\begin_inset Formula $\left[\mathcal{J}_{\infty}(\theta^{0})+o_{s}(1)\right]\stackrel{a.s.}{\rightarrow}\mathcal{J}(\theta^{0}),$
\end_inset

 so 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta^{0})^{-1}\mathcal{I}_{\infty}(\theta^{0})\mathcal{J}_{\infty}(\theta^{0})^{-1}\right]
\]

\end_inset

by the Slutsky Theorem (see Gallant, Theorem 4.6).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Effects-of-"

\end_inset

 shows the effects of the two components, the variability of the gradient,
 and the slope of the gradient.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Effects-of-"

\end_inset

Effects of 
\begin_inset Formula $I_{\mbox{\infty}}$
\end_inset

 and 
\begin_inset Formula $J_{\infty}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/IJ.jpg
	scaleBeforeRotation
	rotateAngle 90

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example: Classical linear model
\end_layout

\begin_layout Standard
Let's use the results to get the asymptotic distribution of the OLS estimator
 applied to the classical model, to verify that we obtain the results seen
 before.
 The OLS criterion is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\beta) & = & \frac{1}{n}\left(y-X\beta\right)^{\prime}\left(y-X\beta\right)\\
 & = & \frac{1}{n}\left(X\beta^{0}+\epsilon-X\beta\right)^{\prime}\left(X\beta^{0}+\epsilon-X\beta\right)\\
 & = & \frac{1}{n}\left(X(\beta^{0}-\beta)+\epsilon\right)^{\prime}\left(X(\beta^{0}-\beta)+\epsilon\right)\\
 & = & \frac{1}{n}\left[\left(\beta^{0}-\beta\right)^{\prime}X^{\prime}X\left(\beta^{0}-\beta\right)+2\epsilon^{\prime}X(\beta_{0}-\beta)+\epsilon^{\prime}\epsilon\right]
\end{eqnarray*}

\end_inset

The first derivative is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{\beta}s_{n}(\beta)=\frac{1}{n}\left[-2X^{\prime}X\left(\beta^{0}-\beta\right)-2X^{\prime}\epsilon\right]
\]

\end_inset

so, evaluating at 
\begin_inset Formula $\beta^{0},$
\end_inset


\begin_inset Newpage newpage
\end_inset


\begin_inset Formula 
\[
D_{\beta}s_{n}(\beta^{0})=-2\frac{X^{\prime}\epsilon}{n}
\]

\end_inset


\end_layout

\begin_layout Itemize
note that this is an average of terms, each of which has expectation zero:
 
\begin_inset Formula $-2\frac{X^{\prime}\epsilon}{n}=-2\frac{1}{n}\sum_{t}x_{t}\epsilon_{t}$
\end_inset


\end_layout

\begin_layout Itemize
thus, a LLN tells us this converges almost surely to 0.
\end_layout

\begin_layout Itemize
to keep this from happening, we can multiply by something that is converging
 to infinity.
 It turns out that 
\begin_inset Formula $\sqrt{n}$
\end_inset

 is the right choice, because then the asymptotic distribution will be stable,
 and a CLT will apply.
\end_layout

\begin_layout Standard
Now, let's get the form of 
\begin_inset Formula $\mathcal{I}_{\infty}$
\end_inset

 of Assumption (c): Considering 
\begin_inset Formula $\sqrt{n}D_{\beta}s_{n}(\beta^{0})$
\end_inset

, it has expectation 0, so the variance is the expectation of the outer
 product (there's no need to subtract the mean):
\begin_inset Formula 
\begin{eqnarray*}
Var\sqrt{n}D_{\beta}s_{n}(\beta^{0}) & = & E\left[\left(-\sqrt{n}2\frac{X^{\prime}\epsilon}{n}\right)\left(-\sqrt{n}2\frac{X^{\prime}\epsilon}{n}\right)^{\prime}\right]\\
 & = & E4\frac{X^{\prime}\epsilon\epsilon^{\prime}X}{n}\\
 & = & 4\sigma_{\epsilon}^{2}E\left(\frac{X^{\prime}X}{n}\right)
\end{eqnarray*}

\end_inset

(because regressors are independent of errors).
 Therefore
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\beta^{0}) & = & \lim_{n\rightarrow\infty}Var\sqrt{n}D_{\beta}s_{n}(\beta^{0})\\
 & = & 4\sigma_{\epsilon}^{2}Q_{X}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $Q_{X}=\lim E\left(\frac{X^{\prime}X}{n}\right),$
\end_inset

 is a finite p.d.
 matrix (by the classical assumption of no perfect collinearity).
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The second derivative is
\begin_inset Formula 
\[
\mathcal{J}_{n}(\beta)=D_{\beta}^{2}s_{n}(\beta^{0})=\frac{1}{n}\left[2X^{\prime}X\right].
\]

\end_inset

A SLLN tells us that this converges almost surely to the limit of its expectatio
n:
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\beta^{0})=2Q_{X}
\]

\end_inset

There's no parameter in that last expression, so uniformity is not an issue.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The asymptotic normality theorem (
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

) tells us that
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}\left(\hat{\beta}-\beta^{0}\right) & \stackrel{d}{\rightarrow} & N\left[0,\mathcal{J}_{\infty}(\beta^{0})^{-1}\mathcal{I}_{\infty}(\beta^{0})\mathcal{J}_{\infty}(\beta^{0})^{-1}\right]
\end{eqnarray*}

\end_inset

which is, given the above,
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(\frac{Q_{X}^{-1}}{2}\right)\left(4\sigma_{\epsilon}^{2}Q_{X}\right)\left(\frac{Q_{X}^{-1}}{2}\right)\right]
\]

\end_inset

or
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\beta}-\beta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,Q_{X}^{-1}\sigma_{\epsilon}^{2}\right].
\]

\end_inset

This is the same thing we saw in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:asymp normality OLS"

\end_inset

, of course.
 So, the theory seems to work :-)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "ols"

\end_inset

Suppose that 
\begin_inset Formula $x_{i}\sim$
\end_inset

 uniform(0,1), and 
\begin_inset Formula $y_{i}=1-x_{i}^{2}+\varepsilon_{i},$
\end_inset

 where 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 is iid(0,
\begin_inset Formula $\sigma^{2}).$
\end_inset

 Suppose we estimate the misspecified model 
\begin_inset Formula $y_{i}=\alpha+\beta x_{i}+\eta_{i}$
\end_inset

 by OLS.
 Find the numeric values of 
\begin_inset Formula $\alpha^{0}$
\end_inset

 and 
\begin_inset Formula $\beta^{0}$
\end_inset

 that are the probability limits of 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 and 
\begin_inset Formula $\hat{\beta}$
\end_inset

.
 Hint: the correct answers are 7/6 and -1.
 To get some help with this exercise, you can use a computer algebra program,
 as was done in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Consistency-of-OLS"

\end_inset

.
 A small modification of that code would solve this problem.
\end_layout

\begin_layout Enumerate
Verify your results using Julia by generating data that follows the above
 model, and calculating the OLS estimator.
 When the sample size is very large the estimator should be very close to
 the analytical results you obtained in question 
\begin_inset CommandInset ref
LatexCommand ref
reference "ols"

\end_inset

.
\end_layout

\begin_layout Enumerate
Use the asymptotic normality theorem to find the asymptotic distribution
 of the ML estimator of 
\begin_inset Formula $\beta^{0}$
\end_inset

 for the model 
\begin_inset Formula $y=x\beta^{0}+\varepsilon,$
\end_inset

 where 
\begin_inset Formula $\varepsilon\sim N(0,1)$
\end_inset

 and is independent of 
\begin_inset Formula $x.$
\end_inset

 This means finding 
\begin_inset Formula $\frac{\partial^{2}}{\partial\beta\partial\beta^{\prime}}s_{n}(\beta)$
\end_inset

, 
\begin_inset Formula $\mathcal{J}(\beta^{0}),\left.\frac{\partial s_{n}(\beta)}{\partial\beta}\right|,$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}(\beta^{0}).$
\end_inset

 The expressions may involve the unspecified density of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Application: a simple DSGE model
\begin_inset CommandInset label
LatexCommand label
name "chap:Application:-a-simple"

\end_inset


\end_layout

\begin_layout Standard
This short chapter presents a simple DSGE model, which will be used to illustrat
e several estimators: ML, GMM, VARs, and Bayesian methods.
 DSGE models are quite widely used by central banks, etc., but they are not
 without their critics: see 
\begin_inset CommandInset href
LatexCommand href
name "Paul Romer's WP \"The Trouble with Macroeconomics\""
target "https://paulromer.net/trouble-with-macroeconomics-update/"
literal "false"

\end_inset

, for example.
 I like the DSGE model as an example because it allow illustrating a variety
 of econometric techniques and methods.
 Any other structural nonlinear model could serve the same purpose.
 
\end_layout

\begin_layout Itemize
We will investigate structural estimation methods, which attempt to estimate
 the actual parameters of the data generating process, and reduced form
 methods, which characterize the data, but which do not recover the parameters
 of the data generating process.
 
\end_layout

\begin_layout Itemize
Knowing the true DGP will allow us to measure sensibly how well the different
 methods work.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
The model is as follows: 
\end_layout

\begin_layout Itemize
The consumer chooses consumption, hours of work, and investment to maximize
 expected discounted utility.
\end_layout

\begin_layout Itemize
Using capital and labor provided by the consumer, a competitive firm produces
 an output to maximize profits, and pays the consumer according to the marginal
 productivity of the inputs.
 
\end_layout

\begin_layout Itemize
The price of the consumption good is normalized to one.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Variables
\end_layout

\begin_layout Standard
There are 9 endogenous variables, listed in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Variables"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Variables"

\end_inset

Variables
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
output
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
consumption
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
capital
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
investment
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
hours
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to labor
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $r$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to capital
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\eta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
preference shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $z$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
technology shock
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\series bold
Parameters
\end_layout

\begin_layout Standard
There are 9 parameters, listed in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Parameters"

\end_inset


\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Parameters"

\end_inset

Parameters
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
production
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
discount
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\delta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
depreciation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\gamma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
risk aversion
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\psi$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MRS
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{z}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
persistence technology shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{z}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
variability technology shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{\eta}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
persistence preference shock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{\eta}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
variability preference shock
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Consumer's problem
\end_layout

\begin_layout Standard
At the beginning of period 
\begin_inset Formula $t$
\end_inset

, the household owns a given amount of capital, 
\begin_inset Formula $k_{t-1}$
\end_inset

, and chooses 
\begin_inset Formula $c_{t}$
\end_inset

, 
\begin_inset Formula $i_{t}$
\end_inset

 and 
\begin_inset Formula $n_{t}$
\end_inset

 to maximize expected discounted utility
\begin_inset Formula 
\[
E_{t}\sum_{s=0}^{\infty}\beta^{s}\left(\frac{c_{t+s}^{1-\gamma}}{1-\gamma}+(1-n_{t+s})\eta_{t}\psi\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
subject to the budget constraint 
\begin_inset Formula $c_{t}+i_{t}=r_{t}k_{t-1}+w_{t}n_{t}$
\end_inset


\end_layout

\begin_layout Itemize
available time 
\begin_inset Formula $0\le n_{t}\le$
\end_inset

1
\end_layout

\begin_layout Itemize
and the accumulation of capital 
\begin_inset Formula $k_{t}=i_{t}+(1-\delta)k_{t-1}$
\end_inset

 
\end_layout

\begin_layout Itemize
There is a shock, 
\begin_inset Formula $\eta_{t}$
\end_inset

, that affects the desirability of leisure relative to consumption 
\end_layout

\begin_deeper
\begin_layout Itemize
The shock evolves according to 
\begin_inset Formula $\ln\eta_{t}=\rho_{\eta}\ln\eta_{t-1}+\sigma_{\eta}\epsilon_{t}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Firm's problem
\end_layout

\begin_layout Standard
The competitive firm maximizes profits 
\begin_inset Formula $y_{t}-w_{t}n_{t}-r_{t}k_{t-1}$
\end_inset

 from production of the good 
\begin_inset Formula $y_{t}$
\end_inset

, taking 
\begin_inset Formula $w_{t}$
\end_inset

 and 
\begin_inset Formula $r_{t}$
\end_inset

 as given, using the constant returns to scale technology
\begin_inset Formula 
\begin{equation}
y_{t}=k_{t-1}^{\alpha}n_{t}^{1-\alpha}z_{t}\label{eq:DSGE production}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Comments:
\end_layout

\begin_layout Itemize
Technology shocks 
\begin_inset Formula $z_{t}$
\end_inset

 also follow an AR(1) process in logarithms: 
\begin_inset Formula $\ln z_{t}=\rho_{z}\ln z_{t-1}+\sigma_{z}u_{t}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The innovations to the preference and technology shocks, 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $u_{t}$
\end_inset

, are both i.i.d.
 standard normal random variables, and are independent of one another.
 
\end_layout

\begin_layout Itemize
The good 
\begin_inset Formula $y_{t}$
\end_inset

 can be allocated by the consumer to consumption or investment: 
\begin_inset Formula $y_{t}=c_{t}+i_{t}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The consumer provides capital and labor to the firm, and is paid at the
 rates 
\begin_inset Formula $r_{t}$
\end_inset

 and 
\begin_inset Formula $w_{t}$
\end_inset

, respectively.
 
\end_layout

\begin_layout Itemize
The representative agent chooses actions in period 
\begin_inset Formula $t$
\end_inset

 using rational expectations, with full information about all variables
 indexed 
\begin_inset Formula $t-1$
\end_inset

 and earlier.
\end_layout

\begin_layout Itemize
The variables available for estimation are 
\begin_inset Formula $y,c,n,w,$
\end_inset

 and 
\begin_inset Formula $r.$
\end_inset

 
\end_layout

\begin_layout Itemize
the model is nonlinear in the parameters, equations depend on multiple endogenou
s variables, and 4 of the endogenous variables are not observed by the econometr
ician (the two shocks, capital and investment).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
First order conditions
\end_layout

\begin_layout Standard
Four definitions are 
\begin_inset Formula 
\begin{eqnarray*}
\textrm{marginal utility of consumption: }MUC_{t} & := & c_{t}^{-\gamma}\\
\textrm{marginal utility of leisure: }MUL_{t} & := & \psi\eta_{t}\\
\textrm{marginal rate of substitution: }MRS_{t} & := & MUC_{t}/MUL_{t}\\
\textrm{marginal product of labor: }MPL_{t} & := & \left(1-\alpha\right)z_{t}k_{t-1}^{\alpha}n_{t}^{-\alpha}\\
\textrm{marginal product of capital: }MPK_{t} & := & \alpha z_{t}k_{t-1}^{\alpha-1}n_{t}^{1-\alpha}
\end{eqnarray*}

\end_inset

With these definitions, the 9 equations that characterize the solution for
 the 9 endogenous variables are:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
MUC_{t} & = & E\left(\beta\cdot MUC_{t+1}\left[1+MPK_{t+1}-\delta\right]\right)\label{eq:Euler}\\
\textrm{ }1/MRS_{t} & = & w_{t}\label{eq:MRS-wage}\\
w_{t} & = & MPL_{t}\label{MPL-wage}\\
r_{t} & = & MPK_{t}\label{eq:-3}\\
\ln\eta_{t} & = & \rho_{\eta}\ln\eta_{t-1}+\sigma_{\eta}\epsilon_{t}\label{eq:}\\
\ln z_{t} & = & \rho_{z}\ln z_{t-1}+\sigma_{z}u_{t}\label{eq:-1}\\
y_{t} & = & z_{t}k_{t-1}^{\alpha}n_{t}^{(1-\alpha)}\label{eq:output}\\
i_{t} & = & y_{t}-c_{t}\label{eq:investment}\\
k_{t} & = & i_{t}+(1-\delta)k_{t-1}\label{eq:capital}
\end{eqnarray}

\end_inset

where the first two are from utility maximization, the second two are from
 profit maximization, and the remaining 5 are directly from the model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
The steady state
\end_layout

\begin_layout Standard
We use a third-order perturbation solution, which combines good accuracy
 with moderate computational demands 
\begin_inset CommandInset citation
LatexCommand cite
key "Aruoba2006"
literal "true"

\end_inset

.
 This is done using Dynare (Juillard, 1996).
 A first step for solving the model using Dynare is to find the deterministic
 steady state.
 Let variables without the 
\begin_inset Formula $t$
\end_inset

 subscript indicate the deterministic steady state level of the variable.
 The deterministic steady state values of the two shocks 
\begin_inset Formula $\eta$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

 are both 1.
 Using equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MRS-wage"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "MPL-wage"

\end_inset

, dropping 
\begin_inset Formula $t$
\end_inset

 subscripts, and setting the two shocks to 1, we obtain 
\begin_inset Formula 
\begin{eqnarray}
\frac{\psi}{c^{-\gamma}} & = & \left(1-\alpha\right)k^{\alpha}n^{-\alpha}\label{eq:MRS-MPL}\\
n & = & \left(\frac{1-\alpha}{\psi}\right)^{1/\alpha}kc^{-\gamma/\alpha}\nonumber 
\end{eqnarray}

\end_inset

Define 
\begin_inset Formula 
\begin{equation}
\theta:=\left(\frac{1-\alpha}{\psi}\right)^{1/\alpha}\label{theta}
\end{equation}

\end_inset

so
\begin_inset Formula 
\begin{equation}
n=\theta kc^{-\gamma/\alpha}\label{eq:steadystate1}
\end{equation}

\end_inset

From the Euler equation (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Euler"

\end_inset

)
\begin_inset Formula 
\begin{eqnarray*}
c & = & \beta c\left(1+\alpha k^{\alpha-1}n^{1-\alpha}\right)\\
n^{1-\alpha} & = & k^{1-\alpha}\left[\frac{1}{\alpha}\left(\frac{1}{\beta}-1+\delta\right)\right]\\
n & = & k\left[\frac{1}{\alpha}\left(\frac{1}{\beta}-1+\delta\right)\right]^{\frac{\text{1}}{1-\alpha}}
\end{eqnarray*}

\end_inset

Define
\begin_inset Formula 
\begin{equation}
\varphi:=\left[\frac{1}{\alpha}\left(\frac{1}{\beta}-1+\delta\right)\right]^{\frac{\text{1}}{1-\alpha}}\label{eq:phi}
\end{equation}

\end_inset

so
\begin_inset Formula 
\begin{equation}
n=\varphi k\label{steadystate2}
\end{equation}

\end_inset

Now set equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:steadystate1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "steadystate2"

\end_inset

 equal, and solve for steady state level of consumption:
\begin_inset Formula 
\begin{eqnarray*}
\theta kc^{-\gamma/\alpha} & = & \varphi k\\
c^{-\gamma/\alpha} & = & \frac{\varphi}{\theta}\\
c & = & \left(\frac{\theta}{\varphi}\right)^{\frac{\alpha}{\gamma}}
\end{eqnarray*}

\end_inset

From equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:capital"

\end_inset

, steady state investment satisfies 
\begin_inset Formula 
\[
i=\delta k
\]

\end_inset

and combining this with equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:output"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:investment"

\end_inset

, we obtain (using equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "steadystate2"

\end_inset

)
\begin_inset Formula 
\[
c+\delta k=k^{\alpha}(\varphi k)^{1-\alpha}
\]

\end_inset

which solves as
\begin_inset Formula 
\[
k=\frac{c}{\varphi^{1-\alpha}-\delta}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

To summarize, the steady state values for the 9 endogenous variables are
 given in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Deterministic-steady-state"

\end_inset

, and given the 9 parameters of the model, these can be solved for in the
 order 
\begin_inset Formula $c,\,k,\,n,\,y,\,i,\,w,r$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Deterministic-steady-state"

\end_inset

Deterministic steady state
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Variable
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Description
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Steady state
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
output
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k^{\alpha}n^{1-\alpha}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $c$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
consumption
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $c=\left(\frac{\theta}{\varphi}\right)^{\frac{\alpha}{\gamma}}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
capital
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k=\frac{c}{\varphi^{1-\alpha}-\delta}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
investment
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y-c$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $n$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
hours
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $n=\varphi k$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to labor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1-\alpha\right)k^{\alpha}n^{-\alpha}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $r$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
return to capital
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha k^{\alpha-1}n^{1-\alpha}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\eta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
preference shock
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $z$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
technology shock
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
True parameter values and priors 
\end_layout

\begin_layout Standard
For this model, given the observable variables, we can recover 
\begin_inset Formula $\alpha=1-\frac{wn}{y}$
\end_inset

, by substituting the production function into the MPL or MPK.
 Once we have 
\begin_inset Formula $\alpha,$
\end_inset

 we can solve for 
\begin_inset Formula $k,$
\end_inset

 and with 
\begin_inset Formula $k,$
\end_inset

 we can recover 
\begin_inset Formula $\delta$
\end_inset

 from the law of motion of capital.
 So, we will take 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\delta$
\end_inset

 as known parameters, as they can be recovered exactly from the observable
 data.
 Henceforth, we set 
\begin_inset Formula $\alpha=0.33$
\end_inset

 and 
\begin_inset Formula $\delta=0.025$
\end_inset

.
\end_layout

\begin_layout Standard
Estimation by GMM does not require specifying a prior distribution for the
 parameters, but Bayesian methods do, so here, I discuss the true parameter
 values used for the simulations, as well as priors.
 Economic intuition can guide choice of priors for most parameters.
 However, we may be less confident specifying a prior for the marginal rate
 of substitution, 
\begin_inset Formula $\psi.$
\end_inset

 Instead, following Ruge-Murcia (2012), we may place a prior on steady state
 hours, 
\begin_inset Formula $n$
\end_inset

.
 Given that hours 
\begin_inset Formula $n_{t}$
\end_inset

 must satisfy the constraint 
\begin_inset Formula $0\le n_{t}\le1$
\end_inset

, and we know that normally around 8 hours per day is dedicated to work,
 it is relatively straightforward to place a prior on 
\begin_inset Formula $n$
\end_inset

.
 If steady state hours, 
\begin_inset Formula $n$
\end_inset

 is given, say as a draw from it's prior, then this, along with the parameters,
 excepting 
\begin_inset Formula $\psi$
\end_inset

, allows us to solve for the steady state values of the other variables
 and for 
\begin_inset Formula $\psi$
\end_inset

, as follows: Given 
\begin_inset Formula $n,$
\end_inset

 we can compute 
\begin_inset Formula $k=n/\varphi,$
\end_inset

 then 
\begin_inset Formula $i=\delta k,$
\end_inset

 then 
\begin_inset Formula $y=k^{\alpha}n^{1-\alpha}$
\end_inset

, then 
\begin_inset Formula $c=y-i$
\end_inset

, and finally, using equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MRS-MPL"

\end_inset

, we can compute the 
\begin_inset Formula $\psi$
\end_inset

 that is consistent with the given steady state 
\begin_inset Formula $n$
\end_inset

 and the other parameters of the model as 
\begin_inset Formula 
\begin{eqnarray*}
\psi & = & c^{-\gamma}\left(1-\alpha\right)k^{\alpha}n^{-\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

We use independent uniform priors for all parameters except 
\begin_inset Formula $\psi,$
\end_inset

 and a uniform prior for steady state hours, 
\begin_inset Formula $n.$
\end_inset

 The true values of the parameters and the supports of the uniform priors
 are given in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:True-parameters-and"

\end_inset

.
 We believe that most economists will find these priors to be quite loose,
 and the parameter values to be reasonable.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:True-parameters-and"

\end_inset

True parameters and support of uniform priors.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Lower bound 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Upper bound 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
True value 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Prior bias 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Prior RMSE
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.95 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.990 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-0.015 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.021
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\gamma$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.000 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.500 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.527
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{z}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.900 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-0.400 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.493
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{z}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.020 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.030 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.042
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{\eta}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.700 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-0.200 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.351
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{\eta}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.040 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.049
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\bar{n}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6/24 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9/24 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1/3 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-0.021 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.042
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Solution of the model and generation of simulated data
\end_layout

\begin_layout Standard
Given a draw of the parameters other than 
\begin_inset Formula $\psi$
\end_inset

, 
\begin_inset Formula $\psi$
\end_inset

 is computed as above, and we can form the full parameter vector, 
\begin_inset Formula $\theta^{s}$
\end_inset

, where 
\begin_inset Formula $s$
\end_inset

 indexes the simulations.
 The model is solved using Dynare, using a third order perturbation about
 the steady state.
 Once the model is solved, a simulation of length 260 is done, initialized
 at the steady state.
 We drop 100 observations, retaining the last 160 observations, which mimic
 40 years of quarterly data.
 The observable variables are 
\begin_inset Formula $y,\,c,\,n,\,w,$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

.
 The capital stock 
\begin_inset Formula $k$
\end_inset

 is not observed.
 The selection of observable variables is in line with much empirical work
 (e.g., 
\begin_inset CommandInset citation
LatexCommand cite
key "Smets2007"
literal "true"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Guerron2010"
literal "true"

\end_inset

).
\end_layout

\begin_layout Standard
The Dynare mod file that is used to solve and simulate data follows (note
 that the Dynare timing regarding when 
\begin_inset Formula $k$
\end_inset

 is available for use is the same as the timing used above):
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

// The two shock model
\end_layout

\begin_layout Plain Layout

// This takes steady state of hours as given, and computes the psi
\end_layout

\begin_layout Plain Layout

// parameter that corresponds.
\end_layout

\begin_layout Plain Layout

close all;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

// Define variables
\end_layout

\begin_layout Plain Layout

var y c k n invest z1 z2 MUC MUL r w;
\end_layout

\begin_layout Plain Layout

varexo e1 e2;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

// Parameters
\end_layout

\begin_layout Plain Layout

parameters alppha betta delta gam  nss rho1 sigma1 rho2 sigma2 psi c1 iss
 yss kss css;
\end_layout

\begin_layout Plain Layout

load parameterfile;
\end_layout

\begin_layout Plain Layout

set_param_value('alppha',0.33)
\end_layout

\begin_layout Plain Layout

set_param_value('betta',beta)
\end_layout

\begin_layout Plain Layout

set_param_value('delta',0.025)
\end_layout

\begin_layout Plain Layout

set_param_value('gam',gam)
\end_layout

\begin_layout Plain Layout

set_param_value('rho1',rho1)
\end_layout

\begin_layout Plain Layout

set_param_value('sigma1',sigma1)
\end_layout

\begin_layout Plain Layout

set_param_value('rho2',rho2)
\end_layout

\begin_layout Plain Layout

set_param_value('sigma2',sigma2)
\end_layout

\begin_layout Plain Layout

set_param_value('nss',nss)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

// Model
\end_layout

\begin_layout Plain Layout

model;
\end_layout

\begin_layout Plain Layout

  MUC = (c)^(-gam);
\end_layout

\begin_layout Plain Layout

  MUL = psi*exp(z2);
\end_layout

\begin_layout Plain Layout

  r = alppha * exp(z1) * (k(-1))^(alppha-1) * n^(1-alppha);
\end_layout

\begin_layout Plain Layout

  w = (1-alppha)*exp(z1)* (k(-1))^alppha * n^(-alppha);
\end_layout

\begin_layout Plain Layout

  MUC = betta*MUC(+1) * (1 + r(+1) - delta);
\end_layout

\begin_layout Plain Layout

  MUL/MUC = w;
\end_layout

\begin_layout Plain Layout

  z1 = rho1*z1(-1) + sigma1*e1;
\end_layout

\begin_layout Plain Layout

  z2 = rho2*z2(-1) + sigma2*e2;
\end_layout

\begin_layout Plain Layout

  y = exp(z1) * ((k(-1))^alppha) * (n^(1-alppha));
\end_layout

\begin_layout Plain Layout

  invest = y - c;
\end_layout

\begin_layout Plain Layout

  k = invest + (1-delta)*k(-1);
\end_layout

\begin_layout Plain Layout

end;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

// Shocks
\end_layout

\begin_layout Plain Layout

shocks;
\end_layout

\begin_layout Plain Layout

var e1 = 1;
\end_layout

\begin_layout Plain Layout

var e2 = 1;
\end_layout

\begin_layout Plain Layout

end;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

// generate data
\end_layout

\begin_layout Plain Layout

stoch_simul(nograph, noprint, nomoments, order=3, periods=260, irf=0) y
 c n r w;
\end_layout

\end_inset

With this file, we can generate samples at the chosen parameter values.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Generating data
\end_layout

\begin_layout Standard
The model is solved using Dynare, which requires Matlab or GNU Octave.
 After a careful price/quality evaluation, I decided to use the free choice,
 Octave.
 Octave may not be too fast, when you don't know some tricks, but it can
 get the job done.
 The script that generates the data is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{make
\backslash
_simdata.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/DSGE/GenDa
ta/make
\backslash
_simdata.m} 
\end_layout

\end_inset

.
 Here's a plot of the 160 observations of the five observed variables.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The DSGE data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/GenData/dsgedata.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
recall that the true parameter values that generate this data were described
 above.
 
\end_layout

\begin_layout Itemize
Note that consumption is much more smooth than output
\end_layout

\begin_layout Itemize
the interest rate is pretty much constant, hours worked is a little more
 responsive
\end_layout

\begin_layout Itemize
wages move around quite a bit in response to shocks
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Maximum likelihood estimation
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Ch.
 5
\end_layout

\begin_layout Standard
The maximum likelihood estimator is important because it uses all of the
 information in a fully specified statistical model.
 Its use of all of the information causes it to have a number of attractive
 properties, foremost of which is asymptotic efficiency.
 For this reason, the ML estimator can serve as a benchmark against which
 other estimators may be measured.
 The ML estimator requires that the statistical model be fully specified,
 which essentially means that there is enough information to draw data from
 the DGP, given the parameter.
 This is a fairly strong requirement, and for this reason we need to be
 concerned about the possible misspecification of the statistical model.
 If this is the case, the ML estimator will not have the nice properties
 that it has under correct specification.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The likelihood function
\end_layout

\begin_layout Standard
Suppose we have a sample of size 
\begin_inset Formula $n$
\end_inset

 of the random vectors 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

.
 Suppose the joint density of 
\begin_inset Formula $Y=\left(\begin{array}{ccc}
y_{1} & \ldots & y_{n}\end{array}\right)$
\end_inset

 and 
\begin_inset Formula $Z=\left(\begin{array}{ccc}
z_{1} & \ldots & z_{n}\end{array}\right)$
\end_inset

 is characterized by a parameter vector 
\begin_inset Formula $\psi_{0}:$
\end_inset


\begin_inset Formula 
\[
f_{YZ}(Y,Z,\psi_{0}).
\]

\end_inset

 This is the joint density of the sample.
 The 
\emph on
likelihood
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
likelihood function
\end_layout

\end_inset

 function
\emph default
 is just this density evaluated at other values 
\begin_inset Formula $\psi$
\end_inset


\begin_inset Formula 
\[
L(Y,Z,\psi)=f(Y,Z,\psi),\psi\in\Psi,
\]

\end_inset

 where 
\begin_inset Formula $\Psi$
\end_inset

 is a 
\emph on
parameter
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
parameter space
\end_layout

\end_inset

 space.
\end_layout

\begin_layout Standard
The 
\emph on
maximum likelihood estimator
\emph default
 of 
\begin_inset Formula $\psi_{0}$
\end_inset

 is the value of 
\begin_inset Formula $\psi$
\end_inset

 that maximizes the likelihood function.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:Count-data.-Suppose"

\end_inset

Count data.
 Suppose we have a sample 
\begin_inset Formula $Y=\{y_{1},...,y_{n}\}$
\end_inset

 where the data are counts: the number of times some event occurs in a given
 interval of time, e.g., number of visits to the doctor in a year.
 The simplest count data density is the Poisson:
\begin_inset Formula 
\[
f_{Y}(y;\lambda)=\frac{e^{-\lambda}\lambda^{y}}{y!}
\]

\end_inset

If the observations are i.i.d.
 distributed according to this density, then the joint density of the sample
 is
\begin_inset Formula 
\[
L(\lambda)=\prod_{i=1}^{n}\frac{e^{-\lambda}\lambda^{y_{i}}}{y_{i}!}=\frac{e^{-n\lambda}\lambda^{\sum y_{i}}}{\prod_{i}y_{i}!}
\]

\end_inset

A little calculus and algebra shows us that the value that maximizes this
 is 
\begin_inset Formula $\hat{\lambda}=\bar{y}$
\end_inset

.
\end_layout

\begin_layout Exercise
Prove this last statement
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Count-data.-Suppose"

\end_inset

 we can compute the ML estimator without much trouble, and we have asymptotic
 theory that will allow us to test hypotheses about 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Itemize
however, now suppose that each observation has its own 
\begin_inset Formula $\lambda_{i}=\exp(x_{i}^{\prime}\beta)$
\end_inset

, which depends on conditioning variables and a new parameter vector.
 We can now write the likelihood function in terms of 
\begin_inset Formula $\beta$
\end_inset

 (as was done in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"

\end_inset

) 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The problem is, we can't find an analytic solution for the ML estimator
 of 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Itemize
Even if we could, the 
\begin_inset Formula $\hat{\beta}$
\end_inset

 which solves the f.o.c.
 is a nonlinear function of the data.
 How could we test hypotheses? The t and F tests developed for the classical
 linear model do not apply.
\end_layout

\begin_layout Itemize
To solve these two problems, we need the methods from Ch.
 11 and the theory from Ch.
 12.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Exogenous variables
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "engle1983exogeneity"
literal "false"

\end_inset

 is a good reference for this part.
 The likelihood function can be factored as 
\begin_inset Formula 
\[
f_{YZ}(Y,Z,\psi)=f_{Y|Z}(Y|Z,\theta)f_{Z}(Z,\rho)
\]

\end_inset

where 
\begin_inset Formula $\theta$
\end_inset

 are whatever elements of 
\begin_inset Formula $\psi$
\end_inset

 that happen to enter in the conditional density, and 
\begin_inset Formula $\rho$
\end_inset

 are the elements that enter into the marginal density.
\end_layout

\begin_layout Standard
Note that if 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\rho$
\end_inset

 share no elements, then the maximizer of the conditional likelihood function
 
\begin_inset Formula $f_{Y|Z}(Y|Z,\theta)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 is the same as the maximizer of the overall likelihood function 
\begin_inset Formula $f_{YZ}(Y,Z,\psi)=f_{Y|Z}(Y|Z,\theta)f_{Z}(Z,\rho)$
\end_inset

, for the elements of 
\begin_inset Formula $\psi$
\end_inset

 that correspond to 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
In this case, the variables 
\begin_inset Formula $Z$
\end_inset

 are said to be 
\emph on
exogenous
\emph default
 for estimation of 
\begin_inset Formula $\theta$
\end_inset

, and we may more conveniently work with the conditional likelihood function
 
\begin_inset Formula $f_{Y|Z}(Y|Z,\theta)$
\end_inset

 for the purposes of estimating 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
With exogeneity of 
\begin_inset Formula $Z$
\end_inset

, the maximum likelihood estimator of 
\begin_inset Formula $\theta_{0}$
\end_inset

 will be 
\begin_inset Formula $\arg\max f_{Y|Z}(Y|Z,\theta)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
We'll suppose this framework holds in what follows.
 If it didn't, for some variables in 
\begin_inset Formula $Z,$
\end_inset

 then just move those variables from 
\begin_inset Formula $Z$
\end_inset

 to 
\begin_inset Formula $Y,$
\end_inset

 until it does hold.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
A convenient factorization of the likelihood function
\end_layout

\begin_layout Itemize
If the 
\begin_inset Formula $n$
\end_inset

 observations are independent, the likelihood function can be written as
 
\begin_inset Formula 
\[
L(Y|Z,\theta)=\prod_{t=1}^{n}f(y_{t}|z_{t},\theta)
\]

\end_inset

 
\end_layout

\begin_layout Itemize
If this is not possible, we can always factor the likelihood into 
\emph on
contributions of observations,
\emph default
 by using the fact that a joint density can be factored into the product
 of a marginal and conditional.
\end_layout

\begin_layout Itemize
Then
\begin_inset Formula 
\[
\begin{array}{c}
\underbrace{f(y_{1,}y_{2},\ldots y_{n-1},y_{n}|Z,\theta)}\\
\mathrm{joint}
\end{array}=\begin{array}{c}
\underbrace{f(y_{n}|y_{1,}y_{2},\ldots y_{n-1},Z,\theta)}\\
\mathrm{conditional}
\end{array}\begin{array}{c}
\underbrace{f(y_{1,}y_{2},\ldots y_{n-1}|Z,\theta)}\\
\mathrm{marginal}
\end{array}
\]

\end_inset


\end_layout

\begin_layout Itemize
do the same thing for 
\begin_inset Formula $y_{n-1}$
\end_inset

 in the last term, and keep iterating.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Then, in the end, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(Y|Z,\theta)=f(y_{n}|y_{1,}y_{2},\ldots y_{n-1},Z,\theta)f(y_{n-1}|y_{1,}y_{2},\ldots y_{n-2},Z,\theta)\cdots f(y_{2}|y_{1},Z,\theta)f(y_{1}|Z,\theta)
\]

\end_inset


\end_layout

\begin_layout Standard
To simplify notation, define 
\begin_inset Formula 
\begin{eqnarray*}
x_{t} & = & \{y_{1},y_{2},...,y_{t-1},Z\}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Usually, for time series data, conditional densities depend only on current
 period exogenous variables, as the effects of lagged exogenous variables
 are transmitted though the realizations of the lagged endogenous variables,
 and economic models normally don't involve leads of exogenous variables.
 If this is the case, 
\begin_inset Formula $x_{1}=z_{1},$
\end_inset

 
\begin_inset Formula $x_{2}=\{y_{1},z_{2}\}$
\end_inset

, 
\emph on
etc
\emph default
.
 - it contains exogenous and predetermined endogenous variables.
 
\end_layout

\begin_layout Itemize
Regardless of the specific contents of 
\begin_inset Formula $x_{t},$
\end_inset

 the likelihood function can now be written as 
\begin_inset Formula 
\[
L(Y|Z;\theta)=\prod_{t=1}^{n}f(y_{t}|x_{t},\theta)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

The criterion function can be defined as the average log-likelihood function:
 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\ln L(Y|Z;\theta)=\frac{1}{n}\sum_{t=1}^{n}\ln f(y_{t}|x_{t};\theta)
\]

\end_inset

 The maximum likelihood estimator may thus be defined equivalently as 
\begin_inset Formula 
\[
\hat{\theta}=\arg\max s_{n}(\theta),
\]

\end_inset

 where the set maximized over is defined below.
 Since 
\begin_inset Formula $\ln(\cdot)$
\end_inset

 is a monotonic increasing function, 
\begin_inset Formula $\ln L$
\end_inset

 and 
\begin_inset Formula $L$
\end_inset

 maximize at the same value of 
\begin_inset Formula $\theta.$
\end_inset

 Dividing by 
\begin_inset Formula $n$
\end_inset

 has no effect on 
\begin_inset Formula $\hat{\theta}.$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
Question: why do we do it, then? 
\emph default
There are both theoretical and practical reasons:
\end_layout

\begin_deeper
\begin_layout Itemize
to get a LLN to apply
\end_layout

\begin_layout Itemize
to avoid loss of precision on a digital computer
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Example
Example: 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Example:-Bernoulli-trial"

\end_inset

Bernoulli trial
\begin_inset Newline newline
\end_inset

Suppose that we are flipping a coin that may be biased, so that the probability
 of a heads may not be 0.5.
 Maybe we're interested in estimating the probability of a heads.
 Let 
\begin_inset Formula $Y=1(heads)$
\end_inset

 be a binary variable that indicates whether or not a heads is observed.
 The outcome of a toss is a Bernoulli random variable:
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y,p_{0}) & = & p_{0}^{y}\left(1-p_{0}\right)^{1-y},y\in\{0,1\}\\
 & = & 0,y\notin\{0,1\}
\end{eqnarray*}

\end_inset

So a representative term that enters the likelihood function is
\begin_inset Formula 
\[
f_{Y}(y,p)=p^{y}\left(1-p\right)^{1-y}
\]

\end_inset

and
\begin_inset Formula 
\[
\ln f_{Y}(y,p)=y\ln p+\left(1-y\right)\ln\left(1-p\right)
\]

\end_inset

For this example, the average log-likelihood function is 
\begin_inset Formula $s_{n}(p)=\frac{1}{n}\sum_{t=1}^{n}y_{t}\ln p+\left(1-y_{t}\right)\ln\left(1-p\right)$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset Newpage newpage
\end_inset

The derivative of a representative term is
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\ln f_{Y}(y,p)}{\partial p} & = & \frac{y}{p}-\frac{\left(1-y\right)}{\left(1-p\right)}\\
 & = & \frac{y-p}{p\left(1-p\right)}
\end{eqnarray*}

\end_inset

so, averaging this over a sample of size 
\begin_inset Formula $n$
\end_inset

, the gradient is
\begin_inset Formula 
\[
\frac{\partial s_{n}(p)}{\partial p}=\frac{1}{n}\sum_{t=1}^{n}\frac{y_{t}-p}{p\left(1-p\right)}.
\]

\end_inset

Setting to zero and solving gives
\begin_inset Formula 
\begin{equation}
\hat{p}=\bar{y}\label{eq:mle Bernoulli}
\end{equation}

\end_inset

So it's easy to calculate the MLE of 
\begin_inset Formula $p_{0}$
\end_inset

 in this case.
\begin_inset Newpage newpage
\end_inset

 
\end_layout

\begin_layout Itemize
We also know that the sample mean converges to the true mean, from basic
 statistics.
 
\end_layout

\begin_layout Itemize
The mean is 
\begin_inset Formula $E(Y)=\sum_{y=0}^{y=1}yp_{0}^{y}\left(1-p_{0}\right)^{1-y}=p_{0}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Thus, the MLE, which is the sample mean, is a consistent estimator of the
 parameter, because the population mean is the parameter.
 
\end_layout

\begin_layout Itemize
For future reference, note that 
\begin_inset Formula $Var(Y)=E(Y^{2})-\left[E(Y)\right]^{2}=p_{0}-p_{0}^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

We see that the ML estimator is consistent, for this model.
 However, let's verify that the consistency theorem for extremum estimators
 gives us the same result:
\end_layout

\begin_layout Itemize
A LLN tells us that, for a given 
\begin_inset Formula $p,$
\end_inset

 the objective function converges to the limit of its expectation:
\begin_inset Formula 
\[
s_{n}(p)=\frac{1}{n}\sum_{t=1}^{n}y_{t}\ln p+\left(1-y_{t}\right)\ln\left(1-p\right)\rightarrow^{a.s.}p_{0}\ln p+(1-p_{0})\ln(1-p).
\]

\end_inset

 
\end_layout

\begin_layout Itemize
The parameter space must be compact.
 We know that 
\begin_inset Formula $p_{0}$
\end_inset

 lies between 0 and 1, so this helps set the parameter space.
\end_layout

\begin_layout Itemize
the objective function is obviously continuous in the parameter
\end_layout

\begin_layout Itemize
we need the objective function to be bounded, for the simple sufficient
 conditions for the consistency theorem to hold.
 So, we need to assume that 
\begin_inset Formula $p$
\end_inset

 can't go to 0 or to 1.
 This means that the parameter space must be a compact subset of 
\begin_inset Formula $\left(0,1\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
With these conditions, the a.s.
 convergence is also uniform.
 
\end_layout

\begin_layout Itemize
The consistency theorem for extremum estimators tells us that the ML estimator
 converges to the value that maximizes the limiting objective function.
 Because 
\begin_inset Formula $s_{\infty}(p)=p_{0}\ln p+(1-p_{0})\ln(1-p)$
\end_inset

, we can easily check that the unique maximizer is 
\begin_inset Formula $p_{0}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
So, the three assumptions of the consistency theorem hold, and thus the
 ML estimator is consistent for the true probability.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example

\emph on
\begin_inset CommandInset label
LatexCommand label
name "exa:Likelihood-function-of"

\end_inset

Likelihood function and MLE of classical linear regression model
\emph default
.
 Let's suppose that a dependent variable is normally distributed: 
\begin_inset Formula $y\sim N(\mu_{0},\sigma_{0}^{2})$
\end_inset

, so
\begin_inset Formula 
\[
f_{y}(y;\mu_{0},\sigma_{0}^{2})=\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}\exp\left(-\frac{(y-\mu_{0})^{2}}{2\sigma_{0}^{2}}\right)
\]

\end_inset

Suppose that the mean, 
\begin_inset Formula $\mu_{0}$
\end_inset

, depends on some regressors, 
\begin_inset Formula $x$
\end_inset

.
 The simplest way to do this is to assume that 
\begin_inset Formula $\mu_{0}=x^{\prime}\beta_{0}.$
\end_inset

 With this, the density, conditional on 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula 
\[
f_{y}(y|x;\beta_{0},\sigma_{0}^{2})=\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}\exp\left(-\frac{(y-x^{\prime}\beta_{0})^{2}}{2\sigma_{0}^{2}}\right)
\]

\end_inset

This is an example of 
\emph on
parameterization 
\emph default
of a density, making some parameters depend on additional variables and
 new parameters.
 With an i.i.d.
 sample of size 
\begin_inset Formula $n$
\end_inset

, the overall conditional density is the product of the conditional density
 of each observation: 
\begin_inset Formula 
\[
f_{y}(y_{1},y_{2},...,y_{n}|x_{1},x_{2},...,x;\beta_{0},\sigma_{0}^{2})=\prod_{t=1}^{n}\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}\exp\left(-\frac{(y_{t}-x_{t}^{\prime}\beta_{0})^{2}}{2\sigma_{0}^{2}}\right)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Taking logarithms, and evaluating at some point in the parameter space,
 we get the log-likelihood function: 
\begin_inset Formula 
\[
\ln L(Y|X;\beta,\sigma^{2})=-n\ln\sqrt{2\pi}-n\ln\sigma-\sum_{t=1}^{n}\frac{\left(y_{t}-x_{t}'\beta\right)^{2}}{2\sigma^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
Observe that the first order conditions for 
\begin_inset Formula $\beta$
\end_inset

 are the same as for the OLS estimator.
 For the 
\begin_inset Formula $\beta$
\end_inset

's, the OLS and ML estimators are identical.
\end_layout

\begin_layout Itemize
We know that the OLS estimator is consistent without making distributional
 assumptions regarding the errors.
 As long as the assumptions for consistency of OLS hold (fundamentally,
 weak exogeneity), then the 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimator will be consistent for 
\begin_inset Formula $\beta$
\end_inset

 as well, even if the normality assumption is not correct.
 This would be an example of 
\emph on
quasi-maximum likelihood
\emph default
 estimation: 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimation of a misspecified model.
 Sometimes the QML estimator is consistent, sometimes it's not.
\end_layout

\begin_layout Itemize
A Julia example that shows how to compute the maximum likelihood estimator
 for data that follows the CLRM with normality is in 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{NormalExample.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/MLE/NormalExample.jl} 
\end_layout

\end_inset

.
 Examine the code, and figure out how the likelihood function is defined.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Consistency of MLE
\end_layout

\begin_layout Itemize
The MLE is an extremum estimator, given basic assumptions it is consistent
 for the value that maximizes the limiting objective function, following
 Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

.
 
\end_layout

\begin_layout Itemize
The question is: what is the value that maximizes 
\begin_inset Formula $s_{\infty}(\theta)?$
\end_inset

 
\end_layout

\begin_layout Itemize
For two cases (Bernoulli trial and ML of the linear model with normality)
 we have seen that the ML estimator converges to the true parameter of the
 d.g.p.
\end_layout

\begin_layout Itemize
Is this a general result?
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Remember that 
\begin_inset Formula $s_{n}(\theta)=\frac{1}{n}\ln L(Y|Z,\theta)$
\end_inset

, and 
\begin_inset Formula $L(Y|Z,\theta_{0})$
\end_inset

 is the true density of the sample data.
 For any 
\begin_inset Formula $\theta\neq\theta_{0}$
\end_inset


\begin_inset Formula 
\[
\mathcal{E}\left(\ln\left(\frac{L(\theta)}{L(\theta_{0})}\right)\right)\leq\ln\left(\mathcal{E}\left(\frac{L(\theta)}{L(\theta_{0})}\right)\right)
\]

\end_inset

 by 
\begin_inset CommandInset href
LatexCommand href
name "Jensen's inequality"
target "http://en.wikipedia.org/wiki/Jensen's_inequality"
literal "false"

\end_inset

 ( 
\begin_inset Formula $\ln\left(\cdot\right)$
\end_inset

 is a concave function).
\end_layout

\begin_layout Standard
Now, the expectation on the RHS is 
\begin_inset Formula 
\[
\mathcal{E}\left(\frac{L(\theta)}{L(\theta_{0})}\right)=\int\frac{L(\theta)}{L(\theta_{0})}L(\theta_{0})dy=1,
\]

\end_inset

 since 
\begin_inset Formula $L(\theta_{0})$
\end_inset

 
\emph on
is
\emph default
 the density function of the observations, and since the integral of any
 density is 1
\begin_inset Formula $.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset

Therefore, since 
\begin_inset Formula $\ln(1)=0,$
\end_inset


\begin_inset Formula 
\[
\mathcal{E}\left(\ln\left(\frac{L(\theta)}{L(\theta_{0})}\right)\right)\leq0,
\]

\end_inset

 or 
\begin_inset Formula 
\begin{align*}
\mathcal{E}\left(s_{n}\left(\theta\right)\right)-\mathcal{E}\left(s_{n}\left(\theta_{0}\right)\right) & \leq0
\end{align*}

\end_inset

or
\begin_inset Formula 
\[
\mathcal{E}\left(s_{n}\left(\theta\right)\right)\leq\mathcal{E}\left(s_{n}\left(\theta_{0}\right)\right)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

Taking limits of each side:
\begin_inset Formula 
\[
s_{\infty}(\theta)\leq s_{\infty}(\theta_{0})
\]

\end_inset

except on a set of zero probability (by assumption b of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

).
 So the true parameter value is the maximizer of the limiting objective
 function (we are in Case 1 of the three cases discussed above - a fully
 correctly specified model).
\end_layout

\begin_layout Standard
If the identification assumption holds, then there is a unique maximizer,
 so the inequality is strict if 
\begin_inset Formula $\theta\neq\theta_{0}$
\end_inset

: 
\begin_inset Formula 
\[
s_{\infty}(\theta)<s_{\infty}(\theta_{0}),\forall\theta\neq\theta_{0},\textnormal{a.s.}
\]

\end_inset

Therefore, 
\begin_inset Formula $\theta_{0}$
\end_inset

 is the unique maximizer of 
\begin_inset Formula $s_{\infty}(\theta),$
\end_inset

 and thus, Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

 tells us that 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\hat{\theta}=\theta_{0},\textrm{ a}.\textrm{s}.
\]

\end_inset

 So, the ML estimator is consistent for the true parameter value.
 In practice, we will need to check identification for the specific model
 under consideration.
\end_layout

\begin_layout Exercise
Verify consistency of the ML estimator of the CLRM with normality by increasing
 
\begin_inset Formula $n$
\end_inset

 in the example 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{NormalExample.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/MLE/NormalExample.jl} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The score function
\end_layout

\begin_layout Description
\noindent
Assumption: (Differentiability) Assume that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is twice continuously differentiable in a neighborhood 
\begin_inset Formula $N(\theta_{0})$
\end_inset

 of 
\begin_inset Formula $\theta_{0}$
\end_inset

, at least when 
\begin_inset Formula $n$
\end_inset

 is large enough.
 
\end_layout

\begin_layout Itemize
with this, and with the result on consistency from above, assumptions (a)
 and (b) of theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

 hold, and 
\begin_inset Formula $\mathcal{J}_{n}(\hat{\theta})\stackrel{a.s.}{\rightarrow}\mathcal{J}_{\infty}(\theta^{0})$
\end_inset


\end_layout

\begin_layout Standard
To maximize the log-likelihood function, take derivatives: 
\begin_inset Formula 
\begin{eqnarray}
g_{n}(Y|Z,\theta) & \equiv & D_{\theta}s_{n}(\theta)\nonumber \\
 & = & \frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f(y_{t}|x_{t},\theta)\label{eq:MLscore}\\
 & \equiv & \frac{1}{n}\sum_{t=1}^{n}g_{t}(\theta).\nonumber 
\end{eqnarray}

\end_inset

This is the 
\emph on
score vector
\emph default
 (with dim 
\begin_inset Formula $K\times1).$
\end_inset

 Note that the score function has 
\begin_inset Formula $Y\;$
\end_inset

as an argument, which implies that it is a random function.
 
\begin_inset Formula $Y$
\end_inset

 (and any exogeneous variables) will often be suppressed for clarity, but
 one should not forget that they are still there.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

The ML estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 sets the derivatives to zero: 
\begin_inset Formula 
\[
g_{n}(\hat{\theta})=\frac{1}{n}\sum_{t=1}^{n}g_{t}(\hat{\theta})\equiv0.
\]

\end_inset


\end_layout

\begin_layout Standard
We will show that 
\begin_inset Formula $E_{\theta}\left[g_{t}(\theta)|x_{t}\right]=0,$
\end_inset

 
\begin_inset Formula $\forall t.$
\end_inset

 
\emph on
This is the expectation taken with respect to the density
\emph default
 
\begin_inset Formula $f(y_{t}|x_{t},\theta),$
\end_inset

 not necessarily 
\begin_inset Formula $f\left(y_{t}|x_{t},\theta_{0}\right).$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
E_{\theta}\left[g_{t}(\theta)|x_{t}\right] & = & \int[D_{\theta}\ln f(y_{t}|x_{t},\theta)]f(y_{t}|x,\theta)dy_{t}\\
 & = & \int\frac{1}{f(y_{t}|x_{t},\theta)}\left[D_{\theta}f(y_{t}|x_{t},\theta)\right]f(y_{t}|x_{t},\theta)dy_{t}\\
 & = & \int D_{\theta}f(y_{t}|x_{t},\theta)dy_{t}.
\end{eqnarray*}

\end_inset

 Given some regularity conditions on boundedness of 
\begin_inset Formula $D_{\theta}f,$
\end_inset

 we can switch the order of integration and differentiation, by the dominated
 convergence theorem.
 This gives 
\begin_inset Formula 
\begin{eqnarray}
E_{\theta}\left[g_{t}(\theta)|x_{t}\right] & = & D_{\theta}\int f(y_{t}|x_{t},\theta)dy_{t}\label{eq:ExpectationScore}\\
 & = & D_{\theta}1\nonumber \\
 & = & 0\nonumber 
\end{eqnarray}

\end_inset

where we use the fact that the integral of the density is 1.
\end_layout

\begin_layout Itemize
So 
\begin_inset Formula $E_{\theta}(g_{t}(\theta)|x_{t})=0:$
\end_inset

 
\emph on
the conditional expectation of the score vector is zero
\emph default
.
 Because this is true for all 
\begin_inset Formula $x_{t},$
\end_inset

 the unconditional expectation is also zero.
\end_layout

\begin_layout Itemize
This hold for all 
\begin_inset Formula $t,$
\end_inset

 so it implies that 
\begin_inset Formula $\mathcal{E}_{\theta}g_{n}(Y|Z,\theta)=0.$
\end_inset


\end_layout

\begin_layout Itemize
This result allows us to show that 
\begin_inset Formula $E_{\theta}(g_{t}(y_{t}|x_{t},\theta)g_{t-s}(y_{t-s}|x_{t-s},\theta)^{\prime})=0$
\end_inset

: the score contributions are uncorrelated.
 This comes from the fact that all random variables in 
\begin_inset Formula $g_{t-s}(y_{t-s}|x_{t-s},\theta)$
\end_inset

 are in the information set 
\begin_inset Formula $x_{t}$
\end_inset

.
 This concept is explained in detail in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Estimation-using-conditional"

\end_inset

, for now let's just accept it.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Asymptotic normality of MLE
\end_layout

\begin_layout Standard
Recall that we assume that the log-likelihood function 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is twice continuously differentiable.
 Take a first order Taylor's series expansion of 
\begin_inset Formula $g(Y,\hat{\theta})$
\end_inset

 about the true value 
\begin_inset Formula $\theta_{0}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
0 & \equiv g(\hat{\theta})= & g(\theta_{0})+\left(D_{\theta^{\prime}}g(\theta^{*})\right)\left(\hat{\theta}-\theta_{0}\right)
\end{eqnarray*}

\end_inset

 or with appropriate definitions
\begin_inset Formula 
\[
\mathcal{J}(\theta^{*})\left(\hat{\theta}-\theta_{0}\right)=-g(\theta_{0}),
\]

\end_inset

where 
\begin_inset Formula $\theta^{*}=\lambda\hat{\theta}+(1-\lambda)\theta_{0},0<\lambda<1.$
\end_inset

 Assume 
\begin_inset Formula $\mathcal{J}(\theta^{*})$
\end_inset

 is invertible (we'll justify this in a minute).
 So 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)=-\mathcal{J}(\theta^{*})^{-1}\sqrt{n}g(\theta_{0})\label{eq:TSexpansionMLgradient}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Now consider 
\begin_inset Formula $\mathcal{J}(\theta^{*}),$
\end_inset

 the matrix of second derivatives of the average log likelihood function.
 This is 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{J}(\theta^{*}) & = & D_{\theta^{\prime}}g(\theta^{*})\\
 & = & D_{\theta}^{2}s_{n}(\theta^{*})\\
 & = & \frac{1}{n}\sum_{t=1}^{n}D_{\theta}^{2}\ln f_{t}(\theta^{*})
\end{eqnarray*}

\end_inset

 where the notation 
\begin_inset Formula 
\[
D_{\theta}^{2}s_{n}(\theta^{*})\equiv\left.\frac{\partial^{2}s_{n}(\theta)}{\partial\theta\partial\theta^{\prime}}\right|_{\theta=\theta^{*}}.
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Given that this is an average of terms, it should usually be the case that
 this satisfies a strong law of large numbers (SLLN).
 
\end_layout

\begin_layout Itemize

\emph on
Regularity conditions
\emph default
 are a set of assumptions that guarantee that this will happen.
 There are different sets of assumptions that can be used to justify appeal
 to different SLLN's.
 For example, the 
\begin_inset Formula $D_{\theta}^{2}\ln f_{t}(\theta^{*})$
\end_inset

 must not be too strongly dependent over time, and their variances must
 not become infinite.
 We don't assume any particular set here, since the appropriate assumptions
 will depend upon the particularities of a given model.
 However, we assume that a SLLN
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

applies.
\end_layout

\begin_layout Standard
Also, since we know that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is consistent, and since 
\begin_inset Formula $\theta^{*}=\lambda\hat{\theta}+(1-\lambda)\theta_{0},$
\end_inset

 we have that 
\begin_inset Formula $\theta^{*}{\overset{a.s.}{\rightarrow}\theta_{0}}$
\end_inset

.
 Also, by the above differentiability assumption, 
\begin_inset Formula $\mathcal{J}(\theta)$
\end_inset

 is continuous in 
\begin_inset Formula $\theta$
\end_inset

.
 Given this, 
\begin_inset Formula $\mathcal{J}(\theta^{*})$
\end_inset

 converges to the limit of its expectation: 
\begin_inset Formula 
\[
\mathcal{J}(\theta^{*})\overset{a.s.}{\rightarrow}\lim_{n\rightarrow\infty}\mathcal{E}\left(D_{\theta}^{2}s_{n}(\theta_{0})\right)=\mathcal{J}_{\infty}(\theta_{0})<\infty
\]

\end_inset

 
\emph on
This matrix converges to a finite limit.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Re-arranging orders of limits and differentiation, which is legitimate given
 certain regularity conditions related to the boundedness of the log-likelihood
 function, we get 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{J}_{\infty}(\theta_{0}) & = & D_{\theta}^{2}\lim_{n\rightarrow\infty}\mathcal{E}\left(s_{n}(\theta_{0})\right)\\
 & = & D_{\theta}^{2}s_{\infty}(\theta_{0},\theta_{0})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We've already seen that
\begin_inset Formula 
\[
s_{\infty}(\theta,\theta_{0})<s_{\infty}(\theta_{0},\theta_{0})
\]

\end_inset

 
\emph on
i.e.,
\emph default
 
\begin_inset Formula $\theta_{0}$
\end_inset

 maximizes the limiting objective function.
 Since there is a unique maximizer, and by the assumption that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is twice continuously differentiable (which holds in the limit), then 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 must be negative definite, and therefore of full rank.
 Therefore the previous inversion is justified, asymptotically, and we have
 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)=-\mathcal{J}(\theta^{*})^{-1}\sqrt{n}g(\theta_{0}).\label{anmle}
\end{equation}

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Now consider 
\begin_inset Formula $\sqrt{n}g(\theta_{0}).$
\end_inset

 For assumption (c) of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

 to apply, this quantity must follow a Central Limit Theorem.
 We have 
\begin_inset Formula 
\begin{eqnarray*}
\sqrt{n}g_{n}(\theta_{0}) & = & \sqrt{n}D_{\theta}s_{n}(\theta)\\
 & = & \sqrt{n}\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f_{t}(y_{t}|x_{t},\theta_{0})\\
 & = & \sqrt{n}\frac{1}{n}\sum_{t=1}^{n}g_{t}(\theta_{0})
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Itemize
We've already seen that 
\begin_inset Formula $\mathcal{E}_{\theta}\left[g_{t}(\theta)\right]=0,$
\end_inset

 for all 
\begin_inset Formula $\theta,$
\end_inset

 and, thus, for 
\begin_inset Formula $\theta_{0}$
\end_inset

, too.
 Thus, the last line, without scaling by 
\begin_inset Formula $\sqrt{n}$
\end_inset

, would converge almost surely to zero, by a LLN (assuming the variances
 of the 
\begin_inset Formula $g_{t}$
\end_inset

 are bounded).
 To get a stable limiting distribution, we need to multiply by something
 that is tending to infinity, at the proper rate.
 It turns out that 
\begin_inset Formula $\sqrt{n}$
\end_inset

 is the quantity that fits the bill (see a proof of a CLT to understand
 why).
\end_layout

\begin_layout Itemize
Also, the elements of the sum are uncorrelated with one another (discussed
 below).
\end_layout

\begin_layout Itemize
As long as the 
\begin_inset Formula $g_{t}$
\end_inset

 have finite variances, a CLT
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

 will apply.
 Checking this requires knowing what specific model we're working with,
 so for this general treatment, we will assume that the model satisfies
 this condition.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Assuming that a CLT applies: 
\begin_inset Formula 
\begin{equation}
\sqrt{n}g_{n}(\theta_{0})\overset{d}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta_{0})\right]\label{eq:asymptoticnormalityofscores}
\end{equation}

\end_inset

 where 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta_{0}) & = & \lim_{n\rightarrow\infty}\mathcal{E}_{\theta_{0}}\left(n\left[g_{n}(\theta_{0})\right]\left[g_{n}(\theta_{0})\right]^{\prime}\right)\\
 & = & \lim_{n\rightarrow\infty}V_{\theta_{0}}\left(\sqrt{n}g_{n}(\theta_{0})\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

 is known as the 
\emph on
information matrix
\emph default
.
 It is the asymptotic variance of the score vector
\end_layout

\begin_layout Itemize
From the previous expression 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)=-\mathcal{J}(\theta^{*})^{-1}\sqrt{n}g(\theta_{0})$
\end_inset

, and noting that 
\begin_inset Formula $\mathcal{J}(\theta^{*})\overset{a.s.}{\rightarrow}\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

, we see that, following the 
\begin_inset CommandInset href
LatexCommand href
name "Slutsky theorem"
target "https://en.wikipedia.org/wiki/Slutsky%27s_theorem"
literal "false"

\end_inset


\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\rightarrow^{d}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right].
\]

\end_inset

 
\emph on
The MLE
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is asymptotically normally distributed.
\end_layout

\begin_layout Itemize
The form of this expression is the same as what we saw for extremum estimators
 in general.
 This is no surprise, as the MLE is an extremum estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The information matrix equality
\end_layout

\begin_layout Standard
We will show that 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta)=-\mathcal{I}_{\infty}(\theta).$
\end_inset

 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f_{t}(\theta)$
\end_inset

 be short for 
\begin_inset Formula $f(y_{t}|x_{t},\theta)$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
1 & = & \int f_{t}(\theta)dy,\textrm{ so}\\
0 & = & \int D_{\theta}f_{t}(\theta)dy\\
 & = & \int\left(D_{\theta}\ln f_{t}(\theta)\right)f_{t}(\theta)dy
\end{eqnarray*}

\end_inset

 Now differentiate again: (Note for lectures: in the second line, start
 with the term in 
\begin_inset Formula $\left\{ \right\} $
\end_inset

 and show you get what's in the first line) 
\begin_inset Formula 
\begin{eqnarray}
0 & = & \int\left[D_{\theta}^{2}\ln f_{t}(\theta)\right]f_{t}(\theta)dy+\int\left[D_{\theta}\ln f_{t}(\theta)\right]\left\{ D_{\theta^{\prime}}f_{t}(\theta)\right\} dy\nonumber \\
 & = & \mathcal{E}_{\theta}\left[D_{\theta}^{2}\ln f_{t}(\theta)\right]+\int\left[D_{\theta}\ln f_{t}(\theta)\right]\left\{ \left[D_{\theta^{\prime}}\ln f_{t}(\theta)\right]f_{t}(\theta)\right\} dy\nonumber \\
 & = & \mathcal{E}_{\theta}\left[D_{\theta}^{2}\ln f_{t}(\theta)\right]+\mathcal{E}_{\theta}\left[D_{\theta}\ln f_{t}(\theta)\right]\left[D_{\theta^{\prime}}\ln f_{t}(\theta)\right]\nonumber \\
 & = & \mathcal{E}_{\theta}\left[\mathcal{J}_{t}(\theta)\right]+\mathcal{E}_{\theta}\left[g_{t}(\theta)\right]\left[g_{t}(\theta)\right]^{\prime}\label{informationmatrixequality,singleobservation}
\end{eqnarray}

\end_inset


\begin_inset Newpage newpage
\end_inset

 Now sum over 
\begin_inset Formula $n$
\end_inset

 and multiply by 
\begin_inset Formula $\frac{1}{n}$
\end_inset


\begin_inset Formula 
\begin{equation}
\mathcal{E}_{\theta}\frac{1}{n}\sum_{t=1}^{n}\left[\mathcal{J}_{t}(\theta)\right]=-\mathcal{E}_{\theta}\left[\frac{1}{n}\sum_{t=1}^{n}\left[g_{t}(\theta)\right]\left[g_{t}(\theta)\right]^{\prime}\right]\label{eq:outerproductsocrecontribs}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Itemize
The scores 
\begin_inset Formula $g_{t}$
\end_inset

 and 
\begin_inset Formula $g_{s}$
\end_inset

 are uncorrelated for 
\begin_inset Formula $t\neq s,$
\end_inset

 since for 
\begin_inset Formula $t>s,$
\end_inset

 
\begin_inset Formula $f_{t}(y_{t}|y_{1},...,y_{t-1},\theta)$
\end_inset

 has conditioned on prior information, so what was random in 
\begin_inset Formula $s$
\end_inset

 is fixed in 
\begin_inset Formula $t$
\end_inset

.
 (This forms the basis for a specification test proposed by White:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

if the scores appear to be correlated one may question the specification
 of the model).
 This allows us to write:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{E}_{\theta}\left[\mathcal{J}_{n}(\theta)\right]=-\mathcal{E}_{\theta}\left(n\left[g(\theta)\right]\left[g(\theta)\right]^{\prime}\right)
\]

\end_inset

 since all cross products between different periods expect to zero.
 Finally take limits, we get 
\begin_inset Formula 
\begin{equation}
\mathcal{J}_{\infty}(\theta)=-\mathcal{I}_{\infty}(\theta).\label{information matrix equality}
\end{equation}

\end_inset

 This holds for all 
\begin_inset Formula $\theta,$
\end_inset

 in particular, for 
\begin_inset Formula $\theta_{0}.$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset

Using this, 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a.s.}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta_{0})^{-1}\mathcal{I}_{\infty}(\theta_{0})\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]
\]

\end_inset

 simplifies to 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a.s.}{\rightarrow}N\left[0,\mathcal{I}_{\infty}(\theta_{0})^{-1}\right]\label{Simple MLE asymp. cov.}
\end{equation}

\end_inset

or
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a.s.}{\rightarrow}N\left[0,-\mathcal{J}_{\infty}(\theta_{0})^{-1}\right]\label{Simple MLE asymp. cov.-1}
\end{equation}

\end_inset


\begin_inset Newpage pagebreak
\end_inset

 To estimate the asymptotic variance, we need estimators of 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})$
\end_inset

.
 We can use 
\begin_inset Formula 
\begin{eqnarray*}
\widehat{\mathcal{I}_{\infty}(\theta_{0})} & = & \frac{1}{n}\sum_{t=1}^{n}g_{t}(\hat{\theta})g_{t}(\hat{\theta})^{\prime}\\
\widehat{\mathcal{J}_{\infty}(\theta_{0})} & = & \mathcal{J}_{n}(\hat{\theta}).
\end{eqnarray*}

\end_inset

as is intuitive if one considers equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:outerproductsocrecontribs"

\end_inset

.
 Note, one can't use
\begin_inset Formula 
\[
\widehat{I_{\infty}(\theta_{0})}=n\left[g_{n}(\hat{\theta})\right]\left[g_{n}(\hat{\theta})\right]^{\prime}
\]

\end_inset

to estimate the information matrix.
 Why not?
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
From this we see that there are alternative ways to estimate 
\begin_inset Formula $V_{\infty}(\theta_{0})$
\end_inset

 that are all valid.
 These include 
\begin_inset Formula 
\begin{eqnarray*}
\widehat{V_{\infty}(\theta_{0})} & = & -\widehat{\mathcal{J}_{\infty}(\theta_{0})}^{-1}\\
\widehat{V_{\infty}(\theta_{0})} & = & \widehat{\mathcal{I}_{\infty}(\theta_{0})}^{-1}\\
\widehat{V_{\infty}(\theta_{0})} & = & \widehat{\mathcal{J}_{\infty}(\theta_{0})}^{-1}\widehat{\mathcal{I}_{\infty}(\theta_{0})}\widehat{\mathcal{J}_{\infty}(\theta_{0})}^{-1}
\end{eqnarray*}

\end_inset

 These are known as the 
\emph on
inverse Hessian, outer product of the gradient
\emph default
 (OPG) and 
\emph on
sandwich
\emph default
 estimators, respectively.
 The sandwich form is the most robust, since it coincides with the covariance
 estimator of the 
\emph on
quasi-
\emph default
ML estimator.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
With a little more detail, the methods are:
\end_layout

\begin_layout Itemize
The sandwich version: 
\begin_inset Formula 
\[
\widehat{V_{\infty}}=n\left\{ \begin{array}{c}
\left\{ \sum_{t=1}^{n}D_{\theta}^{2}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right\} \times\\
\left\{ \sum_{t=1}^{n}\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]^{\prime}\right\} ^{-1}\times\\
\left\{ \sum_{t=1}^{n}D_{\theta}^{2}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right\} 
\end{array}\right\} ^{-1}
\]

\end_inset


\end_layout

\begin_layout Itemize
or the inverse of the negative of the Hessian: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widehat{V_{\infty}}=\left[-1/n\sum_{t=1}^{n}D_{\theta}^{2}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]^{-1},
\]

\end_inset


\end_layout

\begin_layout Itemize
or the inverse of the outer product of the gradient: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widehat{V_{\infty}}=\left\{ 1/n\sum_{t=1}^{n}\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]\left[D_{\theta}\ln f(y_{t}|Y_{t-1},\hat{\theta})\right]^{\prime}\right\} ^{-1}.
\]

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
This simplification is a special result for the MLE estimator - it doesn't
 apply to GMM estimators in general.
\end_layout

\begin_layout Itemize
Asymptotically, if the model is correctly specified, all of these forms
 converge to the same limit.
 In small samples they will differ.
 In particular, there is evidence that the outer product of the gradient
 formula does not perform very well in small samples (
\emph on
e.g., 
\emph default
see Davidson and MacKinnon, pg.
 477).
 
\end_layout

\begin_layout Itemize
White's 
\emph on
Information matrix test
\emph default
 (Econometrica, 1982) is based upon comparing the two ways to estimate the
 information matrix: outer product of gradient or negative of the Hessian.
 If they differ by too much, this is evidence of misspecification of the
 model.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Examine the code in 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimatePoisson.jl}{https://github.com/mcreel/Econometrics/blob/
master/Examples/MEPS-I/EstimatePoisson.jl}
\end_layout

\end_inset

 to figure out how the variance-covariance of the parameters has been estimated.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Alternative-variance-computation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 show the results using the sandwich and OPG forms.
 Note the radical changes in the t-statistics.
 This probably indicates that the Poisson model is not well-specified, following
 the logic of the information matrix test.
 When the t-statistics change so much, it means that the estimates of 
\begin_inset Formula $I_{\infty}$
\end_inset

 and 
\begin_inset Formula $-J_{\infty}$
\end_inset

 are far from one another.
\end_layout

\begin_layout Exercise
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Alternative-variance-computation"

\end_inset

Alternative variance computations for the OBDV Poisson model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/MLE/AlternativeVariances.svg
	width 15cm
	height 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example, Coin flipping, again
\begin_inset CommandInset label
LatexCommand label
name "subsec:Coin-flipping,-again"

\end_inset


\end_layout

\begin_layout Standard
In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Example:-Bernoulli-trial"

\end_inset

 we saw that the MLE for the parameter of a Bernoulli trial, with i.i.d.
 data, is the sample mean: 
\begin_inset Formula $\hat{p}=\bar{y}$
\end_inset

 (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mle Bernoulli"

\end_inset

).
 Now let's find the limiting variance of 
\begin_inset Formula $\sqrt{n}\left(\hat{p}-p_{0}\right)$
\end_inset

.
 We can do this in a simple way:
\begin_inset Formula 
\begin{eqnarray*}
\lim Var\sqrt{n}\left(\hat{p}-p_{0}\right) & = & \lim nVar\left(\hat{p}-p_{0}\right)\\
 & = & \lim nVar\left(\hat{p}\right)\\
 & = & \lim nVar\left(\bar{y}\right)\\
 & = & \lim nVar\left(\frac{\sum y_{t}}{n}\right)\\
 & = & \lim\frac{1}{n}\sum Var(y_{t})\textnormal{ (by independence of obs.)}\\
 & = & \lim\frac{1}{n}nVar(y)\textnormal{ (by identically distributed obs.)}\\
 & = & Var(y)\\
 & = & p_{0}\left(1-p_{0}\right)
\end{eqnarray*}

\end_inset

While that is simple, let's verify this using the methods of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "cha:Asymptotic-properties-of"

\end_inset

 give the same answer.
 The log-likelihood function is
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(p) & = & \frac{1}{n}\sum_{t=1}^{n}\left\{ y_{t}\ln p+\left(1-y_{t}\right)\ln\left(1-p\right)\right\} 
\end{eqnarray*}

\end_inset

so
\begin_inset Formula 
\[
Es_{n}(p)=p^{0}\ln p+\left(1-p^{0}\right)\ln\left(1-p\right)
\]

\end_inset

by the fact that the observations are i.i.d.
 Note that this does not depend on the sample size, 
\begin_inset Formula $n,$
\end_inset

 thus, 
\begin_inset Formula 
\[
s_{\infty}(p)=p^{0}\ln p+\left(1-p^{0}\right)\ln\left(1-p\right).
\]

\end_inset

 A bit of calculation shows that
\begin_inset Formula 
\[
\left.D_{\theta}^{2}s_{\infty}(p)\right|_{p=p^{0}}\equiv\mathcal{J}_{\infty}(p^{0})=\frac{-1}{p^{0}\left(1-p^{0}\right)}.
\]

\end_inset

By results we've seen on MLE, 
\begin_inset Formula $\lim Var\sqrt{n}\left(\hat{p}-p^{0}\right)=-\mathcal{J}_{\infty}^{-1}(p^{0}).$
\end_inset

 And in this case, 
\begin_inset Formula $-\mathcal{J}_{\infty}^{-1}(p^{0})=p^{0}\left(1-p^{0}\right)$
\end_inset

.
 So, we get the same limiting variance using both methods.
\end_layout

\begin_layout Exercise
For this example, find 
\begin_inset Formula $\mathcal{I}_{\infty}(p_{0})$
\end_inset

 by directly computing the covariance of the score vector, after scaling
 by 
\begin_inset Formula $\sqrt{n}$
\end_inset

.
 By the information matrix equality, you already know what the answer should
 be, but verify that your computations give you the same result.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The Cramr-Rao lower bound
\end_layout

\begin_layout Definition
Consistent and asymptotically normal (CAN).
 
\begin_inset CommandInset label
LatexCommand label
name "def:CAN"

\end_inset

An estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 of a parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

 is 
\begin_inset Formula $\sqrt{n}$
\end_inset

-consistent and asymptotically normally distributed if 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{d}{\rightarrow}N\left(0,V_{\infty}\right)$
\end_inset

 where 
\begin_inset Formula $V_{\infty}$
\end_inset

 is a finite positive definite matrix.
\end_layout

\begin_layout Standard
There do exist, in special cases, estimators that are consistent such that
 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{p}{\rightarrow}0.$
\end_inset

 These are known as 
\emph on
superconsistent
\emph default
 estimators, since in ordinary circumstances with stationary data, 
\begin_inset Formula $\sqrt{n}$
\end_inset

 is the highest factor that we can multiply by and still get convergence
 to a stable limiting distribution.
 
\end_layout

\begin_layout Definition
Asymptotically unbiased.
 An estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 of a parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

 is asymptotically unbiased if 
\end_layout

\begin_layout Standard
\begin_inset Formula $\lim_{n\rightarrow\infty}\mathcal{E_{\theta}}(\hat{\theta})=\theta$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Estimators that are CAN are asymptotically unbiased
\emph default
, though not all consistent estimators are asymptotically unbiased.
 Such cases are unusual, though.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Theorem

\emph on
[Cramer-Rao Lower Bound]
\emph default
 
\begin_inset CommandInset label
LatexCommand label
name "[Cramer-Rao-Lower-Bound]"

\end_inset

The limiting variance of a CAN estimator of 
\begin_inset Formula $\theta_{0}$
\end_inset

, say 
\begin_inset Formula $\tilde{\theta}$
\end_inset

, minus the inverse of the information matrix is a positive semidefinite
 matrix.
\end_layout

\begin_layout Theorem
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Proof: Since the estimator is CAN, it is asymptotically unbiased, so 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}_{\theta}(\tilde{\theta}-\theta)=0
\]

\end_inset

 Differentiate wrt 
\begin_inset Formula $\theta^{\prime}:$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
D_{\theta^{\prime}}\lim_{n\rightarrow\infty}\mathcal{E}_{\theta}(\tilde{\theta}-\theta) & = & \lim_{n\rightarrow\infty}\int D_{\theta^{\prime}}\left[f(Y,\theta)\left(\tilde{\theta}-\theta\right)\right]dy\\
 & = & 0.\textrm{ }
\end{eqnarray*}

\end_inset

(The RHS is zero, because we're differentiating something that is zero to
 start with).
 Noting that 
\begin_inset Formula $D_{\theta^{\prime}}f(Y,\theta)=f(\theta)D_{\theta^{\prime}}\ln f(\theta)$
\end_inset

 (a trick we have seen a few times already), and applying the product rule
 to differentiate the two parts that depend on 
\begin_inset Formula $\theta,$
\end_inset

 we can write 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\int\left(\tilde{\theta}-\theta\right)f(\theta)D_{\theta^{\prime}}\ln f(\theta)dy+\lim_{n\rightarrow\infty}\int f(Y,\theta)D_{\theta^{\prime}}\left(\tilde{\theta}-\theta\right)dy=0.
\]

\end_inset

 Now note that 
\begin_inset Formula $D_{\theta^{\prime}}\left(\tilde{\theta}-\theta\right)=-I_{K},$
\end_inset

 and 
\begin_inset Formula $\int f(Y,\theta)(-I_{K})dy=-I_{K}.$
\end_inset

 With this we have 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\int\left(\tilde{\theta}-\theta\right)f(\theta)D_{\theta^{\prime}}\ln f(\theta)dy=I_{K}.
\]

\end_inset

 Playing with powers of 
\begin_inset Formula $n$
\end_inset

 we get 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}\int\sqrt{n}\left(\tilde{\theta}-\theta\right)\sqrt{n}\underbrace{\frac{1}{n}\left[D_{\theta^{\prime}}\ln f(\theta)\right]}f(\theta)dy & = & I_{K}
\end{eqnarray*}

\end_inset

 Note that the bracketed part is just the transpose of the score vector,
 
\begin_inset Formula $g(\theta),$
\end_inset

 so we can write 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\mathcal{E}_{\theta}\left[\sqrt{n}\left(\tilde{\theta}-\theta\right)\sqrt{n}g(\theta)^{\prime}\right]=I_{K}
\]

\end_inset

 This means that the limiting covariance of the score function with 
\begin_inset Formula $\sqrt{n}\left(\tilde{\theta}-\theta\right),$
\end_inset

 for 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 any CAN
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator, is an identity matrix.
 Using this, suppose the variance of 
\begin_inset Formula $\sqrt{n}\left(\tilde{\theta}-\theta\right)$
\end_inset

 tends to 
\begin_inset Formula $V_{\infty}(\tilde{\theta}).$
\end_inset

 Therefore, 
\begin_inset Formula 
\begin{equation}
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}g(\theta)
\end{array}\right]=\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right].\label{Cov. CAN and MLE score}
\end{equation}

\end_inset

 Since this is a covariance matrix, it is positive semi-definite.
 Therefore, for any 
\begin_inset Formula $K$
\end_inset

 -vector 
\begin_inset Formula $\alpha,$
\end_inset


\begin_inset Formula 
\[
\left[\begin{array}{cc}
\alpha^{\prime} & -\alpha^{\prime}\mathcal{I}_{\infty}^{-1}(\theta)\end{array}\right]\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right]\left[\begin{array}{c}
\alpha\\
-\mathcal{I}_{\infty}(\theta)^{-1}\alpha
\end{array}\right]\geq0.
\]

\end_inset

 This simplifies to 
\begin_inset Formula 
\[
\alpha^{\prime}\left[V_{\infty}(\tilde{\theta})-\mathcal{I}_{\infty}^{-1}(\theta)\right]\alpha\geq0.
\]

\end_inset

 Since 
\begin_inset Formula $\alpha$
\end_inset

 is arbitrary, 
\begin_inset Formula $V_{\infty}(\tilde{\theta})-\mathcal{I}_{\infty}^{-1}(\theta)$
\end_inset

 must be positive semidefinite.
 This concludes the proof.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
Interpretation:
\end_layout

\begin_layout Itemize
any linear combination of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 will have an asymptotic variance greater than or equal to the same linear
 combination of the ML estimator.
\end_layout

\begin_layout Itemize

\emph on
e.g., 
\emph default
the individual variances of each 
\begin_inset Formula $\tilde{\theta_{j}}$
\end_inset

 will be no smaller than the corresponding element of the ML estimator,
 
\begin_inset Formula $j=1,2,...,k$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathcal{I}_{\infty}^{-1}(\theta)$
\end_inset

 is a 
\emph on
lower bound
\emph default
 for the asymptotic variance of a CAN
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
(
\emph on
Asymptotic efficiency
\emph default
) Given two CAN estimators of a parameter 
\begin_inset Formula $\theta_{0}$
\end_inset

, say 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}$
\end_inset

, 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is asymptotically efficient with respect to 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 if 
\begin_inset Formula $V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})$
\end_inset

 is a positive semidefinite matrix.
\end_layout

\begin_layout Itemize

\emph on
the MLE is asymptotically efficient with respect to any other CAN estimator.
\end_layout

\begin_layout Itemize
this is the reason that the ML estimator is so important: it provides a
 benchmark for efficiency.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The strong assumptions on which it depends may be questionable, though.
\end_layout

\begin_layout Itemize
If we can find another estimator that obtains an asymptotic variance similar
 to that of ML, but is consistent under weaker assumptions, we might choose
 to use it instead.
 But it's useful to know that this entails a potential loss of efficiency.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Section
Likelihood ratio-type tests
\end_layout

\begin_layout Standard
Suppose we would like to test a set of 
\begin_inset Formula $q$
\end_inset

 possibly nonlinear restrictions 
\begin_inset Formula $r(\theta)=0,$
\end_inset

 where the 
\begin_inset Formula $q\times k$
\end_inset

 matrix 
\begin_inset Formula $D_{\theta^{\prime}}r(\theta)$
\end_inset

 has rank 
\begin_inset Formula $q$
\end_inset

.
 The Wald test can be calculated using the unrestricted model.
 The score test can be calculated using only the restricted model.
 The likelihood ratio test, on the other hand, uses both the restricted
 and the unrestricted estimators.
 The test statistic is 
\begin_inset Formula 
\[
LR=2\left(\ln L(\hat{\theta})-\ln L(\tilde{\theta})\right)=2n\left[s_{n}(\hat{\theta})-s_{n}(\tilde{\theta})\right]
\]

\end_inset

where 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the unrestricted estimate and 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is the restricted estimate.
 We will show that, under the null hypothesis that 
\begin_inset Formula $r(\theta)=0$
\end_inset

, 
\begin_inset Formula 
\[
LR\overset{d}{\rightarrow}\chi^{2}(q).
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

To show that it is asymptotically 
\begin_inset Formula $\chi^{2},$
\end_inset

 take a second order Taylor's series expansion of 
\begin_inset Formula $\ln L(\tilde{\theta})$
\end_inset

 about 
\begin_inset Formula $\hat{\theta}:$
\end_inset


\begin_inset Formula 
\[
\ln L(\tilde{\theta})\simeq\ln L(\hat{\theta})+\frac{n}{2}\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\mathcal{J}(\hat{\theta})\left(\tilde{\theta}-\hat{\theta}\right)
\]

\end_inset

 (note, the first order term drops out since 
\begin_inset Formula $D_{\theta}\ln L(\hat{\theta})\equiv0$
\end_inset

 by the first order necessary conditions, and we need to multiply the second-ord
er term by 
\begin_inset Formula $n$
\end_inset

 since 
\begin_inset Formula $\mathcal{J}(\theta)$
\end_inset

 is defined in terms of 
\begin_inset Formula $\frac{1}{n}\ln L(\theta)$
\end_inset

) so 
\begin_inset Formula 
\[
LR\simeq-n\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\mathcal{J}(\hat{\theta})\left(\tilde{\theta}-\hat{\theta}\right)
\]

\end_inset

 As 
\begin_inset Formula $n\rightarrow\infty,\mathcal{J}(\hat{\theta})\rightarrow\mathcal{J}_{\infty}(\theta_{0})=-\mathcal{I}(\theta_{0}),$
\end_inset

 by the information matrix equality.
 So 
\begin_inset Formula 
\begin{equation}
LR\overset{a}{=}n\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\mathcal{I}_{\infty}(\theta_{0})\left(\tilde{\theta}-\hat{\theta}\right)\label{eq:LR2}
\end{equation}

\end_inset

 We also have that, from the theory on the asymptotic normality of the MLE
 and the information matrix equality 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{a}{=}\mathcal{I}_{\infty}(\theta_{0})^{-1}n^{1/2}g(\theta_{0}).
\]

\end_inset

An analogous result for the restricted estimator is (this is unproven here,
 to prove this set up the Lagrangean for MLE subject to 
\begin_inset Formula $r(\theta)=0,$
\end_inset

 and manipulate the first order conditions.
 Also note, 
\begin_inset Formula $R$
\end_inset

 is notation for 
\begin_inset Formula $R=D_{\theta}r^{\prime}(\theta)$
\end_inset

): 
\begin_inset Formula 
\[
\sqrt{n}\left(\tilde{\theta}-\theta_{0}\right)\overset{a}{=}\mathcal{I}_{\infty}(\theta_{0})^{-1}\left(I_{n}-R^{\prime}\left(R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\right)^{-1}R\mathcal{I}_{\infty}(\theta_{0})^{-1}\right)n^{1/2}g(\theta_{0}).
\]

\end_inset

 Combining the last two equations 
\begin_inset Formula 
\[
\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right)\overset{a}{=}-n^{1/2}\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\left(R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\right)^{-1}R\mathcal{I}_{\infty}(\theta_{0})^{-1}g(\theta_{0})
\]

\end_inset

 so, substituting into [
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LR2"

\end_inset

] 
\begin_inset Formula 
\[
LR\overset{a}{=}\left[n^{1/2}g(\theta_{0})^{\prime}\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\right]\left[R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}\right]^{-1}\left[R\mathcal{I}_{\infty}(\theta_{0})^{-1}n^{1/2}g(\theta_{0})\right]
\]

\end_inset

 But since 
\begin_inset Formula 
\[
n^{1/2}g(\theta_{0})\overset{d}{\rightarrow}N\left(0,\mathcal{I}_{\infty}(\theta_{0})\right)
\]

\end_inset

 the linear function 
\begin_inset Formula 
\[
R\mathcal{I}_{\infty}(\theta_{0})^{-1}n^{1/2}g(\theta_{0})\overset{d}{\rightarrow}N(0,R\mathcal{I}_{\infty}(\theta_{0})^{-1}R^{\prime}).
\]

\end_inset

 We can see that LR is a quadratic form of this rv, with the inverse of
 its variance in the middle, so, by the Continuous Mapping Theorem (also
 known as the Mann-Wald Theorem) (see 
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "true"

\end_inset

 Theorem 4.7 for a statement):
\begin_inset Formula 
\[
LR\overset{d}{\rightarrow}\chi^{2}(q).
\]

\end_inset


\end_layout

\begin_layout Example

\emph on
Likelihood ratio test
\emph default
.
 Continuing with the same code as was used in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Likelihood-function-of"

\end_inset

, 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{LikelihoodRatioTest.jl}{https://github.com/mcreel/Econometrics/b
lob/master/Examples/MLE/LikelihoodRatioTest.jl} 
\end_layout

\end_inset

 computes the LR statistic for a simple linear 
\begin_inset Formula $H_{0}$
\end_inset

.
 This uses the Julia function fmincon to perform the restricted estimation.
\end_layout

\begin_layout Itemize
run the test a number of times to explore size
\end_layout

\begin_layout Itemize
change the value of 
\begin_inset Formula $r$
\end_inset

 in the null hypothesis 
\begin_inset Formula $R\beta=r$
\end_inset

 to make it false, and run the test a number of times, to check power.
\end_layout

\begin_layout Itemize
explore how power depends on the sample size
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*

\series bold
Summary of MLE
\end_layout

\begin_layout Itemize
Consistent
\end_layout

\begin_layout Itemize
Asymptotically normal (CAN)
\end_layout

\begin_layout Itemize
Asymptotically efficient
\end_layout

\begin_layout Itemize
Asymptotically unbiased
\end_layout

\begin_layout Itemize
LR test is available for testing hypothesis
\end_layout

\begin_layout Itemize
The presentation is for general MLE: we haven't specified the distribution
 or the linearity/nonlinearity of the estimator
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
ML of Nerlove model, assuming normality
\end_layout

\begin_layout Standard
As we saw in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Asymptotic-efficiency"

\end_inset

, the ML and OLS estimators of 
\begin_inset Formula $\beta$
\end_inset

 in the linear model 
\begin_inset Formula $y=X\beta+\epsilon$
\end_inset

 coincide when 
\begin_inset Formula $\epsilon$
\end_inset

 is assumed to be i.i.d.
 normally distributed.
 The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{NerloveMLE.jl}{https://github.com/mcreel/Econometrics/blob/maste
r/Examples/MLE/NerloveMLE.jl} 
\end_layout

\end_inset

 verifies this result, for the basic Nerlove model (eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "simple nerlove model"

\end_inset

).
 The output of the script follows:
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MLE/NerloveMLE.out"

\end_inset

Compare the output to that of 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Nerlove.jl}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/OLS/Nerlove.jl} 
\end_layout

\end_inset

, which does OLS.
 The script also provides a basic example of how to use the MLE estimation
 routine 
\family typewriter
mleresults.jl
\family default
, which is in the 
\begin_inset CommandInset href
LatexCommand href
name "Econometrics.jl"
target "https://github.com/mcreel/Econometrics.jl"
literal "false"

\end_inset

 package.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example: Binary response models: theory
\end_layout

\begin_layout Standard
This section extends the Bernoulli trial model to binary response models
 with conditioning variables, as such models arise in a variety of contexts.
\end_layout

\begin_layout Standard
Assume that
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & x^{\prime}\theta-\varepsilon\\
y & = & 1(y^{*}>0)\\
\varepsilon & \sim & N(0,1)
\end{eqnarray*}

\end_inset

Here, 
\begin_inset Formula $y^{*}$
\end_inset

 is an unobserved (latent) continuous variable, and 
\begin_inset Formula $y$
\end_inset

 is a binary variable that indicates whether 
\begin_inset Formula $y^{*}$
\end_inset

is negative or positive.
 Then the 
\emph on
probit 
\emph default
model results, where 
\begin_inset Formula $Pr(y=1|x)=Pr(\varepsilon<x^{\prime}\theta)=\Phi(x^{\prime}\theta)$
\end_inset

, where 
\begin_inset Formula 
\[
\Phi(\cdot)=\int_{-\infty}^{x\beta}(2\pi)^{-1/2}\exp(-\frac{\varepsilon^{2}}{2})d\varepsilon
\]

\end_inset

is the standard normal distribution function.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
logit 
\emph default
model results if the errors 
\begin_inset Formula $\epsilon$
\end_inset

 are not normal, but rather have a logistic distribution.
 This distribution is similar to the standard normal, but has fatter tails.
 The probability, in this case, has the following parameterization
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Pr(y=1|x)=\Lambda(x^{\prime}\theta)=\left(1+\exp(-x^{\prime}\theta)\right)^{-1}.
\]

\end_inset


\end_layout

\begin_layout Standard
In general, a binary response model will require that the choice probability
 be parameterized in some form which could be logit, probit, or something
 else.
 For a vector of explanatory variables 
\begin_inset Formula $x$
\end_inset

, the response probability will be parameterized in some manner
\begin_inset Formula 
\[
Pr(y=1|x)=p(x,\theta)
\]

\end_inset

Again, if 
\begin_inset Formula $p(x,\theta)=\Lambda(x^{\prime}\theta),$
\end_inset

 we have a logit model.
 If 
\begin_inset Formula $p(x,\theta)=\Phi(x^{\prime}\theta),$
\end_inset

 where 
\begin_inset Formula $\Phi(\cdot)$
\end_inset

 is the standard normal distribution function, then we have a probit model.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
The following is another verification of the information matrix equality,
 skip this in lectures.
\end_layout

\begin_layout Standard
Regardless of the parameterization, we are dealing with a Bernoulli density,
 
\begin_inset Formula 
\[
f_{Y_{i}}(y_{i}|x_{i})=p(x_{i},\theta)^{y_{i}}(1-p(x,\theta))^{1-y_{i}}
\]

\end_inset

 so as long as the observations are independent, the maximum likelihood
 (ML) estimator, 
\begin_inset Formula $\hat{\theta},$
\end_inset

 is the maximizer of 
\begin_inset Formula 
\begin{eqnarray}
s_{n}(\theta) & = & \frac{1}{n}\sum_{i=1}^{n}\left(y_{i}\ln p(x_{i},\theta)+(1-y_{i})\ln\left[1-p(x_{i},\theta)\right]\right)\nonumber \\
 & \equiv & \frac{1}{n}\sum_{i=1}^{n}s(y_{i},x_{i},\theta).\label{s(y,A...)}
\end{eqnarray}

\end_inset

 Following the above theoretical results, 
\begin_inset Formula $\hat{\theta}$
\end_inset

 tends in probability to the 
\begin_inset Formula $\theta_{0}$
\end_inset

 that maximizes the uniform almost sure limit of 
\begin_inset Formula $s_{n}(\theta).$
\end_inset

 Noting that 
\begin_inset Formula $\mathcal{E}y_{i}=p(x_{i},\theta_{0}),$
\end_inset

 and following a SLLN for i.i.d.
 processes, 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 converges almost surely to the expectation of a representative term 
\begin_inset Formula $s(y,x,\theta).$
\end_inset

 First one can take the expectation conditional on 
\begin_inset Formula $x$
\end_inset

 to get 
\begin_inset Formula 
\[
\mathcal{E}_{y|x}\left\{ y\ln p(x,\theta)+(1-y)\ln\left[1-p(x,\theta)\right]\right\} =p(x,\theta_{0})\ln p(x,\theta)+\left[1-p(x,\theta_{0})\right]\ln\left[1-p(x,\theta)\right].
\]

\end_inset

 Next taking expectation over 
\begin_inset Formula $x$
\end_inset

 we get the limiting objective function 
\begin_inset Formula 
\begin{equation}
s_{\infty}(\theta)=\int_{\mathcal{X}}\left\{ p(x,\theta_{0})\ln p(x,\theta)+\left[1-p(x,\theta_{0})\right]\ln\left[1-p(x,\theta)\right]\right\} \mu(x)dx,\label{QMLlimitingobjfun}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\mu(x)$
\end_inset

 is the (joint - the integral is understood to be multiple, and 
\begin_inset Formula $\mathcal{X}$
\end_inset

 is the support of 
\begin_inset Formula $x$
\end_inset

) density function of the explanatory variables 
\begin_inset Formula $x$
\end_inset

.
 This is clearly continuous in 
\begin_inset Formula $\theta,$
\end_inset

 as long as 
\begin_inset Formula $p(x,\theta)$
\end_inset

 is continuous, and if the parameter space is compact we therefore have
 uniform almost sure convergence.
 Note that 
\begin_inset Formula $p(x,\theta)$
\end_inset

 is continuous for the logit and probit models, for example.
 The maximizing element of 
\begin_inset Formula $s_{\infty}(\theta),$
\end_inset

 
\begin_inset Formula $\theta^{*},$
\end_inset

 solves the first order conditions 
\begin_inset Formula 
\[
\int_{\mathcal{X}}\left\{ \frac{p(x,\theta_{0})}{p(x,\theta^{*})}\frac{\partial}{\partial\theta}p(x,\theta^{*})-\frac{1-p(x,\theta_{0})}{1-p(x,\theta^{*})}\frac{\partial}{\partial\theta}p(x,\theta^{*})\right\} \mu(x)dx=0
\]

\end_inset

 This is clearly solved by 
\begin_inset Formula $\theta^{*}=\theta_{0}.$
\end_inset

 Provided the solution is unique, 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is consistent.
 Question:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

what's needed to ensure that the solution is unique?
\end_layout

\begin_layout Standard
The asymptotic normality theorem tells us that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta^{0})^{-1}\mathcal{I}_{\infty}(\theta^{0})\mathcal{J}_{\infty}(\theta^{0})^{-1}\right].
\]

\end_inset

 In the case of i.i.d.
 observations 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta_{0})=\lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta_{0})$
\end_inset

 is simply the expectation of a typical element of the outer product of
 the gradient.
\end_layout

\begin_layout Itemize
There's no need to subtract the mean, since it's zero, following the f.o.c.
 in the consistency proof above and the fact that observations are i.i.d.
\end_layout

\begin_layout Itemize
The terms in 
\begin_inset Formula $n$
\end_inset

 also drop out by the same argument: 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta_{0}) & = & \lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}\frac{1}{n}\sum_{t}s(\theta_{0})\\
 & = & \lim_{n\rightarrow\infty}Var\frac{1}{\sqrt{n}}D_{\theta}\sum_{t}s(\theta_{0})\\
 & = & \lim_{n\rightarrow\infty}\frac{1}{n}Var\sum_{t}D_{\theta}s(\theta_{0})\\
 & = & \lim_{n\rightarrow\infty}VarD_{\theta}s(\theta_{0})\\
 & = & VarD_{\theta}s(\theta_{0})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So we get 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta_{0})=\mathcal{E}\left\{ \frac{\partial}{\partial\theta}s(y,x,\theta_{0})\frac{\partial}{\partial\theta^{\prime}}s(y,x,\theta_{0})\right\} .
\]

\end_inset

 Likewise, 
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\theta_{0})=\mathcal{E}\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s(y,x,\theta_{0}).
\]

\end_inset

 Expectations are jointly over 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $x,$
\end_inset

 or equivalently, first over 
\begin_inset Formula $y$
\end_inset

 conditional on 
\begin_inset Formula $x,$
\end_inset

 then over 
\begin_inset Formula $x.$
\end_inset

 From above, a typical element of the objective function is 
\begin_inset Formula 
\[
s(y,x,\theta_{0})=y\ln p(x,\theta_{0})+(1-y)\ln\left[1-p(x,\theta_{0})\right].
\]

\end_inset

 Now suppose that we are dealing with a correctly specified logit model:
 
\begin_inset Formula 
\[
p(x,\theta)=\left(1+\exp(-\mathbf{x}^{\prime}\theta)\right)^{-1}.
\]

\end_inset

 We can simplify the above results in this case.
 We have that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\theta}p(x,\theta) & = & \left(1+\exp(-\mathbf{x}^{\prime}\theta)\right)^{-2}\exp(-\mathbf{x}^{\prime}\theta)\mathbf{x}\\
 & = & \left(1+\exp(-\mathbf{x}^{\prime}\theta)\right)^{-1}\frac{\exp(-\mathbf{x}^{\prime}\theta)}{1+\exp(-\mathbf{x}^{\prime}\theta)}\mathbf{x}\\
 & = & p(x,\theta)\left(1-p(x,\theta)\right)\mathbf{x}\\
 & = & \left(p(x,\theta)-p(x,\theta)^{2}\right)\mathbf{x}.
\end{eqnarray*}

\end_inset

 So 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial}{\partial\theta}s(y,x,\theta_{0}) & = & \left[y-p(x,\theta_{0})\right]\mathbf{x}\label{QMLgradient}\\
\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s(\theta_{0}) & = & -\left[p(x,\theta_{0})-p(x,\theta_{0})^{2}\right]\mathbf{xx}^{\prime}.\nonumber 
\end{eqnarray}

\end_inset

 Taking expectations over 
\begin_inset Formula $y$
\end_inset

 then 
\begin_inset Formula $\mathbf{x}$
\end_inset

 gives 
\begin_inset Formula 
\begin{eqnarray}
\mathcal{I}_{\infty}(\theta_{0}) & = & \int E_{Y}\left[y^{2}-2p(x,\theta_{0})p(x,\theta_{0})+p(x,\theta_{0})^{2}\right]\mathbf{xx}^{\prime}\mu(x)dx\label{I}\\
 & = & \int\left[p(x,\theta_{0})-p(x,\theta_{0})^{2}\right]\mathbf{xx}^{\prime}\mu(x)dx.
\end{eqnarray}

\end_inset

 where we use the fact that 
\begin_inset Formula $E_{Y}(y)=E_{Y}(y^{2})=p(\mathbf{x},\theta_{0})$
\end_inset

.
 Likewise, 
\begin_inset Formula 
\begin{equation}
\mathcal{J}_{\infty}(\theta_{0})=-\int\left[p(x,\theta_{0})-p(x,\theta_{0})^{2}\right]\mathbf{xx}^{\prime}\mu(x)dx.\label{J}
\end{equation}

\end_inset

 Note that we arrive at the expected result:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the information matrix equality holds (that is, 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta_{0})=-\mathcal{I}_{\infty}(\theta_{0}))$
\end_inset

.
 
\end_layout

\begin_layout Standard
On a final note, the logit and standard normal CDF's are very similar -
 the logit distribution is a bit more fat-tailed.
 While coefficients will vary slightly between the two models, functions
 of interest such as estimated probabilities 
\begin_inset Formula $p(x,\hat{\theta})$
\end_inset

 will be virtually identical for the two models.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Estimation of the logit model
\begin_inset CommandInset label
LatexCommand label
name "subsec:Discrete-Choice:logit model"

\end_inset


\end_layout

\begin_layout Standard
In this section we will consider maximum likelihood estimation of the logit
 model for binary 0/1 dependent variables.
 We will use the BFGS algorithm to find the MLE.
 
\end_layout

\begin_layout Standard
A binary response is a variable that takes on only two values, customarily
 0 and 1, which can be thought of as codes for whether or not a condisiton
 is satisfied.
 For example, 0=drive to work, 1=take the bus.
 Often the observed binary variable, say 
\begin_inset Formula $y$
\end_inset

, is related to an unobserved (latent) continuous varable, say 
\begin_inset Formula $y^{*}$
\end_inset

.
 We would like to know the effect of covariates, 
\begin_inset Formula $x$
\end_inset

, on 
\begin_inset Formula $y.$
\end_inset

 The model can be represented as 
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & g(x)-\varepsilon\\
y & = & 1(y^{*}>0)\\
Pr(y=1) & = & F_{\varepsilon}[g(x)]\\
 & \equiv & p(x,\theta)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The log-likelihood function is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}\ln p(x_{i},\theta)+(1-y_{i})\ln\left[1-p(x_{i},\theta)\right]\right)
\]

\end_inset


\end_layout

\begin_layout Standard
For the logit model, the probability has the specific form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(x,\theta)=\frac{1}{1+\exp(-x^{\prime}\theta)}
\]

\end_inset


\end_layout

\begin_layout Standard
You should download and examine 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{LogitDGP.jl}{https://github.com/mcreel/Econometrics/blob/master/
Examples/MLE/LogitDGP.jl} 
\end_layout

\end_inset

, which generates data according to the logit model, 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{logit.jl}{https://github.com/mcreel/Econometrics/blob/master/Exa
mples/MLE/logit.jl} 
\end_layout

\end_inset

, which calculates the loglikelihood, and 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{EstimateLogit.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/MLE/EstimateLogit.jl} 
\end_layout

\end_inset

, which sets things up and calls the estimation routine, which uses the
 BFGS algorithm.
 
\end_layout

\begin_layout Standard
\paragraph_spacing single
Here are some estimation results with 
\begin_inset Formula $n=30,$
\end_inset

 and the true 
\begin_inset Formula $\theta=(0,1)^{\prime}.$
\end_inset

 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MLE/Logit.out"

\end_inset


\end_layout

\begin_layout Standard
The estimation program is calling 
\family typewriter
mleresults
\family default
.jl , which in turn calls other routines.
 See the source code 
\begin_inset CommandInset href
LatexCommand href
name "source code"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/ML/mleresults.jl"
literal "false"

\end_inset

 for more details.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example: the MEPS Data
\end_layout

\begin_layout Standard
We first saw the MEPS data in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"

\end_inset

, where a Poisson model was estimated by maximum likelihood.
 To check the plausibility of the Poisson model for the MEPS data, we can
 compare the sample unconditional variance with the estimated unconditional
 variance according to the Poisson model: 
\begin_inset Formula $\widehat{V(y)}=\frac{\sum_{t=1}^{n}\hat{\lambda}_{t}}{n}$
\end_inset

.
 Using the program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{PoissonVariance.m}{https://github.com/mcreel/Econometrics/blob/m
aster/Examples/MEPS-I/PoissonVariance.m}
\end_layout

\end_inset

, for OBDV and ERV, we get the results in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Marginal-Variances,-Sample"

\end_inset

.
 
\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Marginal-Variances,-Sample"

\end_inset

Marginal Variances, Sample and Estimated (Poisson)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sample
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
38.09
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.151
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Estimated
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.28
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.086
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

We see that even after conditioning, the overdispersion is not captured
 in either case.
 There is huge problem with OBDV, and a significant problem with ERV.
 In both cases the Poisson model does not appear to be plausible.
 You can check this for the other use measures if you like.
\end_layout

\begin_layout Subsubsection
Infinite mixture models: the negative binomial model
\begin_inset CommandInset label
LatexCommand label
name "subsec:Infinite-mixture-models:"

\end_inset


\end_layout

\begin_layout Standard
Reference: Cameron and Trivedi (1998) 
\emph on
Regression analysis of count data,
\emph default
 chapter 4.
\end_layout

\begin_layout Standard
The two measures seem to exhibit extra-Poisson variation.
 To capture unobserved heterogeneity, a possibility is the 
\emph on
random parameters
\emph default
 approach.
 Consider the possibility that the parameter in a Poisson model were random:
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y|\lambda,v) & = & \frac{\exp(-\theta)\theta^{y}}{y!}
\end{eqnarray*}

\end_inset

where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\theta=\lambda v$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
.
 Now 
\begin_inset Formula $\nu$
\end_inset

 is a multiplicative random term.
 The problem is that we don't observe 
\begin_inset Formula $\nu$
\end_inset

, so we will need to marginalize it to get a usable density
\begin_inset Formula 
\[
f_{Y}(y|\lambda,\psi)=\int_{-\infty}^{\infty}\frac{\exp[-\lambda z]\left[\lambda z\right]^{y}}{y!}f_{v}(z;\psi)dz
\]

\end_inset

This density 
\emph on
can
\emph default
 be used directly, perhaps using numerical integration to evaluate the likelihoo
d function.
 In some cases, though, the integral will have an analytic solution.
 For example, if 
\begin_inset Formula $\nu$
\end_inset

 follows a certain one parameter gamma density, then 
\begin_inset Formula 
\begin{equation}
f_{Y}(y|\lambda,\phi)=\frac{\Gamma(y+\psi)}{\Gamma(y+1)\Gamma(\psi)}\left(\frac{\psi}{\psi+\lambda}\right)^{\psi}\left(\frac{\lambda}{\psi+\lambda}\right)^{y}\label{eq:negbindensity}
\end{equation}

\end_inset

where 
\begin_inset Formula $\phi=(\lambda,\psi)$
\end_inset

.
 
\begin_inset Formula $\psi$
\end_inset

 appears since it is the parameter of the gamma density.
 
\end_layout

\begin_layout Itemize
We usually parameterize 
\begin_inset Formula $\lambda=\exp(\mathbf{x}'\beta)$
\end_inset

, as before.
\end_layout

\begin_layout Itemize
The variance depends upon how 
\begin_inset Formula $\psi$
\end_inset

 is parameterized.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $\psi=\lambda/\alpha$
\end_inset

, where 
\begin_inset Formula $\alpha>0$
\end_inset

, then 
\begin_inset Formula $V(y|\mathbf{x})=\lambda+\alpha\lambda$
\end_inset

.
 Note that 
\begin_inset Formula $\lambda$
\end_inset

 is a function of 
\begin_inset Formula $\mathbf{x}$
\end_inset

, so that the variance is too.
 This is referred to as the NB-I model.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\psi=1/\alpha$
\end_inset

, where 
\begin_inset Formula $\alpha>0$
\end_inset

, then 
\begin_inset Formula $V(y|\mathbf{x})=\lambda+\alpha\lambda^{2}$
\end_inset

.
 This is referred to as the NB-II model.
\end_layout

\end_deeper
\begin_layout Standard
So both forms of the NB model allow for overdispersion, with the NB-II model
 allowing for a more radical form.
\end_layout

\begin_layout Standard
Testing reduction of a NB model to a Poisson model cannot be done by testing
 
\begin_inset Formula $\alpha=0$
\end_inset

 using standard Wald or LR procedures.
 The critical values need to be adjusted to account for the fact that 
\begin_inset Formula $\alpha=0$
\end_inset

 is on the boundary of the parameter space.
 Without getting into details, suppose that the data were in fact Poisson,
 so there is equidispersion and the true 
\begin_inset Formula $\alpha=0$
\end_inset

.
 Then about half the time the sample data will be underdispersed, and about
 half the time overdispersed.
 When the data is underdispersed, the MLE of 
\begin_inset Formula $\alpha$
\end_inset

 will be 
\begin_inset Formula $\hat{\alpha}=0$
\end_inset

.
 Thus, under the null, there will be a probability spike in the asymptotic
 distribution of 
\begin_inset Formula $\sqrt{n(}\hat{\alpha}-\alpha)=\sqrt{n}\hat{\alpha}$
\end_inset

 at 0, so standard testing methods will not be valid.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{This program}{https://github.com/mcreel/Econometrics/blob/maste
r/Examples/MEPS-II/EstimateNegBin.m} 
\end_layout

\end_inset

 will do estimation using the NB model (I haven't bothered converting this
 to Julia, but it's easy to do, just program the NB likelihood function).
 
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MEPS-II/NB-I.out"

\end_inset


\end_layout

\begin_layout Standard
Likewise, here are NB-II results:
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MEPS-II/NB-II.out"

\end_inset


\end_layout

\begin_layout Itemize
For the OBDV usage measure, the NB-II model does a slightly better job than
 the NB-I model, in terms of the average log-likelihood and the information
 criteria (more on this last in a moment).
 
\end_layout

\begin_layout Itemize
Note that both versions of the NB model fit much better than does the Poisson
 model (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "PoissonOBDV_results"

\end_inset

).
\end_layout

\begin_layout Itemize
The estimated 
\begin_inset Formula $\alpha$
\end_inset

 is highly significant.
\end_layout

\begin_layout Standard
To check the plausibility of the NB-II model, we can compare the sample
 unconditional variance with the estimated unconditional variance according
 to the NB-II model: 
\begin_inset Formula $\widehat{V(y)}=\frac{\sum_{t=1}^{n}\hat{\lambda}_{t}+\hat{\alpha}\left(\hat{\lambda}_{t}\right)^{2}}{n}$
\end_inset

.
 For OBDV and ERV (estimation results not reported), we get
\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Marginal Variances, Sample and Estimated (NB-II)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sample
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
38.09
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.151
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Estimated
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30.58
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.182
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

For OBDV, the overdispersion problem is significantly better than in the
 Poisson case, but there is still some that is not captured.
 For ERV, the negative binomial model seems to capture the overdispersion
 adequately.
\end_layout

\begin_layout Subsubsection
Finite mixture models: the mixed negative binomial model
\end_layout

\begin_layout Standard
The finite mixture approach to fitting health care demand was introduced
 by Deb and Trivedi (1997).
 The mixture approach has the intuitive appeal of allowing for subgroups
 of the population with different health status.
 If individuals are classified as healthy or unhealthy then two subgroups
 are defined.
 A finer classification scheme would lead to more subgroups.
 Many studies have incorporated objective and/or subjective indicators of
 health status in an effort to capture this heterogeneity.
 The available objective measures, such as limitations on activity, are
 not necessarily very informative about a person's overall health status.
 Subjective, self-reported measures may suffer from the same problem, and
 may also not be exogenous
\end_layout

\begin_layout Standard
Finite mixture models are conceptually simple.
 The density is
\begin_inset Formula 
\[
f_{Y}(y,\phi_{1},...,\phi_{p},\pi_{1},...,\pi_{p-1})=\sum_{i=1}^{p-1}\pi_{i}f_{Y}^{(i)}(y,\phi_{i})+\pi_{p}f_{Y}^{p}(y,\phi_{p}),
\]

\end_inset

where 
\begin_inset Formula $\pi_{i}>0,i=1,2,...,p$
\end_inset

, 
\begin_inset Formula $\pi_{p}=1-\sum_{i=1}^{p-1}\pi_{i}$
\end_inset

, and 
\begin_inset Formula $\sum_{i=1}^{p}\pi_{i}=1$
\end_inset

.
 Identification requires that the 
\begin_inset Formula $\pi_{i}$
\end_inset

 are ordered in some way, for example, 
\begin_inset Formula $\pi_{1}\geq\pi_{2}\geq\cdots\geq\pi_{p}$
\end_inset

 and 
\begin_inset Formula $\phi_{i}\neq\phi_{j},i\neq j$
\end_inset

.
 This is simple to accomplish post-estimation by rearrangement and possible
 elimination of redundant component densities.
 
\end_layout

\begin_layout Itemize
The properties of the mixture density follow in a straightforward way from
 those of the components.
 In particular, the moment generating function is the same mixture of the
 moment generating functions of the component densities, so, for example,
 
\begin_inset Formula $E(Y|x)=\sum_{i=1}^{p}\pi_{i}\mu_{i}(x)$
\end_inset

, where 
\begin_inset Formula $\mu_{i}(x)$
\end_inset

 is the mean of the 
\begin_inset Formula $i^{th}$
\end_inset

 component density.
\end_layout

\begin_layout Itemize
Mixture densities may suffer from overparameterization, since the total
 number of parameters grows rapidly with the number of component densities.
 It is possible to constrained parameters across the mixtures.
\end_layout

\begin_layout Itemize
Testing for the number of component densities is a tricky issue.
 For example, testing for 
\begin_inset Formula $p=1$
\end_inset

 (a single component, which is to say, no mixture) versus 
\begin_inset Formula $p=2$
\end_inset

 (a mixture of two components) involves the restriction 
\begin_inset Formula $\pi_{1}=1$
\end_inset

, which is on the boundary of the parameter space.
 Not that when 
\begin_inset Formula $\pi_{1}=1$
\end_inset

, the parameters of the second component can take on any value without affecting
 the density.
 Usual methods such as the likelihood ratio test are not applicable when
 parameters are on the boundary under the null hypothesis.
 Information criteria means of choosing the model (see below) are valid.
 
\end_layout

\begin_layout Standard
The following results are for a mixture of 2 NB-II models, for the OBDV
 data, which you can replicate using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{this program}{https://github.com/mcreel/Econometrics/blob/maste
r/Examples/MEPS-II/EstimateMixNegBin.m} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/MEPS-II/MixNegBin.out"

\end_inset


\end_layout

\begin_layout Standard
It is worth noting that the mixture parameter is not significantly different
 from zero, but also note that the coefficients of public insurance and
 age, for example, differ quite a bit between the two latent classes.
\end_layout

\begin_layout Subsubsection
Information criteria
\end_layout

\begin_layout Standard
As seen above, a Poisson model can't be tested (using standard methods)
 as a restriction of a negative binomial model.
 But it seems, based upon the values of the likelihood functions and the
 fact that the NB model fits the variance much better, that the NB model
 is more appropriate.
 How can we determine which of a set of competing models is the best?
\end_layout

\begin_layout Standard
The information criteria approach is one possibility.
 Information criteria are functions of the log-likelihood, with a penalty
 for the number of parameters used.
 Three popular information criteria are the Akaike (AIC), Bayes (BIC) and
 consistent Akaike (CAIC).
 The formulae are
\begin_inset Formula 
\begin{eqnarray*}
CAIC & = & -2\ln L(\hat{\theta})+k(\ln n+1)\\
BIC & = & -2\ln L(\hat{\theta})+k\ln n\\
AIC & = & -2\ln L(\hat{\theta})+2k
\end{eqnarray*}

\end_inset

It can be shown that the CAIC and BIC will select the correctly specified
 model from a group of models, asymptotically.
 This doesn't mean, of course, that the correct model is necessarily in
 the group.
 The AIC is not consistent, and will asymptotically favor an over-parameterized
 model over the correctly specified model.
 Here are information criteria values for the models we've seen, for OBDV.
\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Information-Criteria,-OBDV"

\end_inset

Information Criteria, OBDV
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
AIC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
BIC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CAIC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Poisson
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.345
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.355
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.357
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NB-I
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.375
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.386
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.388
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NB-II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.373
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.385
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.386
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MNB-II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.337
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.361
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.365
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

Pretty clearly, the NB models are better than the Poisson.
 The one additional parameter gives a very significant improvement in the
 likelihood function value.
 Between the NB-I and NB-II models, the NB-II is very slightly favored.
 But one should remember that information criteria values are statistics,
 with variances.
 With another sample, it may well be that the NB-I model would be favored,
 since the differences are so small.
 The MNB-II model is favored over the others, by all 3 information criteria.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:DSGE-ML"

\end_inset

Estimation of the DSGE model
\end_layout

\begin_layout Standard
Note: this section will not be converted to the Julia language at the present
 time, as there is presently no Julia language full equivalent of Dynare.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Application:-a-simple"

\end_inset

 introduced a simple dynamic stochastic general equilibrium model.
 The file 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimateCGHK
\backslash
_ML.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/DSGE/ML/Estimat
eCGHK
\backslash
_ML.m} 
\end_layout

\end_inset

 allows you to explore maximum likelihood estimation of the model using
 Kalman or particle filtering, using the 
\begin_inset CommandInset href
LatexCommand href
name "http://www.dynare.org/"
target "http://www.dynare.org/"
literal "false"

\end_inset

 package.
 Some output that can be obtained is:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/ML/dsgeml.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Itemize
this is pretty good! However...
\end_layout

\begin_layout Itemize
Estimation works when the parameters start values are close enough to their
 true values, but it doesn't work when less informative start values are
 tried: probably the objective function is nonconcave, so care must be taken
 to try enough start values.
 
\end_layout

\begin_layout Itemize
When order=1, the estimation process involves forming a linear approximation
 to the true model, which means that the estimator is not actually the true
 maximum likelihood estimator, it is actually a 
\begin_inset Quotes sld
\end_inset

quasi-ML
\begin_inset Quotes srd
\end_inset

 estimator (refer to 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Linearization-of"

\end_inset

).
 The quasi-likelihood is computed by putting the linearized model in state-space
 form, and then computing the likelihood iteratively using Kalman filtering,
 which relies on the assumption that shocks to the model are normally distribute
d.
 State space models and Kalman filtering are introduced in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:State-space-models"

\end_inset

.
 Once the likelihood function is available, the methods studied in this
 Chapter may be applied.
 
\end_layout

\begin_layout Itemize
The linearization of the model, combined with the fact that it has only
 two shocks, leads to a problem of 
\begin_inset Quotes sld
\end_inset

stochastic singularity
\begin_inset Quotes srd
\end_inset

, which means that at most two observed variables may be used to compute
 the likelihood function.
 The code lets you explore the choice.
 It seems to work best using c and n.
 For some choices, the true parameter values will be outside the confidence
 intervals.
 If interested, do a Google search for this term, along with DSGE, and you'll
 find more information.
\end_layout

\begin_layout Itemize
Not using all of the observed variables for estimation is very likely to
 cause problems of lack of identification and inefficiency.
 This can be confirmed if you experiment with the estimation script.
 Using c and n, we get the above results.
 Using other variables, we get results that aren't so good.
 When you don't know the true parameters, how will you choose which results
 to believe?
\end_layout

\begin_layout Itemize
This is not a problem of the ML method, it is a problem due to the fact
 that we are not really estimating the true model, we're working with a
 linear approximation.
 Often, people artificially add measurement error to the variables, which
 gets around the stochastic singularity problem, possibly at the cost of
 introducing a worse problem.
 I have not yet seen a careful study of the effect of estimating assuming
 measurement error when there really is no measurement error.
\end_layout

\begin_layout Itemize
The intention of presenting this example is to show that ML may be used
 for estimation of complex models.
 The problem here is that the econometric model is not complex enough: the
 linearization throws away so much information so that estimation becomes
 impossible.
 A better solution is to try to actually do ML estimation for the true nonlinear
 model: see papers by Fernndez-Villaverde and Rubio-Ramrez, which use
 particle filtering.
 You can also modify the script to do this, by setting order=2 in the code,
 as Dynare supports the particle filter option.
 This is very time-consuming, though.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Suppose that data is generated by the process 
\begin_inset Formula $y=1$
\end_inset

 if 
\begin_inset Formula $(\beta_{0}+\beta_{1}x)>u$
\end_inset

 and 
\begin_inset Formula $y=0$
\end_inset

 otherwise.
 
\begin_inset Formula $x\sim U(0,1)$
\end_inset

 is a scalar exogenous variable, and 
\begin_inset Formula $u$
\end_inset

 is a draw from the standard normal distribution.
 Suppose that 
\begin_inset Formula $n$
\end_inset

 independent observations are generated following this process.
\end_layout

\begin_deeper
\begin_layout Enumerate
explain which of the assumptions of the classical linear regression model
 do not hold.
\end_layout

\begin_layout Enumerate
explain how the parameters of the model could be estimated consistently.
 
\end_layout

\end_deeper
\begin_layout Enumerate
Consider coin tossing with a single possibly biased coin.
 The density function for the random variable 
\begin_inset Formula $y=1(heads)$
\end_inset

 is 
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y,p_{0}) & = & p_{0}^{y}\left(1-p_{0}\right)^{1-y},y\in\{0,1\}\\
 & = & 0,y\notin\{0,1\}
\end{eqnarray*}

\end_inset

Suppose that we have a sample of size 
\begin_inset Formula $n$
\end_inset

.
 We know from above that the ML estimator is 
\begin_inset Formula $\widehat{p_{0}}=\bar{y}$
\end_inset

.
 We also know from the theory above that 
\begin_inset Formula 
\[
\sqrt{n}\left(\bar{y}-p_{0}\right)\overset{a}{\sim}N\left[0,\mathcal{J}_{\infty}(p_{0})^{-1}\mathcal{I}_{\infty}(p_{0})\mathcal{J}_{\infty}(p_{0})^{-1}\right]
\]

\end_inset


\series bold
a)
\series default
 find the analytic expression for 
\begin_inset Formula $g_{t}(\theta)$
\end_inset

 and show that 
\begin_inset Formula $\mathcal{E}_{\theta}\left[g_{t}(\theta)\right]=0$
\end_inset


\begin_inset Newline newline
\end_inset


\series bold
b)
\series default
 find the analytical expressions for 
\begin_inset Formula $\mathcal{J}_{\infty}(p_{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{\infty}(p_{0})$
\end_inset

 for this problem 
\begin_inset Newline newline
\end_inset


\series bold
c)
\series default
 verify that the result for 
\begin_inset Formula $\lim Var\sqrt{n}\left(\hat{p}-p\right)$
\end_inset

 found in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Coin-flipping,-again"

\end_inset

 is equal to 
\begin_inset Formula $\mathcal{J}_{\infty}(p_{0})^{-1}\mathcal{I}_{\infty}(p_{0})\mathcal{J}_{\infty}(p_{0})^{-1}$
\end_inset


\begin_inset Newline newline
\end_inset


\series bold
d)
\series default
 Write an Julia program that does a Monte Carlo study that shows that 
\begin_inset Formula $\sqrt{n}\left(\bar{y}-p_{0}\right)$
\end_inset

 is approximately normally distributed when 
\begin_inset Formula $n$
\end_inset

 is large.
 Please give me histograms that show the sampling frequency of 
\begin_inset Formula $\sqrt{n}\left(\bar{y}-p_{0}\right)$
\end_inset

 for several values of 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Enumerate
The exponential density is 
\begin_inset Formula 
\[
f_{X}(x)=\left\{ \begin{array}{c}
\lambda e^{-\lambda x},\,x\geqslant0\\
0,\,x<0
\end{array}\right.
\]

\end_inset

Suppose we have an independently and identically distributed sample of size
 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $\left\{ x_{i}\right\} ,i=1,2,...,n$
\end_inset

, where each 
\begin_inset Formula $x_{i}$
\end_inset

 follows this exponential distribution.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
write the log likelihood function
\end_layout

\begin_layout Enumerate
compute the maximum likelihood estimator of the parameter 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Enumerate
explain how to estimate the asymptotic variance of the ML estimator.
 That is, if 
\begin_inset Formula $\sqrt{n}\left(\hat{\lambda}-\lambda\right)\rightarrow^{d}N(0,V_{\infty})$
\end_inset

, give a consistent estimator of 
\begin_inset Formula $V_{\infty}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Suppose we have an i.i.d.
 sample of size 
\begin_inset Formula $n$
\end_inset

 from the Poisson density.
 The Poisson density is 
\begin_inset Formula $f_{y}(y;\lambda)=\frac{e^{-\lambda}\lambda^{y}}{y!}$
\end_inset

.
 Verify that the ML estimator is asymptotically distributed as 
\begin_inset Formula $\sqrt{n}\left(\hat{\lambda}-\lambda_{0}\right)\stackrel{d}{\rightarrow}N(0,\lambda_{0})$
\end_inset

, where 
\begin_inset Formula $\lambda_{0}$
\end_inset

 is the true parameter value.
 Hint: compute the asymptotic variance using 
\begin_inset Formula $-\mathcal{J}_{\infty}(\lambda_{0})^{-1}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Consider the model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\alpha\epsilon_{t}$
\end_inset

 where the errors follow the Cauchy (Student-t with 1 degree of freedom)
 density.
 So 
\begin_inset Formula 
\[
f(\epsilon_{t})=\frac{1}{\pi\left(1+\epsilon_{t}^{2}\right)},-\infty<\epsilon_{t}<\infty
\]

\end_inset

The Cauchy density has a shape similar to a normal density, but with much
 thicker tails.
 Thus, extremely small and large errors occur much more frequently with
 this density than would happen if the errors were normally distributed.
 Find the score function 
\begin_inset Formula $g_{n}(\theta)$
\end_inset

 where 
\begin_inset Formula $\theta=\left(\begin{array}{cc}
\beta^{\prime} & \alpha\end{array}\right)^{\prime}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Consider the model classical linear regression model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\epsilon_{t}$
\end_inset

 where 
\begin_inset Formula $\epsilon_{t}\sim IIN(0,\sigma^{2})$
\end_inset

.
 Find the score function 
\begin_inset Formula $g_{n}(\theta)$
\end_inset

 where 
\begin_inset Formula $\theta=\left(\begin{array}{cc}
\beta^{\prime} & \sigma\end{array}\right)^{\prime}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Compare the first order conditions that define the ML estimators of problems
 4 and 5 and interpret the differences.
 
\emph on
Why
\emph default
 are the first order conditions that define an efficient estimator different
 in the two cases?
\end_layout

\begin_layout Enumerate
Assume a d.g.p.
 follows the logit model: 
\begin_inset Formula $\Pr(y=1|x)=\left(1+exp(-\beta^{0}x)\right)^{-1}$
\end_inset

.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Assume that 
\begin_inset Formula $x\sim$
\end_inset

 uniform(-a,a).
 Find the asymptotic distribution of the ML estimator of 
\begin_inset Formula $\beta^{0}$
\end_inset

 (this is a scalar parameter).
\end_layout

\begin_layout Enumerate
Now assume that 
\begin_inset Formula $x\sim$
\end_inset

 uniform(-2a,2a).
 Again find the asymptotic distribution of the ML estimator of 
\begin_inset Formula $\beta^{0}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Comment on the results
\end_layout

\end_deeper
\begin_layout Enumerate
Estimate the simple Nerlove model discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Nerlove-data"

\end_inset

 by ML, assuming that the errors are i.i.d.
 
\begin_inset Formula $N(0,\sigma^{2})$
\end_inset

 and compare to the results you get from running 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Nerlove.jl}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/OLS/Nerlove.jl} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Enumerate
Using the fmincon routinge in Econometrics.jl
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
estimate the Nerlove model with the restriction that 
\begin_inset Formula $\beta_{L}+\beta_{F}+\beta_{K}=1$
\end_inset

 (the cost function satisfies homogeneity of degree one in factor prices).
 Test this restriction using the likelihood ratio test.
\end_layout

\begin_layout Enumerate
test the restriction that 
\begin_inset Formula $\beta_{Q}=\text{1 (the model exhibits constant returns to scale) using the LR test}$
\end_inset

.
\end_layout

\begin_layout Enumerate
test homogeneity of degree 1 and constant returns to scale jointly, using
 the LR test.
\end_layout

\end_deeper
\begin_layout Enumerate
Using 
\begin_inset CommandInset href
LatexCommand href
name "logit.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/ML/Likelihoods/logit.jl"
literal "false"

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{EstimateLogit.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/NonlinearOptimization/EstimateLogit.jl} 
\end_layout

\end_inset

 as templates, write a function to calculate the probit log likelihood,
 and a script to estimate a probit model.
 Run it using data that actually follows a logit model (you can generate
 it in the same way that is done in the logit example).
\end_layout

\begin_layout Enumerate
Study 
\family typewriter

\begin_inset CommandInset href
LatexCommand href
name "mleresults.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/ML/mleresults.jl"
literal "false"

\end_inset


\family default
 to see what it does.
 Examine the functions that
\family typewriter
 it 
\family default
calls.
 Write a complete description of how thechain works.
\end_layout

\begin_layout Enumerate
In Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"

\end_inset

 a model is presented for data on health care usage, along with some Julia
 scripts.
 Look at the Poisson estimation results for the OBDV measure of health care
 use and give an economic interpretation.
 Estimate Poisson models for the other 5 measures of health care usage,
 using the provided scripts.
\end_layout

\begin_layout Enumerate
For practice using fminunc, estimate a Poisson model by ML using the 10
 independent data points
\begin_inset Newline newline
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="11">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

.
 
\begin_inset Newline newline
\end_inset

For the Poisson model, the density 
\begin_inset Formula $f_{Y}(y|x)=\frac{\exp(-\lambda)\lambda^{y}}{y!},$
\end_inset

 
\begin_inset Formula $y=0,1,2,...$
\end_inset

.
 To make the model depend on conditioning variables, use the parameterization
 
\begin_inset Formula $\lambda(x)=\exp(\theta_{1}+\theta_{2}x)$
\end_inset

.
 The example EstimatePoisson.jl, in the notes, should be helpful
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
create a data file that contains these observations
\end_layout

\begin_layout Enumerate
find the log-likelihood function
\end_layout

\begin_layout Enumerate
find the score function
\end_layout

\begin_layout Enumerate
write a Julia function that computes the log-likelihood function.
\end_layout

\begin_layout Enumerate
use fminunc to find the ML estimator.
 You need to use an anonymous function for this.
\end_layout

\begin_layout Enumerate
find the analytic expression for the ML estimator
\end_layout

\begin_layout Enumerate
compute the ML estimator using your analytic expression.
 It should be very close to what you got using fminunc.
 Is it? If not, revise your code to make it work better.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "cha:Generalized-method-of"

\end_inset

Generalized method of moments
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
: 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Ch.
 6; Hamilton Ch.
 14
\begin_inset Formula $^{*}$
\end_inset

; Davidson and MacKinnon, Ch.
 17 (see pg.
 587 for refs.
 to applications), 
\begin_inset CommandInset citation
LatexCommand citet
key "hansen1982"
literal "true"

\end_inset

,  
\begin_inset CommandInset citation
LatexCommand citet
key "HansenSingleton1982"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "NeweyMcfadden"
literal "true"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Moment conditions
\end_layout

\begin_layout Definition
Moment condition: a moment condition 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\bar{m}_{n}(\theta)=\bar{m}_{n}(Z_{n},\theta)$
\end_inset

 is a vector-valued function of the data 
\begin_inset Formula $Z_{n}$
\end_inset

 and the parameter 
\begin_inset Formula $\theta$
\end_inset

 that has mean zero, under the model, when evaluated at the true parameter
 value 
\begin_inset Formula $\theta_{0}$
\end_inset

, and expectation different from zero when evaluated at other parameter
 values:
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\begin_inset Formula 
\begin{align*}
E\bar{m}_{n}(Z_{n},\theta_{0}) & =0\\
E\bar{m}_{n}(Z_{n},\theta) & \ne0,\,\theta\ne\theta_{0}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
The expectation operator 
\begin_inset Formula $E$
\end_inset

 supposes that expectations are taken with respect to the true density of
 the data.
 This may depend on more parameters than appear in 
\begin_inset Formula $\theta$
\end_inset

, if the model is semi-parametric.
\end_layout

\begin_layout Itemize
The moment condition may be vector-valued, with dimension 
\begin_inset Formula $G,$
\end_inset

 say.
\end_layout

\begin_layout Itemize
There are a couple of other details in the definition, which we'll get to.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
Moment contribution: we will be dealing with moment conditions that are
 defined as averages: 
\begin_inset Formula $\bar{m}_{n}(\theta)=\frac{1}{n}\sum_{t}m(Z_{t},\theta)=\frac{1}{n}\sum_{t}m_{t}(\theta)$
\end_inset

.
 The functions 
\begin_inset Formula $m_{t}(\theta)$
\end_inset

 are the 
\emph on
moment contributions.

\emph default
 The 
\begin_inset Formula $t$
\end_inset

th moment contribution 
\begin_inset Formula $m_{t}$
\end_inset

 is a function of the same observation's data.
 I'm casually using 
\begin_inset Formula $m(Z_{t},\theta),$
\end_inset

 
\begin_inset Formula $m_{t}(\theta)$
\end_inset

 and 
\begin_inset Formula $m_{t}$
\end_inset

 to all refer to the same thing.
 This first of these is the full expression, but I will suppress arguments
 when the context makes things clear enough, to reduce the notational burden.
 The main thing is that 
\begin_inset Formula $\bar{m}_{n}$
\end_inset

 refers to the average over the 
\begin_inset Formula $n$
\end_inset

 observations, and 
\begin_inset Formula $m_{t}$
\end_inset

 refers to the terms that are averaged.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
OLS.
 The classical linear model.
 Let 
\begin_inset Formula $\bar{m}_{n}(\beta)=\frac{1}{n}\sum_{t}x_{t}(y_{t}-x_{t}^{\prime}\beta).$
\end_inset

 So the moment contributions are 
\begin_inset Formula $m_{t}(\beta)=x_{t}(y_{t}-x_{t}^{\prime}\beta)$
\end_inset

.
 When 
\begin_inset Formula $\beta=\beta_{0},$
\end_inset

 
\begin_inset Formula $y_{t}-x_{t}^{\prime}\beta_{0}=\epsilon_{t},$
\end_inset

 and 
\begin_inset Formula $m_{t}=x_{t}\epsilon_{t}$
\end_inset

.
 We know that 
\begin_inset Formula $E(x_{t}\epsilon_{t})=0,$
\end_inset

 by the weak exogeneity assumption.
 Thus, the moment contributions, and the moment condition, which is their
 average, have expectation zero when evaluated at the true parameter value.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
ML.
 We have seen (see eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ExpectationScore"

\end_inset

) that the score contributions of the ML estimator have mean zero: 
\begin_inset Formula $E\left(D_{\theta}\ln f(y_{t}|x_{x},\theta_{0})\right)=0$
\end_inset

.
 So, we could set 
\begin_inset Formula $m_{t}(\theta)=D_{\theta}\ln f(y_{t}|x_{x},\theta)$
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
Sampling from 
\begin_inset Formula $\chi^{2}.$
\end_inset

 Suppose we draw a random sample of 
\begin_inset Formula $y_{t}$
\end_inset

 from the 
\begin_inset Formula $\chi^{2}(\theta_{0})$
\end_inset

 distribution.
 Here, 
\begin_inset Formula $\theta_{0}$
\end_inset

 is the parameter of interest.
 If 
\begin_inset Formula $Y\sim\chi^{2}(\theta_{0})$
\end_inset

, then the mean 
\begin_inset Formula $E(Y)=\theta_{0}$
\end_inset

.
 Let the moment contribution be 
\end_layout

\begin_layout Example
\begin_inset Formula 
\[
m_{t}(\theta)=y_{t}-\theta
\]

\end_inset

Then 
\begin_inset Formula 
\begin{align*}
\bar{m}_{n}(\theta) & =\frac{1}{n}\sum_{t=1}^{n}m_{t}(\theta)=\bar{y}-\theta
\end{align*}

\end_inset

We know that the 
\begin_inset Formula $E(\bar{y})=\theta_{0}.$
\end_inset


\end_layout

\begin_layout Itemize
Thus, 
\begin_inset Formula $E\bar{m}_{n}(\theta_{0})=0.$
\end_inset

 
\end_layout

\begin_layout Itemize
However, 
\begin_inset Formula $E\bar{m}_{n}(\theta)=\theta_{0}-\theta\ne$
\end_inset

0 if 
\begin_inset Formula $\theta\ne\theta_{0}.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
When the dimension of the moment conditions is the same as the dimension
 of the parameter vector 
\begin_inset Formula $\theta$
\end_inset

, the 
\emph on
method of moments principle
\emph default
 is to choose the estimator of the parameter to 
\emph on
set the moment condition equal to zero
\emph default
: 
\begin_inset Formula $\bar{m}_{n}(\hat{\theta})\equiv0$
\end_inset

.
 Then the equation is solved for the estimator.
 In the case of OLS, this gives 
\begin_inset Formula $\sum_{t}x_{t}(y_{t}-x_{t}^{\prime}\hat{\beta})=0,$
\end_inset

 which gives a solution that you should already know.
 For the chi-squared example, 
\begin_inset Formula 
\[
\bar{m}(\hat{\theta})=\bar{y}-\hat{\theta}=0
\]

\end_inset

is solved by 
\begin_inset Formula $\hat{\theta}=\bar{y}$
\end_inset

.
 Since 
\begin_inset Formula $\bar{y}=\sum_{t=1}^{n}y_{t}/n\stackrel{p}{\rightarrow}\theta_{0}$
\end_inset

 by the LLN, the estimator is consistent.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula $\chi^{2},$
\end_inset

 version 2.
 The variance of a 
\begin_inset Formula $\chi^{2}(\theta_{0})$
\end_inset

 r.v.
 is 
\begin_inset Formula 
\[
V\left(Y\right)=E\left(Y-\theta_{0}\right)^{2}=2\theta_{0}.
\]

\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula 
\begin{align*}
m_{t}(\theta) & =\frac{n}{n-1}\left(y_{t}-\bar{y}\right)^{2}-2\theta
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Then
\begin_inset Formula 
\[
\bar{m}_{n}(\theta)=\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n-1}-2\theta.
\]

\end_inset

The first term is the unbiased formula for the sample variance, and thus
 has expectation equal to 
\begin_inset Formula $2\theta_{0}.$
\end_inset

 So if we evaluate 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 at 
\begin_inset Formula $\theta_{0},$
\end_inset

 the expectation is zero.
\end_layout

\begin_layout Example
The MM estimator using the variance would set 
\begin_inset Formula 
\[
\bar{m}_{n}(\hat{\theta})=\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n-1}-2\hat{\theta}\equiv0.
\]

\end_inset

Solving for the estimator, it is half the sample variance: 
\begin_inset Formula 
\[
\hat{\theta}=\frac{1}{2}\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n-\text{1}}.
\]

\end_inset

Again, by the LLN, the sample variance is consistent for the true variance,
 that is, 
\begin_inset Formula 
\[
\frac{\sum_{t=1}^{n}\left(y_{t}-\bar{y}\right)^{2}}{n}\stackrel{p}{\rightarrow}2\theta_{0}.
\]

\end_inset

So, this MM is also consistent for 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:chi2 mm"

\end_inset

Try some MM estimation yourself: here's a Julia script that implements the
 two MM estimators discussed above: 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/chi2mm.jl}{https://github.com/mcreel/Econometrics/blob/maste
r/Examples/GMM/chi2mm.jl}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that when you run the script, the two estimators give different results.
 Each of the two estimators is consistent.
 
\end_layout

\begin_layout Itemize
For the 
\begin_inset Formula $\chi^{2}$
\end_inset

 example, we have two alternative moment conditions and only one parameter:
 we have 
\emph on
overidentification,
\emph default
 which means that we have more information than is strictly necessary for
 consistent estimation of the parameter.
\end_layout

\begin_layout Itemize
The idea behind GMM is to combine information from the two moment conditions
 to form a new estimator which will be
\emph on

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

more efficient,
\emph default
 in general (proof of this below).
\end_layout

\begin_layout Itemize
Note that the fact that the data has a chi-squared distribution is not used
 in estimation, it just as easily could have been normally distributed,
 sampling from a 
\begin_inset Formula $N(\theta_{0},2\theta_{0})$
\end_inset

 distribution.
 As long as the assumptions regarding the mean or variance are correct,
 the MM estimators are consistent.
 So, we don't make use of distributional assumptions when doing method of
 moment estimation, we only rely on certain moments being correctly specified.
 In this way, method of moments estimation is 
\emph on
more robust
\emph default
 than is maximum likelihood estimation: we obtain a consistent estimator
 with fewer assumptions.
 There being no free lunch, we should expect to pay something for this,
 of course.
 The cost will be a loss of efficiency, in general.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
To summarize
\series default
, a moment condition is a vector valued function which has expectation zero
 at the true parameter value.
 We have seen some examples of where we might get such functions, and more
 will follow.
 For now, let's take moment conditions as given, and work out the properties
 of the estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Definition of GMM estimator
\end_layout

\begin_layout Standard
For the purposes of this course, the following definition of the GMM estimator
 is sufficiently general:
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "GMM estimator (defn)"

\end_inset

The GMM estimator of the 
\begin_inset Formula $k$
\end_inset

 -dimensional parameter vector 
\begin_inset Formula $\theta_{0},$
\end_inset

 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta),
\]

\end_inset

 
\end_layout

\begin_layout Itemize
where 
\begin_inset Formula $\bar{m}_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}m(Z_{t},\theta)$
\end_inset

 is a 
\begin_inset Formula $g$
\end_inset

-vector valued function, 
\begin_inset Formula $g\geq k,$
\end_inset

 with 
\begin_inset Formula $\mathcal{E}m(\theta_{0})=0,$
\end_inset


\end_layout

\begin_layout Itemize
and 
\begin_inset Formula $W_{n}$
\end_inset

 converges almost surely to a finite 
\begin_inset Formula $g\times g$
\end_inset

 symmetric positive definite matrix 
\begin_inset Formula $W_{\infty}$
\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\emph on
What's the reason for using GMM if MLE is asymptotically efficient?
\emph default
 
\end_layout

\begin_layout Itemize
Robustness: GMM is based upon a limited set of moment conditions.
 For consistency, only these moment conditions need to be correctly specified,
 whereas MLE in effect requires correct specification of 
\emph on
every conceivable
\emph default
 moment condition.
 GMM is 
\emph on
robust with respect to distributional misspecification.

\emph default
 The price for robustness is usually a loss of efficiency with respect to
 the MLE estimator.
 Keep in mind that the true distribution is 
\bar under
not known
\bar default
 so if we erroneously specify a distribution and estimate by MLE, the estimator
 will be inconsistent in general (not always).
 
\end_layout

\begin_layout Itemize
Feasibility: in some cases the MLE estimator is not available, because we
 are not able to deduce or compute the likelihood function.
 More on this in the section on simulation-based estimation.
 The GMM estimator may still be feasible even though MLE is not available.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/chi2gmm.jl}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/GMM/chi2gmm.jl}
\end_layout

\end_inset

 implements GMM using the same 
\begin_inset Formula $\chi^{2}$
\end_inset

 data as was using in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:chi2 mm"

\end_inset

, above.
 The two moment conditions, based on the sample mean and sample variance
 are combined.
 The weight matrix is an identity matrix, 
\begin_inset Formula $I_{2}.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Consistency
\end_layout

\begin_layout Standard
We simply assume that the assumptions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

 hold, so the GMM estimator is strongly consistent.
 The main requirement is that the moment conditions have mean zero at the
 true parameter value, 
\begin_inset Formula $\theta^{0}.$
\end_inset

 This will be the case if our moment conditions are correctly specified.
 With this, it is clear that the minimum of the limiting objective function
 occurs at the true parameter value.
 The only assumption that warrants additional comment is that of identification.
 In Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

, the third assumption reads: (c) 
\shape italic
Identification:
\shape default
 
\begin_inset Formula $s_{\infty}(\cdot)$
\end_inset

 has a unique global maximum at 
\begin_inset Formula $\theta^{0},$
\end_inset

 
\shape italic
i.e.,
\shape default
 
\begin_inset Formula $s_{\infty}(\theta^{0})>s_{\infty}(\theta),$
\end_inset

 
\begin_inset Formula $\forall\theta\neq\theta^{0}.$
\end_inset

 Taking the case of a quadratic objective function 
\begin_inset Formula $s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta),$
\end_inset

 first consider 
\begin_inset Formula $\bar{m}_{n}(\theta).$
\end_inset


\end_layout

\begin_layout Itemize
Applying a uniform law of large numbers, we get 
\begin_inset Formula $\bar{m}_{n}(\theta)\stackrel{a.s.}{\rightarrow}m_{\infty}(\theta).$
\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $E\bar{m}_{n}(\theta^{0})=0$
\end_inset

 by assumption, 
\begin_inset Formula $m_{\infty}(\theta^{0})=0.$
\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $s_{\infty}(\theta^{0})=m_{\infty}(\theta^{0})^{\prime}W_{\infty}m_{\infty}(\theta^{0})=0,$
\end_inset

 in order for asymptotic identification, we need that 
\begin_inset Formula $m_{\infty}(\theta)\neq0$
\end_inset

 for 
\begin_inset Formula $\theta\neq\theta^{0},$
\end_inset

 for at least some element of the vector.
 There can be no other parameter value that sets the moment conditions to
 zero (at least, in the limit).
 
\emph on
Draw picture here.
 
\emph default
This and the assumption that 
\begin_inset Formula $W_{n}\stackrel{a.s.}{\rightarrow}$
\end_inset

 
\begin_inset Formula $W_{\infty},$
\end_inset

 a finite positive 
\begin_inset Formula $g\times g$
\end_inset

 definite 
\begin_inset Formula $g\times g$
\end_inset

 matrix guarantee that 
\begin_inset Formula $\theta^{0}$
\end_inset

 is asymptotically identified.
\end_layout

\begin_layout Itemize
Note that asymptotic identification does not rule out the possibility of
 lack of identification for a given data set - there may be multiple minimizing
 solutions in finite samples.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Increase 
\begin_inset Formula $n$
\end_inset

 in the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/chi2gmm.m}{https://github.com/mcreel/Econometrics/blob/maste
r/Examples/GMM/chi2gmm.m}
\end_layout

\end_inset

 to see evidence of the consistency of the GMM estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Asymptotic normality
\end_layout

\begin_layout Standard
We also simply assume that the conditions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

 hold, so we will have asymptotic normality.
 However, we do need to find the structure of the asymptotic variance-covariance
 matrix of the estimator.
 From Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

, we have 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta^{0})^{-1}\mathcal{I}_{\infty}(\theta^{0})\mathcal{J}_{\infty}(\theta^{0})^{-1}\right]
\]

\end_inset

 where 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta^{0})$
\end_inset

 is the almost sure limit of 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{n}(\theta)$
\end_inset

 when evaluated at 
\begin_inset Formula $\theta^{0}$
\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta^{0})=\lim_{n\rightarrow\infty}Var\sqrt{n}\frac{\partial}{\partial\theta}s_{n}(\theta^{0}).
\]

\end_inset

 We need to determine the form of these matrices given the objective function
 
\begin_inset Formula $s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta).$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Now, using the product rule from section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Notation-for-differentiation"

\end_inset

, 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}s_{n}(\theta)=2\left[\frac{\partial}{\partial\theta}\bar{m}_{n}^{\prime}\left(\theta\right)\right]W_{n}\bar{m}_{n}\left(\theta\right)
\]

\end_inset

 (this is analogous to 
\begin_inset Formula $\frac{\partial}{\partial\beta}\beta^{\prime}X^{\prime}X\beta=2X^{\prime}X\beta$
\end_inset

 which appears when computing the first order conditions for the OLS estimator).
\end_layout

\begin_layout Standard
Define the 
\begin_inset Formula $k\times g$
\end_inset

 matrix 
\begin_inset Formula 
\[
D_{n}(\theta)\equiv\frac{\partial}{\partial\theta}\bar{m}_{n}^{\prime}\left(\theta\right),
\]

\end_inset

 so:
\begin_inset Formula 
\begin{equation}
\frac{\partial}{\partial\theta}s(\theta)=2D(\theta)W\bar{m}\left(\theta\right).\label{gmmscores}
\end{equation}

\end_inset

 (Note that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

, 
\begin_inset Formula $D_{n}(\theta),$
\end_inset

 
\begin_inset Formula $W_{n}$
\end_inset

 and 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 all depend on the sample size 
\begin_inset Formula $n,$
\end_inset

 but it is omitted to unclutter the notation).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To take second derivatives, let 
\begin_inset Formula $D_{i}$
\end_inset

 be the 
\begin_inset Formula $i-$
\end_inset

 th row of 
\begin_inset Formula $D(\theta).$
\end_inset

 This is a 
\begin_inset Formula $1\times g$
\end_inset

 row vector, and 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta_{i}}s(\theta)=2D_{i}(\theta)W\bar{m}\left(\theta\right)
\]

\end_inset


\series bold
is a scalar
\series default
, which is the 
\begin_inset Formula $i$
\end_inset

th row of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
the column vector 
\begin_inset Formula $\frac{\partial}{\partial\theta}s(\theta)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
.
 The 
\begin_inset Formula $i$
\end_inset

th row of the matrix of second derivatives is (using the product rule in
 definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "def: Product-rule.-Let"

\end_inset

), 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\theta^{\prime}}\frac{\partial}{\partial\theta_{i}}s(\theta) & = & \frac{\partial}{\partial\theta^{\prime}}\left[2D_{i}(\theta)W\bar{m}\left(\theta\right)\right]\\
 & = & 2D_{i}WD^{\prime}+2\bar{m}^{\prime}W\left[\frac{\partial}{\partial\theta^{\prime}}D_{i}^{\prime}\right]
\end{eqnarray*}

\end_inset

Note that the first term contains a 
\begin_inset Formula $D^{\prime},$
\end_inset

 which appears due to 
\begin_inset Formula $\frac{\partial}{\partial\theta^{\prime}}\bar{m}_{n}\left(\theta\right)$
\end_inset

.
 When evaluating the second term: 
\begin_inset Formula 
\[
2\bar{m}(\theta)^{\prime}W\left[\frac{\partial}{\partial\theta^{\prime}}D(\theta)_{i}^{\prime}\right]
\]

\end_inset

(where the dependence of 
\begin_inset Formula $D$
\end_inset

 upon 
\begin_inset Formula $\theta$
\end_inset

 is emphasized) at 
\begin_inset Formula $\theta^{0},$
\end_inset

 assume that 
\begin_inset Formula $\frac{\partial}{\partial\theta^{\prime}}D(\theta)_{i}^{\prime}$
\end_inset

 satisfies a LLN (it is an average), so that it converges almost surely
 to a finite limit.
 In this case, we have 
\begin_inset Formula 
\[
2\bar{m}(\theta^{0})^{\prime}W\left[\frac{\partial}{\partial\theta^{\prime}}D(\theta^{0})_{i}^{\prime}\right]\stackrel{a.s.}{\rightarrow}0,
\]

\end_inset

because 
\begin_inset Formula $\bar{m}(\theta^{0})\stackrel{a.s.}{\rightarrow}0$
\end_inset

 and 
\begin_inset Formula $W\stackrel{a.s.}{\rightarrow}$
\end_inset

 
\begin_inset Formula $W_{\infty}$
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Stacking these results over the 
\begin_inset Formula $k$
\end_inset

 rows of 
\begin_inset Formula $D,$
\end_inset

 we get 
\begin_inset Formula 
\[
\lim\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{n}(\theta^{0})=\mathcal{J}_{\infty}(\theta^{0})=2D_{\infty}W_{\infty}D_{\infty}^{\prime},a.s.,
\]

\end_inset

 where we define 
\begin_inset Formula $\lim D=D_{\infty},$
\end_inset

 
\begin_inset Formula $a.s.,$
\end_inset

 and 
\begin_inset Formula $\lim W=W_{\infty},$
\end_inset

 a.s.
 (we assume a LLN holds).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
With regard to 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta^{0})$
\end_inset

, following equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "gmmscores"

\end_inset

, and noting that the scores have mean zero at 
\begin_inset Formula $\theta^{0}$
\end_inset

 (since 
\begin_inset Formula $\mathcal{E}\bar{m}(\theta^{0})=0$
\end_inset

 by assumption), we have
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta^{0}) & = & \lim_{n\rightarrow\infty}Var\sqrt{n}\frac{\partial}{\partial\theta}s_{n}(\theta^{0})\\
 & = & \lim_{n\rightarrow\infty}\mathcal{E}4nDW\bar{m}(\theta^{0})\bar{m}(\theta^{0})^{\prime}WD^{\prime}\\
 & = & \lim_{n\rightarrow\infty}\mathcal{E}4DW\left\{ \sqrt{n}\bar{m}(\theta^{0})\right\} \left\{ \sqrt{n}\bar{m}(\theta^{0})^{\prime}\right\} WD^{\prime}
\end{eqnarray*}

\end_inset

 Now, given that 
\begin_inset Formula $\bar{m}(\theta^{0})$
\end_inset

 is an average of centered (mean-zero) quantities, it is reasonable to expect
 a CLT to apply, after multiplication by 
\begin_inset Formula $\sqrt{n}$
\end_inset

.
 Assuming this, 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\bar{m}(\theta^{0})\stackrel{d}{\rightarrow}N(0,\Omega_{\infty}),\label{eq:CLT applied to moment conditions}
\end{equation}

\end_inset

 where 
\begin_inset Formula 
\[
\Omega_{\infty}=\lim_{n\rightarrow\infty}\mathcal{E}\left[n\bar{m}(\theta^{0})\bar{m}(\theta^{0})^{\prime}\right].
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Using this, and the last equation, we get 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta^{0})=4D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}
\]

\end_inset

 Using these results, the asymptotic normality theorem (
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

) gives us 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}\right],
\]

\end_inset

 the asymptotic distribution of the GMM estimator for arbitrary weighting
 matrix 
\begin_inset Formula $W_{n}.$
\end_inset

 Note that for 
\begin_inset Formula $J_{\infty}$
\end_inset

 to be positive definite, 
\begin_inset Formula $D_{\infty}$
\end_inset

 must have full row rank, 
\begin_inset Formula $\rho(D_{\infty})=k$
\end_inset

.
 This is related to identification.
 If the rows of 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 were not linearly independent of one another, then neither 
\begin_inset Formula $D_{n}$
\end_inset

 nor 
\begin_inset Formula $D_{\infty}$
\end_inset

 would have full row rank.
 Identification plus two times differentiability of the objective function
 lead to 
\begin_inset Formula $J_{\infty}$
\end_inset

 being positive definite.
\end_layout

\begin_layout Standard
There are two things that affect the asymptotic variance:
\end_layout

\begin_layout Itemize
the choice of the moment conditions, 
\begin_inset Formula $\bar{m}_{n}(\theta$
\end_inset

), which determines both 
\begin_inset Formula $D_{\infty}$
\end_inset

 and 
\begin_inset Formula $\Omega_{\infty}$
\end_inset


\end_layout

\begin_layout Itemize
the choice of the weight matrix 
\begin_inset Formula $W_{n}$
\end_inset

, which determines 
\begin_inset Formula $W_{\infty}$
\end_inset


\end_layout

\begin_layout Standard
We would probably like to know how to choose both 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 and 
\begin_inset Formula $W_{n}$
\end_inset

 so that the asymptotic variance is a small as possible.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/AsymptoticNormalityGMM.jl}{https://github.com/mcreel/Econome
trics/blob/master/Examples/GMM/AsymptoticNormalityGMM.jl}
\end_layout

\end_inset

 does a Monte Carlo of the GMM estimator for the 
\begin_inset Formula $\chi^{2}$
\end_inset

 data.
 Histograms for 1000 replications of 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)$
\end_inset

 are given in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Asymptotic-Normality-of"

\end_inset

.
 On the left are results for 
\begin_inset Formula $n=10,$
\end_inset

 on the right are results for 
\begin_inset Formula $n=1000.$
\end_inset

 Note that the two distributions are more or less centered at 0.
 The distribution for the small sample size is somewhat asymmetric, which
 shows that the small sample distribution may be poorly approximated by
 the asymptotic distribution.
 This has mostly disappeared for the larger sample size.
\begin_inset Newpage newpage
\end_inset

 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Asymptotic-Normality-of"

\end_inset

Asymptotic Normality of GMM estimator, 
\begin_inset Formula $\chi^{2}$
\end_inset

 example
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $n=10$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Asnorm_n30.svg
	lyxscale 50
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $n=1000$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Asnorm_n1000.svg
	lyxscale 50
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Choosing the weighting matrix
\end_layout

\begin_layout Standard
\begin_inset Formula $W$
\end_inset

 is a 
\shape italic
weighting matrix,
\shape default
 which determines the relative importance of violations of the individual
 moment conditions.
 For example, if we are much more sure of the first moment condition, which
 is based upon the variance, than of the second, which is based upon the
 fourth moment, we could set 
\begin_inset Formula 
\[
W=\left[\begin{array}{cc}
a & 0\\
0 & b
\end{array}\right]
\]

\end_inset

 with 
\begin_inset Formula $a$
\end_inset

 much larger than 
\begin_inset Formula $b.$
\end_inset

 In this case, errors in the second moment condition have less weight in
 the objective function.
\end_layout

\begin_layout Itemize
Since moments are not independent, in general, we should expect that there
 be a correlation between the moment conditions, so it may not be desirable
 to set the off-diagonal elements to 0.
 
\begin_inset Formula $W$
\end_inset

 may be a random, data dependent matrix.
\end_layout

\begin_layout Itemize
We have already seen that the choice of 
\begin_inset Formula $W$
\end_inset

 will influence the asymptotic distribution of the GMM estimator.
 Since the GMM estimator is already inefficient w.r.t.
 MLE, we might like to choose the 
\begin_inset Formula $W$
\end_inset

 matrix to make the GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator efficient 
\emph on
within the class of GMM estimators
\emph default
 defined by 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
To provide a little intuition, consider the linear model 
\begin_inset Formula $y=\mathbf{x}^{\prime}\beta+\varepsilon,$
\end_inset

 where 
\begin_inset Formula $\varepsilon\sim N(0,\Omega).$
\end_inset

 That is, he have heteroscedasticity and autocorrelation.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The generalized least square estimator minimizes the objective function
 
\begin_inset Formula $(y-\mathbf{X}\beta)^{\prime}\Omega^{-1}(y-\mathbf{X}\beta).$
\end_inset

 We have seen that the GLS estimator is efficient with respect to OLS, when
 there is het.
 and or aut.
 
\end_layout

\begin_layout Itemize
The GLS optimal weighting matrix is seen to be the inverse of the covariance
 matrix of the errors.
 This result carries over to GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimation.
 
\end_layout

\begin_layout Itemize
Note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

this presentation of GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

is not a GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator as defined above, because if we take the errors as 
\begin_inset Quotes sld
\end_inset

moment conditions
\begin_inset Quotes srd
\end_inset

, the dimension is the sample size, 
\begin_inset Formula $n.$
\end_inset

 Thus, the dimension is not fixed.
 Also, they are not averages, as we require - see definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "GMM estimator (defn)"

\end_inset

.
 Later we'll see that GLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

can be expressed in the GMM framework.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "efficient weighting matrix"

\end_inset

If 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator that minimizes 
\begin_inset Formula $\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta),$
\end_inset

 the asymptotic variance of 
\begin_inset Formula $\hat{\theta}$
\end_inset

 will be minimized by choosing 
\begin_inset Formula $W_{n}$
\end_inset

 so that 
\begin_inset Formula $W_{n}\stackrel{a.s}{\rightarrow}W_{\infty}=\Omega_{\infty}^{-1},$
\end_inset

 where 
\begin_inset Formula $\Omega_{\infty}=\lim_{n\rightarrow\infty}\mathcal{E}\left[nm(\theta^{0})m(\theta^{0})^{\prime}\right].$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Proof:
\series default
 For 
\begin_inset Formula $W_{\infty}=\Omega_{\infty}^{-1},$
\end_inset

 the asymptotic variance 
\begin_inset Formula 
\[
\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}
\]

\end_inset

 simplifies to 
\begin_inset Formula $\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}.$
\end_inset

 Now, let 
\begin_inset Formula $A$
\end_inset

 be the difference between the general form and the simplified form: 
\begin_inset Formula 
\begin{eqnarray*}
A=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1} & - & \left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}
\end{eqnarray*}

\end_inset

Set 
\begin_inset Formula $B=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}-\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1}$
\end_inset

.
 One can show that 
\begin_inset Formula $A=B\Omega_{\infty}B^{'}$
\end_inset

.
 This is a quadratic form in a p.d.
 matrix, so it is p.s.d., which concludes the proof.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
The result
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}\right]\label{gmm distribution with optimal weighting matrix}
\end{equation}

\end_inset

 allows us to treat 
\begin_inset Formula 
\[
\hat{\theta}\approx N\left(\theta^{0},\frac{\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}}{n}\right),
\]

\end_inset

 where the 
\begin_inset Formula $\approx$
\end_inset

 means 
\begin_inset Quotes erd
\end_inset

approximately distributed as.
\begin_inset Quotes erd
\end_inset

 To operationalize this we need estimators of 
\begin_inset Formula $D_{\infty}$
\end_inset

 and 
\begin_inset Formula $\Omega_{\infty}.$
\end_inset


\end_layout

\begin_layout Itemize
The obvious estimator of 
\begin_inset Formula $\widehat{D_{\infty}}$
\end_inset

 is simply 
\begin_inset Formula $\frac{\partial}{\partial\theta}\bar{m}_{n}\left(\hat{\theta}\right),$
\end_inset

 which is consistent by the consistency of 
\begin_inset Formula $\hat{\theta},$
\end_inset

 assuming that 
\begin_inset Formula $\frac{\partial}{\partial\theta}\bar{m}_{n}$
\end_inset

 is continuous in 
\begin_inset Formula $\theta.$
\end_inset

 Stochastic equicontinuity results can give us this result even if 
\begin_inset Formula $\frac{\partial}{\partial\theta}\bar{m}_{n}$
\end_inset

 is not continuous.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
To see the effect of using an efficient weight matrix, consider the Julia
 script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/EfficientGMM.jl}{https://github.com/mcreel/Econometrics/blob
/master/Examples/GMM/EfficientGMM.jl}
\end_layout

\end_inset

.
 This modifies the previous Monte Carlo for the 
\begin_inset Formula $\chi^{2}$
\end_inset

 data.
 This new Monte Carlo computes the GMM estimator in two ways:
\begin_inset Newline newline
\end_inset

1) based on an identity weight matrix
\begin_inset Newline newline
\end_inset

2) using an estimated optimal weight matrix.
 The estimated efficient weight matrix is computed as the inverse of the
 estimated covariance of the moment conditions, using the inefficient estimator
 of the first step.
 See the next section for more on how to do this.
\begin_inset Newline newline
\end_inset

Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inefficient-and-Efficient"

\end_inset

 shows the results, plotting densities for 1000 replications of 
\begin_inset Formula $\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)$
\end_inset

.
 Note that the use of the estimated efficient weight matrix leads to much
 better results in this case.
 This is a simple case where it is possible to get a good estimate of the
 efficient weight matrix.
 This is not always so.
 See the next section.
\end_layout

\begin_layout Example
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Inefficient-and-Efficient"

\end_inset

Inefficient and Efficient GMM estimators, 
\begin_inset Formula $\chi^{2}$
\end_inset

 data, 
\begin_inset Formula $n=30$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Efficient.svg
	lyxscale 50
	width 10cm

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example

\end_layout

\begin_layout Section
Estimation of the variance-covariance matrix
\end_layout

\begin_layout Standard

\series bold
(See Hamilton Ch.
 10, pp.
 261-2 and 280-84)
\series default

\begin_inset Formula $^{*}$
\end_inset


\series bold
.
\end_layout

\begin_layout Standard
In the case that we wish to use the optimal weighting matrix, we need an
 estimate of 
\begin_inset Formula $\Omega_{\infty},$
\end_inset

 the limiting variance-covariance matrix of 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\theta^{0})$
\end_inset

.
 In general, we expect that:
\end_layout

\begin_layout Itemize
\begin_inset Formula $m_{t}$
\end_inset

 will be autocorrelated (
\begin_inset Formula $\Gamma_{ts}=\mathcal{E}(m_{t}m_{t-s}^{\prime})\neq0$
\end_inset

).
 Note that this autocovariance will not depend on 
\begin_inset Formula $t$
\end_inset

 if the moment conditions are covariance stationary.
\end_layout

\begin_layout Itemize
contemporaneously correlated, since the individual moment contributions
 will not in general be independent of one another (
\begin_inset Formula $\mathcal{E}(m_{it}m_{jt})\neq0$
\end_inset

).
\end_layout

\begin_layout Itemize
and have different variances (
\begin_inset Formula $\mathcal{E}(m_{it}^{2})=\sigma_{it}^{2}$
\end_inset

 ).
 
\end_layout

\begin_layout Standard
Since we need to estimate so many components, it is unlikely that we would
 arrive at a correct parametric specification.
 For this reason, research has focused on consistent nonparametric estimators
 of 
\begin_inset Formula $\Omega_{\infty}.$
\end_inset

 
\end_layout

\begin_layout Standard
Henceforth we assume that 
\begin_inset Formula $m_{t}$
\end_inset

 is 
\emph on
covariance stationary
\emph default
, so the covariance between 
\begin_inset Formula $m_{t}$
\end_inset

 and 
\begin_inset Formula $m_{t-s}$
\end_inset

 does not depend on 
\begin_inset Formula $t.$
\end_inset

 (See the first part of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Models-for-time"

\end_inset

 for the definition).
 Thus, 
\end_layout

\begin_layout Definition
(Autocovariance).
 Define the 
\begin_inset Formula $s-th$
\end_inset

 autocovariance of covariance stationary moment contributions as 
\begin_inset Formula $\Gamma_{s}=\mathcal{E}(m_{t}m_{t-s}^{\prime}).$
\end_inset

 
\end_layout

\begin_layout Standard
Because of stationarity, 
\begin_inset Formula $\Gamma_{s}$
\end_inset

 does not depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Exercise
Show that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+s}^{\prime})=\Gamma_{s}^{\prime}.$
\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Recall that 
\begin_inset Formula $m_{t}$
\end_inset

 and 
\begin_inset Formula $\bar{m}_{n}$
\end_inset

 are functions of 
\begin_inset Formula $\theta,$
\end_inset

 so for now assume that we have some consistent estimator of 
\begin_inset Formula $\theta^{0}$
\end_inset

.
 With this, a consistent estimator of 
\begin_inset Formula $m_{t}(\theta^{0})$
\end_inset

 is 
\begin_inset Formula $\hat{m}_{t}=m_{t}(\hat{\theta}).$
\end_inset

 Now 
\begin_inset Formula 
\begin{eqnarray*}
\Omega_{n} & = & \mathcal{E}\left[n\bar{m}_{n}(\theta^{0})\bar{m}_{n}(\theta^{0})^{\prime}\right]=\mathcal{E}\left[n\left(1/n\sum_{t=1}^{n}m_{t}\right)\left(1/n\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]\\
 & = & \mathcal{E}\left[1/n\left(\sum_{t=1}^{n}m_{t}\right)\left(\sum_{t=1}^{n}m_{t}^{\prime}\right)\right]\\
 & = & \Gamma_{0}+\frac{n-1}{n}\left(\Gamma_{1}+\Gamma_{1}^{\prime}\right)+\frac{n-2}{n}\left(\Gamma_{2}+\Gamma_{2}^{\prime}\right)\cdots+\frac{1}{n}\left(\Gamma_{n-1}+\Gamma_{n-1}^{\prime}\right)
\end{eqnarray*}

\end_inset

 A natural, consistent estimator of 
\begin_inset Formula $\Gamma_{s}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\Gamma_{s}}=1/n\sum_{t=s+1}^{n}\hat{m}_{t}\hat{m}_{t-s}^{\prime}.
\]

\end_inset

 (you might use 
\begin_inset Formula $n-s$
\end_inset

 in the denominator instead).
\begin_inset Newpage newpage
\end_inset

 So, a natural, but inconsistent, estimator of 
\begin_inset Formula $\Omega_{\infty}$
\end_inset

 would be 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Omega} & = & \widehat{\Gamma_{0}}+\frac{n-1}{n}\left(\widehat{\Gamma_{1}}+\widehat{\Gamma_{1}^{\prime}}\right)+\frac{n-2}{n}\left(\widehat{\Gamma_{2}}+\widehat{\Gamma_{2}^{\prime}}\right)+\cdots+\left(\widehat{\Gamma_{n-1}}+\widehat{\Gamma_{n-1}^{\prime}}\right)\\
 & = & \widehat{\Gamma_{0}}+\sum_{s=1}^{n-1}\frac{n-s}{n}\left(\widehat{\Gamma_{s}}+\widehat{\Gamma_{s}^{\prime}}\right).
\end{eqnarray*}

\end_inset

 This estimator is inconsistent in general, since the number of parameters
 to estimate is more than the number of observations, and increases more
 rapidly than 
\begin_inset Formula $n$
\end_inset

, so information does not build up as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 There is always only one observation to estimate the highest order autocovarian
ce.
\end_layout

\begin_layout Standard
On the other hand, supposing that 
\begin_inset Formula $\Gamma_{s}$
\end_inset

 tends to zero sufficiently rapidly as 
\begin_inset Formula $s$
\end_inset

 tends to 
\begin_inset Formula $\infty,$
\end_inset

 a modified estimator is 
\begin_inset Formula 
\[
\hat{\Omega}=\widehat{\Gamma_{0}}+\sum_{s=1}^{q(n)}\left(\widehat{\Gamma_{s}}+\widehat{\Gamma_{s}^{\prime}}\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
this will be consistent, provided that
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $q(n)\stackrel{p}{\rightarrow}\infty$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

,
\end_layout

\begin_layout Itemize
\begin_inset Formula $q(n)$
\end_inset

 grows sufficiently slowly.
\end_layout

\begin_layout Itemize
The term 
\begin_inset Formula $\frac{n-s}{n}$
\end_inset

 can be dropped because it converges to 1.
\end_layout

\begin_layout Itemize
A disadvantage of this estimator is that it may not be positive definite.
 This could cause one to calculate a negative 
\begin_inset Formula $\chi^{2}$
\end_inset

 statistic, for example!
\begin_inset Newpage newpage
\end_inset

 
\end_layout

\end_deeper
\begin_layout Subsection
Newey-West covariance estimator
\end_layout

\begin_layout Standard
The Newey-West estimator (
\begin_inset CommandInset citation
LatexCommand citealp
key "NeweyWest1987"
literal "true"

\end_inset

) solves the problem of possible non-positive definiteness of the above
 estimator.
 Their estimator is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\Omega}=\widehat{\Gamma_{0}}+\sum_{s=1}^{q(n)}\left[1-\frac{s}{q+1}\right]\left(\widehat{\Gamma_{s}}+\widehat{\Gamma_{s}^{\prime}}\right).
\]

\end_inset

 This estimator is p.d.
 by construction.
 The condition for consistency is that 
\begin_inset Formula $n^{-1/4}q\rightarrow0.$
\end_inset

 Note that this is a very slow rate of growth for 
\begin_inset Formula $q.$
\end_inset

 This estimator is nonparametric - we've placed no parametric restrictions
 on the form of 
\begin_inset Formula $\Omega.$
\end_inset

 It is an example of a 
\shape italic
kernel
\shape default
 estimator.
 Kernel estimators are discussed in more detail in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Nonparametric-inference"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Around the same time as the paper by Newey and West, a number of other similar
 covariance matrix estimators were proposed, but the NW estimator seems
 to be the most widely used in empirical work.
\end_layout

\begin_layout Itemize
If there is no autocorrelation of the moments, then all 
\begin_inset Formula $\Gamma_{s},\,s>0$
\end_inset

 may be set to zero.
 The result is White's heteroscedastic consistent variance covariance estimator,
 
\begin_inset CommandInset citation
LatexCommand citet
key "white1980heteroskedasticity"
literal "true"

\end_inset

.
\end_layout

\begin_layout Itemize
A Julia implementation is in the 
\family typewriter
Econometrics.jl
\family default
 package, at 
\begin_inset CommandInset href
LatexCommand href
name "NeweyWest.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/src/NP/NeweyWest.jl"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
see Gretl User Manual additional information, and use Gretl for examples
 of both:
\end_layout

\begin_deeper
\begin_layout Itemize
het: Nerlove model: do resid.
 plot, add square of log output.
 Test for HET
\end_layout

\begin_layout Itemize
aut: NYSE data, square of log difference of closing price: test for AUT,
 estimate with and without HAC.
 Note how t-stat declines.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Two step and continuously updated GMM estimators
\end_layout

\begin_layout Standard

\series bold
Two step GMM estimator:
\end_layout

\begin_layout Standard
The most common way to do efficient GMM estimation is the two step GMM estimator
:
\end_layout

\begin_layout Enumerate
Set the weight matrix to some positive definite matrix.
 Most commonly, one uses an identity matrix of order 
\begin_inset Formula $g.$
\end_inset

 Obtain the GMM estimator that minimizes 
\begin_inset Formula $s_{n}(\theta)=m_{n}(\theta)^{\prime}Wm_{n}(\theta)$
\end_inset


\end_layout

\begin_layout Enumerate
Based on this initial estimate, 
\begin_inset Formula $\hat{\theta}$
\end_inset

, say, compute the moment contributions 
\begin_inset Formula $m_{t}(\hat{\theta}),\,t=1,2,...,n$
\end_inset

.
 Compute an estimate of 
\begin_inset Formula $\Omega_{\infty}$
\end_inset

 based on the moment contributions, say 
\begin_inset Formula $\hat{\Omega}^{-1}$
\end_inset

.
 The exact way to do this will depend upon the assumptions of the model.
 For example, if moment conditions are autocorrelated, one might use the
 Newey-West estimator.
 Given the estimate, compute the efficient GMM estimator which minimizes
\begin_inset Formula 
\[
s_{n}(\theta)=m_{n}(\theta)^{\prime}\hat{\Omega}^{-1}m_{n}(\theta).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\hat{\Omega}^{-1}$
\end_inset

 is fixed while numeric minimization finds the second step estimator.
 The result is the two step estimator.
 
\end_layout

\begin_layout Itemize
An example of this is given by running 
\family typewriter
gmmresults()
\family default
, which is included in the 
\family typewriter
Econometrics.jl
\family default
 package.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Continuously updated GMM estimator:
\end_layout

\begin_layout Standard
The continuously updated estimator (
\begin_inset CommandInset citation
LatexCommand citet
key "HansenHeatonYaron1996"
literal "true"

\end_inset

) solves a minimization problem where the efficient weight matrix is estimated
 at each iteration of the numeric optimization process.
 The CUE estimator solves the minimization problem 
\begin_inset Formula 
\[
s_{n}(\theta)=m_{n}(\theta)^{\prime}\hat{\Omega}(\theta)^{-1}m_{n}(\theta).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that the covariance of the moment conditions will be updated at each
 trial value of the objective function during the course of minimization.
 
\end_layout

\begin_layout Itemize
This estimator is equivalent to an iterated version of the two step estimator.
 
\end_layout

\begin_layout Itemize
The CUE estimator can be shown to have a smaller bias than does the two
 step estimator, which may have a large small sample bias (
\begin_inset CommandInset citation
LatexCommand citet
key "NeweySmith2003"
literal "true"

\end_inset

).
 
\end_layout

\begin_layout Itemize
An example of CUE estimation is given by running 
\family typewriter
gmmresults()
\family default
, which is included in the 
\family typewriter
Econometrics.jl
\family default
 package.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Estimation-using-conditional"

\end_inset

Estimation using conditional moments
\end_layout

\begin_layout Standard
So far, the moment conditions have been presented as unconditional expectations.
 One common way of defining unconditional moment conditions is based upon
 conditional moment conditions.
\end_layout

\begin_layout Standard
Suppose that a random variable 
\begin_inset Formula $Y$
\end_inset

 has zero expectation conditional on the random variable 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\[
\mathcal{E}_{Y|X}Y=\int Yf(Y|X)dY=0
\]

\end_inset

 Then the unconditional expectation of the product of 
\begin_inset Formula $Y$
\end_inset

 and a function 
\begin_inset Formula $g(X)$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 is also zero.
 
\begin_inset Newpage newpage
\end_inset

To see this, the unconditional expectation is 
\begin_inset Formula 
\[
\mathcal{E}\left(Yg(X)\right)=\int_{\mathcal{X}}\left(\int_{\mathcal{Y}}Yg(X)f(Y,X)dY\right)dX.
\]

\end_inset

 Factor the joint density in to conditional and marginal:
\begin_inset Formula 
\[
\mathcal{E}\left(Yg(X)\right)=\int_{\mathcal{X}}\left(\int_{\mathcal{Y}}Yg(X)f(Y|X)f(X)dY\right)dX.
\]

\end_inset

 Because 
\begin_inset Formula $f(X)$
\end_inset

 and 
\begin_inset Formula $g(X)$
\end_inset

 don't depend on 
\begin_inset Formula $Y$
\end_inset

, they can be pulled out of the integral 
\begin_inset Formula 
\[
\mathcal{E}\left(Yg(X)\right)=\int_{\mathcal{X}}\left(\int_{\mathcal{Y}}Yf(Y|X)dY\right)g(X)f(X)dX.
\]

\end_inset

 But the term in parentheses on the rhs is zero by assumption, so 
\begin_inset Formula 
\[
\mathcal{E}\left(Yg(X)\right)=0
\]

\end_inset

 as claimed.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
This is important from the point of view of constructing an econometric
 model, since economic models often imply restrictions on conditional moments.
\end_layout

\begin_layout Itemize
Suppose a model tells us that the function 
\begin_inset Formula $K(y_{t},x_{t},\theta)$
\end_inset

 has expectation, conditional on the information set 
\begin_inset Formula $I_{t},$
\end_inset

 equal to 
\begin_inset Formula $k(x_{t},\theta),$
\end_inset


\begin_inset Formula 
\[
\mathcal{E}K(y_{t},x_{t},\theta)|I_{t}=k(x_{t},\theta).
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For example, in the context of the classical linear model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\varepsilon_{t},$
\end_inset

 we can set 
\begin_inset Formula $K(y_{t},x_{t},\theta)=y_{t}$
\end_inset

.
 The information set includes the exogenous regressors 
\begin_inset Formula $x_{t},$
\end_inset

 so 
\begin_inset Formula 
\begin{align*}
EK(y_{t},x_{t},\theta)|I_{t} & =E_{\theta}\left(x_{t}^{\prime}\beta+\varepsilon_{t}|x_{t}\right)=k(x_{t},\theta)=x_{t}^{\prime}\beta
\end{align*}

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In the case of the simultaneous equations model of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:demand function"

\end_inset

, we have
\begin_inset Formula 
\[
q_{t}=\alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}m_{t}+\varepsilon_{1t}
\]

\end_inset

The information set includes income 
\begin_inset Formula $m_{t},$
\end_inset

 because it is an exogenous variable.
 The endogenous variables are 
\begin_inset Formula $y_{t}=(q_{t},p_{t})$
\end_inset

 and the exogenous variable is 
\begin_inset Formula $m_{t}$
\end_inset

.
 Set 
\begin_inset Formula $K(y_{t},x_{t},\theta)=q_{t}-\alpha_{2}p_{t}$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{align*}
k(x_{t},\theta) & =E\left(q_{t}-\alpha_{2}p_{t}|m_{t}\right)\\
 & =E(\alpha_{1}+\alpha_{3}m_{t}+\epsilon_{1t}|m_{t})\\
 & =\alpha_{1}+\alpha_{3}m_{t}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

If 
\begin_inset Formula $k(x_{t},\theta)$
\end_inset

 is the conditional expectation of 
\begin_inset Formula $K(y_{t},x_{t},\theta),$
\end_inset

 then the error function 
\begin_inset Formula 
\[
\epsilon_{t}(\theta)=K(y_{t},x_{t})-k(x_{t},\theta)
\]

\end_inset

has conditional expectation equal to zero, by construction: 
\begin_inset Formula 
\[
\mathcal{E}\epsilon_{t}(\theta)|I_{t}=0.
\]

\end_inset

This is a scalar moment condition, which isn't sufficient to identify a
 
\begin_inset Formula $K$
\end_inset

 -dimensional parameter 
\begin_inset Formula $\theta$
\end_inset

 
\begin_inset Formula $(K>1)$
\end_inset

.
 However, the above result allows us to form various unconditional expectations
 
\begin_inset Formula 
\[
m_{t}(\theta)=Z(w_{t})\epsilon_{t}(\theta)
\]

\end_inset

where 
\begin_inset Formula $Z(w_{t})$
\end_inset

 is a 
\begin_inset Formula $g\times1$
\end_inset

-vector valued function of 
\begin_inset Formula $w_{t}$
\end_inset

 and 
\begin_inset Formula $w_{t}$
\end_inset

 is a set of variables drawn from the information set 
\begin_inset Formula $I_{t}.$
\end_inset

 The 
\begin_inset Formula $Z(w_{t})\;$
\end_inset

are 
\emph on
instrumental variables.

\emph default
 We now have 
\begin_inset Formula $g$
\end_inset

 moment conditions, so as long as 
\begin_inset Formula $g>K$
\end_inset

 the necessary condition for identification holds.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
One can form the 
\begin_inset Formula $n\times g$
\end_inset

 matrix 
\begin_inset Formula 
\begin{eqnarray*}
Z_{n} & = & \left[\begin{array}{llll}
Z_{1}(w_{1}) & Z_{2}(w_{1}) & \cdots & Z_{g}(w_{1})\\
Z_{1}(w_{2}) & Z_{2}(w_{2}) &  & Z_{g}(w_{2})\\
\vdots &  &  & \vdots\\
Z_{1}(w_{n}) & Z_{2}(w_{n}) & \cdots & Z_{g}(w_{n})
\end{array}\right]\\
 & = & \left[\begin{array}{c}
Z_{1}^{\prime}\\
Z_{2}^{\prime}\\
\\
Z_{n}^{\prime}
\end{array}\right]
\end{eqnarray*}

\end_inset

 With this we can form the 
\begin_inset Formula $g$
\end_inset

 moment conditions 
\begin_inset Formula 
\begin{eqnarray*}
\bar{m}_{n}(\theta) & = & \frac{1}{n}Z_{n}^{\prime}\left[\begin{array}{l}
\epsilon_{1}(\theta)\\
\epsilon_{2}(\theta)\\
\vdots\\
\epsilon_{n}(\theta)
\end{array}\right]
\end{eqnarray*}

\end_inset

With this, we can write
\begin_inset Formula 
\begin{align*}
\bar{m}_{n}(\theta) & =\frac{1}{n}\sum_{t=1}^{n}Z_{t}\epsilon_{t}(\theta)\\
 & =\frac{1}{n}\sum_{t=1}^{n}m_{t}(\theta)
\end{align*}

\end_inset

where 
\begin_inset Formula $Z_{(t,\cdot)}$
\end_inset

 is the 
\begin_inset Formula $t^{th}$
\end_inset

 row of 
\begin_inset Formula $Z_{n}.$
\end_inset

 This fits the previous treatment.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Example:-Generalized-instrumental"

\end_inset

Generalized instrumental variables estimator for linear models
\end_layout

\begin_layout Standard
The IV estimator may appear a bit unusual at first, but it will grow on
 you over time.
 
\end_layout

\begin_layout Standard
Let's look at the previous section's results in more detail, for the commonly
 encountered special case of a linear model with iid errors, but with correlatio
n between regressors and errors: 
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & x_{t}^{\prime}\theta+\varepsilon_{t}\\
\mathcal{E}(x_{t}\varepsilon_{t}) & \neq & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Let's assume, just to keep things simple, that the errors are iid
\end_layout

\begin_layout Itemize
The model in matrix form is 
\begin_inset Formula $y=X\theta+\epsilon$
\end_inset


\end_layout

\begin_layout Standard

\series bold
We have seen some cases where this problem arises:
\end_layout

\begin_layout Enumerate
measurement error of regressors: Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"

\end_inset


\end_layout

\begin_layout Enumerate
lagged dependent variable and autocorrelated errors: Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:-Dynamic-model"

\end_inset


\end_layout

\begin_layout Enumerate
simultaneous equations: Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Reduced-form"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Let 
\begin_inset Formula $K=dim(x_{t}).$
\end_inset

 Consider some vector 
\begin_inset Formula $z_{t}$
\end_inset

 of dimension 
\begin_inset Formula $G\times1$
\end_inset

, where 
\begin_inset Formula $G\ge K.$
\end_inset

 Assume that 
\begin_inset Formula $E(z_{t}\epsilon_{t})=0.$
\end_inset

 The variables 
\begin_inset Formula $z_{t}$
\end_inset

 are 
\emph on
instrumental variables.

\emph default
 
\end_layout

\begin_layout Standard
Consider the moment conditions
\begin_inset Formula 
\begin{align*}
m_{t}(\theta) & =z_{t}\epsilon_{t}\\
 & =z_{t}\left(y_{t}-x_{t}^{\prime}\theta\right)
\end{align*}

\end_inset

We can arrange the instruments in the 
\begin_inset Formula $n\times G$
\end_inset

 matrix
\begin_inset Formula 
\begin{eqnarray*}
Z & = & \left[\begin{array}{c}
z_{1}^{\prime}\\
z_{2}^{\prime}\\
\vdots\\
z_{n}^{\prime}
\end{array}\right]
\end{eqnarray*}

\end_inset

The average moment conditions are
\begin_inset Formula 
\begin{align*}
\bar{m}_{n}(\theta) & =\frac{1}{n}Z^{\prime}\epsilon\\
 & =\frac{1}{n}(Z^{\prime}y-Z^{\prime}X\theta)
\end{align*}

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The 
\emph on
generalized instrumental variables
\emph default
 estimator is just the GMM estimator based upon these moment conditions.
 
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $G=K$
\end_inset

, we have exact identification, and it is referred to as the instrumental
 variables estimator.
 
\end_layout

\begin_layout Itemize
Given the form of the moment conditions, the general formulae for GMM lead
 to particular forms for the GIV estimator:
\end_layout

\begin_layout Standard
The first order conditions for GMM are 
\begin_inset Formula $D_{n}W_{n}\bar{m}_{n}(\hat{\theta})=0$
\end_inset

, which imply that 
\begin_inset Formula 
\[
D_{n}W_{n}Z^{\prime}X\hat{\theta}_{IV}=D_{n}W_{n}Z^{\prime}y
\]

\end_inset


\end_layout

\begin_layout Exercise
Verify that 
\begin_inset Formula $D_{n}=-\frac{X^{\prime}Z}{n}$
\end_inset

.
 Remember that (assuming differentiability) identification of the GMM estimator
 requires that this matrix must converge to a matrix with full row rank.
 Can just any variable that is uncorrelated with the error be used as an
 instrument, or is there some other condition?
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Exercise
Verify that the efficient weight matrix is 
\begin_inset Formula $W_{n}=\left(\frac{Z^{\prime}Z}{n}\right)^{-1}$
\end_inset

 (up to a constant).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

If we accept what is stated in these two exercises, then
\begin_inset Formula 
\[
D_{n}W_{n}Z^{\prime}X\hat{\theta}_{IV}=D_{n}W_{n}Z^{\prime}y
\]

\end_inset

becomes
\begin_inset Formula 
\[
\frac{X^{\prime}Z}{n}\left(\frac{Z^{\prime}Z}{n}\right)^{-1}Z^{\prime}X\hat{\theta}_{IV}=\frac{X^{\prime}Z}{n}\left(\frac{Z^{\prime}Z}{n}\right)^{-1}Z^{\prime}y
\]

\end_inset

Noting that the powers of 
\begin_inset Formula $n$
\end_inset

 cancel, we get
\begin_inset Formula 
\[
X^{\prime}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}X\hat{\theta}_{IV}=X^{\prime}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}y
\]

\end_inset

or
\begin_inset Formula 
\begin{equation}
\hat{\theta}_{IV}=\left(X^{\prime}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}X\right)^{-1}X^{\prime}Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}y\label{eq:GIV estimator}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Another way of arriving to the same point is to define the projection matrix
 
\begin_inset Formula $P_{Z}$
\end_inset

 
\begin_inset Formula 
\[
P_{Z}=Z(Z^{\prime}Z)^{-1}Z^{\prime}
\]

\end_inset

 Anything that is projected onto the space spanned by 
\begin_inset Formula $Z$
\end_inset

 will be uncorrelated with 
\begin_inset Formula $\varepsilon,$
\end_inset

 by the definition of 
\begin_inset Formula $Z.$
\end_inset

 Transforming the model with this projection matrix we get 
\begin_inset Formula 
\[
P_{Z}y=P_{Z}X\beta+P_{Z}\varepsilon
\]

\end_inset

 or 
\begin_inset Formula 
\[
y^{*}=X^{*}\theta+\varepsilon^{*}
\]

\end_inset

 Now we have that 
\begin_inset Formula $\varepsilon^{*}$
\end_inset

 and 
\begin_inset Formula $X^{*}$
\end_inset

 are uncorrelated, since this is simply 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{E}(X^{*\prime}\varepsilon^{*}) & = & \mathcal{E}(X^{\prime}P_{Z}^{\prime}P_{Z}\varepsilon)\\
 & = & \mathcal{E}(X^{\prime}P_{Z}\varepsilon)
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\[
P_{Z}X=Z(Z^{\prime}Z)^{-1}Z^{\prime}X
\]

\end_inset

 is the fitted value from a regression of 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $Z.$
\end_inset

 This is a linear combination of the columns of 
\begin_inset Formula $Z,$
\end_inset

 so it must be uncorrelated with 
\begin_inset Formula $\varepsilon.$
\end_inset

 This implies that applying OLS to the model 
\begin_inset Formula 
\[
y^{*}=X^{*}\theta+\varepsilon^{*}
\]

\end_inset

 will lead to a consistent estimator, given a few more assumptions.
 
\end_layout

\begin_layout Exercise
Verify algebraically that applying OLS to the above model gives the IV estimator
 of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:GIV estimator"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
With the definition of 
\begin_inset Formula $P_{Z}$
\end_inset

, we can write
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{\theta}_{IV}=(X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}y\label{eq:GIVestimator2}
\end{equation}

\end_inset

 from which we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\theta}_{IV} & = & (X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}(X\theta^{0}+\varepsilon)\\
 & = & \theta^{0}+(X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}\varepsilon
\end{eqnarray*}

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\theta}_{IV}-\theta^{0} & = & (X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}\varepsilon\\
 & = & \left(X^{\prime}Z(Z^{\prime}Z)^{-1}Z^{\prime}X\right)^{-1}X^{\prime}Z(Z^{\prime}Z)^{-1}Z^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

 Now we can introduce factors of 
\begin_inset Formula $n$
\end_inset

 to get 
\begin_inset Formula 
\[
\hat{\theta}_{IV}-\theta^{0}=\left(\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}X}{n}\right)\right)^{-1}\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}\varepsilon}{n}\right)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Assuming that each of the terms with a 
\begin_inset Formula $n$
\end_inset

 in the denominator satisfies a LLN, so that
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{Z^{\prime}Z}{n}\overset{p}{\rightarrow}Q_{ZZ}$
\end_inset

, a finite pd matrix
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{X^{\prime}Z}{n}\overset{p}{\rightarrow}Q_{XZ},$
\end_inset

 a finite matrix with rank 
\begin_inset Formula $K$
\end_inset

 (= cols
\begin_inset Formula $(X)$
\end_inset

 ).
 That is to say, the instruments must be correlated with the regressors.
 More precisely, each regressor must be correlated with at least one instrument.
 Otherwise, the row of 
\begin_inset Formula $Q_{XZ}$
\end_inset

 corresponding to that regressor would be all zeros, and thus the rank of
 the matrix would be less than 
\begin_inset Formula $K.$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{Z^{\prime}\varepsilon}{n}\overset{p}{\rightarrow}0$
\end_inset


\end_layout

\begin_layout Standard
then the plim of the rhs is zero.
 This last term has plim 0 because we started with the assumption that 
\begin_inset Formula $Z$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 are uncorrelated, e.g., 
\begin_inset Formula 
\[
\mathcal{E}(z_{t}^{\prime}\varepsilon_{t})=0,
\]

\end_inset

 Given these assumptions, the IV estimator is consistent 
\begin_inset Formula 
\[
\hat{\theta}_{IV}\overset{p}{\rightarrow}\theta^{0}.
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Furthermore, scaling by 
\begin_inset Formula $\sqrt{n,}$
\end_inset

 we have 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}_{IV}-\theta^{0}\right)=\left(\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}X}{n}\right)\right)^{-1}\left(\frac{X^{\prime}Z}{n}\right)\left(\frac{Z^{\prime}Z}{n}\right)^{-1}\left(\frac{Z^{\prime}\varepsilon}{\sqrt{n}}\right)\label{eq:asvarGIV}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Assuming that the far right term satisfies a CLT, so that
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{Z^{\prime}\varepsilon}{\sqrt{n}}\overset{d}{\rightarrow}N(0,Q_{ZZ}\sigma^{2})$
\end_inset

 
\end_layout

\begin_layout Standard
then we get (using some pleasing cancellations)
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{IV}-\theta^{0}\right)\overset{d}{\rightarrow}N\left(0,(Q_{XZ}Q_{ZZ}^{-1}Q_{XZ}^{\prime})^{-1}\sigma^{2}\right)
\]

\end_inset

The adjustment for heteroscedastic or autocorrelated errors should be apparent.
 We just assume that 
\begin_inset Formula $\frac{Z^{\prime}\varepsilon}{\sqrt{n}}\overset{d}{\rightarrow}N(0,\Omega)$
\end_inset

 and work out the algebra (also, see below, in the 2SLS section).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

The estimators for 
\begin_inset Formula $Q_{XZ}$
\end_inset

 and 
\begin_inset Formula $Q_{ZZ}$
\end_inset

 are the obvious ones.
 An estimator for 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\sigma_{IV}^{2}}=\frac{1}{n}\left(y-X\hat{\theta}_{IV}\right)^{\prime}\left(y-X\hat{\theta}_{IV}\right).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that his is computed using the real regressors, 
\begin_inset Formula $X,$
\end_inset

 not the projected regressors, 
\begin_inset Formula $X^{*}$
\end_inset

.
\end_layout

\begin_layout Itemize
This estimator is consistent following the proof of consistency of the OLS
 estimator of 
\begin_inset Formula $\sigma^{2},$
\end_inset

 when the classical assumptions hold.
\end_layout

\begin_layout Standard
The formula used to estimate the variance of 
\begin_inset Formula $\hat{\theta}_{IV}$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{V}(\hat{\theta}_{IV})=\left(\left(X^{\prime}Z\right)\left(Z^{\prime}Z\right)^{-1}\left(Z^{\prime}X\right)\right)^{-1}\widehat{\sigma_{IV}^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
The GIV
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is
\end_layout

\begin_layout Enumerate
Consistent
\end_layout

\begin_layout Enumerate
Asymptotically normally distributed
\end_layout

\begin_layout Enumerate
Biased in general, because even though 
\begin_inset Formula $\mathcal{E}(X^{\prime}P_{Z}\varepsilon)=0,$
\end_inset

 
\begin_inset Formula $\mathcal{E}(X^{\prime}P_{Z}X)^{-1}X^{\prime}P_{Z}\varepsilon$
\end_inset

 may not be zero, because 
\begin_inset Formula $(X^{\prime}P_{Z}X)^{-1}$
\end_inset

 and 
\begin_inset Formula $X^{\prime}P_{Z}\varepsilon$
\end_inset

 are not independent.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

An important point is that the asymptotic distribution of 
\begin_inset Formula $\hat{\beta}_{IV}$
\end_inset

 depends upon 
\begin_inset Formula $Q_{XZ}$
\end_inset

 and 
\begin_inset Formula $Q_{ZZ},$
\end_inset

 and these depend upon the choice of 
\begin_inset Formula $Z.$
\end_inset

 
\emph on
The choice of instruments influences the efficiency of the estimator
\emph default
.
 This point was made, above, when optimal instruments were discussed.
\end_layout

\begin_layout Itemize
When we have two sets of instruments, 
\begin_inset Formula $Z_{1}$
\end_inset

 and 
\begin_inset Formula $Z_{2}$
\end_inset

 such that 
\begin_inset Formula $Z_{1}\subset Z_{2},$
\end_inset

 then the IV estimator using 
\begin_inset Formula $Z_{2}$
\end_inset

 is at least as efficiently asymptotically as the estimator that used 
\begin_inset Formula $Z_{1}.$
\end_inset

 More instruments leads to more asymptotically efficient estimation, in
 general.
 The same holds for GMM in general: adding moment conditions cannot cause
 the asymptotic variance to become larger.
\end_layout

\begin_layout Itemize
The penalty for indiscriminate use of instruments is that the small sample
 bias of the IV
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator rises as the number of instruments increases.
 The reason for this is that 
\begin_inset Formula $P_{Z}X$
\end_inset

 becomes closer and closer to 
\begin_inset Formula $X$
\end_inset

 itself as the number of instruments increases.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:GIV-example.-Recall"

\end_inset

GIV example.
 Recall Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"

\end_inset

 which deals with a dynamic model with measurement error.
 The model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}^{*} & = & \alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}\\
y_{t} & = & y_{t}^{*}+\upsilon_{t}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 are independent Gaussian white noise errors.
 Suppose that 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is not observed, and instead we observe 
\begin_inset Formula $y_{t}$
\end_inset

.
 If we estimate the equation 
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\nu_{t}
\]

\end_inset

by OLS, we have seen in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"

\end_inset

 that the estimator is biased and inconsistent.
 What about using the GIV estimator? Consider using as instruments 
\begin_inset Formula $Z=\left[1\,x_{t}\,x_{t-1}\,x_{t-2}\right]$
\end_inset

.
 The lags of 
\begin_inset Formula $x_{t}$
\end_inset

 are correlated with 
\begin_inset Formula $y_{t-1}$
\end_inset

 as long as 
\begin_inset Formula $\beta$
\end_inset

 is different from zero, and by assumption 
\begin_inset Formula $x_{t}$
\end_inset

 and its lags are uncorrelated with 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 (and thus they're also uncorrelated with 
\begin_inset Formula $\nu_{t})$
\end_inset

.
 Thus, these are legitimate instruments.
 As we have 4 instruments and 3 parameters, this is an overidentified situation.
 The Julia script  
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/MeasurementErrorIV.jl}{https://github.com/mcreel/Econometric
s/blob/master/Examples/GMM/MeasurementErrorIV.jl} 
\end_layout

\end_inset

 does a Monte Carlo study using 1000 replications, with a sample size of
 100.
 The results are comparable with those in Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"

\end_inset

.
 Using the GIV estimator, descriptive statistics for 1000 replications of
 are
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GMM/MeasurementErrorIV.out"

\end_inset

If you compare these with the results for the OLS estimator, you will see
 that the bias of the GIV estimator is much less for estimation of 
\begin_inset Formula $\rho$
\end_inset

.
 If you increase the sample size, you will see that the GIV estimator is
 consistent, but that the OLS estimator is not.
\end_layout

\begin_layout Example
A histogram for 
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

 is in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:GIV-estimation-results"

\end_inset

.
 You can compare with the similar figure for the OLS estimator, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:measurement error"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:GIV-estimation-results"

\end_inset

GIV estimation results for 
\begin_inset Formula $\hat{\rho}-\rho$
\end_inset

, dynamic model with measurement error
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/givrho.svg
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
2SLS
\end_layout

\begin_layout Standard
We can give an alternative formulation of the GIV estimator.
 Let 
\begin_inset Formula $\hat{X}=Z\left(Z^{\prime}Z\right)^{-1}Z^{\prime}X=P_{Z}X$
\end_inset

.
 These are the fitted values from a regression of the regressors upon the
 instruments.
 Substitute this into eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:GIVestimator2"

\end_inset

, to get
\begin_inset Formula 
\[
\hat{\theta}_{IV}=(X^{\prime}\hat{X})^{-1}\hat{X}^{\prime}y
\]

\end_inset

or
\begin_inset Formula 
\[
\hat{\theta}_{IV}=(\hat{X}^{\prime}\hat{X})^{-1}\hat{X}^{\prime}y.
\]

\end_inset

These are equivalent.
 So, the GIV estimator can be obtained by 
\end_layout

\begin_layout Enumerate
first regressing the regressors on the instruments, and obtaining the predicted
 values
\end_layout

\begin_layout Enumerate
then regressing the dependent variable on the predicted regressors.
\end_layout

\begin_layout Standard
It's clear why it's called 2SLS, no? 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:asvarGIV"

\end_inset

 simplifies to
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}_{IV}-\theta^{0}\right)=\left(\frac{\hat{X}^{\prime}\hat{X}}{n}\right)^{-1}\left(\frac{\hat{X}^{\prime}\varepsilon}{\sqrt{n}}\right)\label{eq:asvarGIV-1}
\end{equation}

\end_inset

From this, we can write (allowing for possible HET/AUT)
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{IV}-\theta^{0}\right)\overset{d}{\rightarrow}N\left(0,(Q_{\hat{X}}^{-1}\Omega Q_{\hat{X}}^{-1}\right)
\]

\end_inset

where 
\begin_inset Formula $\Omega=limV\left(\frac{\hat{X}^{\prime}\varepsilon}{\sqrt{n}}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
this can be estimated using White's or Newey-West estimators, as appropriate,
 or simplified further (as above) if the classical assumption regarding
 homoscedasticity and no autocorrelation hold.
\end_layout

\begin_layout Itemize
In either case, it is important to use the residuals 
\begin_inset Formula $y-X\hat{\theta}_{IV},$
\end_inset

 not 
\begin_inset Formula $y-\hat{X}\hat{\theta}_{IV}$
\end_inset

, to estimate 
\begin_inset Formula $\Omega$
\end_inset

 properly.
\end_layout

\begin_layout Itemize
Go to Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Example:-Klein's-Model"

\end_inset

 for an example.
\end_layout

\begin_layout Itemize
We can also estimate this same model using plain GMM estimation, this is
 done in 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Simeq/KleinGMM.m}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/Simeq/KleinGMM.jl}
\end_layout

\end_inset

.
 This script shows the use of the Newey-West covariance estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:A-specification-test"

\end_inset

The Hansen-Sargan (or J) test
\end_layout

\begin_layout Standard
The first order conditions for minimization, using the an estimate of the
 optimal weighting matrix, are 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}s(\hat{\theta})=2\left[\frac{\partial}{\partial\theta}\bar{m}_{n}\left(\hat{\theta}\right)\right]\hat{\Omega}^{-1}\bar{m}_{n}\left(\hat{\theta}\right)\equiv0
\]

\end_inset

 or
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\hat{\theta})\equiv0
\]

\end_inset


\end_layout

\begin_layout Standard
Consider a Taylor expansion of 
\begin_inset Formula $\bar{m}(\hat{\theta})$
\end_inset

 about the true parameter value:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\bar{m}(\hat{\theta})=\bar{m}_{n}(\theta^{0})+D_{n}^{\prime}(\theta^{*})\left(\hat{\theta}-\theta^{0}\right)\label{TS expansion of moments}
\end{equation}

\end_inset

where 
\begin_inset Formula $\theta^{*}$
\end_inset

 is between 
\begin_inset Formula $\hat{\theta}$
\end_inset

 and 
\begin_inset Formula $\theta^{0}$
\end_inset

.
 Multiplying by 
\begin_inset Formula $D(\hat{\theta})\hat{\Omega}^{-1}$
\end_inset

 we obtain 
\begin_inset Formula 
\[
D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}(\hat{\theta})=D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta^{0})+D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\left(\hat{\theta}-\theta^{0}\right)
\]

\end_inset

 The lhs is zero, by the first order conditions for the GMM estimator, so
 
\begin_inset Formula 
\[
D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta^{0})=-\left[D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right]\left(\hat{\theta}-\theta^{0}\right)
\]

\end_inset

 or
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left(\hat{\theta}-\theta^{0}\right)=-\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta^{0})
\]

\end_inset


\end_layout

\begin_layout Standard
Substitute the RHS into the last part of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "TS expansion of moments"

\end_inset

), and multiply by 
\begin_inset Formula $\sqrt{n}$
\end_inset

, to get 
\begin_inset Formula 
\[
\sqrt{n}\bar{m}_{n}(\hat{\theta})=\sqrt{n}\bar{m}_{n}(\theta^{0})-\sqrt{n}D_{n}^{\prime}(\theta^{*})\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1}\bar{m}_{n}(\theta^{0}).
\]

\end_inset

 With some factoring, this last can be written as
\begin_inset Formula 
\begin{align*}
\sqrt{n}\bar{m}_{n}(\hat{\theta}) & =\left(\hat{\Omega}^{1/2}-D_{n}^{\prime}(\theta^{*})\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1/2}\right)\left(\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\theta^{0})\right)
\end{align*}

\end_inset

(verify it by multiplying out the last expression.
 Also, a note: the matrix square root of a matrix 
\begin_inset Formula $A$
\end_inset

 is any matrix 
\begin_inset Formula $A^{1/2}$
\end_inset

 such that 
\begin_inset Formula $A=A^{1/2}A^{1/2}$
\end_inset

.
 Any positive definite matrix has an invertible matrix square root.)
\end_layout

\begin_layout Standard
Next, multiply by 
\begin_inset Formula $\hat{\Omega}^{-1/2}$
\end_inset

 to get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\hat{\theta}) & =\left(I_{g}-\hat{\Omega}^{-1/2}D_{n}^{\prime}(\theta^{*})\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1/2}\right)\left(\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\theta^{0})\right)\equiv PX\label{eq:JtestIntermediate}
\end{align}

\end_inset

 Now, from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:CLT applied to moment conditions"

\end_inset

 we have 
\begin_inset Formula 
\[
X\equiv\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\theta^{0})\stackrel{d}{\rightarrow}N(0,I_{g})
\]

\end_inset


\end_layout

\begin_layout Itemize
the big matrix 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P=I_{g}-\hat{\Omega}^{-1/2}D_{n}^{\prime}(\theta^{*})\left(D(\hat{\theta})\hat{\Omega}^{-1}D(\theta^{*})^{\prime}\right)^{-1}D(\hat{\theta})\hat{\Omega}^{-1/2}$
\end_inset

 converges in probability to 
\begin_inset Formula $P_{\infty}=I_{g}-\Omega_{\infty}^{-1/2}D_{\infty}^{\prime}\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1/2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
One can easily verify that 
\begin_inset Formula $P_{\infty}$
\end_inset

 is idempotent and has rank 
\begin_inset Formula $g-K,$
\end_inset

 (recall that the rank of an idempotent matrix is equal to its trace)
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $X=\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\theta^{0})$
\end_inset

 converges to a 
\begin_inset Formula $G$
\end_inset

 vector of i.i.d.
 standard normal random variables, by the LLN and the Slutsky theorem.
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Thus, 
\begin_inset Formula $X^{\prime}PX\stackrel{d}{\rightarrow}\chi^{2}(d)$
\end_inset

, by the Continuous Mapping Theorem
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 (
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "true"

\end_inset

, Theorem 4.7) 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 This is because, asymptotically, it is a quadratic form of standard normal
 variables, weighted by an idempotent matrix.
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
S
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
o, the inner product of the r.h.s.
 of eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:JtestIntermediate"
plural "false"
caps "false"
noprefix "false"

\end_inset

 has an asymptotic chi-square distribution.
 The inner product using the l.h.s.
 must also have the same distribution, so we finally get
\begin_inset Formula 
\[
\left(\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\hat{\theta})\right)^{\prime}\left(\sqrt{n}\hat{\Omega}^{-1/2}\bar{m}_{n}(\hat{\theta})\right)=n\bar{m}_{n}(\hat{\theta})^{\prime}\hat{\Omega}^{-1}\bar{m}_{n}(\hat{\theta})\stackrel{d}{\rightarrow}\chi^{2}(g-K)
\]

\end_inset

 
\begin_inset Newpage newpage
\end_inset


\series bold
\emph on
Hansen-Sargan
\series default
 
\emph default
test
\emph on
: 
\emph default
Supposing that the moment conditions actually have expectation zero at the
 true parameter value, and that we are using an estimate of the efficient
 weight matrix, then
\begin_inset Formula 
\[
n\cdot s_{n}(\hat{\theta})\stackrel{d}{\rightarrow}\chi^{2}(g-K).
\]

\end_inset


\end_layout

\begin_layout Itemize
This is a convenient test since we just multiply the optimized value of
 the objective function by 
\begin_inset Formula $n,$
\end_inset

 and compare with a 
\begin_inset Formula $\chi^{2}(g-K)$
\end_inset

 critical value.
 The test is a general test of whether or not the moments used to estimate
 are correctly specified.
\end_layout

\begin_layout Itemize
This won't work when the estimator is just identified.
 The f.o.c.
 are 
\begin_inset Formula 
\[
D_{\theta}s_{n}(\theta)=D\hat{\Omega}^{-1}\bar{m}_{n}(\hat{\theta})\equiv0.
\]

\end_inset

But with exact identification, both 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $\hat{\Omega}$
\end_inset

 are square and invertible (at least asymptotically, assuming that asymptotic
 normality hold), so
\begin_inset Formula 
\[
\bar{m}_{n}(\hat{\theta})\equiv0.
\]

\end_inset

So the moment conditions are zero 
\emph on
regardless
\emph default
 of the weighting matrix used.
 As such, we might as well use an identity matrix and save trouble.
 Also 
\begin_inset Formula $s_{n}(\hat{\theta})=0$
\end_inset

, so the test breaks down.
\end_layout

\begin_layout Itemize
This sort of test often over-rejects in finite samples.
 One should be cautious in rejecting a model when this test rejects.
\end_layout

\begin_layout Itemize
This test goes by several names: Hansen test, Sargan test, Hansen-Sargan
 test, J test.
 I call it the GMM criterion test.
 An old name for GMM estimation is 
\begin_inset Quotes sld
\end_inset

minimum chi-square
\begin_inset Quotes srd
\end_inset

 estimation.
 This makes sense: the criterion function at the estimate (which makes the
 criterion as small as possible), scaled by 
\begin_inset Formula $n$
\end_inset

, has a 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution.
\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/SpecTest.jl}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/GMM/SpecTest.jl} 
\end_layout

\end_inset

 does a Monte Carlo study of the Hansen-Sargan test, for same the dynamic
 model with measurement error as was discussed in Examples 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:Measurement-error-in"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:GIV-example.-Recall"

\end_inset

, which did GIV estimation, and shows that it over-rejects a correctly specified
 model, in this case.
 For example, if the significance level is set to 10%, the test rejects
 about 16% of the time.
 This is a common result for this test.
 Results from a run are:
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GMM/SpecTest.out"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Other estimators interpreted as GMM estimators
\end_layout

\begin_layout Subsection
Maximum likelihood
\end_layout

\begin_layout Standard
In the introduction we argued that ML will in general be more efficient
 than GMM since ML implicitly uses all of the moments of the distribution
 while GMM uses a limited number of moments.
 Actually, a distribution with 
\begin_inset Formula $P$
\end_inset

 parameters can be uniquely characterized by 
\begin_inset Formula $P$
\end_inset

 moment conditions.
 However, some sets of 
\begin_inset Formula $P$
\end_inset

 moment conditions may contain more information than others, since the moment
 conditions could be highly correlated.
 A GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator that chose an optimal set of 
\begin_inset Formula $P$
\end_inset

 moment conditions would be fully efficient.
 The optimal moment conditions are simply the scores of the ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator.
\end_layout

\begin_layout Standard
In the chapter on maximum likelihood, we saw in eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLscore"

\end_inset

 that the first derivative of the average log likelihood function is 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f(y_{t}|x_{x},\theta)
\]

\end_inset

and that the ML estimator is obtained by setting this to zero, and solving.
 We also saw in eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ExpectationScore"

\end_inset

 that the expectation of the score vector is zero, when evaluated at the
 true parameter values.
 Thus, the score vector satisfies the requirement to serve as moment conditions.
 Set 
\begin_inset Formula 
\[
m_{t}(\theta)\equiv D_{\theta}\ln f(y_{t}|x_{t},\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
Recall that the score contributions are both conditionally and unconditionally
 uncorrelated.
 Conditional uncorrelation follows from the fact that 
\begin_inset Formula $m_{t-s}$
\end_inset

 if is a function of lagged endogenous variables, then they are included
 in 
\begin_inset Formula $x_{t}$
\end_inset

, which is what we are conditioning on at time 
\begin_inset Formula $t$
\end_inset

.
 Unconditional uncorrelation follows from the fact that conditional uncorrelatio
n hold regardless of the realization of 
\begin_inset Formula $y_{t-1},$
\end_inset

 so marginalizing with respect to 
\begin_inset Formula $Y_{t-1}$
\end_inset

 preserves uncorrelation (see the section on ML estimation, above).
 
\end_layout

\begin_layout Itemize
The fact that the scores are serially uncorrelated implies that 
\begin_inset Formula $\Omega$
\end_inset

 can be estimated by the estimator of the 0
\begin_inset Formula $^{th}$
\end_inset

 autocovariance of the moment conditions: 
\begin_inset Formula 
\[
\widehat{\Omega}=1/n\sum_{t=1}^{n}m_{t}(\hat{\theta})m_{t}(\hat{\theta})^{\prime}=1/n\sum_{t=1}^{n}\left[D_{\theta}\ln f(y_{t}|x_{t},\hat{\theta})\right]\left[D_{\theta}\ln f(y_{t}|x_{t},\hat{\theta})\right]^{\prime}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
note that this is the estimator of the information matrix, from ML
\end_layout

\begin_layout Itemize
There is no need for a Newey-West style estimator, the heteroscedastic-consisten
t estimator of White is sufficient.
\end_layout

\begin_layout Itemize
Also, the fact that the scores of ML are uncorrelated suggests a means of
 testing the correct specification of the model: see if the fitted scores
 (
\begin_inset Formula $m_{t}(\hat{\theta})$
\end_inset

 show evidence of serial correlation.
 If they do, the correctness of the specification of the model is subject
 to doubt.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
OLS as a GMM estimator - the Nerlove model again
\end_layout

\begin_layout Example
Matlab/Octave code for GMM for Nerlove model.
 Examine and run 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{TwoStepGMM.m}{https://github.com/mcreel/Econometrics/blob/master
/Examples/GMM/TwoStepGMM.m} 
\end_layout

\end_inset

, which illustrates how to do two step GMM for the Nerlove data.
 Note that the GMM results are the same as what you get estimating by OLS.
\end_layout

\begin_layout Exercise
Adapt the Matlab/Octave code for two step GMM for the Nerlove model to the
 Julia language.
\end_layout

\begin_layout Standard
The simple Nerlove model can be estimated using GMM, as we've seen.
 So, OLS is a special case of GMM.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
The Hausman Test
\end_layout

\begin_layout Standard
This section discusses the Hausman test (
\begin_inset CommandInset citation
LatexCommand cite
key "Hausman1978"
literal "true"

\end_inset

).
\end_layout

\begin_layout Standard
Consider the simple linear regression model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\epsilon_{t}.$
\end_inset

 We assume that the functional form and the choice of regressors is correct,
 but that the some of the regressors may be correlated with the error term,
 which as you know will produce inconsistency of 
\begin_inset Formula $\hat{\beta}.$
\end_inset

 For example, this will be a problem if
\end_layout

\begin_layout Itemize
if some regressors are endogenous
\end_layout

\begin_layout Itemize
some regressors are measured with error 
\end_layout

\begin_layout Itemize
some relevant regressors are omitted (equivalent to imposing false restrictions)
\end_layout

\begin_layout Itemize
lagged values of the dependent variable are used as regressors and 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is autocorrelated.
\end_layout

\begin_layout Standard
To illustrate, the Julia program 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{OLSvsIV.jl}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/GMM/Hausman/OLSvsIV.jl} 
\end_layout

\end_inset

 performs a Monte Carlo experiment where errors are correlated with regressors,
 and estimation is by OLS and IV.
 The true value of the slope coefficient used to generate the data is 
\begin_inset Formula $\beta=2.$
\end_inset

 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:OLS-and-IV"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows that the OLS estimator is quite biased and that the IV estimator
 is on average much closer to the true value.
 If you play with the program, increasing the sample size, you can see evidence
 that the OLS estimator is asymptotically biased, while the IV estimator
 is consistent.
 You can also play with the covariances of the instrument and regressor,
 and the covariance of the regressor and the error.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:OLS-and-IV"

\end_inset

OLS and IV
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Hausman/olsiv.svg
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

We have seen that inconsistent and the consistent estimators converge to
 different probability limits.
 This is the idea behind the Hausman test - a pair of consistent estimators
 converge to the same probability limit, while if one is consistent and
 the other is not they converge to different limits.
 If we accept that one is consistent (
\emph on
e.g.
\emph default
, the IV estimator), but we are doubting if the other is consistent (
\emph on
e.g.,
\emph default
 the OLS estimator), we might try to check if the difference between the
 estimators is significantly different from zero.
\end_layout

\begin_layout Itemize
If we're doubting about the consistency of OLS (or QML, 
\emph on
etc
\emph default
.), why should we be interested in testing - why not just use the IV estimator?
 Because the OLS estimator is 
\emph on
more efficient
\emph default
 when the regressors are exogenous and the other classical assumptions (includin
g normality of the errors) hold.
 
\end_layout

\begin_layout Itemize
Play with the above script to convince yourself of this point: make exogeneity
 hold, and compare the variances of OLS and IV
\end_layout

\begin_layout Itemize
When we have a more efficient estimator that relies on stronger assumptions
 (such as exogeneity) than the IV estimator, we might prefer to use it,
 unless we have evidence that the assumptions are false.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
So, let's consider the covariance between the MLE estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 (or any other fully efficient estimator) and some other CAN estimator,
 say 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 Now, let's recall some results from MLE.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "anmle"

\end_inset

 implies:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{d}{\rightarrow}-\mathcal{J}_{\infty}(\theta_{0})^{-1}\sqrt{n}g(\theta_{0}).
\]

\end_inset

Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "information matrix equality"

\end_inset

 is
\begin_inset Formula 
\[
\mathcal{J}{}_{\infty}(\theta)=-\mathcal{I}_{\infty}(\theta).
\]

\end_inset

 Combining these two equations, we get
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\overset{d}{\rightarrow}\mathcal{I}_{\infty}(\theta_{0})^{-1}\sqrt{n}g(\theta_{0}).
\]

\end_inset


\end_layout

\begin_layout Standard
Also, equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "Cov. CAN and MLE score"

\end_inset

 tells us that the asymptotic covariance between any CAN estimator and the
 MLE score vector is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}g(\theta)
\end{array}\right]=\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right].
\]

\end_inset

These results imply that 
\begin_inset Formula 
\[
\begin{bmatrix}I_{K} & 0_{K}\\
0_{K} & I_{\infty}(\theta)^{-1}
\end{bmatrix}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}g(\theta)
\end{array}\right]\rightarrow^{d}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}\left(\hat{\theta}-\theta\right)
\end{array}\right].
\]

\end_inset


\begin_inset Formula 
\[
.
\]

\end_inset

 The asymptotic covariance of this is 
\begin_inset Formula 
\begin{eqnarray*}
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}\left(\hat{\theta}-\theta\right)
\end{array}\right] & = & \begin{bmatrix}I_{K} & 0_{K}\\
0_{K} & I_{\infty}(\theta)^{-1}
\end{bmatrix}\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{K}\\
I_{K} & \mathcal{I}_{\infty}(\theta)
\end{array}\right]\begin{bmatrix}I_{K} & 0_{K}\\
0_{K} & I_{\infty}(\theta)^{-1}
\end{bmatrix}\\
 & = & \left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{\infty}(\theta)^{-1}\\
I_{\infty}(\theta)^{-1} & I_{\infty}(\theta)^{-1}
\end{array}\right],
\end{eqnarray*}

\end_inset

which, for clarity in what follows, we might write as (note to self for
 lectures: the 2,2 element has changed)
\begin_inset Formula 
\[
V_{\infty}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta\right)\\
\sqrt{n}\left(\hat{\theta}-\theta\right)
\end{array}\right]=\left[\begin{array}{cc}
V_{\infty}(\tilde{\theta}) & I_{\infty}(\theta)^{-1}\\
I_{\infty}(\theta)^{-1} & V_{\infty}(\hat{\theta})
\end{array}\right].
\]

\end_inset

So, the asymptotic covariance between the MLE and any other CAN estimator
 is equal to the MLE asymptotic variance (the inverse of the information
 matrix).
\end_layout

\begin_layout Standard
Now, suppose we wish to test whether the the two estimators are in fact
 both converging to 
\begin_inset Formula $\theta_{0}$
\end_inset

, versus the alternative hypothesis that the 
\begin_inset Quotes sld
\end_inset

MLE
\begin_inset Quotes srd
\end_inset

 estimator is not in fact consistent (the consistency of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is a maintained hypothesis).
 Under the null hypothesis that they are, we have
\begin_inset Formula 
\[
\begin{bmatrix}I_{K} & -I_{K}\end{bmatrix}\left[\begin{array}{c}
\sqrt{n}\left(\tilde{\theta}-\theta_{0}\right)\\
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)
\end{array}\right]=\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right),
\]

\end_inset

will be asymptotically normally distributed as (work out on blackboard)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right)\overset{d}{\rightarrow}N\left(0,V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})\right).
\]

\end_inset

 So, 
\begin_inset Formula 
\[
n\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\left(V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})\right)^{-1}\left(\tilde{\theta}-\hat{\theta}\right)\overset{d}{\rightarrow}\chi^{2}(\rho),
\]

\end_inset

where 
\begin_inset Formula $\rho$
\end_inset

 is the rank of the difference of the asymptotic variances.
 A statistic that has the same asymptotic distribution is
\begin_inset Formula 
\[
\left(\tilde{\theta}-\hat{\theta}\right)^{\prime}\left(\hat{V}(\tilde{\theta})-\hat{V}(\hat{\theta})\right)^{-1}\left(\tilde{\theta}-\hat{\theta}\right)\overset{d}{\rightarrow}\chi^{2}(\rho).
\]

\end_inset

This is the Hausman test statistic, in its original form.
 The reason that this test has power under the alternative hypothesis is
 that in that case the 
\begin_inset Quotes sld
\end_inset

MLE
\begin_inset Quotes srd
\end_inset

 estimator will not be consistent, and will converge to 
\begin_inset Formula $\theta_{A}$
\end_inset

, say, where 
\begin_inset Formula $\theta_{A}\neq\theta_{0}$
\end_inset

.
 Then the mean of the asymptotic distribution of vector 
\begin_inset Formula $\sqrt{n}\left(\tilde{\theta}-\hat{\theta}\right)$
\end_inset

 will be 
\begin_inset Formula $\theta_{0}-\theta_{A}$
\end_inset

, a non-zero vector, so the test statistic will eventually reject, regardless
 of how small a significance level is used.
\end_layout

\begin_layout Itemize
The quantity 
\begin_inset Formula $V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})$
\end_inset

 may be a singular matrix, in which case the inverse in 
\begin_inset Formula $\left(V_{\infty}(\tilde{\theta})-V_{\infty}(\hat{\theta})\right)^{-1}$
\end_inset

 must be replaced with a generalized inverse.
 This can occur when the two estimators are defined using some common moment
 conditions, which can introduce some linear dependencies between the estimators.
 
\end_layout

\begin_layout Itemize
When this is the case, the rank, 
\begin_inset Formula $\rho$
\end_inset

, of the difference of the asymptotic variances will be less than the dimension
 of the matrices, and it may be difficult to determine what the true rank
 is.
 If the true rank is lower than what is taken to be true, the test will
 be biased against rejection of the null hypothesis.
 The null is that both estimators are consistent.
 Failure to reject when this hypothesis is false would cause us to use an
 inconsistent estimator: not a desirable outcome! The contrary holds if
 we underestimate the rank.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Incorrect rank and the Hausman test
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/GMM/Hausman/RankProblems.jpg
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
A solution to this problem is to use a rank 1 test, by comparing only a
 single coefficient.
 For example, if a variable is suspected of possibly being endogenous, that
 variable's coefficients may be compared.
\end_layout

\begin_layout Itemize
Note: if the test is based on a sub-vector of the entire parameter vector
 of the MLE, it is possible that the inconsistency of the MLE will not show
 up in the portion of the vector that has been used.
 If this is the case, the test may not have power to detect the inconsistency.
 This may occur, for example, when the consistent but inefficient estimator
 is not identified for all the parameters of the model, so that we estimate
 only some of the parameters using the inefficient estimator, and the test
 does not include the others.
\end_layout

\begin_layout Itemize
This simple formula only holds when the estimator that is being tested for
 consistency is 
\emph on
fully
\emph default
 efficient under the null hypothesis.
 This means that it must be a ML estimator or a fully efficient estimator
 that has the same asymptotic distribution as the ML estimator.
 This is quite restrictive since modern estimators such as GMM, QML, or
 even OLS with heteroscedastic consistent standard errors are not in general
 fully efficient.
\end_layout

\begin_layout Standard
Following up on this last point, let's think of two not necessarily efficient
 estimators, 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

, where one is assumed to be consistent, but the other may not be.
 We assume for expositional simplicity that both 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 belong to the same parameter space, and that each estimator can be expressed
 as generalized method of moments (GMM) estimator.
 The estimators are defined (suppressing the dependence upon data) by
\begin_inset Formula 
\begin{eqnarray*}
\hat{\theta}_{i} & = & \arg\min_{\theta_{i}\in\Theta}\,\bar{m}_{_{i}}(\theta_{i})^{\prime}\,W_{i}\,\bar{m}_{i}(\theta_{i})
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\bar{m}_{i}(\theta_{i})$
\end_inset

 is a 
\begin_inset Formula $g_{i}\times1$
\end_inset

 vector of moment conditions, and 
\begin_inset Formula $W_{i}$
\end_inset

 is a 
\begin_inset Formula $g_{i}\times g_{i}$
\end_inset

 positive definite weighting matrix, 
\begin_inset Formula $i=1,2.$
\end_inset

 Consider the omnibus GMM estimator
\begin_inset Formula 
\begin{equation}
\left(\hat{\theta}_{1},\hat{\theta}_{2}\right)=\arg\min_{\Theta\times\Theta}\,\left[\begin{array}{cc}
\bar{m}_{1}(\theta_{1})^{\prime} & \bar{m}_{2}(\theta_{2})^{\prime}\end{array}\right]\left[\begin{array}{cc}
W_{1} & \mathbf{0}_{\left(g_{1}\times g_{2}\right)}\\
\mathbf{0}_{\left(g_{2}\times g_{1}\right)} & W_{2}
\end{array}\right]\left[\begin{array}{c}
\bar{m}_{1}(\theta_{1})\\
\bar{m}_{2}(\theta_{2})
\end{array}\right].\label{Standard Omnibus}
\end{equation}

\end_inset

Suppose that the asymptotic covariance of the omnibus moment vector is
\begin_inset Formula 
\begin{eqnarray}
\Sigma & = & \lim_{n\rightarrow\infty}Var\left\{ \sqrt{n}\left[\begin{array}{c}
\bar{m}_{1}(\theta_{1})\\
\bar{m}_{2}(\theta_{2})
\end{array}\right]\right\} \label{omnibus variance}\\
 & \equiv & \left(\begin{array}{cc}
\Sigma_{1} & \Sigma_{12}\\
\cdot & \Sigma_{2}
\end{array}\right).\nonumber 
\end{eqnarray}

\end_inset

The standard Hausman test is equivalent to a Wald test of the equality of
 
\begin_inset Formula $\theta_{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}$
\end_inset

 (or subvectors of the two) applied to the omnibus GMM estimator, but with
 the covariance of the moment conditions estimated as 
\begin_inset Formula 
\[
\widehat{\Sigma}=\left(\begin{array}{cc}
\widehat{\Sigma_{1}} & \mathbf{0}_{\left(g_{1}\times g_{2}\right)}\\
\mathbf{0}_{\left(g_{2}\times g_{1}\right)} & \widehat{\Sigma_{2}}
\end{array}\right).
\]

\end_inset

While this is clearly an inconsistent estimator in general, the omitted
 
\begin_inset Formula $\Sigma_{12}$
\end_inset

 term cancels out of the test statistic when one of the estimators is asymptotic
ally efficient, as we have seen above, and thus it need not be estimated.
\end_layout

\begin_layout Standard
The general solution when neither of the estimators is efficient is clear:
 the entire 
\begin_inset Formula $\Sigma$
\end_inset

 matrix must be estimated consistently, since the 
\begin_inset Formula $\Sigma_{12}$
\end_inset

 term will not cancel out.
 Methods for consistently estimating the asymptotic covariance of a vector
 of moment conditions are well-known
\emph on
, e.g.,
\emph default
 the Newey-West estimator discussed previously.
 The Hausman test using a proper estimator of the overall covariance matrix
 will now have an asymptotic 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution when neither estimator is efficient.
 
\end_layout

\begin_layout Standard
However, the test suffers from a loss of power due to the fact that the
 omnibus GMM estimator of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "Standard Omnibus"

\end_inset

 is defined using an inefficient weight matrix.
 A new test can be defined by using an alternative omnibus GMM estimator
\begin_inset Formula 
\begin{equation}
\left(\hat{\theta}_{1},\hat{\theta}_{2}\right)=\arg\min_{\Theta\times\Theta}\left[\begin{array}{cc}
\bar{m}_{1}(\theta_{1})^{\prime} & \bar{m}_{2}(\theta_{2})^{\prime}\end{array}\right]\left(\widetilde{\Sigma}\right)^{-1}\left[\begin{array}{c}
\bar{m}_{1}(\theta_{1})\\
\bar{m}_{2}(\theta_{2})
\end{array}\right],\label{New Omnibus}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\widetilde{\Sigma}$
\end_inset

 is a consistent estimator of the overall covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

 of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "omnibus variance"

\end_inset

.
 By standard arguments, this is a more efficient estimator than that defined
 by equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "Standard Omnibus"

\end_inset

, so the Wald test using this alternative is more powerful.
 See my article in 
\emph on
Applied Economics
\emph default
, 2004, for more details, including simulation results.
 The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{hausman.m}{https://github.com/mcreel/Econometrics/blob/master/Ex
amples/GMM/Hausman/hausman.m} 
\end_layout

\end_inset

 calculates the Wald test corresponding to the efficient joint GMM estimator
 (the 
\begin_inset Quotes sld
\end_inset

H2
\begin_inset Quotes srd
\end_inset

 test in my paper), for a simple linear model, and compares to the standard
 Hausman test.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Application:-Nonlinear-rational"

\end_inset

Application: Hansen-Singleton, 1982
\end_layout

\begin_layout Standard

\series bold
Readings:
\series default
 
\begin_inset CommandInset citation
LatexCommand citep
key "HansenSingleton1982"
literal "true"

\end_inset

; 
\begin_inset CommandInset citation
LatexCommand citep
key "Tauchen1986"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Though GMM estimation has many applications, application to rational expectation
s models is elegant, since theory directly suggests the moment conditions.
 Hansen and Singleton's 1982 paper is also a classic worth studying in itself.
 Though I strongly recommend reading the paper, I'll use a simplified model
 with notation similar to Hamilton's.
 The literature on estimation of these models has grown a lot since these
 early papers.
 After work like the cited papers, people moved to ML estimation of linearized
 models, using Kalman filtering.
 Current methods are usually Bayesian, and involve sophisticated filtering
 methods to compute the likelihood function for nonlinear models with non-normal
 shocks.
 There is a lot of interesting stuff that is beyond the scope of this course.
 I have done some work using simulation-based estimation methods applied
 to such models.
 The methods explained in this section are intended to provide an example
 of GMM estimation.
 They are not the state of the art for estimation of such models.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
We assume a representative consumer maximizes expected discounted utility
 over an infinite horizon.
 Expectations are rational, and the agent has full information (is fully
 aware of the history of the world up to the current time period - how's
 that for an assumption!).
 Utility is temporally additive, and the expected utility hypothesis holds.
 The future consumption stream is the stochastic sequence 
\begin_inset Formula $\left\{ c_{t}\right\} _{t=0}^{\infty}.$
\end_inset

 The objective function at time 
\begin_inset Formula $t$
\end_inset

 is the discounted expected utility 
\begin_inset Formula 
\begin{equation}
\sum_{s=0}^{\infty}\beta^{s}\mathcal{E}\left(u(c_{t+s})|I_{t}\right).\label{umax}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
The parameter 
\begin_inset Formula $\beta$
\end_inset

 is between 0 and 1, and reflects discounting.
\end_layout

\begin_layout Itemize
\begin_inset Formula $I_{t}$
\end_inset

 is the 
\shape italic
information set
\shape default
 at time 
\begin_inset Formula $t,$
\end_inset

 and includes the all realizations of all random variables indexed 
\begin_inset Formula $t$
\end_inset

 and earlier.
\end_layout

\begin_layout Itemize
The choice variable is 
\begin_inset Formula $c_{t}$
\end_inset

 - current consumption, which is constrained to be less than or equal to
 current wealth 
\begin_inset Formula $w_{t}.$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Suppose the consumer can invest in a risky asset.
 A dollar invested in the asset yields a gross return 
\begin_inset Formula 
\[
(1+r_{t+1})=\frac{p_{t+1}+d_{t+1}}{p_{t}}
\]

\end_inset

 where 
\begin_inset Formula $p_{t}$
\end_inset

 is the price and 
\begin_inset Formula $d_{t}$
\end_inset

 is the dividend in period 
\begin_inset Formula $t.$
\end_inset

 Thus, 
\begin_inset Formula $r_{t+1}$
\end_inset

 is the net return on a dollar invested in period 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Itemize
The price of 
\begin_inset Formula $c_{t}$
\end_inset

 is normalized to 
\begin_inset Formula $1.$
\end_inset


\end_layout

\begin_layout Itemize
Current wealth 
\begin_inset Formula $w_{t}=(1+r_{t})i_{t-1}$
\end_inset

, where 
\begin_inset Formula $i_{t-1}$
\end_inset

 is investment in period 
\begin_inset Formula $t-1$
\end_inset

.
 So the problem is to allocate current wealth between current consumption
 and investment to finance future consumption: 
\begin_inset Formula $w_{t}=c_{t}+i_{t}$
\end_inset

.
\end_layout

\begin_layout Itemize
Future net rates of return 
\begin_inset Formula $r_{t+s},s>0$
\end_inset

 are 
\shape italic
not known
\shape default
 in period 
\begin_inset Formula $t$
\end_inset

: the asset is risky.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
A partial set of necessary conditions for utility maximization have the
 form: 
\begin_inset Formula 
\begin{equation}
u^{\prime}(c_{t})=\beta\mathcal{E}\left\{ \left(1+r_{t+1}\right)u^{\prime}(c_{t+1})|I_{t}\right\} .\label{foc}
\end{equation}

\end_inset

 To see that the condition is necessary, suppose that the lhs < rhs.
 Then by reducing current consumption marginally would cause equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "umax"

\end_inset

 to drop by 
\begin_inset Formula $u^{\prime}(c_{t}),$
\end_inset

 since there is no discounting of the current period.
 At the same time, the marginal reduction in consumption finances investment,
 which has gross return 
\begin_inset Formula $\left(1+r_{t+1}\right),$
\end_inset

 which could finance consumption in period 
\begin_inset Formula $t+1.$
\end_inset

 This increase in consumption would cause the objective function to increase
 by 
\begin_inset Formula $\beta\mathcal{E}\left\{ \left(1+r_{t+1}\right)u^{\prime}(c_{t+1})|I_{t}\right\} .$
\end_inset

 Therefore, unless the condition holds, the expected discounted utility
 function is not maximized.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
To use this we need to choose the functional form of utility.
 A constant relative risk aversion (CRRA) form is 
\begin_inset Formula 
\[
u(c_{t})=\frac{c_{t}^{1-\gamma}-1}{1-\gamma}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\gamma$
\end_inset

 is the coefficient of relative risk aversion.
 With this form, 
\begin_inset Formula 
\[
u^{\prime}(c_{t})=c_{t}^{-\gamma}
\]

\end_inset

 so the foc are 
\begin_inset Formula 
\[
c_{t}^{-\gamma}=\beta\mathcal{E}\left\{ \left(1+r_{t+1}\right)c_{t+1}^{-\gamma}|I_{t}\right\} 
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 While it is true that 
\begin_inset Formula 
\[
\mathcal{E}\left(c_{t}^{-\gamma}-\beta\left\{ \left(1+r_{t+1}\right)c_{t+1}^{-\gamma}\right\} \right)|I_{t}=0
\]

\end_inset

 so that we could use this to define moment conditions, it is unlikely that
 
\begin_inset Formula $c_{t}$
\end_inset

 is stationary, even though it is in real terms, and our theory requires
 stationarity.
 To solve this, divide though by 
\begin_inset Formula $c_{t}^{-\gamma}$
\end_inset


\begin_inset Formula 
\[
\mathcal{E}\left(\textrm{1-}\beta\left\{ \left(1+r_{t+1}\right)\left(\frac{c_{t+1}}{c_{t}}\right)^{-\gamma}\right\} \right)|I_{t}=0
\]

\end_inset

 (note that 
\begin_inset Formula $c_{t}$
\end_inset

 can be passed though the conditional expectation since 
\begin_inset Formula $c_{t}$
\end_inset

 is chosen based only upon information available in time 
\begin_inset Formula $t).$
\end_inset

 That is to say, 
\begin_inset Formula $c_{t}$
\end_inset

 is in the information set 
\begin_inset Formula $I_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
Now
\begin_inset Formula 
\[
\textrm{1-}\beta\left\{ \left(1+r_{t+1}\right)\left(\frac{c_{t+1}}{c_{t}}\right)^{-\gamma}\right\} 
\]

\end_inset

is analogous to 
\begin_inset Formula $h_{t}(\theta)$
\end_inset

 defined above: it's a scalar moment condition that has conditional expectation
 equal to zero.
 To get a vector of moment conditions we need some instruments.
 Suppose that 
\begin_inset Formula $\mathbf{z}_{t}$
\end_inset

 is a vector of variables drawn from the information set 
\begin_inset Formula $I_{t}.$
\end_inset

 We can use the necessary conditions to form the expressions 
\begin_inset Formula 
\[
\begin{array}{c}
\left[1-\beta\left(1+r_{t+1}\right)\left(\frac{c_{t+1}}{c_{t}}\right)^{-\gamma}\right]\mathbf{z}_{t}\end{array}\equiv m_{t}(\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta$
\end_inset

 represents 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\gamma.$
\end_inset


\end_layout

\begin_layout Itemize
Therefore, the above expression may be interpreted as a moment condition
 which can be used for GMM estimation of the parameters 
\begin_inset Formula $\theta^{0}.$
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Note that at time 
\begin_inset Formula $t,$
\end_inset

 
\begin_inset Formula $m_{t-s}$
\end_inset

 has been observed, and is therefore an element of the information set.
 By rational expectations, the autocovariances of the moment conditions
 other than 
\begin_inset Formula $\Gamma_{0}$
\end_inset

 should be zero.
 The optimal weighting matrix is therefore the inverse of the variance of
 the moment conditions: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Omega_{_{\infty}}=\lim E\left[n\bar{m}(\theta^{0})\bar{m}(\theta^{0})^{\prime}\right]
\]

\end_inset

 which can be consistently estimated by 
\begin_inset Formula 
\[
\hat{\Omega}=1/n\sum_{t=1}^{n}m_{t}(\hat{\theta})m_{t}(\hat{\theta})^{\prime}
\]

\end_inset

 As before, this estimate depends on an initial consistent estimate of 
\begin_inset Formula $\theta,$
\end_inset

 which can be obtained by setting the weighting matrix 
\begin_inset Formula $W$
\end_inset

 arbitrarily (to an identity matrix, for example).
 After obtaining 
\begin_inset Formula $\hat{\theta},$
\end_inset

 we then minimize 
\begin_inset Formula 
\[
s(\theta)=\bar{m}(\theta)^{\prime}\hat{\Omega}^{-1}\bar{m}(\theta).
\]

\end_inset

 This process can be iterated, e.g., use the new estimate to re-estimate 
\begin_inset Formula $\Omega,$
\end_inset

 use this to estimate 
\begin_inset Formula $\theta^{0},$
\end_inset

 and repeat until the estimates don't change.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In principle, we could use a very large number of moment conditions in estimatio
n, since 
\emph on
any current or lagged variable
\emph default
 could be used in 
\begin_inset Formula $\mathbf{x}_{t}.$
\end_inset

 Since use of more moment conditions will lead to a more (asymptotically)
 efficient estimator, one might be tempted to use many instrumental variables.
 We will do a computer lab that will show that this may not be a good idea
 with finite samples.
 This issue has been studied using Monte Carlos (Tauchen, 
\emph on
JBES,
\emph default
 1986).
 The reason for poor performance when using many instruments is that the
 estimate of 
\begin_inset Formula $\Omega$
\end_inset

 becomes very imprecise.
 
\end_layout

\begin_layout Itemize
Empirical papers that use this approach often have serious problems in obtaining
 precise estimates of the parameters, and identification can be problematic.
 Note that we are basing everything on a single partial first order condition.
 Probably this f.o.c.
 is simply not informative enough.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sec:Empirical-example:-a"

\end_inset

Empirical example: a portfolio model
\end_layout

\begin_layout Standard
The Julia program 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{portfolio.jl}{https://github.com/mcreel/Econometrics/blob/master
/Examples/GMM/portfolio.jl}
\end_layout

\end_inset

 performs GMM estimation of a portfolio model of the sort presented in this
 section, using the data file 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{tauchen.data}{https://github.com/mcreel/Econometrics/blob/master
/Examples/GMM/tauchen.data}
\end_layout

\end_inset

.
 The columns of this data file are 
\begin_inset Formula $c,$
\end_inset

 
\begin_inset Formula $p,$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 in that order.
 There are 95 observations (source: 
\begin_inset CommandInset citation
LatexCommand cite
key "Tauchen1986"
literal "true"

\end_inset

).
 As instruments we use lags of 
\begin_inset Formula $c$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

, as well as a constant.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

For a single lag the estimation results are 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GMM/portfolio1.out"

\end_inset


\begin_inset Newpage newpage
\end_inset

For two lags the estimation results are 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GMM/portfolio2.out"

\end_inset


\begin_inset Newpage newpage
\end_inset

Pretty clearly, the results are sensitive to the choice of instruments.
 Also, if you examine the objective function values, it seems unlikely that
 the global minimum was found in all cases, probably multiple start values
 or global minimization are needed.
 Maybe there is some problem here: poor instruments, or possibly a conditional
 moment that is not very informative.
 Moment conditions formed from Euler conditions sometimes do not identify
 the parameter of a model.
 See 
\begin_inset CommandInset citation
LatexCommand cite
key "HansenHeatonYaron1996"
literal "true"

\end_inset

.
 I believe that this is the case here, though I haven't checked it carefully.
\end_layout

\begin_layout Standard
The Octave/Matlab program 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{HallGMM.m}{https://github.com/mcreel/Econometrics/blob/master/Ex
amples/GMM/HallGMM.m}
\end_layout

\end_inset

 estimates a very similar model, following Chapter 23 of the 
\begin_inset CommandInset href
LatexCommand href
name "Gretl Users Guide"
target "http://ricardo.ecn.wfu.edu/pub/gretl/manual/en/gretl-guide-a4.pdf"
literal "false"

\end_inset

.
 I encourage you to verify that you can obtain the same results using Gretl
 and Octave.
 
\end_layout

\begin_layout Exercise
Translate the HallGMM.m code to run on Julia.
\end_layout

\begin_layout Exercise
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
GMM estimation of the DSGE example
\end_layout

\begin_layout Standard
Here we return to the DSGE model of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Application:-a-simple"

\end_inset

, and derive some moment conditions that can be used for estimation.
 
\end_layout

\begin_layout Itemize
this example shows how moment conditions can be derived from the structure
 of a model
\end_layout

\begin_layout Itemize
it will also illustrate the care that is sometimes needed when doing numeric
 optimization
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
MRS and wage
\end_layout

\begin_layout Standard
From the first order conditions of the model, we have 
\begin_inset Formula 
\begin{eqnarray*}
w_{t} & = & \psi\eta_{t}c_{t}^{\gamma}\\
\eta_{t} & = & \frac{w_{t}}{\psi c_{t}^{\gamma}}\\
\ln\eta_{t} & = & \ln w_{t}-\ln\psi-\gamma\ln c_{t}
\end{eqnarray*}

\end_inset

The real values of this shock are not observed, but, given a guess for the
 parameters and the data, the left hand side of the above equation can be
 calculated.
 Also, we have
\begin_inset Formula 
\[
\ln\eta_{t}=\rho\ln\eta_{t-1}+\sigma_{\eta}\epsilon_{t}.
\]

\end_inset

So, we can regress the calculated 
\begin_inset Formula $\ln\eta_{t}$
\end_inset

 on their lags.
 The FOC for the OLS estimator set the mean of 
\begin_inset Formula 
\[
u_{t}=\ln\eta_{t-1}[\ln\eta_{t}-\rho_{\eta}\ln\eta_{t-1}]
\]

\end_inset

 to zero.
 At the true parameter values, this expression has mean zero, so it can
 be used to define a moment condition.
 We also have that 
\begin_inset Formula 
\[
E\left(u_{t}^{2}-\sigma_{\eta}^{2}\right)=0
\]

\end_inset

 at the true parameters, so this gives us a second moment condition.
 These two moment conditions are informative for all of the parameters that
 enter into their definitions: 
\begin_inset Formula $\gamma,\rho_{\eta},\sigma_{\eta}$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

,
\begin_inset Formula $\beta,$
\end_inset


\begin_inset Formula $\delta$
\end_inset

 and 
\begin_inset Formula $\bar{n}$
\end_inset

 (because 
\begin_inset Formula $\psi$
\end_inset

 depends on them, see above).
 We're only missing 
\begin_inset Formula $\rho_{z}$
\end_inset

 and 
\begin_inset Formula $\sigma_{z}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Euler equation
\end_layout

\begin_layout Standard
The Euler equation is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
c_{t}^{-\gamma}=E\left(\beta\cdot c_{t+1}^{-\gamma}\left[1+MPK_{t+1}-\delta\right]\right),
\]

\end_inset

where the expectation is taken conditional on the information available
 in period 
\begin_inset Formula $t$
\end_inset

 (which include variables indexed 
\begin_inset Formula $t$
\end_inset

 and before).
 But 
\begin_inset Formula $r=MPK,$
\end_inset

 so
\begin_inset Formula 
\[
E\left(\beta\cdot c_{t+1}^{-\gamma}\left[1+r_{t+1}-\delta\right]\right)-c_{t}^{-\gamma}=0
\]

\end_inset

Thus,
\begin_inset Formula 
\begin{equation}
v_{t}=\beta\cdot c_{t+1}^{-\gamma}\left[1+r_{t+1}-\delta\right]-c_{t}^{-\gamma}\label{eq:EulerError}
\end{equation}

\end_inset

has mean zero, conditional on information available in period 
\begin_inset Formula $t.$
\end_inset

 Moment conditions that use this error should be informative for 
\begin_inset Formula $\gamma,$
\end_inset


\begin_inset Formula $\delta$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Estimation by GMM
\end_layout

\begin_layout Standard
A sample of size 160, generated from the model at the true parameter values,
 above, is at 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{dsgedata.txt}{https://github.com/mcreel/Econometrics/blob/master
/Examples/DSGE/GenData/dsgedata.txt}
\end_layout

\end_inset

.
 The columns are y, c, n, r, w.
 
\end_layout

\begin_layout Standard
A Julia function to compute the moment conditions discussed above, and others
 is at 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{DSGEmoments.jl}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/DSGE/GMM/DSGEmoments.jl}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
The script 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{DoGMM.jl}{https://github.com/mcreel/Econometrics/blob/master/Exa
mples/DSGE/GMM/DoGMM.jl}
\end_layout

\end_inset

 implements CUE-GMM estimation of the model, using the selected moment condition
s, using simulated annealing to do the minimization
\end_layout

\begin_layout Itemize
The final estimates, standard errors, and 95% CI bounds, are
\begin_inset Newline newline
\end_inset

 
\family typewriter

\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout

\family typewriter
estimates, st.
 error, and limits of 95% CI
\end_layout

\begin_layout Plain Layout

\family typewriter
estimate std.
 err.
 CI lower CI upper
\end_layout

\begin_layout Plain Layout

\family typewriter
0.98992 0.00142 0.98714 0.99271
\end_layout

\begin_layout Plain Layout

\family typewriter
2.09990 0.32135 1.47004 2.72975
\end_layout

\begin_layout Plain Layout

\family typewriter
0.90879 0.03585 0.83851 0.97906
\end_layout

\begin_layout Plain Layout

\family typewriter
0.01965 0.00156 0.01659 0.02271
\end_layout

\begin_layout Plain Layout

\family typewriter
0.74954 0.41860 -0.07091 1.56999
\end_layout

\begin_layout Plain Layout

\family typewriter
0.00883 0.00553 -0.00202 0.01968
\end_layout

\begin_layout Plain Layout

\family typewriter
0.33277 0.00508 0.32282 0.34272
\end_layout

\begin_layout Plain Layout

\family typewriter
julia> 
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
care is needed to obtain a real global minimum.
 The attempt to use ordinary gradient-based minimization fails.
 A person who tried these methods might conclude that the moments don't
 identify the parameters well, but this is not the case: it is possible
 to obtain good results using GMM.
\end_layout

\begin_layout Itemize
Simulated annealing, on the other hand, converges to the same value in repeated
 runs.
 It is possible that on a given run, a different outcome might be obtained,
 if the cooling rate is to rapid, but I have yet to see this with the current
 setup.
\end_layout

\begin_layout Itemize
SA requires many function evaluations, about 30000 with the setting in the
 example code.
 However, it doesn't take too long, only about 11 seconds.
 This doesn't seem like too much time to get a reliable answer.
\end_layout

\begin_layout Itemize
The estimates are close to the true parameter values (see Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:True-parameters-and"

\end_inset

).
 Some parameters are precisely estimated, but others, especially 
\begin_inset Formula $\rho_{\eta}$
\end_inset

 have fairly large standard errors.
 Perhaps better moments could be found to better identify these parameters.
\end_layout

\begin_layout Itemize
if you attempt to use a less conservative cooling schedule, e.g., setting
 ns = 1, rt = 0.25 estimation using SA will rarely be successful.
 
\end_layout

\begin_layout Itemize
The take home conclusion here is that multiple local minima and irregular
 objective functions really can be a problem, even with simple models like
 this one.
 Imagine what would happen with a large scale DSGE model! For similar problems
 with a model that is much more simple, see 
\begin_inset CommandInset citation
LatexCommand cite
key "HansenHeatonYaron1996"
literal "true"

\end_inset

.
\end_layout

\begin_layout Itemize
the difficulties with extremum estimation may motivate other computational
 methods, such as using a Bayesian approach to compute classical estimators
 as was proposed by 
\begin_inset CommandInset citation
LatexCommand cite
key "ChernozhukovHong2003"
literal "true"

\end_inset

.
 We will return to this idea in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Bayesian-methods"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
\noindent
Suppose you have data on a dependent variable 
\begin_inset Formula $y_{i}$
\end_inset

 and a column vector of regressors 
\begin_inset Formula $x_{i}$
\end_inset

.
 Consider the model
\begin_inset Formula 
\[
y_{i}=x_{i}^{\prime}\beta_{0}+\epsilon_{i}
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\noindent
Suppose that 
\begin_inset Formula $E[\epsilon_{i}|x_{i}]=0$
\end_inset

.
 Use this information to propose a GMM estimator that is equivalent to the
 OLS estimator.
 Your answer should include:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\noindent
state the moment conditions and the GMM objective function clearly
\end_layout

\begin_layout Enumerate
\noindent
compute the first order conditions for minimization of the GMM criterion
 function and solve them to find the expression for the estimator
\end_layout

\end_deeper
\begin_layout Enumerate
Suppose that 
\begin_inset Formula $E[\epsilon_{i}|x_{i}]\ne0$
\end_inset

 but that there is another vector 
\begin_inset Formula $z_{i}$
\end_inset

 with 
\begin_inset Formula $\dim z=\dim x$
\end_inset

 such that 
\begin_inset Formula $E[\epsilon_{i}|z_{i}]=0$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Show that the OLS estimator of 
\begin_inset Formula $\beta_{0}$
\end_inset

 is not consistent, given this information.
\end_layout

\begin_layout Enumerate
Propose a consistent estimator of the parameter vector 
\begin_inset Formula $\beta_{0}$
\end_inset

 that uses this information.
 Your answer should include:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
a clear statement of the moment conditions and the GMM objective function
 to be minimized which defines the estimator.
\end_layout

\begin_layout Enumerate
a closed-form expression (that is, an explicit formula) for the estimator.
\end_layout

\begin_layout Enumerate
a proof that the estimator is consistent.
 If you need to make additional assumptions to prove consistency, state
 them.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Enumerate
Do the exercises in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Generalized-instrumental"

\end_inset

.
\end_layout

\begin_layout Enumerate
Show how the GIV estimator presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Generalized-instrumental"

\end_inset

 can be adapted to account for an error term with HET and/or AUT.
\end_layout

\begin_layout Enumerate
For the GIV estimator presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Example:-Generalized-instrumental"

\end_inset

, find the form of the expressions 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta^{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta^{0})$
\end_inset

 that appear in the asymptotic distribution of the estimator, assuming that
 an efficient weight matrix is used.
\end_layout

\begin_layout Enumerate
The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{meps.m}{https://github.com/mcreel/Econometrics/blob/master/Examp
les/GMM/MEPS/meps.m} 
\end_layout

\end_inset

 estimates a model for office-based doctpr visits (OBDV) using two different
 moment conditions, a Poisson QML approach and an IV approach.
 If all conditioning variables are exogenous, both approaches should be
 consistent.
 If the PRIV variable is endogenous, only the IV approach should be consistent.
 Neither of the two estimators is efficient in any case, since we already
 know that this data exhibits variability that exceeds what is implied by
 the Poisson model (e.g., negative binomial and other models fit much better).
 Test the exogeneity of the variable PRIV with a GMM-based Hausman-type
 test, using the Octave script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{hausman.m}{https://github.com/mcreel/Econometrics/blob/master/Ex
amples/GMM/Hausman/hausman.m} 
\end_layout

\end_inset

 for hints about how to set up the test.
\end_layout

\begin_layout Enumerate
Using Julia, generate data from the logit dgp.
 The script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimateLogit.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/NonlinearOptimization/EstimateLogit.jl} 
\end_layout

\end_inset

 should prove quite helpful.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Recall that 
\begin_inset Formula $E(y_{t}|\mathbf{x}_{t})=\mathbf{p}(\mathbf{x}_{t},\theta)=[1+\exp(-\mathbf{x}_{t}\prime\theta)]^{-1}$
\end_inset

.
 Consider the moment condtions (exactly identified) 
\begin_inset Formula $m_{t}(\theta)=[y_{t}-p(\mathbf{x}_{t},\theta)]\mathbf{x}_{t}$
\end_inset

.
 Estimate by GMM (using 
\family typewriter
gmmresults
\family default
), using these moments.
\end_layout

\begin_layout Enumerate
Estimate by ML (using 
\family typewriter
mleresults
\family default
).
\end_layout

\begin_layout Enumerate
The two estimators should coincide.
 Prove analytically that the estimators coincide.
\end_layout

\end_deeper
\begin_layout Enumerate
When working out the structure of 
\begin_inset Formula $\Omega_{n}$
\end_inset

, show that 
\begin_inset Formula $\mathcal{E}(m_{t}m_{t+s}^{\prime})=\Gamma_{v}^{\prime}.$
\end_inset

 
\end_layout

\begin_layout Enumerate
Verify the missing steps needed to show that 
\begin_inset Formula $n\cdot\bar{m}(\hat{\theta})^{\prime}\hat{\Omega}^{-1}\bar{m}(\hat{\theta})$
\end_inset

 has a 
\begin_inset Formula $\chi^{2}(g-K)$
\end_inset

 distribution.
 That is, show that the monster matrix is idempotent and has trace equal
 to 
\begin_inset Formula $g-K.$
\end_inset


\end_layout

\begin_layout Enumerate
For the portfolio example, experiment with the program using lags of 3 and
 4 periods to define instruments
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Iterate the estimation of 
\begin_inset Formula $\theta=(\beta,\gamma)$
\end_inset

 and 
\begin_inset Formula $\Omega$
\end_inset

 to convergence.
\end_layout

\begin_layout Enumerate
Comment on the results.
 Are the results sensitive to the set of instruments used? Look at 
\begin_inset Formula $\hat{\Omega}$
\end_inset

 as well as 
\begin_inset Formula $\hat{\theta}.$
\end_inset

 Are these good instruments? Are the instruments highly correlated with
 one another? Is there something analogous to collinearity going on here?
\end_layout

\end_deeper
\begin_layout Enumerate
Run the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/chi2gmm.m}{https://github.com/mcreel/Econometrics/blob/maste
r/Examples/GMM/chi2gmm.jl}
\end_layout

\end_inset

 with several sample sizes.
 Do the results you obtain seem to agree with the consistency of the GMM
 estimator? Explain.
 
\end_layout

\begin_layout Enumerate
The GMM estimator with an arbitrary weight matrix has the asymptotic distributio
n
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}\right]
\]

\end_inset

Supposing that you compute a GMM estimator using an arbitrary weight matrix,
 so that this result applies.
 Carefully explain how you could test the hypothesis 
\begin_inset Formula $H_{0}:R\theta^{0}=r$
\end_inset

 versus 
\begin_inset Formula $H_{A}:R\theta^{0}\ne r$
\end_inset

, where 
\begin_inset Formula $R$
\end_inset

 is a given 
\begin_inset Formula $q\times k$
\end_inset

 matrix, and 
\begin_inset Formula $r$
\end_inset

 is a given 
\begin_inset Formula $q\times1$
\end_inset

 vector.
 I suggest that you use a Wald test.
 Explain exactly what is the test statistic, and how to compute every quantity
 that appears in the statistic.
 
\end_layout

\begin_layout Enumerate
(proof that the GMM optimal weight matrix is one such that 
\begin_inset Formula $W_{\infty}=\Omega_{\infty}^{-1})$
\end_inset

 Consider the difference of the asymptotic variance using an arbitrary weight
 matrix, minus the asymptotic variance using the optimal weight matrix:
 
\begin_inset Formula 
\begin{eqnarray*}
A=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1} & - & \left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}
\end{eqnarray*}

\end_inset

Set 
\begin_inset Formula $B=\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}-\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1}$
\end_inset

.
 Verify that 
\begin_inset Formula $A=B\Omega_{\infty}B^{'}$
\end_inset

.
 What is the implication of this? Explain.
\end_layout

\begin_layout Enumerate
\align left
The asymptotic distribution of the GMM estimator, using a non-optimal weight
 matrix, is
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}D_{\infty}W_{\infty}\Omega_{\infty}W_{\infty}D_{\infty}^{\prime}\left(D_{\infty}W_{\infty}D_{\infty}^{\prime}\right)^{-1}\right]
\]

\end_inset

We know that in the case of exact identification, the GMM estimator does
 not depend on the weight matrix, 
\begin_inset Formula $W.$
\end_inset

 If this is the case, the asymptotic covariance matrix must not depend on
 
\begin_inset Formula $W_{\infty}$
\end_inset

, either.
 Prove that this is true, by showing that 
\begin_inset Formula $W$
\end_inset

 cancels out of the asymptotic variance.
 Hint: 
\begin_inset Formula $\left(AB\right)^{-1}=B^{-1}A^{-1}$
\end_inset

 if both 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are invertible matrices.
\end_layout

\begin_layout Enumerate
In the context of the Hansen-Sargan test for correct specification of moments,
 discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:A-specification-test"

\end_inset

, prove that the matrix 
\begin_inset Formula $P_{\infty}=I_{g}-\Omega_{\infty}^{-1/2}D_{\infty}^{\prime}\left(D_{\infty}\Omega_{\infty}^{-1}D_{\infty}^{\prime}\right)^{-1}D_{\infty}\Omega_{\infty}^{-1/2}$
\end_inset

 is idempotent and that its rank is 
\begin_inset Formula $g-K,$
\end_inset

 where 
\begin_inset Formula $g$
\end_inset

 is the number of moment conditions and 
\begin_inset Formula $K$
\end_inset

 is the number of parameters.
\end_layout

\begin_layout Enumerate
Consider the two equation model
\begin_inset Formula 
\begin{eqnarray*}
\text{Demand:\;\ }q_{t} & = & \alpha_{1}+\alpha_{2}p_{t}+\alpha_{3}y_{t}+\varepsilon_{1t}\\
\text{Supply:\;\ }q_{t} & = & \beta_{1}+\beta_{2}p_{t}+\varepsilon_{2t}\\
\mathcal{E}\left(\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}
\end{array}\right]|y_{t}\right) & = & \left[\begin{array}{c}
0\\
0
\end{array}\right]\\
\mathcal{E}\left(\left[\begin{array}{l}
\varepsilon_{1t}\\
\varepsilon_{2t}
\end{array}\right]\left[\begin{array}{ll}
\varepsilon_{1t} & \varepsilon_{2t}\end{array}\right]|y_{t}\right) & = & \left[\begin{array}{ll}
\sigma_{11} & \sigma_{12}\\
\sigma_{12} & \sigma_{22}
\end{array}\right],\forall t
\end{eqnarray*}

\end_inset

The variables 
\begin_inset Formula $q_{t}$
\end_inset

 and 
\begin_inset Formula $p_{t}$
\end_inset

 are endogenous, and the variable 
\begin_inset Formula $y_{t}$
\end_inset

 is weakly exogenous.
 Assume that the observations are independent over time.
 Consider GMM estimation of the parameters of the two equations implemented
 as two stage least squares (2SLS).
 Recall that the 2SLS estimator uses 
\begin_inset Formula $\widehat{p_{t}}$
\end_inset

 as an instrument for the endogenous regressor 
\begin_inset Formula $p_{t}$
\end_inset

, where 
\begin_inset Formula $\widehat{p_{t}}$
\end_inset

 is the fitted value from OLS applied to the equation 
\begin_inset Formula $p_{t}=\pi_{1}+\pi_{2}y_{t}+v_{t}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Show that the regressor 
\begin_inset Formula $p_{t}$
\end_inset

 is correlated with each of the structural errors 
\begin_inset Formula $\varepsilon_{1t}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{2t}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Will OLS give a consistent estimator of the parameters of the supply equation?
 Explain your answer.
\end_layout

\begin_layout Enumerate
Give the exact expression for the 2SLS estimator of the parameters of the
 supply equation, and explain why the estimator is consistent.
\end_layout

\begin_layout Enumerate
Give the exact expression for the 2SLS estimator of the parameters of the
 demand equation, and carefully explain why 2SLS will 
\emph on
not
\emph default
 give a consistent estimator of these parameters.
 Note that the 2SLS estimator is a particular GMM estimator, and it is a
 particular instrumental variables (IV) estimator.
 Keeping this in mind may help you to answer the question.
\end_layout

\end_deeper
\begin_layout Enumerate
Prove that the GMM estimator based upon the 
\begin_inset Formula $g$
\end_inset

 moment conditions 
\begin_inset Formula $\bar{m}_{n}(\theta)=\left[\begin{array}{cc}
p_{n}^{\prime}(\theta) & q_{n}^{\prime}(\theta)\end{array}\right]^{\prime}$
\end_inset

 and the corresponding true optimal weight matrix is asymptotically more
 efficient than the GMM estimator based upon the 
\begin_inset Formula $h<g$
\end_inset

 moment conditions 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p_{n}(\theta)$
\end_inset

 and the corresponding true optimal weight matrix.
\family default
\series default
\shape default
\size default
\bar default
\color inherit

\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Interpret the result
\end_layout

\begin_layout Enumerate
Discuss the importance of the result from an empirical point of view.
 Are there any cautions one should observe when doing applied GMM work?
 Describe any problems you can imagine.
\end_layout

\end_deeper
\begin_layout Enumerate
Recall the dynamic model with measurement error that was discussed in class:
 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}^{*} & = & \alpha+\rho y_{t-1}^{*}+\beta x_{t}+\epsilon_{t}\\
y_{t} & = & y_{t}^{*}+\upsilon_{t}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\upsilon_{t}$
\end_inset

 are independent Gaussian white noise errors.
 Suppose that 
\begin_inset Formula $y_{t}^{*}$
\end_inset

 is not observed, and instead we observe 
\begin_inset Formula $y_{t}$
\end_inset

.
 We can estimate the equation 
\begin_inset Formula 
\[
y_{t}=\alpha+\rho y_{t-1}+\beta x_{t}+\nu_{t}
\]

\end_inset

using GIV, as was done above.
 The Julia script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{GMM/SpecTest.jl}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/GMM/SpecTest.jl} 
\end_layout

\end_inset

 performs a Monte Carlo study of the performance of the GMM criterion test,
 
\begin_inset Formula 
\[
n\cdot s_{n}(\hat{\theta})\stackrel{d}{\rightarrow}\chi^{2}(g-K)
\]

\end_inset

Examine the script and describe what it does.
 Run this script to verify that the test over-rejects.
 Increase the sample size, to determine if the over-rejection problem becomes
 less severe.
 Discuss your findings.
\end_layout

\begin_layout Enumerate
Suppose we have two equations
\begin_inset Formula 
\begin{eqnarray*}
y_{t1} & = & \alpha_{1}+\alpha_{2}y_{t2}+\epsilon_{t1}\\
y_{t2} & = & \beta_{1}+\beta_{2}x_{t}+\epsilon_{t2}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $V(\epsilon_{t1})=\sigma_{1}^{2}>0$
\end_inset

, 
\begin_inset Formula $V(\epsilon_{t2})=\sigma_{2}^{2}>0$
\end_inset

, 
\begin_inset Formula $E(\epsilon_{t1}\epsilon_{t2})=\sigma_{12}\ne0$
\end_inset

.
 The observations are independent over time, and the errors have zero mean.
 The variable 
\begin_inset Formula $x_{t}$
\end_inset

 is strictly exogenous: it is uncorrelated with the two epsilons at all
 time periods.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Is the OLS estimator of the parameters of the first equation consistent
 or not? Explain.
\end_layout

\begin_layout Enumerate
Is the OLS estimator of the parameters of the second equation consistent
 or not? Explain.
\end_layout

\begin_layout Enumerate
If the OLS estimator of the parameters of the first equation is not consistent,
 propose a consistent estimator of the parameters of the first equation
 and explain why the proposed estimator is consistent.
 
\end_layout

\begin_layout Enumerate
If the OLS estimator of the parameters of the second equation is not consistent,
 propose a consistent estimator of the parameters of the second equation
 and explain why the proposed estimator is consistent.
\end_layout

\end_deeper
\begin_layout Enumerate
Estimate a logit model by GMM using the 10 independent data points
\begin_inset Newline newline
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="11">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

.
 
\begin_inset Newline newline
\end_inset

For the logit model, the probability 
\begin_inset Formula $P(y_{t}=1|x_{t})=(1+\exp(-\theta_{1}-\theta_{2}x_{t}))^{-1}$
\end_inset

, and the probability that 
\begin_inset Formula $y_{t}=0$
\end_inset

 is the complement.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
create a data file that contains these observations
\end_layout

\begin_layout Enumerate
find the conditional mean 
\begin_inset Formula $E(y|x)$
\end_inset

 and the conditional variance 
\begin_inset Formula $V(y|x$
\end_inset

)
\end_layout

\begin_layout Enumerate
propose at least 2 moment conditions, using the mean and the variance you
 found in (b)
\end_layout

\begin_layout Enumerate
write a Julia function that computes the GMM estimator using your two moment
 conditions
\end_layout

\begin_layout Enumerate
compute the two step efficient GMM estimator
\end_layout

\begin_layout Enumerate
comment on the results
\end_layout

\end_deeper
\begin_layout Enumerate
Given the 10 independent data points
\begin_inset Newline newline
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="11">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

.
 
\begin_inset Newline newline
\end_inset

For the Poisson model, the density 
\begin_inset Formula $f_{Y}(y|x)=\frac{\exp(-\lambda)\lambda^{y}}{y!},$
\end_inset

 
\begin_inset Formula $y=0,1,2,...$
\end_inset

.
 To make the model depend on conditioning variables, use the parameterization
 
\begin_inset Formula $\lambda(x)=\exp(\theta_{1}+\theta_{2}x)$
\end_inset

.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
The mean of a Poisson distribution with parameter 
\begin_inset Formula $\lambda$
\end_inset

 is equal to 
\begin_inset Formula $\lambda,$
\end_inset

 and so is the variance.
 Propose moment conditions to an overidentified 
\begin_inset Formula $(g>k)$
\end_inset

 GMM estimator of 
\begin_inset Formula $\theta_{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Discuss how your proposed moment conditions relate to the score function
 of the maximum likelihood estimator.
\end_layout

\begin_layout Enumerate
Estimate the parameters using two-step efficient GMM, using the moment condition
s you have proposed.
\end_layout

\begin_layout Enumerate
Discuss the results, and compare them to your ML estimates for the similar
 problem in the chapter on ML estimation.
\end_layout

\end_deeper
\begin_layout Enumerate
Consider the model
\begin_inset Formula 
\begin{align*}
y_{t} & =\alpha+\rho_{1}y_{t-1}+\rho_{2}y_{t-2}+\beta x_{t}+\epsilon_{t}
\end{align*}

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is a 
\begin_inset Formula $N(0,1)$
\end_inset

 white noise error.
 This is an autoregressive model of order 2 (AR2) model, with an additional
 exogenous regressor.
 Suppose that data is generated from the AR2 model, but the econometrician
 mistakenly decides to estimate an AR1 model, 
\begin_inset Formula $y_{t}=\alpha+\rho_{1}y_{t-1}+\beta x_{t}+v_{t}$
\end_inset

.
 This is a case of omitted relevant variables.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
show that weak exogeneity fails for the AR1 model.
\end_layout

\begin_layout Enumerate
Consider IV estimation of the AR1 model, using lags of 
\begin_inset Formula $x_{t}$
\end_inset

 as instruments.
 Is this a consistent estimator?
\end_layout

\begin_layout Enumerate
simulate data from the correct AR2 model, using 
\begin_inset Formula $\alpha=0$
\end_inset

, 
\begin_inset Formula $\rho_{1}=0.5$
\end_inset

, 
\begin_inset Formula $\rho_{2}=0.4$
\end_inset

, 
\begin_inset Formula $\beta=2$
\end_inset

, and 
\begin_inset Formula $x_{t}\sim IIN(0,1).$
\end_inset

 Use a sample size of 30 observations.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
estimate the incorrectly specified AR1 model by OLS
\end_layout

\begin_layout Enumerate
estimate the correctly specified AR2 model by OLS
\end_layout

\begin_layout Enumerate
implement your proposed IV estimator of the AR1 model
\end_layout

\begin_layout Enumerate
embed the simulations and estimations in a loop, to do a Monte Carlo study
 using 1000 replications.
 Provide histograms for the distribtions of the estimators of the parameter
 
\begin_inset Formula $\rho_{1}$
\end_inset

 for the 3 estimators.
\end_layout

\end_deeper
\begin_layout Enumerate
discuss all results thoroughly, focusing on bias and standard errors of
 the estimators of the autoregressive parameters
\end_layout

\end_deeper
\begin_layout Enumerate
Estimate the investment equation of the Klein Model 1 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Example:-Klein's-Model"

\end_inset

) using GMM.
 See the example at the end of the discussion of 2SLS for a good hint.
\end_layout

\begin_layout Enumerate
Verify the missing steps needed to show that 
\begin_inset Formula $n\cdot m(\hat{\theta})^{\prime}\hat{\Omega}^{-1}m(\hat{\theta})$
\end_inset

 has a 
\begin_inset Formula $\chi^{2}(g-K)$
\end_inset

 distribution.
 That is, show that the big ugly matrix is idempotent and has trace equal
 to 
\begin_inset Formula $g-K.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Models-for-time"

\end_inset

Models for time series data
\end_layout

\begin_layout Standard
Hamilton, 
\emph on
Time Series Analysis
\emph default
 is a good reference for this chapter.
 
\end_layout

\begin_layout Standard
Up to now we've considered the behavior of the dependent variable 
\begin_inset Formula $y_{t}$
\end_inset

 as a function of other variables 
\begin_inset Formula $x_{t}.$
\end_inset

 These variables can of course contain lagged dependent variables, e.g., 
\begin_inset Formula $x_{t}=(w_{t},y_{t-1},...,y_{t-j}).$
\end_inset

 Pure time series methods consider the behavior of 
\begin_inset Formula $y_{t}$
\end_inset

 as a function only of its own lagged values, unconditional on other observable
 variables.
 One can think of this as modeling the behavior of 
\begin_inset Formula $y_{t}$
\end_inset

 after marginalizing out all other variables.
 But, of course, general models will include lagged dependent variables
 and other explanatory variables.
 This Chapter gives a brief description of some of the widely used models.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Basic concepts
\end_layout

\begin_layout Definition
[Stochastic process]
\begin_inset CommandInset label
LatexCommand label
name "Stochastic process"

\end_inset

 A stochastic process is a sequence of random variables, indexed by time:
 
\begin_inset Formula $\{Y_{t}\}_{t=-\infty}^{\infty}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Definition
[Time series]
\begin_inset CommandInset label
LatexCommand label
name "Time series"

\end_inset

 A time series is 
\series bold
one
\series default
 observation of a stochastic process, over a specific interval: 
\begin_inset Formula $\{y_{t}\}_{t=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
So a time series is a sample of size 
\begin_inset Formula $n$
\end_inset

 from a stochastic process.
 It's important to keep in mind that conceptually, one could draw another
 sample, and that the values would be different.
\end_layout

\begin_layout Definition
[Autocovariance] The 
\begin_inset Formula $j^{th}$
\end_inset

 autocovariance of a stochastic process is 
\begin_inset Formula $\gamma_{jt}=\mathcal{E}(y_{t}-\mu_{t})(y_{t-j}-\mu_{t-j})$
\end_inset

 where 
\begin_inset Formula $\mu_{t}=\mathcal{E}\left(y_{t}\right).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
[Covariance (weak) stationarity] A stochastic process is covariance stationary
 if it has time constant mean and autocovariances of all orders: 
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{eqnarray*}
\mu_{t} & =\mu, & \forall t\\
\gamma_{jt} & =\gamma_{j}, & \forall t
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As we've seen, this implies that 
\begin_inset Formula $\gamma_{j}=\gamma_{-j}:$
\end_inset

 the autocovariances depend only one the interval between observations,
 but not the time of the observations.
\end_layout

\begin_layout Definition
[Strong stationarity] A stochastic process is strongly stationary if the
 joint distribution of an arbitrary collection of the 
\begin_inset Formula $\left\{ Y_{t}\right\} $
\end_inset

, e.g., 
\begin_inset Formula $(Y_{t-j},Y_{t-k},...,Y_{t},...,Y_{t+l},Y_{t+m}\}$
\end_inset

, doesn't depend on 
\begin_inset Formula $t.$
\end_inset


\end_layout

\begin_layout Standard
Since moments are determined by the distribution, strong stationarity
\begin_inset Formula $\Rightarrow$
\end_inset

weak stationarity.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

How can we estimate the mean of 
\begin_inset Formula $Y_{t}?$
\end_inset

 The time series is one sample from the stochastic process, and each of
 the random variables over the sample interval is sampled only once.
 One could think of 
\begin_inset Formula $M$
\end_inset

 repeated samples from the stoch.
 proc., e.g., 
\begin_inset Formula $\left\{ y_{tm}\right\} _{m=1}^{M}$
\end_inset

 By a LLN, we would expect that 
\begin_inset Formula 
\[
\frac{1}{M}\sum_{m=1}^{M}y_{tm}\overset{p}{\rightarrow}\mathcal{E}(Y_{t})
\]

\end_inset

as 
\begin_inset Formula $M$
\end_inset

 gets large.
 The problem is, we have only one sample to work with, since we can't go
 back in time and collect another.
 How can 
\begin_inset Formula $\mathcal{E}(Y_{t})$
\end_inset

 be estimated then? It turns out that 
\emph on
ergodicity
\emph default
 is the needed property.
\end_layout

\begin_layout Definition
[Ergodicity].
 A stationary stochastic process is ergodic (for the mean) if the time average
 converges to the mean
\begin_inset Formula 
\begin{equation}
\frac{1}{n}\sum_{t=1}^{n}y_{t}\overset{p}{\rightarrow}\mu
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
A sufficient condition for ergodicity is that the autocovariances be absolutely
 summable: 
\begin_inset Formula 
\[
\sum_{j=0}^{\infty}|\gamma_{j}|<\infty
\]

\end_inset

This implies that the autocovariances die off, so that the 
\begin_inset Formula $y_{t}$
\end_inset

 are not so strongly dependent that they don't satisfy a LLN.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Definition
[Autocorrelation] The 
\begin_inset Formula $j^{th}$
\end_inset

 autocorrelation, 
\begin_inset Formula $\rho_{j}$
\end_inset

 is just the 
\begin_inset Formula $j^{th}$
\end_inset

 autocovariance divided by the variance: 
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{equation}
\rho_{j}=\frac{\gamma_{j}}{\gamma_{0}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Definition
[White noise] White noise is just the time series literature term for a
 classical error.
 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is white noise if i) 
\begin_inset Formula $\mathcal{E}(\epsilon_{t})=0,\forall t,$
\end_inset

 ii) 
\begin_inset Formula $V(\epsilon_{t})=\sigma^{2},\forall t$
\end_inset

 and iii) 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{s}$
\end_inset

 are independent, 
\begin_inset Formula $t\neq s.$
\end_inset

 Gaussian white noise just adds a normality assumption.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
US quarterly macro data, used in 
\begin_inset CommandInset citation
LatexCommand citet
key "stock2011introduction"
literal "true"

\end_inset

, Chapter 14.
 The original materials are at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://wps.pearsoned.co.uk/ema_ge_stock_ieupdate_3/251/64413/16489878.cw/index.html
\end_layout

\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{The data file}{https://github.com/mcreel/Econometrics/blob/mast
er/Examples/Data/us
\backslash
_macro
\backslash
_quarterly.gdt} 
\end_layout

\end_inset

, in GRETL format.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{The data description file.}{https://github.com/mcreel/Econometri
cs/blob/master/Examples/Data/usmacro
\backslash
_quarterly
\backslash
_description.pdf} 
\end_layout

\end_inset

.
 Use GRETL to:
\end_layout

\begin_layout Itemize
plot the GDP data, and notice that it's nonstationary.
\end_layout

\begin_layout Itemize
Plot the growth rate, and note that it's stationary.
\end_layout

\begin_layout Itemize
compute the autocorrelations for the annual growth rate of GDP, using the
 GRETL correlogram option: they die off fairly quickly, so ergodicity seems
 to hold
\end_layout

\begin_layout Itemize
compute the autocorrelations of GDP.
 HIGHLY PERSISTENT.
 Doubtful that the ergodicity condition will hold.
\end_layout

\begin_layout Itemize
we are going to want to work with stationary data, if we want to apply standard
 regression methods and inference.
\end_layout

\begin_layout Itemize
working with nonstationary data can give very misleading results, if we
 rely on standard theory for stationary data, as we will see.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
ARMA models
\end_layout

\begin_layout Standard
With these concepts, we can discuss ARMA models.
 These are closely related to the AR and MA error processes that we've already
 discussed.
 The main difference is that the lhs variable is observed directly now.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
MA(q) processes
\end_layout

\begin_layout Standard
A 
\begin_inset Formula $q^{th}$
\end_inset

 order moving average (MA) process is 
\begin_inset Formula 
\[
y_{t}=\mu+\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}+\theta_{2}\varepsilon_{t-2}+\cdots+\theta_{q}\varepsilon_{t-q}
\]

\end_inset

 where 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 is white noise.
 The variance is 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{0} & = & \mathcal{E}\left(y_{t}-\mu\right)^{2}\\
 & = & \mathcal{E}\left(\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}+\theta_{2}\varepsilon_{t-2}+\cdots+\theta_{q}\varepsilon_{t-q}\right)^{2}\\
 & = & \sigma^{2}\left(1+\theta_{1}^{2}+\theta_{2}^{2}+\cdots+\theta_{q}^{2}\right)
\end{eqnarray*}

\end_inset

 Similarly, the autocovariances are 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{j} & = & \mathcal{E}\left[\left(y_{t}-\mu\right)\left(y_{t-j}-\mu\right)\right]\\
 & = & \sigma^{2}(\theta_{j}+\theta_{j+1}\theta_{1}+\theta_{j+2}\theta_{2}+\cdots+\theta_{q}\theta_{q-j}),j\leq q\\
 & = & 0,j>q
\end{eqnarray*}

\end_inset

 Therefore an MA(q) process is necessarily covariance stationary and ergodic,
 as long as 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and all of the 
\begin_inset Formula $\theta_{j}$
\end_inset

 are finite.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
For example, if we have an MA(1) model, then 
\begin_inset Formula $E(y_{t})=\mu$
\end_inset

, 
\begin_inset Formula $V(y_{t})=\sigma^{2}(1+\theta_{1}^{2})$
\end_inset

, and 
\begin_inset Formula $\gamma_{1}=\sigma^{2}\theta_{1}$
\end_inset

.
 The higher order autocovariances are zero.
 
\end_layout

\begin_layout Itemize
Thus, if the model is MA(1) with normally distributed shocks, the density
 of the vector of 
\begin_inset Formula $n$
\end_inset

 observations, 
\begin_inset Formula $y$
\end_inset

, is 
\begin_inset Formula 
\begin{align}
f_{Y}(y|\rho) & =\frac{\text{1}}{\sqrt{\left(2\pi\right)^{n}\left|\Sigma\right|}}\exp\left(-\frac{1}{2}\left(y-\mu\right)^{\prime}\Sigma^{-1}\left(y-\mu\right)\right)\label{eq:MA1likelihood}
\end{align}

\end_inset

where 
\begin_inset Formula 
\[
\Sigma=\sigma^{2}\left[\begin{array}{ccccc}
1+\theta_{1}^{2} & \theta_{1} & 0 & \cdots & 0\\
\theta_{1} & \ddots & \ddots & \ddots & \vdots\\
0 & \ddots & \ddots & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & \theta_{1}\\
0 & \cdots & 0 & \theta_{1} & 1+\theta_{1}^{2}
\end{array}\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
With this, it is very easy to program the log-likelihood function.
 For higher order MA models, the only difference is the structure of 
\begin_inset Formula $\Sigma$
\end_inset

 becomes more complicated.
 In this form, one needs a lot of computer memory.
 A more economical approach uses the Kalman filter, which we'll see in the
 discussion of state space models.
\end_layout

\end_deeper
\begin_layout Itemize
If we don't make assumptions on the distribution of the shocks, then method
 of moments estimation can be used.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Generate data that follows a simple MA(1) model: 
\begin_inset Formula $y_{t}=\mu+\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}$
\end_inset

 for 
\begin_inset Formula $\mu=0$
\end_inset

 and 
\begin_inset Formula $\theta_{1}=0.5$
\end_inset

, and with 
\begin_inset Formula $\epsilon_{t}+1$
\end_inset

 distributed independently and identically 
\begin_inset Formula $\chi^{2}(1)$
\end_inset

.
 Do estimation by GMM, and verify experimentally (by increasing the sample
 size) that the estimator is consistent.
 Hint: generate 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 as the square of a standard normal, minus 1.
\end_layout

\begin_layout Exercise
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
An issue to be aware of is that MA models are not identified, in that there
 exist multiple parameter values that give the same value of the likelihood
 function.
\end_layout

\begin_layout Itemize
For example, the MA(1) model with 
\begin_inset Formula $\tilde{\sigma}^{2}=\theta^{2}\sigma^{2}$
\end_inset

 and 
\begin_inset Formula $\tilde{\theta}_{1}=\frac{1}{\theta_{1}}$
\end_inset

 has identical first and second moments to the original model, so the likelihood
 function has the same value.
\end_layout

\begin_layout Itemize
Normally, the parameterization that leads to an 
\emph on
invertible
\emph default
 MA model is the one that is selected.
 An invertible MA model is one that has a representation as a AR(
\begin_inset Formula $\infty)$
\end_inset

 model.
 For the MA(1) model, the invertible parameterization has 
\begin_inset Formula $\left|\theta_{1}\right|<1$
\end_inset

.
\end_layout

\begin_layout Itemize
This implies that parameter restrictions will need to be imposed when estimating
 the MA model, to enforce selection of the invertible model.
\end_layout

\begin_layout Itemize
Maximization of the conditional likelihood is also used for estimation,
 sometimes.
 Assuming that 
\begin_inset Formula $\epsilon_{0}$
\end_inset

 is known (for example, equal to zero), then one can compute 
\begin_inset Formula $\epsilon_{1}$
\end_inset

, given the parameters.
 Then one works forward recursively to get all of the 
\begin_inset Formula $\epsilon_{t}$
\end_inset

.
 With these, the likelihood function is very easy to compute.
 This is a convenient shortcut, but it's not recommended if the sample is
 not large, especially since it's not hard to compute the exact likelihood
 function.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
AR(p) processes
\end_layout

\begin_layout Standard
An AR(p) process can be represented as 
\begin_inset Formula 
\[
y_{t}=c+\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\cdots+\phi_{p}y_{t-p}+\varepsilon_{t}
\]

\end_inset

where 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is white noise.
 This is just a linear regression model, and assuming stationarity, we can
 estimate the parameters by OLS.
 What is needed for stationarity?
\end_layout

\begin_layout Standard
The dynamic behavior of an AR(p) process can be studied by writing this
 
\begin_inset Formula $p^{th}$
\end_inset

 order difference equation as a vector first order difference equation (this
 is known as the companion form): 
\begin_inset Formula 
\[
\left[\begin{array}{l}
y_{t}\\
y_{t-1}\\
\vdots\\
y_{t-p+1}
\end{array}\right]=\left[\begin{array}{l}
c\\
0\\
\vdots\\
0
\end{array}\right]+\left[\begin{array}{lllll}
\phi_{1} & \phi_{2} & \cdots &  & \phi_{p}\\
1 & 0 & 0 &  & 0\\
0 & 1 & 0 & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & 0\cdots\\
0 & \cdots & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{l}
y_{t-1}\\
y_{t-2}\\
\vdots\\
y_{t-p}
\end{array}\right]+\left[\begin{array}{l}
\varepsilon_{t}\\
0\\
\vdots\\
0
\end{array}\right]
\]

\end_inset

 or 
\begin_inset Formula 
\[
Y_{t}=C+FY_{t-1}+E_{t}
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 With this, we can recursively work forward in time: 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t+1} & = & C+FY_{t}+E_{t+1}\\
 & = & C+F\left(C+FY_{t-1}+E_{t}\right)+E_{t+1}\\
 & = & C+FC+F^{2}Y_{t-1}+FE_{t}+E_{t+1}
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
Y_{t+2} & = & C+FY_{t+1}+E_{t+2}\\
 & = & C+F\left(C+FC+F^{2}Y_{t-1}+FE_{t}+E_{t+1}\right)+E_{t+2}\\
 & = & C+FC+F^{2}C+F^{3}Y_{t-1}+F^{2}E_{t}+FE_{t+1}+E_{t+2}
\end{eqnarray*}

\end_inset

 or in general 
\begin_inset Formula 
\[
Y_{t+j}=C+FC+\cdots+F^{j}C+F^{j+1}Y_{t-1}+F^{j}E_{t}+F^{j-1}E_{t+1}+\cdots+FE_{t+j-1}+E_{t+j}
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Consider the impact of a shock in period 
\begin_inset Formula $t$
\end_inset

 on 
\begin_inset Formula $y_{t+j}.$
\end_inset

 This is simply 
\begin_inset Formula 
\[
\frac{\partial Y_{t+j}}{\partial E_{t}^{\prime}}_{(1,1)}=F_{(1,1)}^{j}
\]

\end_inset

 If the system is to be stationary, then as we move forward in time this
 impact must die off.
 Otherwise a shock causes a permanent change in the mean of 
\begin_inset Formula $y_{t}.$
\end_inset

 Therefore, stationarity requires that 
\begin_inset Formula 
\[
\lim_{j\rightarrow\infty}F_{(1,1)}^{j}=0
\]

\end_inset


\end_layout

\begin_layout Itemize
Save this result, we'll need it in a minute.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Consider the eigenvalues of the matrix 
\begin_inset Formula $F.$
\end_inset

 These are the 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula 
\[
|F-\lambda I_{P}|=0
\]

\end_inset

 The determinant here can be expressed as a polynomial.
 For example, for 
\begin_inset Formula $p=1,$
\end_inset

 the matrix 
\begin_inset Formula $F$
\end_inset

 is simply 
\begin_inset Formula 
\[
F=\phi_{1}
\]

\end_inset

 so 
\begin_inset Formula 
\[
|\phi_{1}-\lambda|=0
\]

\end_inset

 can be written as 
\begin_inset Formula 
\[
\phi_{1}-\lambda=0
\]

\end_inset

 When 
\begin_inset Formula $p=2,$
\end_inset

 the matrix 
\begin_inset Formula $F$
\end_inset

 is 
\begin_inset Formula 
\[
F=\left[\begin{array}{ll}
\phi_{1} & \phi_{2}\\
1 & 0
\end{array}\right]
\]

\end_inset

 so 
\begin_inset Formula 
\[
F-\lambda I_{P}=\left[\begin{array}{ll}
\phi_{1}-\lambda & \phi_{2}\\
1 & -\lambda
\end{array}\right]
\]

\end_inset

 and 
\begin_inset Formula 
\[
|F-\lambda I_{P}|=\lambda^{2}-\lambda\phi_{1}-\phi_{2}
\]

\end_inset

 So the eigenvalues are the roots of the polynomial 
\begin_inset Formula 
\[
\lambda^{2}-\lambda\phi_{1}-\phi_{2}
\]

\end_inset

 which can be found using the quadratic equation.
 This generalizes.
 For a 
\begin_inset Formula $p^{th}$
\end_inset

 order AR process, the eigenvalues are the roots of 
\begin_inset Formula 
\[
\lambda^{p}-\lambda^{p-1}\phi_{1}-\lambda^{p-2}\phi_{2}-\cdots-\lambda\phi_{p-1}-\phi_{p}=0
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Supposing that all of the roots of this polynomial are distinct, then the
 matrix 
\begin_inset Formula $F$
\end_inset

 
\begin_inset CommandInset href
LatexCommand href
name "can be factored as"
target "http://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix"
literal "false"

\end_inset

 
\begin_inset Formula 
\[
F=T\Lambda T^{-1}
\]

\end_inset

 where 
\begin_inset Formula $T$
\end_inset

 is the matrix which has as its columns the eigenvectors of 
\begin_inset Formula $F,$
\end_inset

 and 
\begin_inset Formula $\Lambda$
\end_inset

 is a diagonal matrix with the eigenvalues on the main diagonal.
 Using this decomposition, we can write 
\begin_inset Formula 
\[
F^{j}=\left(T\Lambda T^{-1}\right)\left(T\Lambda T^{-1}\right)\cdots\left(T\Lambda T^{-1}\right)
\]

\end_inset

 where 
\begin_inset Formula $T\Lambda T^{-1}$
\end_inset

 is repeated 
\begin_inset Formula $j$
\end_inset

 times.
 This gives 
\begin_inset Formula 
\[
F^{j}=T\Lambda^{j}T^{-1}
\]

\end_inset

 and 
\begin_inset Formula 
\[
\Lambda^{j}=\left[\begin{array}{llll}
\lambda_{1}^{j} & 0 &  & 0\\
0 & \lambda_{2}^{j}\\
 &  & \ddots\\
0 &  &  & \lambda_{p}^{j}
\end{array}\right]
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 Supposing that the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 
\begin_inset Formula $i=1,2,...,p$
\end_inset

 are all real valued, it is clear that 
\begin_inset Formula 
\[
\lim_{j\rightarrow\infty}F_{(1,1)}^{j}=0
\]

\end_inset

 requires that 
\begin_inset Formula 
\[
|\lambda_{i}|<1,i=1,2,...,p
\]

\end_inset

 e.g., the eigenvalues must be less than one in absolute value.
\end_layout

\begin_layout Itemize
It may be the case that some eigenvalues are complex-valued.
 The previous result generalizes to the requirement that the eigenvalues
 be less than one in 
\emph on
modulus,
\emph default
 where the modulus of a complex number 
\begin_inset Formula $a+bi$
\end_inset

 is 
\begin_inset Formula 
\[
mod(a+bi)=\sqrt{a^{2}+b^{2}}
\]

\end_inset

 This leads to the famous statement that 
\begin_inset Quotes eld
\end_inset

stationarity requires the roots of the determinantal polynomial to lie inside
 the complex unit circle.
\begin_inset Quotes erd
\end_inset

 
\emph on
draw picture here
\emph default
.
\end_layout

\begin_layout Itemize
When there are roots on 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the unit circle (unit roots) or outside the unit circle, we leave the world
 of stationary processes.
\end_layout

\begin_layout Itemize
Dynamic multipliers: 
\begin_inset Formula $\partial y_{t+j}/\partial\varepsilon_{t}=F_{(1,1)}^{j}$
\end_inset

 is a 
\emph on
dynamic multiplier
\emph default
 or an 
\emph on
impulse-response
\emph default
 function.
 Real eigenvalues lead to steady movements, whereas complex eigenvalues
 lead to oscillatory behavior.
 Of course, when there are multiple eigenvalues the overall effect can be
 a mixture.
 
\emph on
pictures
\emph default

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Moments of AR(p) process
\end_layout

\begin_layout Standard
The AR(p) process is 
\begin_inset Formula 
\[
y_{t}=c+\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\cdots+\phi_{p}y_{t-p}+\varepsilon_{t}
\]

\end_inset

 Assuming stationarity, 
\begin_inset Formula $\mathcal{E}(y_{t})=\mu,\forall t,$
\end_inset

 so 
\begin_inset Formula 
\[
\mu=c+\phi_{1}\mu+\phi_{2}\mu+...+\phi_{p}\mu
\]

\end_inset

 so 
\begin_inset Formula 
\[
\mu=\frac{c}{1-\phi_{1}-\phi_{2}-...-\phi_{p}}
\]

\end_inset

 and 
\begin_inset Formula 
\[
c=\mu-\phi_{1}\mu-...-\phi_{p}\mu
\]

\end_inset

 so 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}-\mu & = & \mu-\phi_{1}\mu-...-\phi_{p}\mu+\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\cdots+\phi_{p}y_{t-p}+\varepsilon_{t}-\mu\\
 & = & \phi_{1}(y_{t-1}-\mu)+\phi_{2}(y_{t-2}-\mu)+...+\phi_{p}(y_{t-p}-\mu)+\varepsilon_{t}
\end{eqnarray*}

\end_inset


\begin_inset Newpage newpage
\end_inset

 With this, the second moments are easy to find: The variance is 
\begin_inset Formula 
\[
\gamma_{0}=\phi_{1}\gamma_{1}+\phi_{2}\gamma_{2}+...+\phi_{p}\gamma_{p}+\sigma^{2}
\]

\end_inset

 The autocovariances of orders 
\begin_inset Formula $j\geq1$
\end_inset

 follow the rule 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{j} & = & \mathcal{E}\left[\left(y_{t}-\mu\right)\left(y_{t-j}-\mu)\right)\right]\\
 & = & \mathcal{E}\left[\left(\phi_{1}(y_{t-1}-\mu)+\phi_{2}(y_{t-2}-\mu)+...+\phi_{p}(y_{t-p}-\mu)+\varepsilon_{t}\right)\left(y_{t-j}-\mu\right)\right]\\
 & = & \phi_{1}\gamma_{j-1}+\phi_{2}\gamma_{j-2}+...+\phi_{p}\gamma_{j-p}
\end{eqnarray*}

\end_inset

 Using the fact that 
\begin_inset Formula $\gamma_{-j}=\gamma_{j},$
\end_inset

 one can take the 
\begin_inset Formula $p+1$
\end_inset

 equations for 
\begin_inset Formula $j=0,1,...,p$
\end_inset

, which have 
\begin_inset Formula $p+1$
\end_inset

 unknowns (
\begin_inset Formula $\sigma^{2},$
\end_inset

 
\begin_inset Formula $\gamma_{0},\gamma_{1},...,\gamma_{p})$
\end_inset

 and solve for the unknowns.
 With these, the 
\begin_inset Formula $\gamma_{j}$
\end_inset

 for 
\begin_inset Formula $j>p$
\end_inset

 can be solved for recursively.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
ARMA model
\end_layout

\begin_layout Standard
An ARMA(
\begin_inset Formula $p,q$
\end_inset

) model is 
\begin_inset Formula $(1+\phi_{1}L+\phi_{2}L^{2}+...+\phi_{p}L^{p})y_{t}=c+(1+\theta_{1}L+\theta_{2}L^{2}+...+\theta_{q}L^{q})\epsilon_{t}$
\end_inset

.
 These are popular in applied time series analysis.
 A high order AR process 
\emph on
may
\emph default
 be well approximated by a low order MA process, and a high order MA process
 
\emph on
may
\emph default
 be well approximated by a low order AR process.
 By combining low order AR and MA processes in the same model, one can hope
 to fit a wide variety of time series using a parsimonious number of parameters.
 There is much literature on how to choose 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q,$
\end_inset

 which is outside the scope of this course.
 Estimation can be done using the Kalman filter, assuming that the errors
 are normally distributed.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Using GRETL, try out various models to explain the unemployment rate, using
 the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{S&W US quarterly macro data.}{https://github.com/mcreel/Economet
rics/blob/master/Examples/Data/us
\backslash
_macro
\backslash
_quarterly.gdt} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
estimate a MA(4) model for the unemployment rate
\end_layout

\begin_layout Itemize
estimate a AR(4) model for the unemployment rate
\end_layout

\begin_layout Itemize
estimate an ARMAX(1,1) model using 12 lags of the GDP growth rate (don't
 use current value)
\end_layout

\begin_deeper
\begin_layout Itemize
interpret the estimated coefficients.
 What can we say about persistence and speed of transmission of effects
 in the economy?
\end_layout

\begin_layout Itemize
look at fit and residuals.
 Observe the 
\begin_inset Quotes eld
\end_inset

Great Moderation
\begin_inset Quotes erd
\end_inset

 of the 1990's, and the return to volatility after the 2007 Great Recession.
\end_layout

\begin_layout Itemize
restrict the estimation sample to before 1994, estimate, and forecast.
\end_layout

\end_deeper
\begin_layout Itemize
look at the BIC to help to decide which model to use
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:VAR-models"

\end_inset

VAR models
\end_layout

\begin_layout Standard
Consider the model
\begin_inset Formula 
\begin{align}
y_{t} & =C+A_{1}y_{t-1}+\epsilon_{t}\label{eq:VAR1}\\
E(\epsilon_{t}\epsilon_{t}^{\prime}) & =\Sigma\nonumber \\
E(\epsilon_{t}\epsilon_{s}^{\prime}) & =0,t\ne s\nonumber 
\end{align}

\end_inset

where 
\begin_inset Formula $y_{t}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 are 
\begin_inset Formula $G\times1$
\end_inset

 vectors, 
\begin_inset Formula $C$
\end_inset

 is a 
\begin_inset Formula $G\times1$
\end_inset

 of constants, and 
\begin_inset Formula $A_{1}$
\end_inset

 is a 
\begin_inset Formula $G\times G$
\end_inset

 matrix of parameters.
 The matrix 
\begin_inset Formula $\Sigma$
\end_inset

 is a 
\begin_inset Formula $G\times G$
\end_inset

 covariance matrix.
 Assume that we have 
\begin_inset Formula $n$
\end_inset

 observations.
 This is a 
\emph on
vector autoregressive
\emph default
 model, of order 1 - commonly referred to as a VAR(1) model.
 It is a collection of 
\begin_inset Formula $G$
\end_inset

 AR(1) models, augmented to include lags of other endogenous variables,
 and the 
\begin_inset Formula $G$
\end_inset

 equations are contemporaneously correlated.
 The extension to a VAR(p) model is quite obvious.
\end_layout

\begin_layout Itemize
As shown in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:EstimationRF"

\end_inset

, it is efficient to estimate a VAR model using OLS equation by equation,
 there is no need to use GLS, in spite of the cross equation correlations.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
A VAR model of this form can be thought of as the reduced form of a dynamic
 simultaneous equations system, with all of the variables treated as endogenous,
 and with lags of all of the endogenous variables present:
\end_layout

\begin_layout Itemize
The simultaneous equations model is (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SIMEQ structural form"

\end_inset

)
\begin_inset Formula 
\[
Y_{t}^{\prime}\Gamma=X_{t}^{\prime}B+E_{t}^{\prime}
\]

\end_inset


\end_layout

\begin_layout Itemize
this can be written after transposing (and adapting notation to use small
 case, pulling the constant out of 
\begin_inset Formula $X_{t}$
\end_inset

 and using 
\begin_inset Formula $v_{t}$
\end_inset

 for the error) as 
\begin_inset Formula $\Gamma^{\prime}y_{t}=a+B^{\prime}x_{t}+v_{t}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $x_{t}=y_{t-1}.$
\end_inset

 Then we have 
\begin_inset Formula $\Gamma^{\prime}y_{t}=a+B^{\prime}y_{t-1}+v_{t}.$
\end_inset


\end_layout

\begin_layout Itemize
Premultiplying by the inverse of 
\begin_inset Formula $\Gamma^{\prime}$
\end_inset

 gives
\begin_inset Formula 
\[
y_{t}=\left(\Gamma^{\prime}\right)^{-1}a+\left(\Gamma^{\prime}\right)^{-1}B^{\prime}y_{t-1}+\left(\Gamma^{\prime}\right)^{-1}v_{t}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Finally define 
\begin_inset Formula $C=\left(\Gamma^{\prime}\right)^{-1}a$
\end_inset

, 
\begin_inset Formula $A_{1}=\left(\Gamma^{\prime}\right)^{-1}B^{\prime}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{t}=\left(\Gamma^{\prime}\right)^{-1}v_{t}$
\end_inset

, and we have the VAR(1) model of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:VAR1"

\end_inset

.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
C.
 Sims originally proposed reduced form VAR models as an alternative to structura
l simultaneous equations models, which were perceived to require too many
 unrealistic assumptions for their identification.
 
\end_layout

\begin_layout Itemize
However, the search for structural interpretations of VAR models slowly
 crept back into the literature, leading to 
\begin_inset Quotes sld
\end_inset

structural VARs
\begin_inset Quotes srd
\end_inset

.
 
\end_layout

\begin_layout Itemize
A structural VAR model is really just a certain form of dynamic linear simultane
ous equations model, with other imaginative and hopefully more realistic
 methods used for identification.
 
\end_layout

\begin_layout Itemize
The issue of identifying the structural parameters 
\begin_inset Formula $\Gamma$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is more or less the same problem that was studied in the context of simultaneou
s equations.
 
\end_layout

\begin_layout Itemize
There, identification was obtained through zero restrictions.
 In the structural VAR literature, zero restrictions are often used, but
 other information may also be used, such as covariance matrix restrictions
 or sign restrictions.
\end_layout

\begin_layout Itemize
Interest often focuses on the impulse-response functions.
 Identification of the impact of structural shocks (how to estimate the
 impact-response functions) is complicated, with many alternative methodologies,
 and is often a topic of much disagreement among practitioners.
 The estimated impulse response functions are often sensitive to the identificat
ion strategy that is used.
 There is a large literature.
 
\end_layout

\begin_layout Itemize
Papers by C.
 Sims are a good place to start, if one wants to learn more.
 He also offers a good deal of useful software on his web page.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An issue which arises when a VAR(p) model 
\begin_inset Formula $y_{t}=C+A_{1}y_{t-1}+\cdots+A_{p}y_{t-p}+\epsilon_{t}$
\end_inset

 is contemplated is that the number of parameters increases rapidly in p,
 which introduces severe collinearity problems.
 
\end_layout

\begin_layout Itemize
One can use Bayesian methods such as the 
\begin_inset Quotes sld
\end_inset

Minnesota prior
\begin_inset Quotes srd
\end_inset

 (search for papers by Litterman), which is a prior that each variable separatel
y follows a random walk (an AR(1) model with 
\begin_inset Formula $\rho=1)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The prior on 
\begin_inset Formula $A_{1}$
\end_inset

 is that it is an identity matrix
\end_layout

\begin_layout Itemize
and the prior on the 
\begin_inset Formula $A_{j},\,j>1$
\end_inset

 is that they are zero matrices
\end_layout

\begin_layout Itemize
thus, each variable follows a random walk, according to the prior
\end_layout

\end_deeper
\begin_layout Itemize
This can be done using stochastic restrictions similar to what was in the
 discussion of collinearity and ridge regression.
 For example, a VAR(2) model in de-meaned variables, with 
\begin_inset Formula $G$
\end_inset

 variables, can be written as
\begin_inset Formula 
\[
Y=\left[\begin{array}{cc}
Y_{-1} & Y_{-2}\end{array}\right]\left[\begin{array}{c}
A_{1}\\
A_{2}
\end{array}\right]+\epsilon
\]

\end_inset

We can impose the stochastic restriction that 
\begin_inset Formula $A_{1}=I_{2}-v_{1}$
\end_inset

 and that 
\begin_inset Formula $A_{2}=0_{2}-v_{2}$
\end_inset

.
 Augmenting the data with these 4 
\begin_inset Quotes sld
\end_inset

artificial observations
\begin_inset Quotes srd
\end_inset

, we get
\begin_inset Formula 
\[
\left[\begin{array}{c}
Y\\
I_{G}\\
0_{G}
\end{array}\right]=\left[\begin{array}{cc}
Y_{-1} & Y_{-2}\\
I_{G} & 0_{G}\\
0_{G} & I_{G}
\end{array}\right]\left[\begin{array}{c}
A_{1}\\
A_{2}
\end{array}\right]+\left[\begin{array}{c}
\epsilon\\
v_{1}\\
v_{2}
\end{array}\right]
\]

\end_inset

Then we can impose how important the restrictions are by weighting the stochasti
c restrictions, along the lines of a GLS heteroscedasticity correction:
 
\begin_inset Formula 
\[
\left[\begin{array}{c}
Y\\
k_{1}I_{G}\\
0_{G}
\end{array}\right]=\left[\begin{array}{cc}
Y_{-1} & Y_{-2}\\
k_{1}I_{G} & 0_{G}\\
0_{G} & k_{2}I_{G}
\end{array}\right]\left[\begin{array}{c}
A_{1}\\
A_{2}
\end{array}\right]+\left[\begin{array}{c}
\epsilon\\
k_{1}v_{1}\\
k_{2}v_{2}
\end{array}\right]
\]

\end_inset

Then we fit by OLS.
 When 
\begin_inset Formula $k_{1}$
\end_inset

 is large, the estimated 
\begin_inset Formula $A_{1}$
\end_inset

 will be forced to be close to an identity matrix.
 When 
\begin_inset Formula $k_{2}$
\end_inset

 is large, the second lag coefficients are all forced to zero.
 Jointly, these restrictions push the model in the direction of separate
 random walks for each variable.
 The degree to which the model is pushed depends on the 
\begin_inset Formula $ks$
\end_inset

.
 When the 
\begin_inset Formula $ks$
\end_inset

 are small, the fit is close to the unrestricted OLS fit, when they are
 large, it is close to separate random walks.
 
\end_layout

\begin_layout Standard
\begin_inset Quotes sld
\end_inset

Bayesian VARs
\begin_inset Quotes srd
\end_inset

 is a now a substantial body of literature.
 An introduction to more formal Bayesian methods is given in a chapter that
 follows.
 For highly parameterized models, Bayesian methods can help to impose structure.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
Using GRETL, using the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{S&W US quarterly macro data.}{https://github.com/mcreel/Economet
rics/blob/master/Examples/Data/us
\backslash
_macro
\backslash
_quarterly.gdt} 
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
compute the term spread using the US Macro data.
 (term spread is GS10 - TB3MS)
\end_layout

\begin_layout Itemize
estimate a VAR(1) model for unemployment rate, GDP growth rate, and the
 term spread
\end_layout

\begin_layout Itemize
examine the impulse-response functions
\end_layout

\begin_layout Itemize
using the BIC, is the equation for the unemployment rate preferred, compared
 to the models of the previous example?
\end_layout

\begin_layout Itemize
See Stock and Watson, Ch.
 14 for more discussion
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Get the simulation data from the 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{example DSGE model}{https://github.com/mcreel/Econometrics/blob
/master/Examples/DSGE/GenData/dsgedata.gdt} 
\end_layout

\end_inset

.
 Recall that this simulated data intends to be representative of 40 years
 of quarterly data.
 
\begin_inset Newline newline
\end_inset

1.
 Estimate a VAR(1) model.
 Do an analysis of collinearity.
 Compute impulse-response functions.
\begin_inset Newline newline
\end_inset

2.
 Estimate an AR(1) model for output.
 Compare 
\begin_inset Formula $R^{2}$
\end_inset

 to the AR(1) model.Note that matching impulse response functions has sometimes
 been used for estimation of DSGE models.
 Perhaps we'll see this idea again.
\end_layout

\begin_layout Exercise
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
ARCH, GARCH and Stochastic volatility
\end_layout

\begin_layout Standard
ARCH (autoregressive conditionally heteroscedastic) models appeared in the
 literature in 1982, in Engle, Robert F.
 (1982).
 "Autoregressive Conditional Heteroscedasticity with Estimates of Variance
 of United Kingdom Inflation", Econometrica 50:987-1008.
 This paper stimulated a very large growth in the literature for a number
 of years afterward.
 The related GARCH (generalized ARCH) model is now one of the most widely
 used models for financial time series.
\end_layout

\begin_layout Standard
Financial time series often exhibit several type of behavior:
\end_layout

\begin_layout Itemize
volatility clustering: periods of low variation can be followed by periods
 of high variation
\end_layout

\begin_layout Itemize
fat tails, or 
\begin_inset CommandInset href
LatexCommand href
name "excess kurtosis"
target "http://en.wikipedia.org/wiki/Kurtosis"
literal "false"

\end_inset

: the marginal density of a series is more strongly peaked and has fatter
 tails than does a normal distribution with the same mean and variance.
\end_layout

\begin_layout Itemize
leverage (negative correlation between returns and volatility), which often
 shows up as negative 
\begin_inset CommandInset href
LatexCommand href
name " skewness"
target "http://en.wikipedia.org/wiki/Skewness"
literal "false"

\end_inset

 of returns
\end_layout

\begin_layout Itemize
perhaps slight autocorrelation within the bounds allowed by arbitrage
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The data set 
\begin_inset Quotes sld
\end_inset

nysewk.gdt
\begin_inset Quotes srd
\end_inset

, which is provided with Gretl, provides an example.
 If we compute 100 times the growth rate of the series, using log differences,
 we can obtain the plots in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dow-Jones-close,"

\end_inset

 (Julia code for this is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{here}{https://github.com/mcreel/Econometrics/blob/master/Exampl
es/TimeSeries/MakePlots.jl} 
\end_layout

\end_inset

).
 In the first we clearly see volatility clusters, and in the second, we
 see excess kurtosis, skew, and tails fatter than the normal distribution.
 The skewness suggests that leverage may be present.
 We'll see how the third plot was made in the chapter on nonparametric estimatio
n.
\end_layout

\begin_layout Itemize
compute descriptive statistics: negative skew and positive excess kurtosis
\end_layout

\begin_layout Itemize
regress returns on its own lag and on squared returns and lags: low predictabili
ty
\end_layout

\begin_layout Itemize
regress squared returns on its own lags and on returns: more predictable,
 evidence of leverage
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dow-Jones-close,"

\end_inset

NYSE weekly close price, 100 
\begin_inset Formula $\text{\times}$
\end_inset

log differences
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/TimeSeries/nyse.svg
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The presence of volatility clusters indicates that the variance of the series
 is not constant over time, conditional on past events.
 Engle's ARCH paper was the first to model this feature.
 
\end_layout

\begin_layout Itemize
The frequency plot shows excess kurtosis and skew (leverage)
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
ARCH
\end_layout

\begin_layout Standard
A basic ARCH specification is
\begin_inset Formula 
\begin{align*}
y_{t} & =\mu+\rho y_{t-1}+\epsilon_{t}\\
 & \equiv g_{t}+\epsilon_{t}\\
\epsilon_{t} & =\sigma_{t}u_{t}\\
\sigma_{t}^{2} & =\omega+\sum_{i=1}^{q}\alpha_{i}\epsilon_{t-i}^{2}
\end{align*}

\end_inset

where the 
\begin_inset Formula $u_{t}$
\end_inset

 are Gaussian white noise shocks.
 The ARCH variance is a moving average process.
 Previous large shocks to the series cause the conditional variance of the
 series to increase.
 There is no leverage: negative shocks have the same impact on the future
 variance as do positive shocks.
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
 
\end_layout

\begin_layout Itemize
for 
\begin_inset Formula $\sigma_{t}^{2}$
\end_inset

 to be positive for all realizations of 
\begin_inset Formula $\left\{ \epsilon_{t}\right\} $
\end_inset

, we need 
\begin_inset Formula $\omega>0$
\end_inset

, 
\begin_inset Formula $\alpha_{i}\ge0$
\end_inset

, 
\begin_inset Formula $\forall i$
\end_inset

.
\end_layout

\begin_layout Itemize
to ensure that the model is covariance stationary, we need 
\begin_inset Formula $\sum_{i}\alpha_{i}<1$
\end_inset

.
 Otherwise, the variances will explode off to infinity.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Given that 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is normally distributed, to find the likelihood in terms of the observable
 
\begin_inset Formula $y_{t}$
\end_inset

 instead of the unobservable 
\begin_inset Formula $\epsilon_{t}$
\end_inset

, first note that the series 
\begin_inset Formula $u_{t}=\left(y_{t}-g_{t}\right)/\sigma_{t}=\frac{\epsilon_{t}}{\sigma_{t}}$
\end_inset

 is iid Gaussian, so the likelihood is simply the product of standard normal
 densities.
 
\begin_inset Formula 
\begin{eqnarray*}
u & \sim & N(0,I),\textrm{ so}\\
f(u) & = & \prod_{t=1}^{n}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{u_{t}^{2}}{2}\right)
\end{eqnarray*}

\end_inset

 The joint density for 
\begin_inset Formula $y$
\end_inset

 can be constructed using a change of variables:
\end_layout

\begin_layout Itemize
We have 
\begin_inset Formula $u_{t}=\left(y_{t}-\mu-\rho y_{t-1}\right)/\sigma_{t}$
\end_inset

, so 
\begin_inset Formula $\frac{\partial u_{t}}{\partial y_{t}}=\frac{1}{\sigma_{t}}$
\end_inset

 and 
\begin_inset Formula $|\frac{\partial u}{\partial y^{\prime}}|=\prod_{t=1}^{n}\frac{1}{\sigma_{t}},$
\end_inset

 
\end_layout

\begin_layout Itemize
doing a change of variables, 
\begin_inset Formula 
\[
f(y;\theta)=\prod_{t=1}^{n}\frac{1}{\sqrt{2\pi}}\frac{1}{\sigma_{t}}\exp\left(-\frac{1}{2}\left(\frac{y_{t}-\mu-\rho y_{t-1}}{\sigma_{t}}\right)^{2}\right)
\]

\end_inset

where 
\begin_inset Formula $\theta$
\end_inset

 is the vector of all parameters (the parameters in 
\begin_inset Formula $g_{t}$
\end_inset

, and the 
\begin_inset Formula $\omega$
\end_inset

 and alpha parameters of the ARCH specification.
 Taking logs, 
\begin_inset Formula 
\[
\ln L(\theta)=-n\ln\sqrt{2\pi}-\sum_{t=1}^{n}\ln\sigma_{t}-\frac{1}{2}\sum_{t=1}^{n}\left(\frac{y_{t}-\mu-\rho y_{t-1}}{\sigma_{t}}\right)^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

In principle, this is easy to maximize.
 Some complications can arise when the restrictions for positivity and stationar
ity are imposed.
 Consider a fairly short data series with low volatility in the initial
 part, and high volatility at the end.
 This data appears to have a nonstationary variance sequence.
 If one attempts to estimate and ARCH model with stationarity imposed, the
 data and the restrictions are saying two different things, which can make
 maximization of the likelihood function difficult.
\end_layout

\begin_layout Itemize
use GRETL to estimate ARCH(1) and ARCH(4)
\end_layout

\begin_layout Itemize
if interested, adapt the Julia code for GARCH(1,1), below, to estimate an
 ARCH model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
GARCH
\end_layout

\begin_layout Standard
Note that an ARCH model specifies the variance process as a moving average.
 For the same reason that an ARMA model may be used to parsimoniously model
 a series instead of a high order AR or MA, one can do the same thing for
 the variance series.
 A basic GARCH(p,q) (Bollerslev, Tim (1986).
 "Generalized Autoregressive Conditional Heteroskedasticity", Journal of
 Econometrics, 31:307-327) specification is
\begin_inset Formula 
\begin{align*}
y_{t} & =\mu+\rho y_{t-1}+\epsilon_{t}\\
\epsilon_{t} & =\sigma_{t}u_{t}\\
\sigma_{t}^{2} & =\omega+\sum_{i=1}^{q}\alpha_{i}\epsilon_{t-i}^{2}+\sum_{i=1}^{p}\beta_{i}\sigma_{t-i}^{2}
\end{align*}

\end_inset

The idea is that a GARCH model with low values of p and q may fit the data
 as well or better than an ARCH model with large q.
\end_layout

\begin_layout Itemize
the model also requires restrictions for positive variance and stationarity,
 which are:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\omega>0$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha_{i}\ge0,\,i=1,...,q$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{i}\ge0,\,i=1,...,p$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\sum_{i=1}^{q}\alpha_{i}$
\end_inset

+
\begin_inset Formula $\sum_{i=1}^{p}\beta_{i}<1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
to estimate a GARCH model, you need to initialize 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 at some value.
 The sample unconditional variance is one possibility.
 Another choice could be the sample variance of the initial elements of
 the sequence.
 One can also 
\begin_inset Quotes sld
\end_inset

backcast
\begin_inset Quotes srd
\end_inset

 the conditional variance.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The GARCH model also requires restrictions on the parameters to ensure stationar
ity and positivity of the variance.
 
\end_layout

\begin_layout Itemize
A useful modification is the EGARCH model (exponential GARCH, Nelson, D.
 B.
 (1991).
 "Conditional heteroskedasticity in asset returns: A new approach", Econometrica
 59: 347-370).
 This model treats the logarithm of the variance as an ARMA process, so
 the variance will be positive without restrictions on the parameters.
\end_layout

\begin_layout Itemize
There are many variants that introduce asymmetry (leverage) and non-normality.
\end_layout

\begin_layout Itemize
GARCH(1,1) is a highly popular model in financial analysis.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Garch11Example.jl}{https://github.com/mcreel/Econometrics/blob/m
aster/Examples/TimeSeries/Garch11Example.jl} 
\end_layout

\end_inset

 illustrates estimation of a GARCH(1,1) model, using the NYSE closing price
 data.
 Results:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

 julia> include("Garch11Example.jl")
\end_layout

\begin_layout Plain Layout

**************************************************
\end_layout

\begin_layout Plain Layout

GARCH(1,1) example
\end_layout

\begin_layout Plain Layout

MLE Estimation Results
\end_layout

\begin_layout Plain Layout

BFGS convergence: Normal convergence
\end_layout

\begin_layout Plain Layout

Average Log-L: -2.07859
\end_layout

\begin_layout Plain Layout

Observations: 2115
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

            estimate   st.
 err    t-stat   p-value
\end_layout

\begin_layout Plain Layout

         1   0.17647   0.04030   4.37857   0.00001
\end_layout

\begin_layout Plain Layout

         2   0.00168   0.02287   0.07347   0.94144
\end_layout

\begin_layout Plain Layout

         3   0.15989   0.05657   2.82613   0.00476
\end_layout

\begin_layout Plain Layout

         4   0.11238   0.02601   4.32127   0.00002
\end_layout

\begin_layout Plain Layout

         5   0.85350   0.03039  28.08748   0.00000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Information Criteria
\end_layout

\begin_layout Plain Layout

               Crit.
    Crit/n
\end_layout

\begin_layout Plain Layout

     CAIC 8835.73256   4.17765
\end_layout

\begin_layout Plain Layout

      BIC 8830.73256   4.17529
\end_layout

\begin_layout Plain Layout

      AIC 8802.44851   4.16191
\end_layout

\begin_layout Plain Layout

**************************************************
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
examine the code to see how start values were determined, and how the variance
 loop was initialized.
\end_layout

\begin_layout Itemize
The AR(1) in the mean is probably not needed.
\end_layout

\begin_layout Itemize
Compare BIC to ARCH(1) and ARCH(4), which you can obtain using GRETL.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

You can get the same results quickly and easily using Gretl: 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Model 1: GARCH, using observations 670078-672192 (T = 2115)
\end_layout

\begin_layout Plain Layout

Dependent variable: y
\end_layout

\begin_layout Plain Layout

Standard errors based on Hessian
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

             coefficient   std.
 error      z        p-value 
\end_layout

\begin_layout Plain Layout

  ----------------------------------------------------------
\end_layout

\begin_layout Plain Layout

  const      0.177119      0.0387575     4.570     4.88e-06  ***
\end_layout

\begin_layout Plain Layout

  y_1        0.00148067    0.0232384     0.06372   0.9492   
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  alpha(0)   0.155435      0.0451241     3.445     0.0006    ***
\end_layout

\begin_layout Plain Layout

  alpha(1)   0.111397      0.0171598     6.492     8.48e-11  ***
\end_layout

\begin_layout Plain Layout

  beta(1)    0.855317      0.0228815    37.38      8.18e-306 ***
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Mean dependent var   0.129001   S.D.
 dependent var   2.061158
\end_layout

\begin_layout Plain Layout

Log-likelihood      4396.923   Akaike criterion     8805.846
\end_layout

\begin_layout Plain Layout

Schwarz criterion    8839.786   Hannan-Quinn         8818.273
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
There are some minor differences, because the Julia code initializes the
 variance in a different way, using only the first 10 observations.
 Also, the Julia code uses sandwich standard errors, while GRETL uses the
 Hessian, which tends to inflate t-statistics.
\end_layout

\begin_layout Itemize
Note that the 
\begin_inset Formula $\beta_{1}$
\end_inset

 parameter is highly significant.
 If you compare likelihood values or information criteria values with the
 ARCH results, you'll see that this model is favored - it fits better with
 fewer parameters.
\end_layout

\begin_layout Itemize
Gretl has a number of other ARCH/GARCH style models available.
\end_layout

\begin_layout Itemize
With Gretl, run the GARCH variants GJR(1,1) with skewed t shocks.
\end_layout

\begin_deeper
\begin_layout Itemize
Do a density plot
\end_layout

\begin_layout Itemize
note the BIC value
\end_layout

\begin_layout Itemize
there are a lot of options to explore
\end_layout

\end_deeper
\begin_layout Itemize
Note that the test of homoscedasticity against ARCH or GARCH involves parameters
 being on the boundary of the parameter space, which means that standard
 asymptotics do not apply.
 Also, the reduction of GARCH to ARCH has the same problem.
 Testing needs to be done taking this into account.
 See Demos and Sentana (1998) 
\emph on
Journal of Econometrics
\emph default
.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Stochastic-volatility"

\end_inset

Stochastic volatility
\end_layout

\begin_layout Standard
In ARCH and GARCH models, the same shocks that affect the level also affect
 the variance.
 The stochastic volatility model allows the variance to have its own random
 component.
 A simple example is
\begin_inset Formula 
\begin{align*}
y_{t} & =\sigma_{\epsilon}\exp(h_{t}/2)\epsilon_{t}\\
h_{t} & =\rho h_{t-1}+\sigma_{u}u_{t}
\end{align*}

\end_inset

In this model, the log of the variance of the observed sequence follows
 an AR(1) model.
 Once can introduce leverage by allowing correlation between 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $u_{t}.$
\end_inset

 A mini-package that let's you generate data from this model is here: 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{https://github.com/mcreel/SV}{https://github.com/mcreel/SV}
\end_layout

\end_inset

.
 Typical data and a nonparametric density plot look like what we see in
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SV-model,-typical"

\end_inset

.
 Note the volatility clusters, leptokurtosis, and the fat tails of the density.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SV-model,-typical"

\end_inset

SV model, typical data and density
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/TimeSeries/svdata.svg
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
While the ARCH and GARCH models have a link between the shocks to 
\begin_inset Formula $y_{t}$
\end_inset

 and the dynamics of the variance of 
\begin_inset Formula $y_{t}$
\end_inset

, the stochastic volatility model has latent shocks to the variance which
 are not directly linked to the observed dependent variable.
 This may be perfectly reasonable: even when volatility is high, the mean
 of shocks to the observables may be zero.
 An ARCH model could not account for an increase in volatility without having
 a realized extreme shock to the level.
 The SV model can allow for this.
 
\end_layout

\begin_layout Itemize
The latent shocks complicate estimation.
 Many estimation methods have been proposed, and this sort of model helped
 to popularize Bayesian methods in econometrics: see Jacquier, E., Polson,
 N.G.
 and Rossi, P.E., 2002.
 Bayesian analysis of stochastic volatility models.
 Journal of Business & Economic Statistics, 20(1), pp.69-87.
 We will see an examples of estimation in the chapter on simulation-based
 estimation.
\end_layout

\begin_layout Itemize
Variants of this sort of model are widely used to model financial data,
 competing with the GARCH(1,1) model for being the most popular choice.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Diffusion-models"

\end_inset

Diffusion models
\end_layout

\begin_layout Standard
Financial data is often modeled using a continuous time specification.
 An example is the following model, taken from a paper of mine (JEF, 2015,
 with D.
 Kristensen).
\end_layout

\begin_layout Standard
A basic model is a simple continuous time stochastic volatility model with
 leverage.
 Log price 
\begin_inset Formula $p_{t}=\log\left(P_{t}\right)$
\end_inset

, solves the following pure diffusion model,
\begin_inset Formula 
\[
dp_{t}=\left(\mu_{0}+\mu_{1}\text{\exp}\left(h_{t}-\alpha\right)\right)dt+\text{\exp}\left(\frac{h_{t}}{2}\right)dW_{1,t}
\]

\end_inset

where the spot volatility (the instantaneous variance of returns), 
\begin_inset Formula $\exp(h_{t})$
\end_inset

 is modeled using its logarithm:
\begin_inset Formula 
\[
dh_{t}=\kappa(\alpha-h_{t})dt+\sigma dW_{2,t}.
\]

\end_inset

Here, 
\begin_inset Formula $W_{1,t}$
\end_inset

 and 
\begin_inset Formula $W_{2,t}$
\end_inset

 are two standard Brownian motions with instantaneous correlation 
\begin_inset Formula $\rho=Cov\left(dW_{1,t},dW_{2,t}\right)$
\end_inset

.
 The parameters are interpreted as follows: 
\begin_inset Formula $\mu_{0}$
\end_inset

 is the baseline drift of returns; 
\begin_inset Formula $\mu_{1}$
\end_inset

 allows drift to depend upon spot volatility; 
\begin_inset Formula $\alpha$
\end_inset

 is the mean of log volatility; 
\begin_inset Formula $\kappa$
\end_inset

 is the speed of mean reversion of log volatility, such that low values
 of 
\begin_inset Formula $\kappa$
\end_inset

 imply high persistence of log volatility; 
\begin_inset Formula $\sigma$
\end_inset

 is the so-called volatility of volatility; and 
\begin_inset Formula $\rho$
\end_inset

 is a leverage parameter that affects the correlation between returns and
 log volatility.
 We collect the parameters in 
\begin_inset Formula $\theta=\left(\mu_{0},\mu_{1},\alpha,\kappa,\sigma,\rho\right)$
\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An extension is to add jumps to the above model.
 These occur with Poisson frequency, and are conditionally normally distributed.
 More specifically, log-price 
\begin_inset Formula $p_{t}$
\end_inset

 solves the following continuous-time jump-diffusion model,
\begin_inset Formula 
\[
dp_{t}=\left(\mu_{0}+\mu_{1}\text{\exp}\left(h_{t}/2\right)\right)dt+\text{\exp}\left(\frac{h_{t}}{2}\right)dW_{1,t}+J_{t}dN_{t}.
\]

\end_inset

The Poisson process 
\begin_inset Formula $N_{t}$
\end_inset

 counts the number of jumps up to time 
\begin_inset Formula $t,$
\end_inset

 and has jump intensity 
\begin_inset Formula $\lambda_{t}=\lambda_{0}+\lambda_{1}\text{\exp}\left(h_{t}-\alpha\right)$
\end_inset

 that varies with the volatility, while jump sizes, conditional on the occurrenc
e of a jump, are independent and conditionally normally distributed: 
\begin_inset Formula $J_{t}\sim N(\mu_{J},\sigma_{J}^{2})$
\end_inset

.
 The inclusion of the jump component adds four parameters to 
\begin_inset Formula $\theta$
\end_inset

 as defined above, 
\begin_inset Formula $\mu_{J}$
\end_inset

, 
\begin_inset Formula $\sigma_{J}^{2}$
\end_inset

 and 
\begin_inset Formula $\lambda_{0}$
\end_inset

, and 
\begin_inset Formula $\lambda_{1}$
\end_inset

.
 This jump model was considered in, for example, Andersen, Benzoni and Lund
 (2002).
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
An example of how returns, 
\begin_inset Formula $r_{t}=100(p_{t}-p_{t-1})$
\end_inset

, generated by such a model might look is given in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Returns-from-jump-diffusion"

\end_inset

.
 The spot volatility is plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Spot-volatility,-jump-diffusion"

\end_inset

.
 Returns are observable, but spot volatility is not.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Returns-from-jump-diffusion"

\end_inset

Returns from jump-diffusion model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/rets.svg
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Spot-volatility,-jump-diffusion"

\end_inset

Spot volatility, jump-diffusion model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/spotvol.svg
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

One might want to try to infer the parameters of the model, and also the
 latent spot volatility, using the observable data.
 
\end_layout

\begin_layout Itemize
Estimation of the parameters of such models is complicated by the fact that
 data is available in discrete time: 
\begin_inset Formula $p_{1},p_{2},...p_{n},$
\end_inset

 but the model is in continuous time.
 
\end_layout

\begin_layout Itemize
One can 
\begin_inset Quotes sld
\end_inset

discretize
\begin_inset Quotes srd
\end_inset

 the model, to obtain something like the discrete time SV model of the previous
 section, but the discrete time transition density implied by the approximating
 model is not the same as the true transition density 
\begin_inset Formula 
\[
p_{t}\sim f_{p}\left(p_{t}|p_{t-1},h_{t-1};\theta\right),
\]

\end_inset

induced by the continuous time model.
 
\end_layout

\begin_layout Itemize
This true density is unknown, however, so using it for ML estimation is
 not possible.
 If one estimates the discrete time version treating it as the actual density,
 there is an approximation misspecification that causes the estimates to
 be inconsistent: we're not doing ML, we're doing quasi-ML, which is in
 general an inconsistent estimator.
 
\end_layout

\begin_layout Itemize
Consistent estimation of parameters is discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:sim-baed estimation differential eqns"

\end_inset

, in the Chapter on simulation-based estimation.
 A means of learning about spot volatility, 
\begin_inset Formula $h_{t},$
\end_inset

 given estimated parameters and the history of observable variables, is
 discussed in the chapter on nonparametric inference, in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Limited-information-nonparametri"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:State-space-models"

\end_inset

State space models
\end_layout

\begin_layout Standard
For linear time series models with Gaussian shocks, it is often useful to
 put the model in state space form, as in this form, the Kalman filter provides
 a convenient way to compute the likelihood function.
 For example, with an MA model, we can compute the likelihood function using
 the joint density of the whole sample, 
\begin_inset Formula $y\sim N(0,\Sigma$
\end_inset

) where 
\begin_inset Formula $\Sigma$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 matrix that depends on 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and 
\begin_inset Formula $\phi$
\end_inset

,  The log likelihood is 
\begin_inset Formula $f(y|\sigma^{2},\phi)$
\end_inset

, as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MA1likelihood"

\end_inset

.
 That form of writing the likelihood uses a lot of computer memory, as the
 entire 
\begin_inset Formula $\Sigma$
\end_inset

 matrix must be stored.
 A more efficient method is to write the MA model as a linear Gaussian state-spa
ce model, and to use Kalman filtering to compute the likelihood.
 
\end_layout

\begin_layout Standard
For Kalman filtering, see Hamilton, 
\emph on
Time Series Analysis, 
\emph default
Chapter 13.
 Also, see 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "Mikusheva's MIT Open Courseware notes"
target "http://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2013/lecture-notes/"
literal "false"

\end_inset


\bar default
, lectures 21 and 22, and also the summary in the introduction of 
\begin_inset CommandInset citation
LatexCommand citet
key "LopesTsayPArticleFilter2011"
literal "true"

\end_inset

.
 Additional sources are 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "Fernndez-Villaverde's Kalman filter notes"
target "http://economics.sas.upenn.edu/~jesusfv/Chapter_4_Likelihood.pdf"
literal "false"

\end_inset


\bar default
 and the 
\begin_inset CommandInset href
LatexCommand href
name "Quantitative Economics Kalman filter"
target "http://quant-econ.net/jl/linear_models.html"
literal "false"

\end_inset

 tutorial (includes example code using the Julia language).
\end_layout

\begin_layout Standard
For nonlinear state space models, or non-Gaussian state space models, the
 basic Kalman filter cannot be used, and the particle filter is becoming
 a widely-used means of computing the likelihood.
 This is a fairly new, computationally demanding technique, and is currently
 (this was written in 2013) an active area of research.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "LopesTsayPArticleFilter2011"
literal "true"

\end_inset

 for a review.
 Papers by Fernndez-Villaverde and Rubio-Ramrez provide interesting and
 reasonably accessible applications in the context of estimating macroeconomic
 (DSGE) models.
\end_layout

\begin_layout Section
Nonstationarity and cointegration
\end_layout

\begin_layout Standard
I'm going to follow Karl Whelan's notes, which are available at
\bar under
 
\begin_inset CommandInset href
LatexCommand href
name "Whelan notes"
target "http://www.karlwhelan.com/Teaching/MA%20Econometrics/part4.pdf"
literal "false"

\end_inset


\bar default
.
 A Gretl script file which generates data following the random walk with
 drift example is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{RandomWalks.inp}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/TimeSeries/RandomWalks.inp} 
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
run the script to generate data.
 Then set the data set structure to time series.
\end_layout

\begin_layout Itemize
do a time series plot of the y and x series
\end_layout

\begin_layout Itemize
run an OLS of y on x
\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
Use Matlab/Octave to estimate the same GARCH(1,1) model as in the GarchExample.jl
 script provided above (hint: get version 1.0 of these notes: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/mcreel/Econometrics/tree/v1.0
\end_layout

\end_inset

).
 Also, estimate an ARCH(4) model for the same data.
 If unconstrained estimation does not satisfy stationarity restrictions,
 then do contrained estimation.
 Compare likelihood values.
 Which of the two models do you prefer? But do the models have the same
 number of parameters? Find out what is the 
\begin_inset Quotes sld
\end_inset

consistent Akaike information criterion
\begin_inset Quotes srd
\end_inset

 or the 
\begin_inset Quotes sld
\end_inset

Bayes information criterion
\begin_inset Quotes srd
\end_inset

 and what they are used for.
 Compute one or the other, or both, and discuss what they tell you about
 selecting between the two models.
\end_layout

\begin_layout Enumerate
Use Gretl to estimate (by ML) the same Garch(1,1) model as in the previous
 problem using the nysewk.gdt data set.
 Do you get the same parameter estimates?
\end_layout

\begin_layout Enumerate
Write a Matlab/Julia/your favorite package script that generates two independent
 random walks, 
\begin_inset Formula $x_{t}=x_{t-1}+u_{t}$
\end_inset

 and 
\begin_inset Formula $y_{t}=y_{t-1}+u_{t}$
\end_inset

, where the initial conditions are 
\begin_inset Formula $x_{0}=0$
\end_inset

 and 
\begin_inset Formula $y_{0}=0$
\end_inset

, and the two errors are both iid N(0,1).
 Use a sample size of 1000: 
\begin_inset Formula $t=1,2,...,1000.$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
regress y upon x and a constant.
\end_layout

\begin_layout Enumerate
discuss your findings, especially the slope coefficient, the t statistic
 of the slope, and 
\begin_inset Formula $R^{2}$
\end_inset

.
 Are the findings sensible, given that we know that 
\begin_inset Formula $x$
\end_inset

 has nothing to do with 
\begin_inset Formula $y$
\end_inset

?
\end_layout

\begin_layout Enumerate
compute the variance of 
\begin_inset Formula $y_{t}$
\end_inset

 and 
\begin_inset Formula $x_{t}$
\end_inset

 conditional on the initial conditions 
\begin_inset Formula $y_{0}=0$
\end_inset

 and 
\begin_inset Formula $x_{0}=0.$
\end_inset

 Does the variance depend on 
\begin_inset Formula $t$
\end_inset

?
\end_layout

\begin_layout Enumerate
which of the assumptions of the classical linear regression model are not
 satisfied by this data generating process?
\end_layout

\begin_layout Enumerate
present estimation results using transformation(s) of y and/or x so that
 the regression using the transformed variables confirms that there is no
 relationship between the variables.
 Explain why the transformation(s) you use are successful in eliminating
 the problem of a spurious relationship.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Bayesian-methods"

\end_inset

Bayesian methods
\end_layout

\begin_layout Standard
This chapter provides a brief introduction to Bayesian methods, which form
 a large part of econometric research, especially in the last two decades.
 Advances in computational methods (e.g., MCMC, particle filtering), combined
 with practical advantages of Bayesian methods (e.g., no need for minimization
 and improved identification coming from the prior) have contributed to
 the popularity of this approach.
 References I have used to prepare these notes: 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Chapter 13; 
\begin_inset CommandInset citation
LatexCommand cite
key "ChernozhukovHong2003"
literal "true"

\end_inset


\emph on
; 
\emph default
Gallant and Tauchen, 
\begin_inset Quotes sld
\end_inset

EMM: A program for efficient method of moments estimation
\begin_inset Quotes srd
\end_inset

; Hoogerheide, van Dijk and van Oest (2007) 
\begin_inset Quotes sld
\end_inset

Simulation Based Bayesian Econometric Inference: Principles and Some Recent
 Computational Advances
\begin_inset Quotes srd
\end_inset

.
 You might also like to read Mikusheva's MIT OpenCourseWare notes, lectures
 23-26: 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "Bayesian notes"
target "http://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2013/lecture-notes/"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Definitions
\end_layout

\begin_layout Standard
The Bayesian approach summarizes beliefs about parameters using a density
 function:
\end_layout

\begin_layout Itemize
There is a true unknown parameter vector, 
\begin_inset Formula $\theta_{0}$
\end_inset

, and the density 
\begin_inset Formula $\pi(\theta)$
\end_inset

, which is known as the 
\emph on
prior
\emph default
, reflects current beliefs about the parameter, before observing the sample.
 It is assumed that the econometrician can provide this density.
\end_layout

\begin_layout Itemize
We also have sample information, 
\begin_inset Formula $y$
\end_inset

=
\begin_inset Formula $\left\{ y_{1},y_{2},...y_{n}\right\} $
\end_inset

.
 We're already familiar with the 
\emph on
likelihood function,
\emph default
 
\begin_inset Formula $f(y|\theta)$
\end_inset

, which is the density of the sample given a parameter value.
 
\end_layout

\begin_layout Standard
Given these two pieces, we can write the joint density of the sample and
 the beliefs:
\begin_inset Formula 
\[
f(y,\theta)=f(y|\theta)\pi(\theta)
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

We can get the 
\emph on
marginal likelihood 
\emph default
by integrating out the parameter, integrating over its support 
\begin_inset Formula $\Theta$
\end_inset

:
\emph on

\begin_inset Formula 
\[
f(y)=\int_{\Theta}f(y,\theta)d\theta
\]

\end_inset


\emph default
The last step is to get the 
\emph on
posterior 
\emph default
of the parameter.
 This is simply the density of the parameter conditional on the sample,
 and we get it in the normal way we get a conditional density, using Bayes'
 theorem:
\begin_inset Formula 
\[
f(\theta|y)=\frac{f(y,\theta)}{f(y)}=\frac{f(y|\theta)\pi(\theta)}{f(y)}
\]

\end_inset


\end_layout

\begin_layout Itemize
The movement from the prior to the posterior reflects the learning that
 occurs about the parameter when one receives the sample information.
\end_layout

\begin_layout Itemize
The sources of information used to make the posterior are the prior and
 the likelihood function.
 
\end_layout

\begin_layout Itemize
Once we have the posterior, one can provide a complete probabilistic description
 about our updated beliefs about the parameter, using quantiles or moments
 of the posterior.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The posterior mean or median provide the Bayesian analogue of the frequentist
 point estimator, in the form of the ML estimator.
 
\end_layout

\begin_layout Itemize
One can show that these point estimators converge to the true 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
We can define regions analogous to confidence intervals by using quantiles
 of the posterior, or the marginal posterior.
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
So far, this is pretty straightforward.
 The complications are mostly computational.
 To illustrate, the posterior mean is
\begin_inset Formula 
\[
E(\theta|y)=\int_{\Theta}\theta f(\theta|y)d\theta=\frac{\int_{\Theta}\theta f(y|\theta)\pi(\theta)d\theta}{\int_{\Theta}f(y,\theta)d\theta}
\]

\end_inset

 
\end_layout

\begin_layout Itemize
One can see that a means of integrating will be needed.
\end_layout

\begin_layout Itemize
Only in very special cases will the integrals have analytic solutions.
\end_layout

\begin_layout Itemize
Otherwise, computational methods will be needed.
 Advances in computational methods are what have lead to the increased use
 of Bayesian methods.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Philosophy, etc.
\end_layout

\begin_layout Standard
So, the classical paradigm views the data as generated by a data generating
 process, which is a perhaps unknown model characterized by a parameter
 vector, and the data is generated from the model at a particular value
 of the parameter vector, 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Bayesians view data as given, and update beliefs about a parameter using
 the information about the parameter contained in the data.
 There's nothing obviously contradictory in these views.
 Nevertheless, it's not hard to find discussions where there are disagreements.
\end_layout

\begin_layout Standard
Here, I'm trying to address a model with a fixed non-random parameter about
 which we would like to learn.
 As long as the object of interest - the dgp and it's parameter - is agreed
 upon, then we can contemplate using any convenient tool.
\end_layout

\begin_layout Standard
Even if one is a strict frequentist, one shouldn't reinvent the wheel each
 time we get a new sample: previous samples have information about the parameter
, and we should use all of the available information.
 A pure frequentist 
\begin_inset Quotes sld
\end_inset

full information
\begin_inset Quotes srd
\end_inset

 approach would require writing the joint likelihood of all samples, which
 would almost certainly constitute an impossible task.
 The Bayesian approach concentrates all of the information coming from previous
 work in the form of a prior.
 A fairly simple, easy to use prior may not 
\emph on
exactly
\emph default
 capture all previous information, but it could offer a handy and reasonably
 accurate summary, and it's almost certainly better than simply pretending
 that all of that previous information simply doesn't exist.
 So, the idea of a prior as a summary of what we have learned may simply
 be viewed as a practical solution to the problem of using all the available
 information.
 Given that it's a summary, one may as well use a convenient form, as long
 as it's plausible and the results don't depend too exaggeratedly particular
 form used.
\end_layout

\begin_layout Standard
As long as one takes the view that there is a fixed unknown parameter value
 
\begin_inset Formula $\theta_{0}$
\end_inset

 which generates all samples, then frequentist and Bayesian methods are
 trying to inform us about the same object, and the choice between tools
 may become one of convenience.
 It turns out that one can analyze Bayesian estimators from a classical
 (frequentist) perspective.
 It also turns out that Bayesian estimators may be easier to compute reliably
 than analogous classical estimators.
 These computational advantages, combined with the ability to use information
 from previous work in an intelligent way, make the study of Bayesian methods
 attractive for frequentists.
 If a Bayesian takes the view that there is a fixed data generating process,
 and Bayesian learning leads in the limit to the same fixed true value that
 frequentists posit, then the study of frequentist theory will be useful
 to a Bayesian practitioner.
 For example, the GMM estimator is closely related to some versions of 
\begin_inset CommandInset href
LatexCommand href
name "Approximate Bayesian Computing"
target "http://en.wikipedia.org/wiki/Approximate_Bayesian_computation"
literal "false"

\end_inset

 (ABC).
 Thus, knowledge of theory and practical experience with GMM can be a useful
 guide to implementing ABC estimators.
\end_layout

\begin_layout Itemize
For the rest of this, I will adopt the classical, frequentist perspective,
 and study the behavior of Bayesian estimators in this context.
\end_layout

\begin_layout Itemize
One should note that the traditional Bayesian approach requires the likelihood
 function, just as is the case with ML.
 Thus, it uses 
\emph on
strong assumptions
\emph default
, for a given model.
 
\end_layout

\begin_layout Itemize
There are Bayesian methods for choosing between models
\end_layout

\begin_layout Itemize
There are also recent Bayesian-inspired methods that attempt to work without
 knowledge of the likelihood function.
 For instance, 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHong2003"
literal "true"

\end_inset

 use Bayesian methods to compute a GMM estimator.
 Some such methods, e.g.
 
\begin_inset CommandInset href
LatexCommand href
name "Approximate Bayesian Computing"
target "http://en.wikipedia.org/wiki/Approximate_Bayesian_computation"
literal "false"

\end_inset

 require the model to be simulable, in which case, essentially the same
 strong assumptions as underlie ML are being used.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example
\end_layout

\begin_layout Standard
Suppose data is generated by i.i.d.
 sampling from an exponential distribution with mean 
\begin_inset Formula $\theta$
\end_inset

.
 An exponential random variable takes values on the positive real numbers.
 Waiting times are often modeled using the exponential distribution.
\end_layout

\begin_layout Itemize
The density of a typical sample element is 
\begin_inset Formula $f(y|\theta)=\frac{1}{\theta}e^{-y/\theta}$
\end_inset

.
 The likelihood is simply the product of the sample contributions.
\end_layout

\begin_layout Itemize
Suppose the prior for 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula $\theta\sim$
\end_inset

lognormal(1,1).
 This means that the logarithm of 
\begin_inset Formula $\theta$
\end_inset

 is standard normal.
 We use a lognormal prior because it enforces the requirement that the parameter
 of the exponential density be positive.
\end_layout

\begin_layout Itemize
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{BayesExample1.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/Bayesian/BayesExample1.jl} 
\end_layout

\end_inset

 implements Bayesian estimation for this setup.
\end_layout

\begin_layout Standard
With a sample of 10 observations, we obtain the results in panel (a) of
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Bayesian-estimation,-exponential"

\end_inset

, while with a sample of size 50 we obtain the results in panel (b).
 Note how the posterior is more concentrated around the true parameter value
 in panel (b).
 Also note how the posterior mean is closer to the prior mean when the sample
 is small.
 When the sample is small, the likelihood function has less weight, and
 more of the information comes from the prior.
 When the sample is larger, the likelihood function will have more weight,
 and its effect will dominate the prior's.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Bayesian-estimation,-exponential"

\end_inset

Bayesian estimation, exponential likelihood, lognormal prior
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
N=10
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/BayesExampleN10.svg
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
N=50
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/BayesExampleN50.svg
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Standard
Chernozhukov and Hong (2003) 
\begin_inset Quotes sld
\end_inset

An MCMC Approach to Classical Estimation
\begin_inset Quotes srd
\end_inset

 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.sciencedirect.com/science/article/pii/S0304407603001003
\end_layout

\end_inset

 is a very interesting article that shows how Bayesian methods may be used
 with criterion functions that are associated with classical estimation
 techniques.
 For example, it is possible to compute a posterior mean version of a GMM
 estimator.
 Chernozhukov and Hong provide their Theorem 2, which proves consistency
 and asymptotic normality for a general class of such estimators.
 When the criterion function 
\begin_inset Formula $L_{n}(\theta)$
\end_inset

 in their paper is set to the log-likelihood function, the pseudo-prior
 
\begin_inset Formula $\pi(\theta)$
\end_inset

 is a real Bayesian prior, and the penalty function 
\begin_inset Formula $\rho_{n}$
\end_inset

 is the squared loss function (see the paper), then the class of estimators
 discussed by CH reduces to the ordinary Bayesian posterior mean.
 As such, their Theorem 2, in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Chernozhukov-and-Hong,"

\end_inset

 tells us that this estimator is consistent and asymptotically normally
 distributed.
 In particular, the Bayesian posterior mean has the same asymptotic distribution
 as does the ordinary maximum likelihood estimator.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Chernozhukov-and-Hong,"

\end_inset

Chernozhukov and Hong, Theorem 2
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/Theorem2.png
	lyxscale 50
	width 20cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
the intuition is clear: as the amount of information coming from the sample
 increases, the likelihood function brings an increasing amount of information,
 relative to the prior.
 Eventually, the prior is no longer important for determining the shape
 of the posterior.
\end_layout

\begin_layout Itemize
when the sample is large, the shape of the posterior depends on the likelihood
 function.
 The likelihood function collapses around 
\begin_inset Formula $\theta_{0}$
\end_inset

 when the sample is generated at 
\begin_inset Formula $\theta_{0}.$
\end_inset

 The same is true of the posterior, it narrows around 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 This causes the posterior mean to converge to the true parameter value.
 In fact, all quantiles of the posterior converge to 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Chernozhukov and Hong discuss estimators defined using quantiles.
\end_layout

\begin_layout Itemize
For an econometrician coming from the frequentist perspective, this is attractiv
e.
 The Bayesian estimator has the same asymptotic behavior as the MLE.
 There may be computational advantages to using the Bayesian approach, because
 there is no need for optimization.
 If the objective function that defines the classical estimator is irregular
 (multiple local optima, nondifferentiabilities, noncontinuities...), then
 optimization may be very difficult.
 However, Bayesian methods that use integration may be more tractable.
 This is the main motivation of CH's paper.
 Additional advantages include the benefits if an informative prior is available.
 When this is the case, the Bayesian estimator can have better small sample
 performance than the maximum likelihood estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Computational methods
\end_layout

\begin_layout Itemize
To compute the posterior mean, we need to evaluate
\begin_inset Formula 
\begin{align*}
E(\theta|y) & =\int_{\Theta}\theta f(\theta|y)d\theta\\
= & \frac{\int_{\Theta}\theta f(y|\theta)\pi(\theta)d\theta}{\int_{\Theta}f(y,\theta)d\theta}.
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Note that both of the integrals are multiple integrals, with the dimension
 given by that of the parameter, 
\begin_inset Formula $\theta.$
\end_inset


\end_layout

\begin_layout Itemize
Under some special circumstances, the integrals may have analytic solutions:
 e.g., Gaussian likelihood with a Gaussian prior leads to a Gaussian posterior.
 
\end_layout

\begin_layout Itemize
When the dimension of the parameter is low, quadrature methods may be used.
 What was done in 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{BayesExample1.m}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/Bayesian/BayesExample1.m} 
\end_layout

\end_inset

 is an unsophisticated example of this.
 More sophisticated methods use an intelligently chosen grid to reduce the
 number of function evaluations.
 Still, these methods only work for dimensions up to 3 or so.
\end_layout

\begin_layout Itemize
Otherwise, some form of simulation-based 
\begin_inset Quotes sld
\end_inset

Monte Carlo
\begin_inset Quotes srd
\end_inset

 integration must be used.
 The basic idea is that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $E(\theta|y)$
\end_inset

 can be approximated by 
\begin_inset Formula $(1/S)\sum_{s=1}^{S}\theta^{s}$
\end_inset

, where 
\begin_inset Formula $\theta^{s}$
\end_inset

 is a random draw from the posterior distribution 
\begin_inset Formula $f(\theta|y)$
\end_inset

.
 The 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
trick is
\emph on
 how to make draws from the posterior
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 when in general we can't compute the posterior.
\family default
\series default
\shape default
\size default
\bar default
\color inherit

\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
the law of large numbers tells us that this average will converge to the
 desired expectation as 
\begin_inset Formula $S$
\end_inset

 gets large
\end_layout

\begin_layout Itemize
convergence will be more rapid if the random draws are independent of one
 another, but insisting on independence may have computational drawbacks.
\end_layout

\end_deeper
\begin_layout Standard
Monte Carlo methods include importance sampling, Markov chain Monte Carlo
 (MCMC) and sequential Monte Carlo (SMC, also known as particle filtering).
 The great expansion of these methods over the years has caused Bayesian
 econometrics to become much more widely used than it was in the not so
 distant (for some of us) past.
 There is much literature - here we will only look at a basic example that
 captures the main ideas.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
MCMC
\end_layout

\begin_layout Standard
Variants of Markov chain Monte Carlo have become a very widely used means
 of computing Bayesian estimates.
 See Tierney (1994) 
\begin_inset Quotes sld
\end_inset

Markov Chains for Exploring Posterior Distributions
\begin_inset Quotes srd
\end_inset

 
\emph on
Annals of Statistics
\emph default
 and Chib and Greenberg (1995) 
\begin_inset Quotes sld
\end_inset

Understanding the Metropolis-Hastings algorithm
\begin_inset Quotes srd
\end_inset

 
\emph on
The American Statistician.
\end_layout

\begin_layout Standard
Let's consider the basic Metropolis-Hastings MCMC algorithm.
 We will generate a long realization of a Markov chain process for 
\begin_inset Formula $\theta$
\end_inset

, as follows:
\end_layout

\begin_layout Itemize
The prior density is 
\begin_inset Formula $\pi(\theta)$
\end_inset

, as above.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $g(\theta^{*};\theta^{s})$
\end_inset

 be a proposal density, which describes the density of a trial value 
\begin_inset Formula $\theta^{*}$
\end_inset

 conditional on starting at 
\begin_inset Formula $\theta^{s}$
\end_inset

.
 It must be possible to sample from the proposal.
 This gives a new trial parameter value 
\begin_inset Formula $\theta^{*}$
\end_inset

, given the most recently accepted parameter value 
\begin_inset Formula $\theta^{s}$
\end_inset

.
 A proposal will be accepted if
\begin_inset Formula 
\[
\frac{f(\theta^{*}|y)}{f(\theta^{s}|y)}\frac{g(\theta^{s};\theta^{*})}{g(\theta^{*};\theta^{s})}>\alpha
\]

\end_inset

where 
\begin_inset Formula $\alpha$
\end_inset

 is a 
\begin_inset Formula $U(0,1)$
\end_inset

 random variate.
 
\end_layout

\begin_layout Standard
There are two parts to the numerator and denominator: the posterior, and
 the proposal density.
 
\end_layout

\begin_layout Itemize
Focusing on the numerator, when the trial value of the proposal has a higher
 posterior, acceptance is favored.
 
\end_layout

\begin_layout Itemize
The other factor is the density associated with returning to 
\begin_inset Formula $\theta^{s}$
\end_inset

 when starting at 
\begin_inset Formula $\theta^{*}$
\end_inset

, which has to do with the reversibility of the Markov chain.
 If this is too low, acceptance is not favored.
 We don't want to jump to a new region if we will never get back, as we
 need to sample from the entire support of the posterior.
 
\end_layout

\begin_layout Itemize
The two together mean that we will jump to a new area only if we are able
 to eventually jump back with a reasonably high probability.
 The probability of jumping is higher when the new area has a higher posterior
 density, but lower if it's hard to get back.
 
\end_layout

\begin_layout Itemize
The idea is to sample from all regions of the posterior, those with high
 and low density, sampling more heavily from regions of high density.
 We want to go occasionally to regions of low density, but it is important
 not to get stuck there.
\end_layout

\begin_layout Itemize
Consider a bimodal density: we want to explore the area around both modes.
 To be able to do that, it is important that the proposal density allows
 us to be able to jump between modes.
 
\end_layout

\begin_layout Itemize
Understanding in detail why this makes sense is the tricky and elegant part
 of the theory, see the references for more information.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Note that the ratio of posteriors is equal to the ratio of likelihoods times
 the ratio of priors:
\begin_inset Formula 
\[
\frac{f(\theta^{*}|y)}{f(\theta^{s}|y)}=\frac{f(y|\theta^{*})}{f(y|\theta^{s})}\frac{\pi(\theta^{*})}{\pi(\theta^{s})}
\]

\end_inset

because the marginal likelihood 
\begin_inset Formula $f(y)$
\end_inset

 is the same in both cases.
 We don't need to compute that integral! We don't need to know the posterior,
 either.
 The acceptance criterion can be written as: accept if
\begin_inset Formula 
\[
\frac{f(y|\theta^{*})}{f(y|\theta^{s})}\frac{\pi(\theta^{*})}{\pi(\theta^{s})}\frac{g(\theta^{s};\theta^{*})}{g(\theta^{*};\theta^{s})}>\alpha
\]

\end_inset

otherwise, reject
\end_layout

\begin_layout Itemize
From this, we see that the information needed to determine if a proposal
 is accepted or rejected is the prior, the proposal density, and the likelihood
 function 
\begin_inset Formula $f(y|\theta)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
in principle, the prior is non-negotiable.
 In practice, people often chose priors with convenience in mind
\end_layout

\begin_layout Itemize
the likelihood function is what it is
\end_layout

\begin_layout Itemize
the place where artistry comes to bear is the choice of the proposal density
\end_layout

\end_deeper
\begin_layout Itemize
when the proposal density is 
\emph on
symmetric, 
\emph default
so that 
\begin_inset Formula $g(\theta^{s};\theta^{*})=g(\theta^{*};\theta^{s})$
\end_inset

, the acceptance criterion simplifies to 
\begin_inset Formula 
\[
\frac{f(y|\theta^{*})}{f(y|\theta^{s})}\frac{\pi(\theta^{*})}{\pi(\theta^{s})}>\alpha
\]

\end_inset

A random walk proposal, where the trial value is the current value plus
 a shock that doesn't depend on the current value, satisfies symmetry.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
the steps are:
\end_layout

\begin_layout Enumerate
the algorithm is initialized at some 
\begin_inset Formula $\theta^{1}$
\end_inset


\end_layout

\begin_layout Enumerate
for 
\begin_inset Formula $s=2,...,S,$
\end_inset

 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
draw 
\begin_inset Formula $\theta^{*}$
\end_inset

 from 
\begin_inset Formula $g(\theta^{*};\theta^{s})$
\end_inset


\end_layout

\begin_layout Enumerate
according to the acceptance/rejection criterion, if the result is acceptance,
 set 
\begin_inset Formula $\theta^{s+1}=\theta^{*}$
\end_inset

, otherwise set 
\begin_inset Formula $\theta^{s+1}=\theta^{s}$
\end_inset


\end_layout

\begin_layout Enumerate
iterate
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Once the chain is considered to have stabilized, say at iteration 
\begin_inset Formula $r$
\end_inset

, the values of 
\begin_inset Formula $\theta^{s}$
\end_inset

 for 
\begin_inset Formula $s>r$
\end_inset

 are taken to be draws from the posterior.
 The posterior mean is computed as the simple average of the value.
 Quantiles, etc., can be computed in the appropriate fashion.
\end_layout

\begin_layout Itemize
the art of applying these methods consists of providing a good proposal
 density so that the acceptance rate is reasonably high, but not too high.
 There is a vast literature on this, and the vastness of the literature
 should serve as a warning that getting this to work in practice is not
 necessarily a simple matter.
 If it were, there would be fewer papers on the topic.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
too high acceptance rate: this is usually due to a proposal density that
 gives proposals very close to the current value, e.g, a random walk with
 very low variance.
 This means that the posterior is being explored inefficiently, we travel
 around through the support at a very low rate, which means the chain will
 have to run for a (very, very...) long time to do a thorough exploration.
\end_layout

\begin_layout Itemize
too low acceptance rate: this means that the steps are too large, and we
 attempt to move to low posterior density regions too frequently.
 The chain will become highly autocorrelated, as it stays in the same place
 due to rejections, so long periods convey little additional information
 relative to a subset of the values in the interval 
\end_layout

\end_deeper
\begin_layout Itemize
look at 
\begin_inset CommandInset href
LatexCommand href
name "Geoff Gordon's mh.h"
target "http://www.cs.cmu.edu/~ggordon/MCMC/"
literal "false"

\end_inset

 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
MCMC for the simple example
\end_layout

\begin_layout Standard
The simple exponential example with log-normal prior can be implemented
 using MH MCMC, and this is done in the Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{BayesExample2.jl}{https://github.com/mcreel/Econometrics/blob/ma
ster/Examples/Bayesian/BayesExample2.jl} 
\end_layout

\end_inset

.
 Play around with the sample size and the tuning parameter, and note the
 effects on the computed posterior mean and on the acceptance rate.
 An example of output is given in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Metropolis-Hastings-MCMC,-expone"

\end_inset

, which shows the final draws of the chain, and the posterior density (computed
 using non-parametric density estimation, more on that later).
 In that Figure, the chain is probably too spiky: too many draws are being
 accepted (it's around 0.6, which you'll see if you run the code), meaning
 that the tuning parameter needs to be increased, to lower the acceptance
 rate.
 If you increase the sample size, you'll see how the posterior concentrates
 around the true value, 3.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Metropolis-Hastings-MCMC,-expone"

\end_inset

Metropolis-Hastings MCMC, exponential likelihood, lognormal prior
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
chain, last 1000 draws
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/chain.svg
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
posterior
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Bayesian/posterior.svg
	width 10cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Bayesian estimation of DSGE model
\end_layout

\begin_layout Standard
In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:DSGE-ML"

\end_inset

, a simple DSGE model was estimated by ML.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimateCGHK
\backslash
_Bayes.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/DSGE/Bayesia
n/EstimateCGHK
\backslash
_Bayes.m} 
\end_layout

\end_inset

 is a script which estimates the same model, using Bayesian methods, with
 MCMC or particle filtering.
 Adjust the .mod file mentioned in the script to change options.
\end_layout

\begin_layout Standard
Some results one can obtain by playing with this:
\end_layout

\begin_layout Itemize
MCMC is a bit time consuming...
\end_layout

\begin_layout Itemize
Estimation of the parameters is quite good, when order=1 (linearized, which
 allows for Kalman filtering), giving the results in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MCMC-results-for"

\end_inset

.
 Most of the true parameter values are inside the 90% HPD intervals, though
 this is not the case for 
\begin_inset space ~
\end_inset


\begin_inset Formula $\beta,$
\end_inset


\begin_inset Formula $\rho_{2},$
\end_inset

and 
\begin_inset Formula $\sigma_{2}$
\end_inset

.
 Perhaps this is in part due to the linearized model being used to specify
 the 
\begin_inset Quotes sld
\end_inset

likelihood
\begin_inset Quotes srd
\end_inset

.
\end_layout

\begin_layout Itemize
Estimation by particle filtering (set order=2) will take a looong time...
\end_layout

\begin_layout Itemize
It seems that the Bayesian estimation has given us better results than ML
 and GMM However, one still has the stochastic singularity problem, and
 the results that are obtained will depend on which variables are selected
 for estimation.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:MCMC-results-for"

\end_inset

MCMC results for simple DSGE example model (two different runs)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/Bayesian/MCMC.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/Bayesian/MCMC2.png
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:CGHK-model,-posteriors"

\end_inset

 plots the priors and posteriors.
 Note that the posterior is substantially different than the prior: we learn
 a lot from the sample
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:CGHK-model,-posteriors"

\end_inset

CGHK model, posteriors
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/DSGE/Bayesian/posterior.png
	width 22cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
if the model is solved using a higher order solution, the Kalman filter
 cannot be used, and Dynare uses particle filtering instead.
 This is very time consuming, as you can check.
\end_layout

\begin_layout Itemize
Note that you can only use two observed variables for estimation, as there
 are only two shocks in the model.
 This is because of 
\begin_inset Quotes sld
\end_inset

stochastic singularity
\begin_inset Quotes srd
\end_inset

 of the linearized model.
 When the solution order is higher than 1, there is no reason to limit the
 number of observed variables to be the same as the number of shocks, but
 Dynare still imposes the limit (as of early 2018).
 Some researchers artificially introduce more shocks to a model by assuming
 measurement error.
\end_layout

\begin_layout Standard
For tips on using Dynare for MCMC estimation, see 
\begin_inset CommandInset href
LatexCommand href
name "these notes by Wouter den Haan."
target "http://www.wouterdenhaan.com/numerical/slidesdynareestimation.pdf"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Bayesian GMM for the DSGE model
\end_layout

\begin_layout Standard
A script which show how to do Bayesian GMM as proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHong2003"
literal "false"

\end_inset

 is 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{DoMCMC.jl}{https://github.com/mcreel/Econometrics/blob/master/Ex
amples/DSGE/MCMC/DoMCMC.jl} 
\end_layout

\end_inset

.
 This can be used to compute posterior densities for the parameters, for
 example, the estimated posterior for 
\begin_inset Formula $\gamma$
\end_inset

 follows.
 Recall that the true value that generated the sample is 
\begin_inset Formula $\gamma=2$
\end_inset

, so, for this sample, the method worked fairly well.
 However, the true values of 
\begin_inset Formula $\gamma$
\end_inset

 and two of the other parameters are still outside of the 90% HPD intervals.
 The point estimation is working pretty well, but inference still has some
 weaknesses.
 This is only for a single sample, though, it may be that things work better
 on average! It wouldn't be too hard to find out.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/MCMC/gamma.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The results for all parameters are
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Examples/DSGE/MCMC/results.png
	lyxscale 75
	width 20cm

\end_inset


\end_layout

\begin_layout Standard
The final line are statistics for the acceptance rate.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
Experiment with the examples to learn about tuning, etc.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Introduction-to-panel"

\end_inset

Introduction to panel data
\end_layout

\begin_layout Standard
Reference: 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Part V, Chapters 21 and 22 (plus 23 if you have special interest in the
 topic).
 The GRETL manual also has two chapters, which are a nice reference.
\end_layout

\begin_layout Standard
In this chapter we'll look at panel data.
 Panel data is an important area in applied econometrics, simply because
 much of the available data has this structure.
 Also, it provides an example where things we've already studied (GLS, endogenei
ty, GMM, Hausman test) come into play.
 There has been much work in this area, and the intention is not to give
 a complete overview, but rather to highlight the issues and see how the
 tools we have studied can be applied.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Generalities
\end_layout

\begin_layout Standard
Panel data combines cross sectional and time series data: we have a time
 series for each of the agents observed in a cross section.
\end_layout

\begin_layout Itemize
The addition of temporal information to a cross sectional model can in principle
 allow us to investigate issues such as persistence, habit formation, and
 dynamics.
\end_layout

\begin_layout Itemize
Starting from the perspective of a single time series, the addition of cross-sec
tional information allows investigation of heterogeneity.
\end_layout

\begin_layout Itemize
In both cases, if parameters are common across units or over time, the additiona
l data allows for more precise estimation.
 This is simply an example of estimation subject to restrictions, which
 improves efficiency 
\emph on
if the restrictions are correct
\end_layout

\begin_layout Standard
The basic idea is to allow variables to have two indices, 
\begin_inset Formula $i=1,2,...,n$
\end_inset

 and 
\begin_inset Formula $t=1,2,...,T$
\end_inset

.
 The simple linear model
\begin_inset Formula 
\[
y_{i}=\alpha+x_{i}\beta+\epsilon_{i}
\]

\end_inset

 becomes
\begin_inset Formula 
\[
y_{it}=\alpha+x_{it}\beta+\epsilon_{it}
\]

\end_inset

We could think of allowing the parameters to change over time and over cross
 sectional units.
 This would give
\begin_inset Formula 
\[
y_{it}=\alpha_{it}+x_{it}\beta_{it}+\epsilon_{it}
\]

\end_inset

The problem here is that there are more parameters than observations, so
 the model is not identified.
 We need some restraint! The proper restrictions to use of course depend
 on the problem at hand, and a single model is unlikely to be appropriate
 for all situations.
 For example, one could have time and cross-sectional dummies, and slopes
 that vary by time:
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+\gamma_{t}+x_{it}\beta_{t}+\epsilon_{it}
\]

\end_inset

There is a lot of room for playing around here.
 We also need to consider whether or not 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

 are fixed or growing.
 We'll need at least one of them to be growing in order to do asymptotics.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To provide some focus, we'll consider common slope parameters, but agent-specifi
c intercepts, which:
\begin_inset Formula 
\begin{equation}
y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}\label{eq:simple linear panel model}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
I will refer to this as the 
\begin_inset Quotes sld
\end_inset

simple linear panel model
\begin_inset Quotes srd
\end_inset

.
\end_layout

\begin_layout Itemize
This is the model most often encountered in the applied literature.
 It is like the original cross-sectional model, in that the 
\begin_inset Formula $\beta's$
\end_inset

 are constant over time for all 
\begin_inset Formula $i.$
\end_inset

 However we're now allowing for the constant to vary across 
\begin_inset Formula $i$
\end_inset

 (some individual heterogeneity).
 The 
\begin_inset Formula $\beta's$
\end_inset

 are fixed over time, which is a testable restriction, of course.
\end_layout

\begin_layout Itemize
We can consider what happens as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 but 
\begin_inset Formula $T$
\end_inset

 is fixed.
 This would be relevant for microeconometric panels, (e.g., the PSID data)
 where a survey of a large number of individuals may be done for a limited
 number of time periods.
 
\end_layout

\begin_layout Itemize
Macroeconometric applications might look at longer time series for a small
 number of cross-sectional units (e.g., 40 years of quarterly data for 15
 European countries).
 For that case, we could keep 
\begin_inset Formula $n$
\end_inset

 fixed (seems appropriate when dealing with the EU countries), and do asymptotic
s as 
\begin_inset Formula $T$
\end_inset

 increases, as is normal for time series.
\end_layout

\begin_layout Itemize
The asymptotic results depend on how we do this, of course.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
Why bother using panel data, what are the benefits?
\series default
 The model 
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

 is a restricted version of
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+x_{it}\beta_{i}+\epsilon_{it}
\]

\end_inset

which could be estimated for each 
\begin_inset Formula $i$
\end_inset

 in turn.
 Why use the panel approach?
\end_layout

\begin_layout Itemize
Because the restrictions that 
\begin_inset Formula $\beta_{i}=\beta_{j}=...=\beta,$
\end_inset

 if true, lead to more efficient estimation.
 Estimation for each 
\begin_inset Formula $i$
\end_inset

 in turn will be very uninformative if 
\begin_inset Formula $T$
\end_inset

 is small.
\end_layout

\begin_layout Itemize
Another reason is that panel data allows us to estimate parameters that
 are not identified by cross sectional (time series) data.
 For example, if the model is 
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+\gamma_{t}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

and we have only cross sectional data, we cannot estimate the 
\begin_inset Formula $\alpha_{i}$
\end_inset

.
 If we have only time series data on a single cross sectional unit 
\begin_inset Formula $i=1$
\end_inset

, we cannot estimate the 
\begin_inset Formula $\gamma_{t}$
\end_inset

.
 Cross-sectional variation allows us to estimate parameters indexed by time,
 and time series variation allows us to estimate parameters indexed by cross-sec
tional unit.
 Parameters indexed by both 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 will require other forms of restrictions in order to be estimable.
\end_layout

\begin_layout Itemize
Another important reason is that 
\begin_inset Formula $\alpha_{i}$
\end_inset

 can absorb any missing variables in the regression that don't change over
 time, and 
\begin_inset Formula $\gamma_{t}$
\end_inset

 can absorb missing variables that don't change across 
\begin_inset Formula $i$
\end_inset

.
 This may help to obtain consistent estimates of 
\begin_inset Formula $\beta,$
\end_inset

 if any missing variables are correlated with 
\begin_inset Formula $x_{it}.$
\end_inset


\end_layout

\begin_layout Itemize
Exercise: write the form of the regressor matrix for the above model, under
 the assumption of only time series or only cross sectional data.
 Show that there is perfect collinearity in each case.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\series bold
The main issues are:
\end_layout

\begin_layout Itemize
can 
\begin_inset Formula $\beta$
\end_inset

 be estimated consistently? This is almost always a goal.
\end_layout

\begin_layout Itemize
can the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 be estimated consistently? This is often of secondary interest.
\end_layout

\begin_layout Itemize
sometimes, we're interested in estimating the distribution of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 across 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
are the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 correlated with 
\begin_inset Formula $x_{it}$
\end_inset

?
\end_layout

\begin_layout Itemize
does the presence of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 complicate estimation of 
\begin_inset Formula $\beta$
\end_inset

?
\end_layout

\begin_layout Standard
what about the covariance structure? We're likely to have HET and AUT, so
 GLS issues will probably be relevant.
 Potential for efficiency gains.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Static models and correlations between variables
\end_layout

\begin_layout Standard
To begin with, assume that:
\end_layout

\begin_layout Itemize
the 
\begin_inset Formula $x_{it}$
\end_inset

 are weakly exogenous variables (uncorrelated with 
\begin_inset Formula $\epsilon_{it})$
\end_inset


\end_layout

\begin_layout Itemize
the model is static: 
\begin_inset Formula $x_{it}$
\end_inset

 does not contain lags of 
\begin_inset Formula $y_{it}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
then the basic problem we have in the panel data model 
\begin_inset Formula $y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}$
\end_inset

 is the presence of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

.
 These are individual-specific parameters.
 Or, possibly more accurately, they can be thought of as individual-specific
 variables that are not observed (latent variables).
 The reason for thinking of them as variables is because the agent may choose
 their values following some process, or may choose other variable taking
 these ones as given.
\end_layout

\begin_layout Standard
Define 
\begin_inset Formula $\alpha=E(\alpha_{i})$
\end_inset

, so 
\begin_inset Formula $E(\alpha_{i}-\alpha)=0,$
\end_inset

 where the expectation is with respect to the density that describes the
 distribution of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 in the population.
 Our model 
\begin_inset Formula $y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}$
\end_inset

 may be written 
\begin_inset Formula 
\begin{align*}
y_{it} & =\alpha_{i}+x_{it}\beta+\epsilon_{it}\\
 & =\alpha+x_{it}\beta+(\alpha_{i}-\alpha+\epsilon_{it})\\
 & =\alpha+x_{it}\beta+\eta_{it}
\end{align*}

\end_inset

Note that 
\begin_inset Formula $E(\eta_{it})=0.$
\end_inset

 A way of thinking about the data generating process is this:
\end_layout

\begin_layout Itemize
First, 
\begin_inset Formula $\alpha_{i}$
\end_inset

 is drawn, from the population density
\end_layout

\begin_layout Itemize
then 
\begin_inset Formula $T$
\end_inset

 values of 
\begin_inset Formula $x_{it}$
\end_inset

 are drawn from 
\begin_inset Formula $f_{X}(z|\alpha_{i}).$
\end_inset

 
\end_layout

\begin_layout Itemize
the important point is that the distribution of 
\begin_inset Formula $x$
\end_inset

 
\emph on
may vary depending on the realization of 
\begin_inset Formula $\alpha_{i}$
\end_inset

.

\emph default
 
\end_layout

\begin_layout Itemize
For example, if 
\begin_inset Formula $y$
\end_inset

 is the quantity demanded of a luxury good, then a high value of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 means that agent 
\begin_inset Formula $i$
\end_inset

 will buy a large quantity, on average.
 This may be possible only when the agent's income is also high.
 Thus, it may be possible to draw high values of 
\begin_inset Formula $\alpha_{i}$
\end_inset

 only when income is also high, otherwise, the budget constraint would be
 violated.
 If income is one of the variables in 
\begin_inset Formula $x_{it},$
\end_inset

 then 
\begin_inset Formula $\alpha_{i}$
\end_inset

 and 
\begin_inset Formula $x_{it}$
\end_inset

 are not independent.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Itemize
Another example: consider returns to education, modeling wage as a function
 of education.
 
\begin_inset Formula $\alpha_{i}$
\end_inset

 could be an individual specific measure of ability.
 Ability could affect wages, but it could also affect the number of years
 of education.
 When education is a regressor and ability is a component of the error,
 we may expect an endogeneity problem.
\end_layout

\begin_layout Itemize
Thus, there may be correlation between 
\begin_inset Formula $\alpha_{i}$
\end_inset

 and 
\begin_inset Formula $x_{it}$
\end_inset

, in which case 
\begin_inset Formula $E(x_{it}\eta_{it})\ne$
\end_inset

0 in the above equation.
 
\end_layout

\begin_deeper
\begin_layout Itemize
This means that OLS estimation of the model would lead to biased and inconsisten
t estimates.
 
\end_layout

\begin_layout Itemize
However, it is possible (but unlikely for economic data) that 
\begin_inset Formula $x_{it}$
\end_inset

 and 
\begin_inset Formula $\eta_{it}$
\end_inset

 are independent or at least uncorrelated, if the distribution of 
\begin_inset Formula $x_{it}$
\end_inset

 is constant with respect to the realization of 
\begin_inset Formula $\alpha_{i}$
\end_inset

.
 In this case OLS estimation would be consistent.
\end_layout

\end_deeper
\begin_layout Standard

\series bold
\begin_inset Newpage newpage
\end_inset

Fixed effects
\series default
: when 
\begin_inset Formula $E(x_{it}$
\end_inset


\begin_inset Formula $\eta_{it})\ne$
\end_inset

0, the model is called the 
\begin_inset Quotes sld
\end_inset

fixed effects model
\begin_inset Quotes srd
\end_inset


\end_layout

\begin_layout Standard

\series bold
Random effects
\series default
: when 
\begin_inset Formula $E(x_{it}\eta_{it})=0,$
\end_inset

 the model is called the 
\begin_inset Quotes sld
\end_inset

random effects model
\begin_inset Quotes srd
\end_inset

.
\end_layout

\begin_layout Standard
I find this to be pretty poor nomenclature, because the issue is not whether
 
\begin_inset Quotes sld
\end_inset

effects
\begin_inset Quotes srd
\end_inset

 are fixed or random (they are always random, unconditional on 
\begin_inset Formula $i$
\end_inset

).
 The issue is whether or not the 
\begin_inset Quotes sld
\end_inset

effects
\begin_inset Quotes srd
\end_inset

 are correlated with the other regressors.
 In economics, it seems likely that the unobserved variable 
\begin_inset Formula $\alpha$
\end_inset

 is probably correlated with the observed regressors, 
\begin_inset Formula $x$
\end_inset

 (this is simply the presence of collinearity between observed and unobserved
 variables, and collinearity is usually the rule rather than the exception).
 So, we expect that the 
\begin_inset Quotes sld
\end_inset

fixed effects
\begin_inset Quotes srd
\end_inset

 model is probably the relevant one unless special circumstances imply that
 the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are uncorrelated with the 
\begin_inset Formula $x_{it}$
\end_inset

.
\series bold

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Estimation of the simple linear panel model
\end_layout

\begin_layout Subsection
\begin_inset Quotes sld
\end_inset

Fixed effects
\begin_inset Quotes srd
\end_inset

: The 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator
\end_layout

\begin_layout Standard
How can we estimate the parameters of the simple linear panel model (equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:simple linear panel model"

\end_inset

) and what properties do the estimators have? First, we assume that the
 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are correlated with the 
\begin_inset Formula $x_{it}$
\end_inset

 (
\begin_inset Quotes sld
\end_inset

fixed effects
\begin_inset Quotes srd
\end_inset

 model ).
 The model can be written as 
\begin_inset Formula $y_{it}=\alpha+x_{it}\beta+\eta_{it}$
\end_inset

, and we have that 
\begin_inset Formula $E(x_{it}\eta_{it})\ne$
\end_inset

0.
 As such, OLS estimation of this model will give biased an inconsistent
 estimated of the parameters 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
 The 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator is a solution - this involves subtracting the time series average
 from each cross sectional unit.
\begin_inset Formula 
\begin{align}
\overline{x}_{i} & =\frac{1}{T}\sum_{t=1}^{T}x_{it}\nonumber \\
\overline{\epsilon_{i}} & =\frac{1}{T}\sum_{t=1}^{T}\epsilon_{it}\nonumber \\
\overline{y}_{i} & =\frac{1}{T}\sum_{t=1}^{T}y_{it}=\alpha_{i}+\frac{1}{T}\sum_{t=1}^{T}x_{it}\beta+\frac{1}{T}\sum_{t=1}^{T}\epsilon_{it}\nonumber \\
\overline{y}_{i} & =\alpha_{i}+\overline{x}_{i}\beta+\overline{\epsilon_{i}}\label{eq:time averages}
\end{align}

\end_inset

The transformed model is
\begin_inset Formula 
\begin{align}
y_{it}-\overline{y}_{i} & =\alpha_{i}+x_{it}\beta+\epsilon_{it}-\alpha_{i}-\overline{x}_{i}\beta-\overline{\epsilon_{i}}\label{eq:within estimator}\\
y_{it}^{*} & =x_{it}^{*}\beta+\epsilon_{it}^{*}\nonumber 
\end{align}

\end_inset

where 
\begin_inset Formula $x_{it}^{*}=x_{it}-\overline{x}_{i}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{it}^{*}=\epsilon_{it}-\overline{\epsilon}_{i}$
\end_inset

.
 In this model, it is clear that 
\begin_inset Formula $x_{it}^{*}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{it}^{*}$
\end_inset

 are uncorrelated, as long as the original regressors 
\begin_inset Formula $x_{it}$
\end_inset

 are 
\emph on
strongly
\emph default
 exogenous with respect to the original error 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 (
\begin_inset Formula $E(x_{it}\epsilon_{is})=0,\,\forall t,s$
\end_inset

).
 In this case, OLS will give consistent estimates of the parameters of this
 model, 
\begin_inset Formula $\beta.$
\end_inset


\end_layout

\begin_layout Exercise
Explain why we need strong exogeneity of the 
\begin_inset Formula $x_{it}$
\end_inset

 with respect to 
\begin_inset Formula $\epsilon_{it}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
What about the 
\begin_inset Formula $\alpha_{i}?$
\end_inset

 Can they be consistently estimated? An estimator is
\begin_inset Formula 
\[
\hat{\alpha}_{i}=\frac{1}{T}\sum_{t=1}^{T}\left(y_{it}-x_{it}\hat{\beta}\right)
\]

\end_inset

It's fairly obvious that this is a consistent estimator 
\emph on
if 
\emph default

\begin_inset Formula $T\rightarrow\infty$
\end_inset

.
 For a short panel with fixed 
\begin_inset Formula $T,$
\end_inset

 this estimator is not consistent.
 Nevertheless, the variation in the 
\begin_inset Formula $\hat{\alpha}_{i}$
\end_inset

 can be fairly informative about the heterogeneity.
 A couple of notes:
\end_layout

\begin_layout Itemize
an equivalent approach is to estimate the model
\begin_inset Formula 
\[
y_{it}=\sum_{j=1}^{n}d_{j,it}\alpha_{i}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

 by OLS.
 The 
\begin_inset Formula $d_{j}$
\end_inset

, 
\begin_inset Formula $j=1,2,...,n$
\end_inset

 are 
\begin_inset Formula $n$
\end_inset

 dummy variables that take on the value 
\begin_inset Formula $1$
\end_inset

 if 
\begin_inset Formula $j=i,$
\end_inset

 zero otherwise.
 They are indicators of the cross sectional unit of the observation.
 (Write out form of regressor matrix on blackboard).
 Estimating this model by OLS gives numerically exactly the same results
 as the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator, and you get the 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\hat{\alpha}_{i}$
\end_inset

 automatically.
 See Cameron and Trivedi, section 21.6.4 for details.
 An interesting and important result known as the Frisch-Waugh-Lovell Theorem
 can be used to show that the two means of estimation give identical results.
\end_layout

\begin_layout Itemize
This last expression makes it clear why the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator cannot estimate slope coefficients corresponding to variables
 that have no time variation.
 Such variables are perfectly collinear with the cross sectional dummies
 
\begin_inset Formula $d_{j}$
\end_inset

.
 The corresponding coefficients are not identified.
\end_layout

\begin_layout Itemize
OLS estimation of the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 model is consistent, but probably not efficient, because it is highly probable
 that the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not iid.
 There is very likely heteroscedasticity across the 
\begin_inset Formula $i$
\end_inset

 and autocorrelation between the 
\begin_inset Formula $T$
\end_inset

 observations corresponding to a given 
\begin_inset Formula $i$
\end_inset

.
 
\emph on
One needs to estimate the covariance matrix of the parameter estimates taking
 this into account
\emph default
.
 It is possible to use GLS corrections if you make assumptions regarding
 the het.
 and autocor.
 Quasi-GLS, using a possibly misspecified model of the error covariance,
 can lead to more efficient estimates than simple OLS.
 One can then combine it with subsequent panel-robust covariance estimation
 to deal with the misspecification of the error covariance, which would
 invalidate inferences if ignored.
 The White heteroscedasticity consistent covariance estimator is easily
 extended to panel data with independence across 
\begin_inset Formula $i$
\end_inset

, but with heteroscedasticity and autocorrelation within 
\begin_inset Formula $i$
\end_inset

, and heteroscedasticity between 
\begin_inset Formula $i.$
\end_inset

 See Cameron and Trivedi, Section 21.2.3.
\series bold

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Estimation with random effects
\end_layout

\begin_layout Standard
The original model is
\begin_inset Formula 
\[
y_{it}=\alpha_{i}+x_{it}\beta+\epsilon_{it}
\]

\end_inset

This can be written as 
\begin_inset Formula 
\begin{eqnarray}
y_{it} & = & \alpha+x_{it}\beta+\left(\alpha_{i}-\alpha+\epsilon_{it}\right)\nonumber \\
y_{it} & = & \alpha+x_{it}\beta+\eta_{it}\label{eq:simple linear panel random effects}
\end{eqnarray}

\end_inset

Under random effects, the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are assumed not to be correlated with the 
\begin_inset Formula $x_{it},$
\end_inset

 so 
\begin_inset Formula $E(\eta_{it})=0,$
\end_inset

 and 
\begin_inset Formula $E(x_{it}\eta_{it})=0$
\end_inset

.
 As such, the OLS estimator of this model is consistent.
 We can recover estimates of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 as discussed above.
 It is to be noted that the error 
\begin_inset Formula $\eta_{it}$
\end_inset

 is almost certainly heteroscedastic and autocorrelated, so OLS will not
 be efficient, and inferences based on OLS need to be done taking this into
 account.
 One could attempt to use GLS, or panel-robust covariance matrix estimation,
 or both, as above.
\end_layout

\begin_layout Standard
There are other estimators when we have random effects, a well-known example
 being the 
\begin_inset Quotes sld
\end_inset

between
\begin_inset Quotes srd
\end_inset

 estimator, which operates on the time averages of the cross sectional units.
 There is no advantage to doing this, as the overall estimator is already
 consistent, and averaging looses information (efficiency loss).
 One would still need to deal with cross sectional heteroscedasticity when
 using the between estimator, so there is no gain in simplicity, either.
\end_layout

\begin_layout Standard
It is to be emphasized that 
\begin_inset Quotes sld
\end_inset

random effects
\begin_inset Quotes srd
\end_inset

 is not a plausible assumption with most economic data, so use of this estimator
 is discouraged, even if your statistical package offers it as an option.
 Think carefully about whether the assumption is warranted before trusting
 the results of this estimator.
\series bold

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Hausman test
\end_layout

\begin_layout Standard
Suppose you're doubting about whether fixed or random effects are present.
\end_layout

\begin_layout Itemize
If we have correlation between 
\begin_inset Formula $x_{it}$
\end_inset

 and 
\begin_inset Formula $\alpha_{i}$
\end_inset

 (fixed effects), then the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator will be consistent, but the random effects estimator of the previous
 section will not.
 
\end_layout

\begin_layout Itemize
Evidence that the two estimators are converging to different limits is evidence
 in favor of fixed effects, not random effects.
\end_layout

\begin_layout Itemize
A Hausman test statistic can be computed, using the difference between the
 two estimators.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The null hypothesis is that the effects are uncorrelated with the regressors
 in 
\begin_inset Formula $x_{it}$
\end_inset

 (
\begin_inset Quotes sld
\end_inset

random effects
\begin_inset Quotes srd
\end_inset

) so that both estimators are consistent under the null.
 
\end_layout

\begin_layout Itemize
When the test rejects, we conclude that fixed effects are present, so the
 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator should be used.
 
\end_layout

\begin_layout Itemize
Now, what happens if the test does not reject? One could optimistically
 turn to the random effects model, but it's probably more realistic to conclude
 that the test may have low power.
 Failure to reject does not mean that the null hypothesis is true.
 After all, estimation of the covariance matrices needed to compute the
 Hausman test is a non-trivial issue, and is a source of considerable noise
 in the test statistic (noise=low power).
\end_layout

\begin_layout Itemize
Finally, the simple version of the Hausman test requires that the estimator
 under the null be fully efficient.
 Achieving this goal is probably a utopian prospect.
 A conservative approach would acknowledge that neither estimator is likely
 to be efficient, and to operate accordingly.
 I have a little paper on this topic, Creel, 
\emph on
Applied Economics, 
\emph default
2004.
 See also Cameron and Trivedi, section 21.4.3.
\end_layout

\end_deeper
\begin_layout Standard

\series bold
In class, do the first part of the example at the end of the chapter at
 this time
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Dynamic panel data
\end_layout

\begin_layout Standard
When we have panel data, we have information on both 
\begin_inset Formula $y_{it}$
\end_inset

 as well as 
\begin_inset Formula $y_{i,t-1}$
\end_inset

.
 One may naturally think of including 
\begin_inset Formula $y_{i,t-1}$
\end_inset

 as a regressor, to capture dynamic effects that can't be analyed with only
 cross-sectional data.
 Excluding dynamic effects is often the reason for detection of spurious
 AUT of the errors.
 With dynamics, there is likely to be less of a problem of autocorrelation,
 but one should still be concerned that some might still be present.
 The model, using a single lag of the dependent variable, becomes
\begin_inset Formula 
\begin{eqnarray*}
y_{it} & = & \alpha_{i}+\gamma y_{i,t-1}+x_{it}\beta+\epsilon_{it}\\
y_{it} & = & \alpha+\gamma y_{i,t-1}+x_{it}\beta+\left(\alpha_{i}-\alpha+\epsilon_{it}\right)\\
y_{it} & = & \alpha+\gamma y_{i,t-1}+x_{it}\beta+\eta_{it}
\end{eqnarray*}

\end_inset

We assume that the 
\begin_inset Formula $x_{it}$
\end_inset

 are uncorrelated with 
\begin_inset Formula $\epsilon_{it}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\alpha_{i}$
\end_inset

 is a component that determines both 
\begin_inset Formula $y_{it}$
\end_inset

 and its lag, 
\begin_inset Formula $y_{i,t-1}$
\end_inset

.
 Thus, 
\begin_inset Formula $\alpha_{i}$
\end_inset

 and 
\begin_inset Formula $y_{i,t-1}$
\end_inset

 are correlated, even if the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are pure random effects (uncorrelated with 
\begin_inset Formula $x_{it}$
\end_inset

).
\end_layout

\begin_layout Itemize
So, 
\begin_inset Formula $y_{i,t-1}$
\end_inset

 is correlated with 
\begin_inset Formula $\eta_{it}$
\end_inset

.
\end_layout

\begin_layout Itemize
For this reason, OLS estimation is inconsistent even for the random effects
 model, and it's also of course still inconsistent for the fixed effects
 model.
 
\end_layout

\begin_layout Itemize
When regressors are correlated with the errors, the natural thing to do
 is start thinking of instrumental variables estimation, or GMM.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Arellano-Bond estimator
\end_layout

\begin_layout Standard
The first thing is to realize that the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 that are a component of the error are correlated with all regressors in
 the general case of fixed effects.
 Getting rid of the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 is a step in the direction of solving the problem.
 We could subtract the time averages, as above for the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimator, but this would give us problems later when we need to define
 instruments.
 Instead, consider the model in first differences
\begin_inset Formula 
\begin{eqnarray*}
y_{it}-y_{i,t-1} & = & \left(\alpha_{i}+\gamma y_{i,t-1}+x_{it}\beta+\epsilon_{it}\right)-\left(\alpha_{i}+\gamma y_{i,t-2}+x_{i,t-1}\beta+\epsilon_{i,t-1}\right)\\
 & = & \gamma\left(y_{i,t-1}-y_{i,t-2}\right)+\left(x_{it}-x_{i,t-1}\right)\beta+\epsilon_{it}-\epsilon_{i,t-1}
\end{eqnarray*}

\end_inset

or
\begin_inset Formula 
\[
\Delta y_{it}=\gamma\Delta y_{i,t-1}+\Delta x_{it}\beta+\Delta\epsilon_{it}
\]

\end_inset


\end_layout

\begin_layout Itemize
Now the pesky 
\begin_inset Formula $\alpha_{i}$
\end_inset

 are no longer in the picture.
 
\end_layout

\begin_layout Itemize
Note that we loose one observation when doing first differencing.
\end_layout

\begin_layout Itemize
OLS estimation of this model will still be inconsistent, because 
\begin_inset Formula $y_{i,t-1}$
\end_inset

is clearly correlated with 
\begin_inset Formula $\epsilon_{i,t-1}.$
\end_inset

 
\end_layout

\begin_layout Itemize
Note also that the error 
\begin_inset Formula $\Delta\epsilon_{it}$
\end_inset

 is serially correlated even if the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not.
 
\end_layout

\begin_layout Itemize
There is no problem of correlation between 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\Delta x_{it}$
\end_inset

 and 
\begin_inset Formula $\Delta\epsilon_{it}$
\end_inset

.
 Thus, to do GMM, we need to find instruments for 
\begin_inset Formula $\Delta y_{i,t-1}$
\end_inset

, but the variables in 
\begin_inset Formula $\Delta x_{it}$
\end_inset

 can serve as their own instruments.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

How about using 
\begin_inset Formula $y_{i.t-2}$
\end_inset

 as an instrument?
\end_layout

\begin_layout Itemize
It is clearly correlated with 
\begin_inset Formula $\Delta y_{i,t-1}=\left(y_{i,t-1}-y_{i,t-2}\right)$
\end_inset


\end_layout

\begin_layout Itemize

\emph on
as long as the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not serially correlated
\emph default
, then 
\begin_inset Formula $y_{i.t-2}$
\end_inset

 is not correlated with 
\begin_inset Formula $\Delta\epsilon_{it}=\epsilon_{it}-\epsilon_{i,t-1}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
We can also use additional lags 
\begin_inset Formula $y_{i.t-s}$
\end_inset

, 
\begin_inset Formula $s\ge2$
\end_inset

 to increase efficiency, because GMM with additional instruments is asymptotical
ly more efficient than with less instruments (but small sample bias may
 become a serious problem).
\end_layout

\begin_layout Standard
This sort of estimator is widely known in the literature as an Arellano-Bond
 estimator, due to the influential 1991 paper of Arellano and Bond (1991).
\end_layout

\begin_layout Itemize
Note that this sort of estimators requires 
\begin_inset Formula $T=3$
\end_inset

 at a minimum.
 
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $t=1$
\end_inset

 and 
\begin_inset Formula $t=2,$
\end_inset

 we cannot compute the moment conditions.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $t=1,$
\end_inset

 we do not have 
\begin_inset Formula $y_{i,t-1}=y_{i,0},$
\end_inset

 so we can't compute dependent variable.
\end_layout

\begin_layout Itemize
for 
\begin_inset Formula $t=2,$
\end_inset

 we can compute the dependent variable 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\Delta y_{i2}$
\end_inset

, but not the regressor 
\begin_inset Formula $\Delta y_{i,2-1}=y_{i,1}-y_{i,0}.$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
for 
\begin_inset Formula $t=3,$
\end_inset

 we can compute the dep.
 var.
 
\begin_inset Formula $\Delta y_{i,3}$
\end_inset

, the regressor 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\Delta y_{i,2}=y_{i,2}-y_{i,1},$
\end_inset

 and we have 
\begin_inset Formula $y_{i,1,}$
\end_inset

 to serve as an instrument for 
\begin_inset Formula $\Delta y_{i,2}$
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $T>3,$
\end_inset

 then when 
\begin_inset Formula $t=4,$
\end_inset

 we can use both 
\begin_inset Formula $y_{i,1}$
\end_inset

 and 
\begin_inset Formula $y_{i,2}$
\end_inset

 as instruments.
 This sort of unbalancedness in the instruments requires a bit of care when
 programming.
 Also, additional instruments increase asymptotic efficiency but can lead
 to increased small sample bias, so one should be a little careful with
 using too many instruments.
 Some robustness checks, looking at the stability of the estimates are a
 way to proceed.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

One should note that serial correlation of the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 will cause this estimator to be inconsistent.
 Serial correlation of the errors 
\emph on
may 
\emph default
be due to dynamic misspecification, and this can be solved by including
 additional lags of the dependent variable.
 However, too many lags leads to a reduction of the sample size, so there's
 a limit to what can be done without having variances explode.
 However, serial correlation may also be due to factors not captured in
 lags of the dependent variable.
 If this is a possibility, then the validity of the Arellano-Bond type instrumen
ts is in question.
\end_layout

\begin_layout Itemize
A final note is that the error 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\Delta\epsilon_{it}$
\end_inset

 is serially correlated even when the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are not, and very likely heteroscedastic across 
\begin_inset Formula $i$
\end_inset

.
 One needs to take this into account when computing the covariance of the
 GMM estimator.
 One can also attempt to use GLS style weighting to improve efficiency.
 There are many possibilities.
\end_layout

\begin_layout Itemize
there is a 
\begin_inset Quotes sld
\end_inset

system
\begin_inset Quotes srd
\end_inset

 version of this sort of estimator that adds additional moment conditions,
 to improve efficiency
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Example
\end_layout

\begin_layout Standard
Use the GRETL data set abdata.gdt to illustrate fixed effects, random effects,
 and DPD estimation For FE and RE, use the model 
\begin_inset Formula 
\[
n_{it}=\alpha_{i}+\beta_{t}+\gamma w_{it}+\delta k_{it}+\phi ys_{it}+\epsilon_{it}
\]

\end_inset


\end_layout

\begin_layout Itemize
open abdata.gdt in GRETL
\end_layout

\begin_layout Itemize
read dataset info: 9 years of data on 140 companies in manufacturing sector
 (different industries).
\end_layout

\begin_deeper
\begin_layout Itemize
examine the variables: note that the data set is not 
\begin_inset Quotes sld
\end_inset

balanced
\begin_inset Quotes srd
\end_inset

: some companies are not observed in some years
\end_layout

\begin_layout Itemize
taking care of this problem is annoying without using a well written panel
 data package.
\end_layout

\end_deeper
\begin_layout Itemize
estimate fixed effects
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
note the pattern of the coefficients of the yearly dummies: the 
\begin_inset Quotes sld
\end_inset

Margaret Thatcher effect
\begin_inset Quotes srd
\end_inset


\end_layout

\begin_layout Itemize
signs of coefficients seem ok.
 Exogeneity to be trusted?
\end_layout

\begin_layout Itemize
save fixed effects, save residuals
\end_layout

\begin_layout Itemize
do residuals appear to be normally distributed? Test, and nonparametric
 density.
 If not normal, then random effects is not fully efficient, even if exogeneity
 of effects is valid.
\end_layout

\begin_layout Itemize
is there evidence of serial correlation of residuals? Run AR(1) on residuals:
 significant autocorrelation.
 Suggests an omitted dynamic effect.
\end_layout

\begin_layout Itemize
do nonparametric density plot of fixed effects: mean is 1, but significant
 variation across companies (different industries have different labor intensity
)
\end_layout

\end_deeper
\begin_layout Itemize
run random effects (tradition, but not logic, demands that we do it)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hausman test: rejects RE (unsurprisingly): we should favor FE.
 However, if errors are not normal, or if there is serial correlation, the
 test is not valid.
 Nevertheless, FE is probably favored on strictly theoretical grounds.
\end_layout

\end_deeper
\begin_layout Itemize
Given that the residuals seem to be serially correlated, we need to introduce
 dynamic structure.
 For DPD, use the model 
\begin_inset Formula 
\[
n_{it}=\alpha_{i}+\beta_{t}+\rho_{1}n_{i,t-1}+\gamma w_{it}+\delta k_{it}+\phi ys_{it}+\epsilon_{it}
\]

\end_inset


\end_layout

\begin_layout Itemize
the estimate of 
\begin_inset Formula $\rho_{1}$
\end_inset

 is economically and statistically significant
\end_layout

\begin_layout Itemize
note the important differences in the other coefficients compared to the
 FE model
\end_layout

\begin_layout Standard
check the serial correlation of the residuals: if it exists, the instruments
 are not valid.
 Possible solution is to augment the AR order, but the sample size gets
 smaller with every additional lag.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
In the context of a dynamic model with fixed effects, why is the differencing
 used in the 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimation approach (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:within estimator"

\end_inset

) problematic? That is, why does the Arellano-Bond estimator operate on
 the model in first differences instead of using the within approach?
\end_layout

\begin_layout Enumerate
Consider the simple linear panel data model with random effects (equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:simple linear panel random effects"

\end_inset

).
 Suppose that the 
\begin_inset Formula $\epsilon_{it}$
\end_inset

 are independent across cross sectional units, so that 
\begin_inset Formula $E(\epsilon_{it}\epsilon_{js})=0,\,i\ne j,\,\forall t,s$
\end_inset

.
 With a cross sectional unit, the errors are independently and identically
 distributed, so 
\begin_inset Formula $E(\epsilon_{it}^{2})=\sigma_{i}^{2},$
\end_inset

 but 
\begin_inset Formula $E(\epsilon_{it}\epsilon_{is})=0,\,t\ne s.$
\end_inset

 More compactly, let 
\begin_inset Formula $\epsilon_{i}=\left[\begin{array}{cccc}
\epsilon_{i1} & \epsilon_{i2} & \cdots & \epsilon_{iT}\end{array}\right]^{\prime}$
\end_inset

 .
 Then the assumptions are that 
\begin_inset Formula $E(\epsilon_{i}\epsilon_{i}^{\prime})=\sigma_{i}^{2}I_{T},$
\end_inset

 and 
\begin_inset Formula $E(\epsilon_{i}\epsilon_{j}^{\prime})=0,\,i\ne j$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
write out the form of the entire covariance matrix (
\begin_inset Formula $nT\times nT$
\end_inset

) of all errors, 
\begin_inset Formula $\Sigma=E(\epsilon\epsilon^{\prime})$
\end_inset

, where 
\begin_inset Formula $\epsilon=\left[\begin{array}{cccc}
\epsilon_{1}^{\prime} & \epsilon_{2}^{\prime} & \cdots & \epsilon_{T}^{\prime}\end{array}\right]^{\prime}$
\end_inset

 is the column vector of 
\begin_inset Formula $nT$
\end_inset

 errors.
\end_layout

\begin_layout Enumerate
suppose that 
\begin_inset Formula $n$
\end_inset

 is fixed, and consider asymptotics as 
\begin_inset Formula $T$
\end_inset

 grows.
 Is it possible to estimate the 
\begin_inset Formula $\Sigma_{i}$
\end_inset

 consistently? If so, how?
\end_layout

\begin_layout Enumerate
suppose that 
\begin_inset Formula $T$
\end_inset

 is fixed, and consider asymptotics an 
\begin_inset Formula $n$
\end_inset

 grows.
 Is it possible to estimate the 
\begin_inset Formula $\Sigma_{i}$
\end_inset

 consistently? If so, how?
\end_layout

\begin_layout Enumerate
For one of the two preceeding parts (b) and (c), consistent estimation is
 possible.
 For that case, outline how to do 
\begin_inset Quotes sld
\end_inset

within
\begin_inset Quotes srd
\end_inset

 estimation using a GLS correction.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Nonparametric-inference"

\end_inset

Nonparametric inference
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Ch.
 9; 
\begin_inset CommandInset citation
LatexCommand cite
key "li2007nonparametric"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
What do we mean by the term 
\begin_inset Quotes eld
\end_inset

nonparametric inference
\begin_inset Quotes erd
\end_inset

? Simply, this means inferences that are possible without restricting the
 functions of interest to belong to a parametric family.
 
\end_layout

\begin_layout Example
A parametric demand function: 
\begin_inset Formula $x=\alpha+\beta p+\gamma m+\epsilon$
\end_inset

, where 
\begin_inset Formula $\epsilon\sim N(0,\sigma^{2})$
\end_inset

.
 Here, the functional form of the conditional mean is restricted to be linear
 in the parameter and the regressors, and the distribution of the error
 is restricted to the set of mean zero normal distributions.
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
A nonparametric demand function: 
\begin_inset Formula $x=x(p,m)+\epsilon$
\end_inset

, where 
\begin_inset Formula $E(\epsilon|p,m)=0.$
\end_inset

 The conditional mean is the function 
\begin_inset Formula $x(p,m),$
\end_inset

 but the form is not restricted.
 Also, the error has conditional mean zero, but may have any distribution
 that follows this restriction.
\end_layout

\begin_layout Itemize
Normally, it is good to use parametric restrictions 
\emph on
if we are confident that they are at least approximately true
\emph default
, as this will lead to low variance, low bias estimation.
\end_layout

\begin_layout Itemize
If we impose parametric restrictions for which we have little or no justificatio
n, we may provoke serious biases which can lead to incorrect conclusions.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Motivation 
\series default
(see 
\begin_inset CommandInset citation
LatexCommand citet
key "white1980using"
literal "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
In this section we return to an example which we've already seen: approximating
 a nonlinear in the variables regression line using a linear in the variables
 regression line.
 
\end_layout

\begin_layout Standard
We suppose that data is generated by random sampling of 
\begin_inset Formula $(y,x)$
\end_inset

, where 
\begin_inset Formula $y=f(x)$
\end_inset

 
\begin_inset Formula $+\varepsilon$
\end_inset

, 
\begin_inset Formula $x$
\end_inset

 is uniformly distributed on 
\begin_inset Formula $(0,2\pi),$
\end_inset

 and 
\begin_inset Formula $\varepsilon$
\end_inset

 is a classical error with variance equal to 1.
 Suppose that the regression function is truly a quadratic function:
\begin_inset Formula 
\[
f(x)=1+\frac{3x}{2\pi}-\left(\frac{x}{2\pi}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
If we knew the functional form but not the coefficients, we could just estimate
 by least squares.
\begin_inset Newpage newpage
\end_inset

 
\end_layout

\begin_layout Itemize
But, let's assume that we do not know the functional form, to make things
 interesting
\end_layout

\begin_layout Itemize
Suppose that the problem of interest is to estimate the elasticity of 
\begin_inset Formula $f(x)$
\end_inset

 with respect to 
\begin_inset Formula $x,$
\end_inset

 throughout the range of 
\begin_inset Formula $x$
\end_inset

.
 Recall that the elasticity is an elasticity is the marginal function divided
 by the average function: 
\begin_inset Formula 
\[
\varepsilon(x)=\frac{f^{\prime}(x)}{f(x)/x}
\]

\end_inset


\end_layout

\begin_layout Itemize
We would like to be able to estimate this quantity well for any arbitrary
 value 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

In general, the functional form of 
\begin_inset Formula $f(x)$
\end_inset

 is unknown.
 One idea is to take a Taylor's series approximation to 
\begin_inset Formula $f(x)$
\end_inset

 about some point 
\begin_inset Formula $x_{0}.$
\end_inset

 Flexible functional forms such as the transcendental logarithmic (usually
 known as the translog) can be interpreted as second order Taylor's series
 approximations.
 We'll work with a first order approximation, for simplicity.
 Approximating about 
\begin_inset Formula $x_{0}$
\end_inset

: 
\begin_inset Formula 
\[
h(x)=f(x_{0})+D_{x}f(x_{0})\left(x-x_{0}\right)
\]

\end_inset

 If the approximation point is 
\begin_inset Formula $x_{0}=0,$
\end_inset

 we can write 
\begin_inset Formula 
\[
h(x)=a+bx
\]

\end_inset

 The coefficient 
\begin_inset Formula $a$
\end_inset

 is the value of the function at 
\begin_inset Formula $x=0,$
\end_inset

 and the slope is the value of the derivative at 
\begin_inset Formula $x=0.$
\end_inset

 These are of course not known.
 One might try estimation by ordinary least squares.
 The objective function is 
\begin_inset Formula 
\[
s(a,b)=1/n\sum_{t=1}^{n}\left(y_{t}-h(x_{t})\right)^{2}.
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 The limiting objective function, following the argument we used to get
 equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "olslim"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlslim"

\end_inset

 is 
\begin_inset Formula 
\[
s_{\infty}(a,b)=\int_{0}^{2\pi}\left(f(x)-h(x)\right)^{2}dx+C
\]

\end_inset

 The theorem regarding the consistency of extremum estimators (Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

) tells us that 
\begin_inset Formula $\hat{a}$
\end_inset

 and 
\begin_inset Formula $\hat{b}$
\end_inset

 will converge almost surely to the values that minimize the limiting objective
 function.
 Solving the first order conditions
\begin_inset Foot
status open

\begin_layout Plain Layout
The following results were obtained using the free computer algebra system
 (CAS) 
\begin_inset CommandInset href
LatexCommand href
name "Maxima"
target "http://maxima.sourceforge.net/"
literal "false"

\end_inset

.
 Unfortunately, I have lost the source code to get the results.
 It's not hard to do, though: see 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Consistency-of-OLS"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\end_inset

 reveals that 
\begin_inset Formula $s_{\infty}(a,b)$
\end_inset

 obtains its minimum at 
\begin_inset Formula $\left\{ a^{0}=\frac{7}{6},b^{0}=\frac{1}{\pi}\right\} .$
\end_inset

 The estimated approximating function 
\begin_inset Formula $\hat{h}(x)$
\end_inset

 therefore tends almost surely to
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\infty}(x)=7/6+x/\pi
\]

\end_inset


\begin_inset Newpage newpage
\end_inset

 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-and-simple"

\end_inset

 we see the true function and the limit of the approximation to see the
 asymptotic bias as a function of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-and-simple"

\end_inset

True and simple approximating functions
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/linearfit.png
	width 4in

\end_inset


\end_layout

\end_inset

(The approximating model is the straight line, the true model has curvature.)
 Note that the approximating model is in general inconsistent, even at the
 approximation point.
 This shows that ``flexible functional forms
\begin_inset Quotes erd
\end_inset

 based upon Taylor's series approximations do not in general lead to consistent
 estimation of functions.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The approximating model seems to fit the true model fairly well, asymptotically,
 so maybe the approximation problem is not too important? 
\end_layout

\begin_layout Itemize
However, we are interested in the elasticity of the function.
 Recall that the elasticity is an elasticity is the marginal function divided
 by the average function: 
\begin_inset Formula 
\[
\varepsilon(x)=\frac{f^{\prime}(x)}{f(x)/x}
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Good approximation of the elasticity over the range of 
\begin_inset Formula $x$
\end_inset

 will require a good approximation of both 
\begin_inset Formula $f(x)$
\end_inset

 and 
\begin_inset Formula $f^{\prime}(x)$
\end_inset

 over the range of 
\begin_inset Formula $x.$
\end_inset

 The approximating elasticity is 
\begin_inset Formula 
\[
\eta(x)=\frac{h^{\prime}(x)}{h(x)/x}
\]

\end_inset


\end_layout

\begin_layout Itemize
The question is: how well does 
\begin_inset Formula $\eta(x)$
\end_inset

 approximate 
\begin_inset Formula $\varepsilon(x)$
\end_inset

?
\begin_inset Newpage newpage
\end_inset

 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-and-approximating"

\end_inset

 we see the true elasticity and the elasticity obtained from the limiting
 approximating model.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-and-approximating"

\end_inset

True and approximating elasticities
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/linearelasticity.png
	width 4in

\end_inset


\end_layout

\end_inset

The true elasticity is the line that has negative slope for large 
\begin_inset Formula $x.$
\end_inset

 Visually we see that the elasticity is not approximated so well.
 Root mean squared error in the approximation of the elasticity is 
\begin_inset Formula 
\[
\left(\int_{0}^{2\pi}\left(\varepsilon(x)-\eta(x)\right)^{2}dx\right)^{1/2}=0.31546
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Now suppose we use the leading terms of a trigonometric series as the approximat
ing model.
 The reason for using a trigonometric series as an approximating model is
 motivated by the asymptotic properties of the Fourier flexible functional
 form (Gallant, 1981, 1982), which is an example of a 
\emph on
sieve estimator
\emph default
.
 Normally with this type of model the number of basis functions is an increasing
 function of the sample size.
 Here we hold the set of basis function fixed.
 We will consider the asymptotic behavior of a fixed model, which we interpret
 as an approximation to the estimator's behavior in finite samples.
 Consider the set of basis functions:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z(x)=\left[\begin{array}{cccccc}
1 & x & \cos(x) & \sin(x) & \cos(2x) & \sin(2x)\end{array}\right].
\]

\end_inset

 The approximating model is 
\begin_inset Formula 
\[
g_{K}(x)=Z(x)\alpha.
\]

\end_inset

 Maintaining these basis functions as the sample size increases, we find
 that the limiting objective function is minimized at 
\begin_inset Formula 
\[
\left\{ a_{1}=\frac{7}{6},a_{2}=\frac{1}{\pi},a_{3}=-\frac{1}{\pi^{2}},a_{4}=0,a_{5}=-\frac{1}{4\pi^{2}},a_{6}=0\right\} .
\]

\end_inset

 Substituting these values into 
\begin_inset Formula $g_{K}(x)$
\end_inset

 we obtain the almost sure limit of the approximation 
\begin_inset Formula 
\begin{equation}
g_{\infty}(x)=7/6+x/\pi+\left(\cos x\right)\left(-\frac{1}{\pi^{2}}\right)+\left(\sin x\right)0+\left(\cos2x\right)\left(-\frac{1}{4\pi^{2}}\right)+\left(\sin2x\right)0\label{g}
\end{equation}

\end_inset


\begin_inset Newpage newpage
\end_inset

 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-function-and"

\end_inset

 we have the approximation and the true function:
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-function-and"

\end_inset

True function and more flexible approximation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/fourierfit.png
	width 4in

\end_inset


\end_layout

\end_inset

Clearly the truncated trigonometric series model offers a better approximation,
 asymptotically, than does the linear model.
 
\begin_inset Newpage newpage
\end_inset

In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:True-elasticity-and"

\end_inset

 we have the more flexible approximation's elasticity and that of the true
 function:
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:True-elasticity-and"

\end_inset

True elasticity and more flexible approximation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/fourierelasticity.png
	width 4in

\end_inset


\end_layout

\end_inset

On average, the fit is better, though there is some implausible waviness
 in the estimate.
 
\begin_inset Newpage newpage
\end_inset

Root mean squared error in the approximation of the elasticity is 
\begin_inset Formula 
\[
\left(\int_{0}^{2\pi}\left(\varepsilon(x)-\frac{g_{\infty}^{\prime}(x)x}{g_{\infty}(x)}\right)^{2}dx\right)^{1/2}=0.16213,
\]

\end_inset

about half that of the RMSE when the first order approximation is used.
 
\end_layout

\begin_layout Itemize
Sieve estimators allow the number of regressors to grow as the sample size
 grows.
 
\end_layout

\begin_deeper
\begin_layout Itemize
this must be done in a controlled way: there is a variance/bias tradeoff:
\end_layout

\begin_deeper
\begin_layout Itemize
more regressors -> less bias
\end_layout

\begin_layout Itemize
more regressors-> more variance
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
It can be shown that if we introduce regressors at a slow rate, it is possible
 to drive the RMSE of the approximation to the regression function and to
 the elasticity to zero, as the sample size grows.
 
\end_layout

\begin_layout Itemize
This is why sieve estimators (and other nonparametric regression estimators)
 are of interest: we can obtain consistent estimates of regression functions
 without knowledge of the functional form of the true regression line.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Estimation of regression functions
\end_layout

\begin_layout Standard
Here, we will see two examples of methods of estimating regression functions
 without knowledge of the true functional form: kernel regression and neural
 nets.
 There are other methods, for example sieve estimators, 
\begin_inset CommandInset href
LatexCommand href
name "nearest neighbors"
target "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
literal "false"

\end_inset

, etc.
\end_layout

\begin_layout Subsection
Kernel regression estimators
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
: 
\begin_inset CommandInset citation
LatexCommand cite
key "li2007nonparametric"
literal "true"

\end_inset

, Ch.
 2; 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Ch.
 9; 
\begin_inset CommandInset citation
LatexCommand citet
key "bierens1987kernel"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Kernel regression estimation is an example of fully nonparametric estimation
 (others are splines, nearest neighbors, etc.).
 We'll consider the Nadaraya-Watson kernel regression estimator in a simple
 case.
\end_layout

\begin_layout Itemize
Suppose we have an iid sample from the joint density 
\begin_inset Formula $f(x,y),$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula $k$
\end_inset

 -dimensional.
 The model is 
\begin_inset Formula 
\[
y_{t}=g(x_{t})+\varepsilon_{t},
\]

\end_inset

 where 
\begin_inset Formula 
\[
E(\varepsilon_{t}|x_{t})=0.
\]

\end_inset


\end_layout

\begin_layout Itemize
The conditional expectation of 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula $g(x).$
\end_inset

 By definition of the conditional expectation, we have 
\begin_inset Formula 
\begin{eqnarray*}
g(x) & = & \int y\frac{f(x,y)}{h(x)}dy\\
 & = & \frac{1}{h(x)}\int yf(x,y)dy,
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $h(x)$
\end_inset

 is the marginal density of 
\begin_inset Formula $x:$
\end_inset


\begin_inset Formula 
\[
h(x)=\int f(x,y)dy.
\]

\end_inset


\end_layout

\begin_layout Itemize
This suggests that we could estimate 
\begin_inset Formula $g(x)$
\end_inset

 by estimating 
\begin_inset Formula $h(x)$
\end_inset

 and 
\begin_inset Formula $\int yf(x,y)dy.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Estimation of the denominator
\end_layout

\begin_layout Standard
A kernel estimator for 
\begin_inset Formula $h(x)$
\end_inset

 has the form 
\begin_inset Formula 
\[
\hat{h}(x)=\frac{1}{n}\sum_{t=1}^{n}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}},
\]

\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the sample size and 
\begin_inset Formula $k$
\end_inset

 is the dimension of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Itemize
The function 
\begin_inset Formula $K(\cdot)$
\end_inset

 (the kernel) is absolutely integrable: 
\begin_inset Formula 
\[
\int|K(x)|dx<\infty,
\]

\end_inset

 and 
\begin_inset Formula $K(\cdot)$
\end_inset

 integrates to 
\begin_inset Formula $1:$
\end_inset


\begin_inset Formula 
\[
\int K(x)dx=1.
\]

\end_inset

 In this respect, 
\begin_inset Formula $K(\cdot)$
\end_inset

 is like a density function, but we do not necessarily restrict 
\begin_inset Formula $K(\cdot)$
\end_inset

 to be nonnegative.
\end_layout

\begin_layout Itemize
The 
\emph on
window width
\emph default
 parameter, 
\begin_inset Formula $\gamma_{n}$
\end_inset

 is a sequence of positive numbers that satisfies 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}\gamma_{n} & = & 0\\
\lim_{n\rightarrow\infty}n\gamma_{n}^{k} & = & \infty
\end{eqnarray*}

\end_inset

 So, the window width must tend to zero, but not too quickly.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
To show pointwise consistency of 
\begin_inset Formula $\hat{h}(x)$
\end_inset

 for 
\begin_inset Formula $h(x),$
\end_inset

 first consider the expectation of the estimator (because the estimator
 is an average of iid terms, we only need to consider the expectation of
 a representative term): 
\begin_inset Formula 
\[
E\left[\hat{h}(x)\right]=\int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]h(z)dz.
\]

\end_inset

 Change variables as 
\begin_inset Formula $z^{*}=(x-z)/\gamma_{n},$
\end_inset

 so 
\begin_inset Formula $z=x-\gamma_{n}z^{*}$
\end_inset

 and 
\begin_inset Formula $|\frac{dz}{dz^{*\prime}}|=\gamma_{n}^{k},$
\end_inset

 we obtain
\begin_inset Formula 
\begin{eqnarray*}
E\left[\hat{h}(x)\right] & = & \int\gamma_{n}^{-k}K\left(z^{*}\right)h(x-\gamma_{n}z^{*})\gamma_{n}^{k}dz^{*}\\
 & = & \int K\left(z^{*}\right)h(x-\gamma_{n}z^{*})dz^{*}.
\end{eqnarray*}

\end_inset

 Now, asymptotically, 
\begin_inset Formula 
\begin{eqnarray*}
\lim_{n\rightarrow\infty}E\left[\hat{h}(x)\right] & = & \lim_{n\rightarrow\infty}\int K\left(z^{*}\right)h(x-\gamma_{n}z^{*})dz^{*}\\
 & = & \int\lim_{n\rightarrow\infty}K\left(z^{*}\right)h(x-\gamma_{n}z^{*})dz^{*}\\
 & = & \int K\left(z^{*}\right)h(x)dz^{*}\\
 & = & h(x)\int K\left(z^{*}\right)dz^{*}\\
 & = & h(x),
\end{eqnarray*}

\end_inset

 since 
\begin_inset Formula $\gamma_{n}\rightarrow0$
\end_inset

 and 
\begin_inset Formula $\int K\left(z^{*}\right)dz^{*}=1$
\end_inset

 by assumption.
 (Note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

that we can pass the limit through the integral is a result of the dominated
 convergence theorem.
 For this to hold we need that 
\begin_inset Formula $h(\cdot)$
\end_inset

 be dominated by an absolutely integrable function.)
\end_layout

\begin_layout Itemize
Next, considering the variance of 
\begin_inset Formula $\hat{h}(x),$
\end_inset

 we have, due to the iid assumption 
\begin_inset Formula 
\begin{eqnarray*}
n\gamma_{n}^{k}V\left[\hat{h}(x)\right] & = & n\gamma_{n}^{k}\frac{1}{n^{2}}\sum_{t=1}^{n}V\left\{ \frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}\right\} \\
 & = & \gamma_{n}^{-k}\frac{1}{n}\sum_{t=1}^{n}V\left\{ K\left[\left(x-x_{t}\right)/\gamma_{n}\right]\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
By the representative term argument, this is
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
n\gamma_{n}^{k}V\left[\hat{h}(x)\right]=\gamma_{n}^{-k}V\left\{ K\left[\left(x-z\right)/\gamma_{n}\right]\right\} 
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Also, since 
\begin_inset Formula $V(x)=E(x^{2})-E(x)^{2}$
\end_inset

 we have 
\begin_inset Formula 
\begin{eqnarray*}
n\gamma_{n}^{k}V\left[\hat{h}(x)\right] & = & \gamma_{n}^{-k}E\left\{ \left(K\left[\left(x-z\right)/\gamma_{n}\right]\right)^{2}\right\} -\gamma_{n}^{-k}\left\{ E\left(K\left[\left(x-z\right)/\gamma_{n}\right]\right)\right\} ^{2}\\
 & = & \int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]^{2}h(z)dz-\gamma_{n}^{k}\left\{ \int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]h(z)dz\right\} ^{2}\\
 & = & \int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]^{2}h(z)dz-\gamma_{n}^{k}E\left[\widehat{h}(x)\right]^{2}
\end{eqnarray*}

\end_inset

 The second term converges to zero: 
\begin_inset Formula 
\[
\gamma_{n}^{k}E\left[\widehat{h}(x)\right]^{2}\rightarrow0,
\]

\end_inset

 by the previous result regarding the expectation and the fact that 
\begin_inset Formula $\gamma_{n}\rightarrow0.$
\end_inset

 Therefore, 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}n\gamma_{n}^{k}V\left[\hat{h}(x)\right]=\lim_{n\rightarrow\infty}\int\gamma_{n}^{-k}K\left[\left(x-z\right)/\gamma_{n}\right]^{2}h(z)dz.
\]

\end_inset

 Using exactly the same change of variables as before, this can be shown
 to be 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}n\gamma_{n}^{k}V\left[\hat{h}(x)\right]=h(x)\int\left[K(z^{*})\right]^{2}dz^{*}.
\]

\end_inset

 Since both 
\begin_inset Formula $\int\left[K(z^{*})\right]^{2}dz^{*}$
\end_inset

 and 
\begin_inset Formula $h(x)$
\end_inset

 are bounded, the RHS is bounded, and since 
\begin_inset Formula $n\gamma_{n}^{k}\rightarrow\infty$
\end_inset

 by assumption, we have that 
\begin_inset Formula 
\[
V\left[\hat{h}(x)\right]\rightarrow0.
\]

\end_inset


\end_layout

\begin_layout Itemize
Since the bias and the variance both go to zero, we have pointwise consistency
 (convergence in quadratic mean implies convergence in probability).
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Estimation of the numerator
\end_layout

\begin_layout Standard
To estimate 
\begin_inset Formula $\int yf(x,y)dy,$
\end_inset

 we need an estimator of 
\begin_inset Formula $f(x,y).$
\end_inset

 The estimator has the same form as the estimator for 
\begin_inset Formula $h(x),$
\end_inset

 only with one dimension more: 
\begin_inset Formula 
\[
\hat{f}(x,y)=\frac{1}{n}\sum_{t=1}^{n}\frac{K_{*}\left[\left(y-y_{t}\right)/\gamma_{n},\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k+1}}
\]

\end_inset

 The kernel 
\begin_inset Formula $K_{*}\left(\cdot\right)$
\end_inset

 is required to have mean zero: 
\begin_inset Formula 
\[
\int yK_{*}\left(y,x\right)dy=0
\]

\end_inset

 and to marginalize to the previous kernel for 
\begin_inset Formula $h(x):$
\end_inset


\begin_inset Formula 
\[
\int K_{*}\left(y,x\right)dy=K(x).
\]

\end_inset

 With this kernel, we have (not obviously, see Li and Racine, Ch.
 2, section 2.1) 
\begin_inset Formula 
\[
\int y\hat{f}(y,x)dy=\frac{1}{n}\sum_{t=1}^{n}y_{t}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}
\]

\end_inset

 by marginalization of the kernel, so we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\hat{g}(x) & := & \frac{1}{\hat{h}(x)}\int y\hat{f}(y,x)dy\\
 & = & \frac{\frac{1}{n}\sum_{t=1}^{n}y_{t}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}}{\frac{1}{n}\sum_{t=1}^{n}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}}\\
 & = & \frac{\sum_{t=1}^{n}y_{t}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}
\end{eqnarray*}

\end_inset

 This is the Nadaraya-Watson kernel regression estimator.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Discussion
\end_layout

\begin_layout Itemize
defining 
\begin_inset Formula 
\[
w_{t}=\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]},
\]

\end_inset

the kernel regression estimator for 
\begin_inset Formula $g(x_{t})$
\end_inset

 can be written as
\begin_inset Formula 
\begin{align*}
\hat{g}(x) & =\sum_{t=1}^{n}y_{t}w_{t},
\end{align*}

\end_inset

a weighted average of the 
\begin_inset Formula $y_{j},\,j=1,2,...,n$
\end_inset

, where higher weights are associated with points that are closer to 
\begin_inset Formula $x_{t}.$
\end_inset

 The weights sum to 1.
 See this 
\begin_inset CommandInset href
LatexCommand href
name "link for a graphic interpretation."
target "https://en.wikipedia.org/wiki/Kernel_smoother"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
The window width parameter 
\begin_inset Formula $\gamma_{n}$
\end_inset

 imposes smoothness.
 The estimator is increasingly flat as 
\begin_inset Formula $\gamma_{n}\rightarrow\infty,$
\end_inset

 since in this case each weight tends to 
\begin_inset Formula $1/n.$
\end_inset


\end_layout

\begin_layout Itemize
A large window width reduces the variance (strong imposition of flatness),
 but increases the bias.
\end_layout

\begin_layout Itemize
A small window width reduces the bias, but makes very little use of information
 except points that are in a small neighborhood of 
\begin_inset Formula $x_{t}.$
\end_inset

 Since relatively little information is used, the variance is large when
 the window width is small.
\end_layout

\begin_layout Itemize
The standard normal density is a popular choice for 
\begin_inset Formula $K(.)\;$
\end_inset

 and 
\begin_inset Formula $K_{*}(y,x),$
\end_inset

 though there are possibly better alternatives.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Choice of the window width:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

Cross-validation
\end_layout

\begin_layout Standard
The selection of an appropriate window width is important.
 One popular method is cross validation.
 This consists of splitting the sample into two parts (e.g., 50%-50%).
 The first part is the 
\begin_inset Quotes eld
\end_inset

in sample
\begin_inset Quotes erd
\end_inset

 data, which is used for estimation, and the second part is the 
\begin_inset Quotes eld
\end_inset

out of sample
\begin_inset Quotes erd
\end_inset

 data, used for evaluation of the fit though RMSE or some other criterion.
 The steps are:
\end_layout

\begin_layout Enumerate
Split the data.
 The out of sample data is 
\begin_inset Formula $y^{out}$
\end_inset

 and 
\begin_inset Formula $x^{out}$
\end_inset

 (these are the first 
\begin_inset Formula $n_{out}$
\end_inset

 observations, say-
\end_layout

\begin_layout Enumerate
Choose a window width 
\begin_inset Formula $\gamma$
\end_inset

.
\end_layout

\begin_layout Enumerate
With the in sample data, fit 
\begin_inset Formula $\hat{y}_{t}^{out}(\gamma)$
\end_inset

 corresponding to each 
\begin_inset Formula $x_{t}^{out}.$
\end_inset

 This fitted value is a function of the window width, the in sample data,
 as well as the evaluation point 
\begin_inset Formula $x_{t}^{out}$
\end_inset

, but it does not involve 
\begin_inset Formula $y_{t}^{out}.$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat for all out of sample points.
\end_layout

\begin_layout Enumerate
Calculate RMSE
\begin_inset Formula $(\gamma)=\sqrt{\frac{1}{n_{out}}\sum_{t=1}^{n_{out}}\left(y_{t}^{out}-\hat{y}_{t}^{out}(\gamma)\right)^{2}}$
\end_inset


\end_layout

\begin_layout Enumerate
Go to step 
\begin_inset Formula $2,$
\end_inset

 or to the next step if enough window widths have been tried.
\end_layout

\begin_layout Enumerate
Select the 
\begin_inset Formula $\gamma$
\end_inset

 that minimizes RMSE(
\begin_inset Formula $\gamma)$
\end_inset

 (Verify that a minimum has been found, for example by plotting RMSE as
 a function of 
\begin_inset Formula $\gamma).$
\end_inset


\end_layout

\begin_layout Enumerate
Re-estimate using the best 
\begin_inset Formula $\gamma$
\end_inset

 and all of the data.
 
\end_layout

\begin_layout Itemize
there is a variation known as leave-one-out cross validation, where each
 
\begin_inset Formula $y_{t}^{out}$
\end_inset

 is fit in turn using all of the remaining observations, omitting the 
\begin_inset Formula $t^{th}$
\end_inset

 observation.
 This is the recommended procedure.
 It is somewhat more demanding computationally, but works better.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Neural nets
\end_layout

\begin_layout Standard
Neural networks are a well known tool in many fields, and there are many
 presentations, both academic and more informal, of various structures that
 can be used.
 For this reason, the presentation here is brief.
 For more details and references, see 
\begin_inset CommandInset citation
LatexCommand citet
key "KuanWhiteNNSurvey1994"
literal "true"

\end_inset

.
 A very useful practical guide is given by 
\begin_inset CommandInset citation
LatexCommand citet
key "LeCun2012"
literal "true"

\end_inset

.
 A good 
\begin_inset CommandInset href
LatexCommand href
name "practical introduction is here"
target "https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/understanding-deep-learning-parameter-tuning-with-mxnet-h2o-package-in-r/tutorial/"
literal "false"

\end_inset

.
 Papers by 
\begin_inset CommandInset citation
LatexCommand cite
key "GallantWhiteNeural88"
literal "true"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Hornik1989MFN:70405.70408"
literal "true"

\end_inset

 show that some types of neural networks can be thought of as as nonparametric
 regression estimators, but this discussion seems to still be open, in the
 case of the 
\begin_inset Quotes sld
\end_inset

deep learning
\begin_inset Quotes srd
\end_inset

 nets that are popular nowadays.
 The discussion below is based on 
\begin_inset CommandInset citation
LatexCommand cite
key "creel2016neural"
literal "true"

\end_inset

, and code for the example below is at 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/mcreel/NeuralNetsForIndirectInference.jl"
target "https://github.com/mcreel/NeuralNetsForIndirectInference.jl"
literal "false"

\end_inset

.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Suppose we are interested in the regression model 
\begin_inset Formula $y=g(x)+\epsilon,$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $K$
\end_inset

-vector and 
\begin_inset Formula $y$
\end_inset

 is a 
\begin_inset Formula $G$
\end_inset

-vector.
 
\end_layout

\begin_deeper
\begin_layout Itemize
This is a multivariate (more than one dependent variable) multiple (more
 than one regressor) regression model.
 
\end_layout

\begin_layout Itemize
Because we don't specify the form of 
\begin_inset Formula $g(x)=E(y|x),$
\end_inset

 it is a nonparametric regression model.
\end_layout

\begin_layout Itemize
Let's model this using a neural net.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Consider a simple feed forward neural net for regression of an output in
 
\begin_inset Formula $R^{K}$
\end_inset

 upon an input in 
\begin_inset Formula $R^{G}$
\end_inset

.
 A typical feed forward net is depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:A-simple-neural"

\end_inset

, which maps 3 inputs to 2 outputs.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The inputs to the net, 
\begin_inset Formula $I_{1}$
\end_inset

, 
\begin_inset Formula $I_{2}$
\end_inset

, and 
\begin_inset Formula $I_{3}$
\end_inset

, are scalar real numbers, as are the outputs 
\begin_inset Formula $O_{1}$
\end_inset

 and 
\begin_inset Formula $O_{2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The net has two hidden layers, formed by 4 and 3 hidden nodes or neurons,
 
\begin_inset Formula $h_{1}$
\end_inset

,
\begin_inset Formula $h_{2}$
\end_inset

,...,
\begin_inset Formula $h_{7}$
\end_inset

, 
\end_layout

\begin_layout Itemize
and an output layer, which gives the values of the two outputs 
\begin_inset Formula $O_{1}$
\end_inset

 and 
\begin_inset Formula $O_{2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The values 
\begin_inset Formula $\alpha_{1},\,\alpha_{2}$
\end_inset

 and 
\begin_inset Formula $\alpha_{3}$
\end_inset

 are vectors of 
\begin_inset Quotes eld
\end_inset

bias
\begin_inset Quotes erd
\end_inset

 parameters, which are discussed below.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:A-simple-neural"

\end_inset

A simple neural net
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/michael/Desktop/Papers/NN/EconometricsAndStatistics/PublishedVersion/Figures/neuralnet.pdf
	width 14cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
In general, a net is a series of transformations of the inputs.
 
\end_layout

\begin_layout Itemize
Each of the transformations is referred to as a layer.
 
\end_layout

\begin_layout Itemize
The inputs themselves constitute the zero-th layer, and the final result
 of the transformations is the output layer.
 
\end_layout

\begin_layout Itemize
A layer, 
\begin_inset Formula $H_{j}$
\end_inset

, is a vector of real numbers, which is the result of the 
\begin_inset Formula $j^{th}$
\end_inset

 in the series of transformations.
\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $H_{0}$
\end_inset

 be the 
\begin_inset Formula $G$
\end_inset

 dimensional vector of inputs.
 
\end_layout

\begin_layout Itemize
Suppose that there are 
\begin_inset Formula $P$
\end_inset

 layers.
 
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $n_{j}$
\end_inset

 be the number of neurons in the 
\begin_inset Formula $j^{th}$
\end_inset

 layer, 
\begin_inset Formula $j=1,2,...,P.$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
The value taken by a neuron in the 
\begin_inset Formula $j^{th}$
\end_inset

 layer is the result of the layer's 
\begin_inset Quotes eld
\end_inset

activation function
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Formula $f_{j}(\cdot)$
\end_inset

, applied on an element-by-element basis to an affine function of the inputs
 to the layer.
 The relationship between the layers is given by 
\begin_inset Formula 
\begin{align}
H_{j} & =f_{j}(\alpha_{j}+\beta_{j}H_{j-1}),\,j=1,2,...,P,\label{eq:LayersOfNet}
\end{align}

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\alpha_{j}$
\end_inset

 is a 
\begin_inset Formula $n_{j}$
\end_inset

 dimensional vector of parameters (these are known as bias parameters in
 the neural net literature)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta_{j}$
\end_inset

 is a 
\begin_inset Formula $n_{j}\times n_{j-1}$
\end_inset

 matrix of parameters.
 
\end_layout

\begin_layout Itemize
The layers 
\begin_inset Formula $1,\,2,...,\,P-1$
\end_inset

 are referred to as hidden layers
\end_layout

\begin_layout Itemize
layer 
\begin_inset Formula $P$
\end_inset

 is the output layer.
\end_layout

\begin_layout Itemize
The input to the first hidden layer, known as the input layer, is simply
 the input data, 
\begin_inset Formula $H_{0}\in\mathbb{R}^{G}$
\end_inset

.
 The output of the net is the final layer, 
\begin_inset Formula $H_{P}\in\mathbb{\mathbb{R}}^{K}$
\end_inset

.
 When using a net for regression, the last activation function, 
\begin_inset Formula $f_{P}(\cdot)$
\end_inset

, is simply an identity function, so that 
\begin_inset Formula $H_{P}=\alpha_{P}+\beta_{P}H_{P-1}.$
\end_inset

 The reason that an activation function is used with the hidden layers is
 that this is what allows the net to approximate a nonlinear mapping.
 If all activation functions were identity functions, the entire net would
 reduce to an over-parameterized linear regression model.
 In this paper, the activation function that is used for the hidden layers
 is the 
\begin_inset Quotes eld
\end_inset

rectified linear unit
\begin_inset Quotes erd
\end_inset

 (ReLU) function, 
\begin_inset Formula $f(x)=\max(0,\,x)$
\end_inset

, a very widely used choice in modern deep learning applications.
\end_layout

\end_deeper
\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

A neural net may contain many, many hidden parameters
\end_layout

\begin_layout Itemize
Suppose the number of inputs, 
\begin_inset Formula $G,$
\end_inset

 is 40, and the number of outputs, 
\begin_inset Formula $K,$
\end_inset

 is 9.
\end_layout

\begin_layout Itemize
Suppose the net has two hidden layers, of size 300 and 40, respectively.
 
\end_layout

\begin_layout Itemize
Then there are 
\begin_inset Formula $300\times40$
\end_inset

 parameters in the 
\begin_inset Formula $\beta_{1}$
\end_inset

 matrix of the first layer and 40 elements in the 
\begin_inset Formula $\alpha_{1}$
\end_inset

 vector.
 
\end_layout

\begin_layout Itemize
Similarly, in the second hidden layer, there are 
\begin_inset Formula $40\times300+40$
\end_inset

 parameters
\end_layout

\begin_layout Itemize
there are 
\begin_inset Formula $9\times40+9$
\end_inset

 parameters corresponding to the output layer.
 
\end_layout

\begin_layout Itemize
Thus, the total number of parameters is 24449.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
A neural net is a nonlinear regression model that may be highly parameterized
\end_layout

\begin_deeper
\begin_layout Itemize
may be more parameters than observations in a single sample
\end_layout

\begin_layout Itemize
lack of identification: neurons can be reordered
\end_layout

\begin_layout Itemize
multiple local minima
\end_layout

\end_deeper
\begin_layout Standard
Partial solutions:
\end_layout

\begin_layout Itemize
for a simulable model, we can generate multiple data sets to train the net.
 With much data, even a large net can be trained well.
\end_layout

\begin_layout Itemize
For the multiple local minima problem, 
\begin_inset Quotes sld
\end_inset

stochastic gradient descent
\begin_inset Quotes srd
\end_inset

 and techniques related to cross validation can help a lot:
\end_layout

\begin_deeper
\begin_layout Itemize
compute the gradient using a small number of observations from the training
 set.
 This is called a stochastic gradient, because it depends on the observations
 that were chosen.
\end_layout

\begin_layout Itemize
take a small step in that direction.
 The step size is called the 
\begin_inset Quotes sld
\end_inset

learning rate
\begin_inset Quotes srd
\end_inset

 in the NN literature.
\end_layout

\begin_layout Itemize
evaluate the new fit using a testing set
\end_layout

\begin_layout Itemize
iterate gradient/learning, with the learning rate (step size) getting smaller
 as learning proceeds, until the fit to the testing set no longer improves.
 
\end_layout

\end_deeper
\begin_layout Standard
Modern software exists to make this quite easy to do.
 For Julia, 
\begin_inset CommandInset href
LatexCommand href
name "see this page"
target "https://juliacomputing.com/domains/ml-and-ai.html"
literal "false"

\end_inset

 to get started.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
For this to work well, you need a lot of data, to train the net.
\end_layout

\begin_layout Itemize
Simulation based econometric methods can give us a lot of simulated data,
 so using neural nets when doing simulation based estimation is very natural
\end_layout

\begin_layout Itemize
A neural net indirect inference estimator is not an extremum estimator:
 how to test hypotheses? 
\end_layout

\begin_deeper
\begin_layout Itemize
bootstrapping?
\end_layout

\begin_layout Itemize
One can use the NN estimator as a statistic for indirect inference or related
 methods, and then use the asymptotic theory for those methods.
 This works pretty well - see the table.
\end_layout

\end_deeper
\begin_layout Itemize
We use a NN to identify which are the important variables for explaining
 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Neural net indirect inference
\series default
: see 
\begin_inset CommandInset citation
LatexCommand citet
key "creel2017neural"
literal "true"

\end_inset


\end_layout

\begin_layout Itemize
see 
\begin_inset CommandInset href
LatexCommand href
name "this repository"
target "https://github.com/mcreel/NeuralNetsForIndirectInference.jl"
literal "false"

\end_inset

.
 Part of that repository has been copied into the ./Examples/DSGE/NNII directory
 of the example files that accompany these notes.
 See especially 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{this file}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/DSGE/NNII/NN
\backslash
_estimate.m} 
\end_layout

\end_inset

 for an example that shows that shows that an accurate neural net estimator
 can be computed in extremely little time.
 Using these methods to estimate the model discussed in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Application:-a-simple"

\end_inset

, we can obtain the results in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:DSGE-model.-Monte"

\end_inset

.
 (note to self: the program isn't translated to Julia because it uses Dynare
 to draw from the true model).
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:DSGE-model.-Monte"

\end_inset

DSGE model.
 Bias and RMSE as a percentage of the true parameter value, and 90% confidence
 interval inclusion of true value.
 NN=direct estimator using output of the net.
 II=indirect inference estimator using the output of the net as the auxiliary
 statistic.
 ABC=approximate Bayesian computation estimated posterior mean, using the
 output of the net as the auxiliary statistic.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="10">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
% Bias
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
% RMSE
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
90% CI
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Param.
 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
True
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NN
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ABC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NN
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ABC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ABC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.33 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.27
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.30
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
0.27
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
89
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
94
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.99 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.04
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.06
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
0.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
92
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
86
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\delta$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.025 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 1.35
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.27
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.76
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 3.52
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 3.09
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
3.09
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
87
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
79
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\gamma$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.00 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 3.85
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
-1.29
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.65
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 5.90
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 4.47
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
4.21
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
92
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
88
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{z}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.90 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
-0.29
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.08
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.26
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 1.71
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 1.54
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
1.26
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
97
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
94
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{z}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
-3.18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
-0.13
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 3.09
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 8.92
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 8.70
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
8.27
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
82
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
91
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\rho_{\eta}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.70 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
-1.03
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
-1.71
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
-1.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 7.90
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 8.30
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
8.15
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
92
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
92
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{\eta}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.89
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 1.41
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 3.54
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
11.13
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
11.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
9.39
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
84
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
86
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\bar{n}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1/3 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.07
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.14
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.83
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 0.80
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
0.98
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
93
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
85
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
For simple code to get started with neural nets, 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/mcreel/Econometrics.jl"
target "https://github.com/mcreel/Econometrics.jl"
literal "false"

\end_inset

, especially the file 
\begin_inset CommandInset href
LatexCommand href
name "neural_net_example.jl"
target "https://github.com/mcreel/Econometrics.jl/blob/master/examples/neural_net_example.jl"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
(run this in class, and discuss)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Density function estimation
\end_layout

\begin_layout Subsection
Kernel density estimation
\end_layout

\begin_layout Standard
The previous discussion suggests that a kernel density estimator may easily
 be constructed.
 We have already seen how joint densities may be estimated.
 If were interested in a conditional density, for example of 
\begin_inset Formula $y$
\end_inset

 conditional on 
\begin_inset Formula $x$
\end_inset

, then the kernel estimate of the conditional density is simply
\begin_inset Formula 
\begin{eqnarray*}
\widehat{f}_{y|x} & = & \frac{\hat{f}(x,y)}{\hat{h}(x)}\\
 & = & \frac{\frac{1}{n}\sum_{t=1}^{n}\frac{K_{*}\left[\left(y-y_{t}\right)/\gamma_{n},\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k+1}}}{\frac{1}{n}\sum_{t=1}^{n}\frac{K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\gamma_{n}^{k}}}\\
 & = & \frac{1}{\gamma_{n}}\frac{\sum_{t=1}^{n}K_{*}\left[\left(y-y_{t}\right)/\gamma_{n},\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}
\end{eqnarray*}

\end_inset

where we obtain the expressions for the joint and marginal densities from
 the section on kernel regression.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Example
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{ExampleKernelDensity.jl}{https://github.com/mcreel/Econometrics/
blob/master/Examples/Nonparametric/ExampleKernelDensity.jl}
\end_layout

\end_inset

 draws data from a 
\begin_inset Formula $\chi^{2}(3)$
\end_inset

 distribution and plots a kernel density fit, plus the true density.
 We see that they're pretty close, when the sample size is large enough
 for the kernel estimate to be precise.
 Try playing around with a smaller sample, and see what happens.
\end_layout

\begin_layout Example
\begin_inset Graphics
	filename Examples/Nonparametric/NPdensity.svg
	width 10cm

\end_inset


\end_layout

\begin_layout Subsection
Semi-nonparametric maximum likelihood
\begin_inset CommandInset label
LatexCommand label
name "subsec:Semi-nonparametric-maximum-likel"

\end_inset


\end_layout

\begin_layout Standard

\series bold
Readings:
\series default
 Gallant and Nychka, 
\emph on
Econometrica
\emph default
, 1987.
 For a Fortran program to do this and a useful discussion in the user's
 guide, see 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{this link}{http://www.econ.duke.edu/~get/snp.html}
\end_layout

\end_inset

.
 See also Cameron and Johansson, 
\emph on
Journal of Applied Econometrics
\emph default
, V.
 12, 1997.
\end_layout

\begin_layout Standard
MLE is the estimation method of choice when we are confident about specifying
 the density.
 Is is possible to obtain the benefits of MLE when we're not so confident
 about the specification? In part, yes.
\end_layout

\begin_layout Standard
Suppose we're interested in the density of 
\begin_inset Formula $y$
\end_inset

 conditional on 
\begin_inset Formula $x$
\end_inset

 (both may be vectors).
 Suppose that the density 
\begin_inset Formula $f(y|x,\phi)$
\end_inset

 is a reasonable starting approximation to the true density.
 This density can be reshaped by multiplying it by a squared polynomial.
 The new density is 
\begin_inset Formula 
\[
g_{p}(y|x,\phi,\gamma)=\frac{h_{p}^{2}(y|\gamma)f(y|x,\phi)}{\eta_{p}(x,\phi,\gamma)}
\]

\end_inset

 where 
\begin_inset Formula 
\[
h_{p}(y|\gamma)=\sum_{k=0}^{p}\gamma_{k}y^{k}
\]

\end_inset

 and 
\begin_inset Formula $\eta_{p}(x,\phi,\gamma)$
\end_inset

 is a normalizing factor to make the density integrate (sum) to one.
 Because 
\begin_inset Formula $h_{p}^{2}(y|\gamma)/\eta_{p}(x,\phi,\gamma)$
\end_inset

 is a homogenous function of 
\begin_inset Formula $\theta$
\end_inset

 it is necessary to impose a normalization to identify the parameters: 
\begin_inset Formula $\gamma_{0}$
\end_inset

 is set to 1.
 The normalization factor 
\begin_inset Formula $\eta_{p}(\phi,\gamma)$
\end_inset

 is calculated (following Cameron and Johansson) using
\begin_inset Formula 
\begin{eqnarray*}
E(Y^{r}) & = & \sum_{y=0}^{\infty}y^{r}f_{Y}(y|\phi,\gamma)\\
 & = & \sum_{y=0}^{\infty}y^{r}\frac{\left[h_{p}\left(y|\gamma\right)\right]^{2}}{\eta_{p}(\phi,\gamma)}f_{Y}(y|\phi)\\
 & = & \sum_{y=0}^{\infty}\sum_{k=0}^{p}\sum_{l=0}^{p}y^{r}f_{Y}(y|\phi)\gamma_{k}\gamma_{l}y^{k}y^{l}/\eta_{p}(\phi,\gamma)\\
 & = & \sum_{k=0}^{p}\sum_{l=0}^{p}\gamma_{k}\gamma_{l}\left\{ \sum_{y=0}^{\infty}y^{r+k+l}f_{Y}(y|\phi)\right\} /\eta_{p}(\phi,\gamma)\\
 & = & \sum_{k=0}^{p}\sum_{l=0}^{p}\gamma_{k}\gamma_{l}m_{k+l+r}/\eta_{p}(\phi,\gamma).
\end{eqnarray*}

\end_inset

By setting 
\begin_inset Formula $r=0$
\end_inset

 we get that the normalizing factor is
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand ref
reference "normfactor"

\end_inset


\begin_inset Formula 
\begin{equation}
\eta_{p}(\phi,\gamma)=\sum_{k=0}^{p}\sum_{l=0}^{p}\gamma_{k}\gamma_{l}m_{k+l}\label{normfactor}
\end{equation}

\end_inset

Recall that 
\begin_inset Formula $\gamma_{0}$
\end_inset

 is set to 1 to achieve identification.
 The 
\begin_inset Formula $m_{r}$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "normfactor"

\end_inset

 are the raw moments of the baseline density.
 Gallant and Nychka (1987) give conditions under which such a density may
 be treated as correctly specified, asymptotically.
 Basically, the order of the polynomial must increase as the sample size
 increases.
 However, there are technicalities.
\end_layout

\begin_layout Standard
Similarly to Cameron and Johannson (1997), we may develop a negative binomial
 polynomial (NBP) density for count data.
 The negative binomial baseline density may be written (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:negbindensity"

\end_inset

) as
\begin_inset Formula 
\[
f_{Y}(y|\phi)=\frac{\Gamma(y+\psi)}{\Gamma(y+1)\Gamma(\psi)}\left(\frac{\psi}{\psi+\lambda}\right)^{\psi}\left(\frac{\lambda}{\psi+\lambda}\right)^{y}
\]

\end_inset

 where 
\begin_inset Formula $\phi=\{\lambda,\psi\},$
\end_inset

 
\begin_inset Formula $\lambda>0$
\end_inset

 and 
\begin_inset Formula $\psi>0$
\end_inset

.
 The usual means of incorporating conditioning variables 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the parameterization 
\begin_inset Formula $\lambda=e^{\mathbf{x}^{\prime}\beta}$
\end_inset

.
 When 
\begin_inset Formula $\psi=\lambda/\alpha$
\end_inset

 we have the negative binomial-I model (NB-I).
 When 
\begin_inset Formula $\psi=1/\alpha$
\end_inset

 we have the negative binomial-II (NP-II) model.
 For the NB-I density, 
\begin_inset Formula $V(Y)=\lambda+\alpha\lambda$
\end_inset

.
 In the case of the NB-II model, we have 
\begin_inset Formula $V(Y)=\lambda+\alpha\lambda^{2}$
\end_inset

.
 For both forms, 
\begin_inset Formula $E(Y)=\lambda$
\end_inset

.
 
\end_layout

\begin_layout Standard
The reshaped density, with normalization to sum to one, is
\begin_inset Formula 
\begin{equation}
f_{Y}(y|\phi,\gamma)=\frac{\left[h_{p}\left(y|\gamma\right)\right]^{2}}{\eta_{p}(\phi,\gamma)}\frac{\Gamma(y+\psi)}{\Gamma(y+1)\Gamma(\psi)}\left(\frac{\psi}{\psi+\lambda}\right)^{\psi}\left(\frac{\lambda}{\psi+\lambda}\right)^{y}.\label{NBP}
\end{equation}

\end_inset

To get the normalization factor, we need the moment generating function:
\begin_inset Formula 
\begin{equation}
M_{Y}(t)=\psi^{\psi}\left(\lambda-e^{t}\lambda+\psi\right)^{-\psi}.\label{nbmgf}
\end{equation}

\end_inset

To illustrate, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Negative-binomial-raw"

\end_inset

 shows calculation of the first four raw moments of the NB density, calculated
 using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{MuPAD}{http://www.mupad.org}
\end_layout

\end_inset

, which is a Computer Algebra System that (used to be?) free for personal
 use.
 These are the moments you would need to use a second order polynomial 
\begin_inset Formula $(p=2)$
\end_inset

.
 MuPAD will output these results in the form of C code, which is relatively
 easy to edit to write the likelihood function for the model.
 This has been done in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{NegBinSNP.cc}{https://github.com/mcreel/Econometrics/blob/master
/MyOctaveFiles/OctFiles/NegBinSNP.cc}
\end_layout

\end_inset

, which is a C++ version of this model that can be compiled to use with
 octave using the 
\family typewriter
mkoctfile
\family default
 command.
 Note the impressive length of the expressions when the degree of the expansion
 is 4 or 5! This is an example of a model that would be difficult to formulate
 without the help of a program like 
\emph on
MuPAD.
\emph default

\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Negative-binomial-raw"

\end_inset

Negative binomial raw moments
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/mupad.png
	width 5in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is possible that there is conditional heterogeneity such that the appropriate
 reshaping should be more local.
 This can be accomodated by allowing the 
\begin_inset Formula $\gamma_{k}$
\end_inset

 parameters to depend upon the conditioning variables, for example using
 polynomials.
\end_layout

\begin_layout Standard
Gallant and Nychka, 
\emph on
Econometrica
\emph default
, 1987 prove that this sort of density can approximate a wide variety of
 densities arbitrarily well as the degree of the polynomial increases with
 the sample size.
 This approach is not without its drawbacks: the sample objective function
 can have an 
\emph on
extremely
\emph default
 large number of local maxima that can lead to numeric difficulties.
 If someone could figure out how to do in a way such that the sample objective
 function was nice and smooth, they would probably get the paper published
 in a good journal.
 Any ideas?
\end_layout

\begin_layout Standard
Here's a plot of true and the limiting SNP approximations (with the order
 of the polynomial fixed) to four different count data densities, which
 variously exhibit over and underdispersion, as well as excess zeros.
 The baseline model is a negative binomial density.
\end_layout

\begin_layout Standard
\begin_inset VSpace 0.5001cm
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Examples/Figures/SNP.pdf
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 0.5001cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
MEPS health care usage data
\end_layout

\begin_layout Standard
We'll use the MEPS OBDV data to illustrate kernel regression and semi-nonparamet
ric maximum likelihood.
\end_layout

\begin_layout Subsubsection
Kernel regression estimation
\end_layout

\begin_layout Standard
Let's try a kernel regression fit for the OBDV data.
 The program 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{OBDVkernel.m}{https://github.com/mcreel/Econometrics/blob/master
/Examples/Nonparametric/OBDVkernel.m} 
\end_layout

\end_inset

 loads the MEPS OBDV data, computes kernel regression estimates using the
 same conditioning variables as in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"

\end_inset

, and plots the fitted OBDV usage versus AGE and INCOME.
 The plots are in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kernel-regression-fits,"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that usage increases with age, just as we've seen with the parametric
 models.
 
\end_layout

\begin_layout Itemize
Note that for income, there is a U shape.
 Previously, we found that income appeared to be insignificant (run EstimatePois
son to see it again).
 
\end_layout

\begin_deeper
\begin_layout Itemize
Perhaps that insignificance was due to omitting a nonlinear effect (e.g.,
 quadratic).
\end_layout

\begin_layout Itemize
The U shape could also be due to ignoring endogeneity of income.
 If a person is seriously ill, they may make more doctor visits, but may
 also suffer loss of income due to reduces work hours.
\end_layout

\begin_layout Itemize
Another explanation might be that kernel regression has a high variance
 in regions of data sparseness, so that for very low or high incomes, an
 outlier or two can have a big impact 
\end_layout

\end_deeper
\begin_layout Itemize
Nonparametric analysis can help us to learn what might be appropriate parametric
 models, by helping to identify potential problems with a parametric model
 
\end_layout

\begin_layout Itemize
Once could use bootstrapping or other methods to generate a confidence intervals
 for the fits.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kernel-regression-fits,"

\end_inset

Kernel regression fits, OBDV health care usage versus AGE and INCOME
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Kernel-fitted-OBDV"

\end_inset

Kernel fitted OBDV usage versus AGE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/kernelfit.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Kernel-fitted-OBDV-income"

\end_inset

Kernel fitted OBDV usage versus INCOME
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/OBDVvsIncome.png
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Financial data and volatility
\end_layout

\begin_layout Standard
The data set 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{rates}{https://github.com/mcreel/Econometrics/blob/master/Examp
les/Nonparametric/SpotRate/rates}
\end_layout

\end_inset

 contains the growth rate (100
\begin_inset Formula $\times$
\end_inset

log difference) of the daily spot $/euro and $/yen exchange rates at New
 York, noon, from January 04, 1999 to February 12, 2008.
 There are 2291 observations.
 See the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{README}{https://github.com/mcreel/Econometrics/blob/master/Exam
ples/Nonparametric/SpotRate/README}
\end_layout

\end_inset

 file for details.
 Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dollar-Euro"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dollar-Yen"

\end_inset

 show the data and their histograms.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dollar-Euro"

\end_inset

Dollar-Euro
\end_layout

\end_inset


\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/Nonparametric/dollar_euro_historgram.png
	lyxscale 10
	width 6cm

\end_inset


\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/Nonparametric/dollar_euro_series.png
	lyxscale 10
	width 6cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dollar-Yen"

\end_inset

Dollar-Yen
\end_layout

\end_inset


\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/Nonparametric/dollar_yen_histogram.png
	lyxscale 10
	width 6cm

\end_inset


\begin_inset Graphics
	filename /home/michael/Mystuff/Econometrics/Examples/Nonparametric/dollar_yen_series.png
	lyxscale 10
	width 6cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
at the center of the histograms, the bars extend above the normal density
 that best fits the data, and the tails are fatter than those of the best
 fit normal density.
 This feature of the data is known as 
\emph on
leptokurtosis
\begin_inset Index idx
status open

\begin_layout Plain Layout
leptokurtosis
\end_layout

\end_inset


\emph default
.
\end_layout

\begin_layout Itemize
in the series plots, we can see that the variance of the growth rates is
 not constant over time.
 Volatility clusters are apparent, alternating between periods of stability
 and periods of more wild swings.
 This is known as 
\begin_inset Index idx
status open

\begin_layout Plain Layout
conditional heteroscedasticity
\end_layout

\end_inset


\emph on
conditional heteroscedasticity
\emph default
.
 
\begin_inset Index idx
status open

\begin_layout Plain Layout
ARCH
\end_layout

\end_inset

ARCH and 
\begin_inset Index idx
status open

\begin_layout Plain Layout
GARCH
\end_layout

\end_inset

GARCH well-known models that are often applied to this sort of data.
\end_layout

\begin_layout Itemize
Many structural economic models often cannot generate data that exhibits
 conditional heteroscedasticity without directly assuming shocks that are
 conditionally heteroscedastic.
 It would be nice to have an economic explanation for how conditional heterosced
asticity, leptokurtosis, and other (leverage, etc.) features of financial
 data result from the behavior of economic agents, rather than from a black
 box that provides shocks.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{kernelfit.m}{https://github.com/mcreel/Econometrics/blob/master/
Examples/Nonparametric/SpotRate/kernelfit.m}
\end_layout

\end_inset

 performs kernel regression to fit 
\begin_inset Formula $E(y_{t}^{2}|y_{t-1,}^{2}y_{t-2}^{2})$
\end_inset

, and generates the plots in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kernel reg spot rates"

\end_inset

.
\end_layout

\begin_layout Itemize
From the point of view of learning the practical aspects of kernel regression,
 note how the data is compactified in the example script.
\end_layout

\begin_layout Itemize
In the Figure, note how current volatility depends on lags of the squared
 return rate - it is high when both of the lags are high, but drops off
 quickly when either of the lags is low.
\end_layout

\begin_layout Itemize
The fact that the plots are not flat suggests that this conditional moment
 contain information about the process that generates the data.
 Perhaps attempting to match this moment might be a means of estimating
 the parameters of the dgp.
 We'll come back to this later.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kernel reg spot rates"

\end_inset

Kernel regression fitted conditional second moments, Yen/Dollar and Euro/Dollar
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Yen/Dollar
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/SpotRate/yendollar.png
	width 8cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Euro/Dollar
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Nonparametric/SpotRate/eurodollar.png
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Additional kernel regression examples
\end_layout

\begin_layout Standard
There is a basic example of kernel regression and kernel density estimation
 in 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{kernel
\backslash
_example.m}{https://github.com/mcreel/Econometrics/blob/master/MyOctaveFiles/Econo
metrics/Kernel/kernel
\backslash
_example.m} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
There is another example of local constant and local linear kernel regression
 in 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{kernel
\backslash
_local
\backslash
_linear
\backslash
_example.m}{https://github.com/mcreel/Econometrics/blob/master/MyOctaveFiles/Econo
metrics/Kernel/kernel
\backslash
_local
\backslash
_linear
\backslash
_example.m} 
\end_layout

\end_inset

.
 With that, you can experiment with different bandwidths.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
Seminonparametric ML estimation and the MEPS data
\end_layout

\begin_layout Standard
Now let's estimate a seminonparametric density for the OBDV data.
 We'll reshape a negative binomial density, as discussed above.
 The program 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{EstimateNBSNP.m}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/Nonparametric/EstimateNBSNP.m} 
\end_layout

\end_inset

 loads the MEPS OBDV data and estimates the model, using a NB-I baseline
 density and a 2nd order polynomial expansion.
 The output is:
\end_layout

\begin_layout Standard
\paragraph_spacing single
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/Nonparametric/NBSNP.out"

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that the CAIC and BIC are lower for this model than for the models
 presented in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Information-Criteria,-OBDV"

\end_inset

.
 This model fits well, still being parsimonious.
 You can play around trying other use measures, using a NP-II baseline density,
 and using other orders of expansions.
 Density functions formed in this way may have 
\series bold
MANY
\series default
 local maxima, so you need to be careful before accepting the results of
 a casual run.
 To guard against having converged to a local maximum, one can try using
 multiple starting values, or one could try simulated annealing as an optimizati
on method.
 If you uncomment the relevant lines in the program, you can use SA to do
 the minimization.
 This will take a 
\emph on
lot
\emph default
 of time, compared to the default BFGS minimization.
 The chapter on parallel computations might be interesting to read before
 trying this.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Limited-information-nonparametri"

\end_inset

Limited information nonparametric filtering
\end_layout

\begin_layout Standard
Add discussion from JEF paper.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Exercises
\end_layout

\begin_layout Enumerate
In Octave, type 
\begin_inset Quotes sld
\end_inset


\family typewriter
edit kernel_example
\family default

\begin_inset Quotes srd
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Look this script over, and describe in words what it does.
\end_layout

\begin_layout Enumerate
Run the script and interpret the output.
\end_layout

\begin_layout Enumerate
Experiment with different bandwidths, and comment on the effects of choosing
 small and large values.
\end_layout

\end_deeper
\begin_layout Enumerate
In Octave, type 
\begin_inset Quotes sld
\end_inset


\family typewriter
help kernel_regression
\family default

\begin_inset Quotes srd
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
How can a kernel fit be done without supplying a bandwidth?
\end_layout

\begin_layout Enumerate
How is the bandwidth chosen if a value is not provided?
\end_layout

\begin_layout Enumerate
What is the default kernel used?
\end_layout

\end_deeper
\begin_layout Enumerate
Using the Octave script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
htmladdnormallink{OBDVkernel.m}{https://github.com/mcreel/Econometrics/blob/master
/Examples/Nonparametric/OBDVkernel.m} 
\end_layout

\end_inset

 as a model, plot kernel regression fits for OBDV visits as a function of
 income and education.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Quantile regression
\end_layout

\begin_layout Standard
References: 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Chapter 4, 
\begin_inset CommandInset citation
LatexCommand citet
key "koenker1978"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "koenker2001quantile"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHansen"
literal "true"

\end_inset

, and Chernozhukov's MIT OpenCourseWare notes, lecture 8 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "Chernozhukov's quantile reg notes"
target "http://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/lecture08.pdf"
literal "false"

\end_inset


\bar default
.
\end_layout

\begin_layout Standard
This chapter gives an outline of quantile regression.
 The quantile IV estimator provides an opportunity to explore MCMC methods.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\series bold
Conditional quantile, definition
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\alpha$
\end_inset

 quantile of a random variable 
\begin_inset Formula $Y$
\end_inset

, conditional on 
\begin_inset Formula $X=x$
\end_inset

 (notation: 
\begin_inset Formula $Y_{\alpha|X=x})$
\end_inset

 is the smallest value 
\begin_inset Formula $z$
\end_inset

 such that 
\begin_inset Formula $Pr(Y\leq z|X=x)=\alpha$
\end_inset

.
 
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $F_{Y|X=x}$
\end_inset

 is the conditional CDF of 
\begin_inset Formula $Y,$
\end_inset

 then the 
\begin_inset Formula $\alpha$
\end_inset

-conditional quantile is 
\begin_inset Formula 
\[
Y_{\alpha|X=x}=\text{\ensuremath{\inf}\,}y:\alpha\leq F_{Y|X=x}(y).
\]

\end_inset

 
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $\alpha=0.5,$
\end_inset

 we are talking about the conditional median 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $Y_{0.5|X=x}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
, but we could be interested in other quantiles, too.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The linear regression model is focused on the conditional mean of the dependent
 variable.
 
\end_layout

\begin_layout Itemize
However, when looking at economic policies, we're often interested in distributi
onal effects:
\end_layout

\begin_deeper
\begin_layout Itemize
we may like to know how the rich and poor may be differentially affected
 by a policy that provides a public good
\end_layout

\begin_layout Itemize
or we might like to know how a training program affects low-performing students
 compared to high-performing students 
\end_layout

\end_deeper
\begin_layout Itemize
For these sorts of issues, we're not concerned with the average agent: we
 want to know about the extremes, too.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Quantiles of the linear regression model
\end_layout

\begin_layout Standard
The classical linear regression model 
\begin_inset Formula $y_{t}=x_{t}^{\prime}\beta+\epsilon_{t}$
\end_inset

 with normal errors implies that the distribution of 
\begin_inset Formula $y_{t}$
\end_inset

 conditional on 
\begin_inset Formula $x_{t}$
\end_inset

 is
\begin_inset Formula 
\[
y_{t}\sim N(x_{t}^{\prime}\beta,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $Pr(Y<x^{\prime}\beta|X=x)=0.5$
\end_inset

 when the model follows the classical assumptions with normal errors, because
 the normal distribution is symmetric about the mean, so the mean and the
 median are the same, that is, 
\begin_inset Formula $Y_{0.5|X=x}=x^{\prime}\beta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
One can estimate the conditional median just by using the fitted conditional
 mean, because the mean and median are the same, given normality.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
How about other quantiles? We have 
\begin_inset Formula $y=x^{\prime}\beta+\epsilon$
\end_inset

 and 
\begin_inset Formula $\epsilon\sim N(0,\sigma^{2})$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Conditional on 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $x^{\prime}\beta$
\end_inset

 is given, and the distribution of 
\begin_inset Formula $\epsilon$
\end_inset

 does not depend on 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\epsilon/\sigma$
\end_inset

 is standard normal, and the 
\begin_inset Formula $\alpha$
\end_inset

 quantile of 
\begin_inset Formula $\epsilon/\sigma$
\end_inset

 is simply the inverse of the standard normal CDF evaulated at 
\begin_inset Formula $\alpha,$
\end_inset

 
\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

, where 
\begin_inset Formula $\Phi$
\end_inset

 is the standard normal CDF function.
 
\end_layout

\begin_layout Itemize
The probit function 
\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

 is tabulated (or can be found in Julia using 
\family typewriter
using Distributions; y = quantile.(Normal(),range(0.001,stop=0.999,length=200))
\family default
.
 It is plotted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inverse-CDF-for"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Inverse-CDF-for"

\end_inset

Inverse CDF for N(0,1)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/norminv.svg
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\alpha$
\end_inset

 quantile of 
\begin_inset Formula $\epsilon$
\end_inset

 is 
\begin_inset Formula $\sigma\Phi^{-1}(\alpha).$
\end_inset

 Thus, the 
\begin_inset Formula $\alpha$
\end_inset

 conditional quantile of 
\begin_inset Formula $y$
\end_inset

 is 
\begin_inset Formula $Y_{\alpha|X=x}=x^{\prime}\beta+$
\end_inset


\begin_inset Formula $\sigma$
\end_inset


\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

.
 Some quantiles are pictured in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Quantiles-of-classical"

\end_inset

.
 These give confidence intervals for the the fitted value, 
\begin_inset Formula $x^{\prime}\beta$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Quantiles-of-classical"

\end_inset

Quantiles of classical linear regression model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/quantiles.jpg
	lyxscale 25
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The conditional quantiles for the classical model are 
\emph on
parallel
\emph default
, 
\emph on
linear
\emph default
 functions of 
\begin_inset Formula $x$
\end_inset


\end_layout

\begin_layout Itemize
all have the same slope: the only thing that changes with 
\begin_inset Formula $\alpha$
\end_inset

 is the intercept 
\begin_inset Formula $\sigma$
\end_inset


\begin_inset Formula $\Phi^{-1}(\alpha)$
\end_inset

.
\end_layout

\begin_layout Itemize
If the error is heteroscedastic, so that 
\begin_inset Formula $\sigma=\sigma(x)$
\end_inset

, quantiles can have 
\bar under
different slopes
\bar default
, and given quantiles may be nonlinear functions of 
\begin_inset Formula $x$
\end_inset

, depending on the form of the heteroscedasticity.
 
\emph on
Draw a picture.
\emph default

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Fully nonparametric conditional quantiles
\end_layout

\begin_layout Standard
To compute conditional quantiles for the classical linear model, we used
 the assumption of normality.
 Can we estimate conditional quantiles without making distributional assumptions
? Yes, we can! (nod to Obama) (a note from 2018: those were the good old
 days!).
 You can do fully nonparametric conditional density estimation, as in Chapter
 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Nonparametric-inference"

\end_inset

, and use the fitted conditional density to compute quantiles.
\end_layout

\begin_layout Itemize
Note that estimating quantiles where 
\begin_inset Formula $\alpha$
\end_inset

 is close to 0 or 1 is difficult, because you have few observations that
 lie in the neighborhood of the quantile, so you should expect a large variance
 if you go the nonparametric route.
 For more central quantiles, like the median, this will be less of a problem.
\end_layout

\begin_layout Itemize
For this reason, we may go the 
\emph on
semi-parametric
\emph default
 route, which imposes more structure.
 When people talk about quantile regression, they usually mean the semi-parametr
ic approach.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Quantile regression as a semi-parametric estimator
\end_layout

\begin_layout Standard
The most widely used method does not take either of the extreme positions,
 it is not fully parametric, like the linear regression model with known
 distribution of errors, but some parametric restrictions are made, to improve
 efficiency compared to the fully nonparametric approach.
\end_layout

\begin_layout Itemize
The assumption is that the 
\begin_inset Formula $\alpha$
\end_inset

-conditional quantile of the dependent variable 
\begin_inset Formula $Y$
\end_inset

 is a linear function of the conditioning variables 
\begin_inset Formula $X$
\end_inset

: 
\begin_inset Formula $Y_{\alpha|X=x}=x^{\prime}\beta_{\alpha}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
This is a generalization of what we get from the classical model with normality,
 where the slopes of the quantiles with respect to the regressors are constant
 for all 
\begin_inset Formula $\alpha$
\end_inset

: 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For the classical model with normality, 
\begin_inset Formula $\frac{\partial}{\partial x}Y_{\alpha|X=x}=\beta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
With the assumption of linear quantiles without distributional assumptions,
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\frac{\partial}{\partial x}Y_{\alpha|X=x}=\beta_{\alpha}$
\end_inset

, so the slopes (and constants) are allowed to change with 
\begin_inset Formula $\alpha$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
This is a step in the direction of flexibility, but it also means we need
 to estimate many parameters if we're interested in many quantiles: there
 may be an efficiency loss due to using many parameters to avoid distributional
 assumptions.
\end_layout

\begin_layout Itemize
The question is how to estimate 
\begin_inset Formula $\beta_{\alpha}$
\end_inset

 when we don't make distributional assumptions.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
It turns out that the problem can be expressed as an extremum estimator:
 
\begin_inset Formula $\widehat{\beta_{\alpha}}=\arg\min s_{n}(\beta)$
\end_inset

 where
\begin_inset Formula 
\[
s_{n}(\beta)=\sum_{i=1}^{n}\left[1(y_{i}\geq x_{i}^{\prime}\beta_{\alpha})\alpha+1(y_{i}<x_{i}^{\prime}\beta_{\alpha})(1-\alpha)\right]\left|y_{i}-x_{i}^{\prime}\beta_{\alpha}\right|
\]

\end_inset

First, suppose that 
\begin_inset Formula $\alpha=0.5,$
\end_inset

 so we are estimating the median.
 Then the objective simplifies to minimizing the absolute deviations:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{n}(\beta)=\sum_{i=1}^{n}\left|y_{i}-x_{i}^{\prime}\beta_{\alpha}\right|
\]

\end_inset


\end_layout

\begin_layout Standard
The presence of the weights in the general version accounts for the fact
 that if we're estimating the 
\begin_inset Formula $\alpha=0.1$
\end_inset

 quantile, we expect 90% of the 
\begin_inset Formula $y_{i}$
\end_inset

 to be greater than 
\begin_inset Formula $x_{i}^{\prime}\beta_{\alpha}$
\end_inset

, and only 10% to be smaller.
 We need to down-weight the likely events and up-weight the unlikely events
 so that the objective function minimizes at the appropriate place.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
One note is that median regression may be a useful means of dealing with
 data that satisfies the classical assumptions, except for contamination
 by outliers.
 
\emph on
In class, use Gretl to show this.
\end_layout

\begin_layout Itemize
Note that the quantile regression objective function is discontinuous.
 Minimization can be done quickly using linear programming.
 BFGS won't work.
\end_layout

\begin_layout Itemize
the asymptotic distribution is normal, with the sandwich form typical of
 extremum estimators.
 Estimation of the terms is not completely straightforward, so methods like
 bootstrapping may be preferable.
\end_layout

\begin_layout Itemize
the asymptotic variance depends upon which quantile we're estimating.
 When 
\begin_inset Formula $\alpha$
\end_inset

 is close to 0 or 1, the asymptotic variance becomes large, and the asymptotic
 approximation is unreliable for the small sample distribution.
\end_layout

\begin_layout Itemize
Extreme quantiles are hard to estimate with precision, because the data
 is sparse in those regions.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
The artificial data set 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{quantile.gdt}{https://github.com/mcreel/Econometrics/blob/master
/Examples/Quantile/quantile.gdt} 
\end_layout

\end_inset

 allows you to explore quantile regression with GRETL, and to see how median
 regression can help to deal with data contamination.
 
\end_layout

\begin_layout Itemize
If you do quantile regression of the variable y versus x, we are in a situation
 where the assumptions of the classical model hold.
 Quantiles all have approximately the same slope (the true value is 1).
\end_layout

\begin_layout Itemize
With heteroscedastic data, the quantiles have different slopes.
\end_layout

\begin_layout Itemize
see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Quantile-regression-results"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Quantile-regression-results"

\end_inset

Quantile regression results
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
homoscedastic data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/qreg1.png
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
heteroscedastic data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/qreg2.png
	lyxscale 25
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Exercise
Suppose that 
\begin_inset Formula $y$
\end_inset

 depends on a single regressor, 
\begin_inset Formula $x$
\end_inset

.
 Think about how you could do quantile regression estimation where the quantiles
 are nonlinear functions of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Returns to schooling: quantile regression, quantile IV regression, and Bayesian
 GMM via MCMC
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "card1993using"
literal "true"

\end_inset

 presents an analysis of returns to schooling using the data from the National
 Longitudinal Survey of Young Men, for those interviewed in 1976.
 Card presents OLS and instrumental variables estimates for a number of
 specifications, using college proximity as an instrument for years of education
, and age as an instrument for experience.
 Here, we work with the simple model from column (1) of Card's Table 2.
 Let's consider estimation of conditional quantiles for the model.
 The model is 
\begin_inset Formula 
\begin{align*}
Q_{\ln W|X}(\tau) & =\beta_{0}(\tau)+\beta_{EDUC}(\tau)EDUC+\beta_{X}(\tau)EXP+\beta_{EXP^{2}}(\tau)\frac{EXP^{2}}{100}\\
 & +\beta_{BLACK}(\tau)BLACK+\beta_{SMSA}(\tau)SMSA+\beta_{SOUTH}(\tau)SOUTH\\
 & \equiv X\beta(\tau)
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
the dependent variable 
\begin_inset Formula $\ln W$
\end_inset

 is log hourly earnings (in cents)
\end_layout

\begin_layout Itemize
the regressors are years of education (EDUC), experience (EXP), experience
 squared divided by 100, a black indicator (BLACK), a metropolitan area
 indicator (SMSA), and a South indicator (SOUTH).
\end_layout

\begin_layout Itemize
We explore estimation of quantiles treating all variables as exogenous,
 or treating education and experience as endogenous, and the others as exogenous.
\end_layout

\begin_layout Itemize
When education and experience are treated as endogenous, we use proximity
 to an accredited four year college (NEARC4) as an instrumental variable.
 EXPER is defined as EXPER = AGE-EDUC-6, so if EDUC is endogenous, so is
 EXPER.
 We use AGE as an instrument for EXPER.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
If all variables are taken as exogenous, then quantile regression (QR) estimates
 may be obtained by standard methods, as implemented in the GRETL software
 package.
 
\end_layout

\begin_layout Itemize
The Card data set is provided with the Wooldridge data set for GRETL, see
 the GRETL web page.
 A version prepared for the model used here is 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{card.gdt}{https://github.com/mcreel/Econometrics/blob/master/Exa
mples/Data/card.gdt} 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Itemize
QR results from GRETL for EDUC are in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:QR-results-for"

\end_inset

.
 Note that the QR results are pretty close to the OLS results, for all quantiles
, and there's no clear pattern of the effect of education differing across
 quantiles.
 
\end_layout

\begin_layout Itemize
The effect of an additional year of education on earnings is about 7-8%,
 all across the distribution.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:QR-results-for"

\end_inset

QR results for the Card data, 
\begin_inset Formula $\tau$
\end_inset

 sequence
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{EDUC}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

, with 95% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/QReduc.pdf
	width 12cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{0}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

, with 95% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/QRconst.pdf
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
If education and experience are taken as endogenous, ordinary quantile regressio
n will give biased estimates, just as OLS is biased and inconsistent with
 endogenous regressors.
 
\end_layout

\begin_layout Itemize
We may use an instrumental variables version of quantile regression, due
 to 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHansen"
literal "true"

\end_inset

.
 They show that the moment conditions 
\begin_inset Formula 
\[
m_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}Z_{i}\left(\tau-1\left[y_{i}\le X_{i}\beta(\tau)\right]\right)
\]

\end_inset

(where 
\begin_inset Formula $\theta=\beta(\tau))$
\end_inset

 have expectation zero at the true parameter values, and thus can be used
 for GMM estimation.
 
\end_layout

\begin_layout Itemize
We can show that, at the true parameter values 
\begin_inset Formula 
\[
\sqrt{n}m_{n}(\theta_{0})\rightarrow^{d}N(0,(\tau-\tau^{2})Q_{Z})
\]

\end_inset

so an estimate of the efficient weight matrix is the inverse of 
\begin_inset Formula $\hat{\Sigma}=\frac{(\tau-\tau^{2})}{n}\sum_{i}Z_{i}Z_{i}^{\prime}.$
\end_inset

 
\end_layout

\begin_layout Itemize
The problem is that these moment conditions are discontinuous functions
 of the parameters, due to the indicator function, so gradient-based optimizatio
n methods will not work for computing the GMM estimates.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
To deal with this problem, we can consider using the MCMC methods proposed
 by 
\begin_inset CommandInset citation
LatexCommand citet
key "ChernozhukovHong2003"
literal "true"

\end_inset

 to compute a Bayesian version of the GMM estimator.
 
\end_layout

\begin_layout Itemize
This estimator works with the asymptotic distribution of the moment conditions
 to define the likelihood used in MCMC, rather than the full sample likelihood
 function, but otherwise, it is standard MCMC.
\end_layout

\begin_deeper
\begin_layout Itemize
the use of moment conditions is a dimension reducing operation: the full
 sample likelihood requires knowing the distribution of 
\begin_inset Formula $n$
\end_inset

 (growing with the sample size) random variables, while the use of moment
 conditions and their asymptotic distribution only requires knowing the
 (asymptotic) distribution of 
\begin_inset Formula $G$
\end_inset

 (fixed and finite) random variables
\end_layout

\begin_layout Itemize
thus, GMM is like a limited information ML estimator, with the asymptotic
 distribution substituting the actual small sample distribution.
\end_layout

\end_deeper
\begin_layout Itemize
The model is implemented in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{QIVmodel.jl}{https://github.com/mcreel/Econometrics/blob/master/
Examples/Quantile/QIVmodel.jl} 
\end_layout

\end_inset

, and the estimation by MCMC is done in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{QIVbyMCMC}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/Quantile/QIVbyMCMC.jl} 
\end_layout

\end_inset

.
 It may be of interest to examine the code to see how posterior means and
 90% confidence intervals are computed using the Chernozhukhov-Hong method.
\end_layout

\begin_layout Itemize
For those of you interested in MCMC, there is the file 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{PlayWithMCMC.jl}{https://github.com/mcreel/Econometrics/blob/mas
ter/Examples/Quantile/PlayWithMCMC.jl} 
\end_layout

\end_inset

, which studies the MCMC chain a bit.
 The basic proposal draws the parameter from independent normal densities.
 The second proposal draws the parameters from a joint normal density that
 accounts for correlations in the posterior.
 The empirical results reported below don't depend on which proposal is
 used, though, as a long enough chain was used so that the difference washes
 out.
 This is an issue of computational efficiency, not one of statistical reliabilit
y.
 To obtain reliable results with a shorter chain, the proposal density should
 be chosen to ensure good mixing (sampling from the whole support of the
 posterior).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Two-chains-for"

\end_inset

Two chains for 
\begin_inset Formula $\beta_{0}(\tau=0.5),$
\end_inset

 independent and correlated proposals
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Independent proposals, poor mixing
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/chain1.svg
	width 20cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Correlated proposals, better mixing
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/chain2.svg
	width 20cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The results are in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:IV-QR-results"

\end_inset

, for 
\begin_inset Formula $\beta_{EDUC}(\tau)$
\end_inset

 and 
\begin_inset Formula $\beta_{0}(\tau)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
We can see that the IVQR results are substantially different from the ordinary
 QR results in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:QR-results-for"

\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The effect of education, according to the IVQR results, is substantially
 larger, for all quantiles, with an additional year of education increasing
 all quantiles, except the 40th, by more than 20%.
 This is good news for the people in the U.S.
 that have to take out enormous student loans.
 Given the cost of college tuition in the U.S., the miserable 7% return that
 OLS and ordinary QR find would probably not be enough to induce people
 to take out loans.
 So, we have external reasons to believe that this higher number may be
 more realistic.
 It would be interesting to study the evolution of returns over time, and
 compare them to the cost of education.
\end_layout

\begin_layout Itemize
There is a U shape, with a greater effect at the lower and higher quantiles.
 
\end_layout

\end_deeper
\begin_layout Itemize
The confidence bands are broader for the IV version, which is to be expected.
 This is similar to what happens with ordinary IV and OLS.
\end_layout

\begin_layout Itemize
The results are quite similar to those of 
\begin_inset CommandInset citation
LatexCommand citet
key "chernozhukovHansen2006instrumental"
literal "true"

\end_inset

, who estimate a similar model using the 
\begin_inset CommandInset citation
LatexCommand citet
key "angrist1991does"
literal "true"

\end_inset

 data (this is the influential paper that used quarter of birth as an instrument
 for education).
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:IV-QR-results"

\end_inset

IV-QR results
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{EDUC}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

, with 90% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/Educ.svg
	width 12cm

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated 
\begin_inset Formula $\beta_{0}(\tau)$
\end_inset

 as a function of 
\begin_inset Formula $\tau$
\end_inset

, with 90% confidence band
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Quantile/Constant.svg
	width 12cm

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Simulation-based methods for estimation and inference
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
: 
\begin_inset CommandInset citation
LatexCommand cite
key "cameron2005microeconometrics"
literal "true"

\end_inset

, Ch.
 12; 
\begin_inset CommandInset citation
LatexCommand citet
key "gourieroux1996simulation"
literal "true"

\end_inset

.
 There are many articles.
 Some of the seminal papers are 
\begin_inset CommandInset citation
LatexCommand cite
key "McFadden1989MSM"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "PakesPollard"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "GourierouxMonfortIndirect"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "smith1993estimating"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "duffie1993simulated"
literal "true"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "emm"
literal "true"

\end_inset


\emph on
.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
Human brain power is perhaps growing over time, but not as fast as thumb
 dexterity, I would argue.
 
\end_layout

\begin_layout Itemize
On the other hand, computing power is growing more or less exponentially,
 according to 
\begin_inset CommandInset href
LatexCommand href
name "Moore's Law"
target "https://en.wikipedia.org/wiki/Moore%27s_law"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Any economist would argue that we need to use inputs in proportion to their
 relative prices, which means that we should increasingly be using computers
 to make advancements in econometrics (and, maybe, our thumbs).
 
\end_layout

\begin_layout Itemize
Simulation-based methods do just that.
 When intensive use of computer power is contemplated, it is possible to
 do things that are otherwise infeasible:
\end_layout

\begin_deeper
\begin_layout Itemize
obtaining more accurate results that what asymptotic theory gives us, using
 methods like bootstrapping,
\end_layout

\begin_layout Itemize
performing estimation of models that are complex enough so that analytic
 expressions for objective functions that define conventional estimators
 (e.g., ML, GMM) are not available.
 Once you go down this rabbit hole, you can estimate 
\emph on
very 
\emph default
complex models.
\end_layout

\end_deeper
\begin_layout Itemize
Simulation based estimation, especially the simulated method of moments,
 has become quite standard in applied research, so it is important to understand
 how it works
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Motivation
\end_layout

\begin_layout Standard
Simulation methods are of interest when the DGP
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

is fully characterized by a parameter vector, so that simulated data can
 be generated, but the likelihood function and analytic moments of the observabl
e variables are not calculable, so that MLE or GMM estimation is not possible.
 
\end_layout

\begin_layout Itemize
Many moderately complex models result in intractable likelihoods or moments,
 as we will see.
\end_layout

\begin_layout Itemize
Simulation-based estimation methods open up the possibility to estimate
 truly complex models.
 
\end_layout

\begin_layout Itemize
The desirability introducing a great deal of complexity may be an issue
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Remember that a model is an abstraction from reality, and abstraction helps
 us to isolate the important features of a phenomenon.
\end_layout

\end_inset

, but it least it becomes a possibility.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example: Multinomial and/or dynamic discrete response models
\end_layout

\begin_layout Standard
(following 
\begin_inset CommandInset citation
LatexCommand cite
key "McFadden1989MSM"
literal "true"

\end_inset

, which is one of the seminal articles behind the simulation-based estimation
 boom)
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $y_{i}^{*}$
\end_inset

 be a latent random vector of dimension 
\begin_inset Formula $m.$
\end_inset

 Suppose that 
\begin_inset Formula 
\[
y_{i}^{*}=X_{i}\beta+\varepsilon_{i}
\]

\end_inset

 where 
\begin_inset Formula $X_{i}$
\end_inset

 is 
\begin_inset Formula $m\times K.$
\end_inset

 Suppose that 
\begin_inset Formula 
\begin{equation}
\varepsilon_{i}\sim N(0,\Omega)\label{MNPerrors}
\end{equation}

\end_inset

 Henceforth drop the 
\begin_inset Formula $i$
\end_inset

 subscript when it is not needed for clarity.
\end_layout

\begin_layout Itemize
\begin_inset Formula $y^{*}$
\end_inset

 is not observed.
 Rather, we observe a many-to-one mapping 
\begin_inset Formula 
\[
y=\tau(y^{*})
\]

\end_inset

 This mapping is such that each element of 
\begin_inset Formula $y$
\end_inset

 is either zero or one (in some cases only one element will be one).
\end_layout

\begin_layout Itemize
Define 
\begin_inset Formula 
\[
A_{i}=A(y_{i})=\{y^{*}|y_{i}=\tau(y^{*})\}
\]

\end_inset

 Suppose random sampling of 
\begin_inset Formula $(y_{i},X_{i})$
\end_inset

.
 In this case the elements of 
\begin_inset Formula $y_{i}$
\end_inset

 may not be independent of one another (and clearly are not if 
\begin_inset Formula $\Omega$
\end_inset

 is not diagonal).
 However, 
\begin_inset Formula $y_{i}$
\end_inset

 is independent of 
\begin_inset Formula $y_{j}$
\end_inset

, 
\begin_inset Formula $i\neq j.$
\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\theta=(\beta^{\prime},(vec^{*}\Omega)^{\prime})^{\prime}$
\end_inset

 be the vector of parameters of the model.
 The contribution of the 
\begin_inset Formula $i^{th}$
\end_inset

 observation to the likelihood function is 
\begin_inset Formula 
\[
p_{i}(\theta)=\int_{A_{i}}\phi(y_{i}^{*}-X_{i}\beta,\Omega)dy_{i}^{*}
\]

\end_inset

 where 
\begin_inset Formula 
\[
\phi(\varepsilon,\Omega)=(2\pi)^{-M/2}\left|\Omega\right|^{-1/2}\exp\left[\frac{-\varepsilon^{\prime}\Omega^{-1}\varepsilon}{2}\right]
\]

\end_inset

 is the multivariate normal density of an 
\begin_inset Formula $M$
\end_inset

 -dimensional random vector.
 The log-likelihood function is 
\begin_inset Formula 
\[
\ln\mathcal{L}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\ln p_{i}(\theta).
\]

\end_inset

 
\end_layout

\begin_layout Itemize
The problem is that evaluation of 
\begin_inset Formula $\mathcal{L}_{i}(\theta)$
\end_inset

 and its derivative w.r.t.
 
\begin_inset Formula $\theta$
\end_inset

 by standard methods of numeric integration such as quadrature is computationall
y infeasible when 
\begin_inset Formula $m$
\end_inset

 (the dimension of 
\begin_inset Formula $y)$
\end_inset

 is higher than 3 or 4 (as long as there are no restrictions on 
\begin_inset Formula $\Omega).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
The mapping 
\begin_inset Formula $\tau(y^{*})$
\end_inset

 has not been made specific so far.
 This setup is quite general:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\end_layout

\end_inset

for different choices of 
\begin_inset Formula $\tau(y^{*})$
\end_inset

 it nests the case of dynamic binary discrete choice models as well as the
 case of multinomial discrete choice (the choice of one out of a finite
 set of alternatives).
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Multinomial discrete choice is illustrated by a (very simple) job search
 model.
 We have cross sectional data on individuals' matching to a set of 
\begin_inset Formula $m$
\end_inset

 jobs that are available (one of which is unemployment).
 The utility of alternative 
\begin_inset Formula $j$
\end_inset

 is 
\begin_inset Formula 
\[
u_{j}=X_{j}\beta+\varepsilon_{j}
\]

\end_inset

 Utilities of jobs, stacked in the vector 
\begin_inset Formula $u_{i}$
\end_inset

 are not observed.
 Rather, we observe the vector formed of elements 
\begin_inset Formula 
\[
y_{j}=1\left[u_{j}>u_{k},\forall k\in m,k\neq j\right]
\]

\end_inset

 Only one of these elements is different than zero.
\end_layout

\begin_layout Itemize
Dynamic discrete choice is illustrated by repeated choices over time between
 two alternatives.
 Let alternative 
\begin_inset Formula $j$
\end_inset

 have utility 
\begin_inset Formula 
\begin{eqnarray*}
u_{jt} & = & W_{jt}\beta-\varepsilon_{jt},\\
j & \in & \{1,2\}\\
t & \in & \{1,2,...,m\}
\end{eqnarray*}

\end_inset

 Then 
\begin_inset Formula 
\begin{eqnarray*}
y^{*} & = & u_{2}-u_{1}\\
 & = & (W_{2}-W_{1})\beta+\varepsilon_{2}-\varepsilon_{1}\\
 & \equiv & X\beta+\varepsilon
\end{eqnarray*}

\end_inset

 Now the mapping is (element-by-element) 
\begin_inset Formula 
\[
y=1\left[y^{*}>0\right],
\]

\end_inset

 that is 
\begin_inset Formula $y_{it}=1$
\end_inset

 if individual 
\begin_inset Formula $i$
\end_inset

 chooses the second alternative in period 
\begin_inset Formula $t,$
\end_inset

 zero otherwise.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Example:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\end_layout

\end_inset

Marginalization of latent variables
\end_layout

\begin_layout Standard
Economic data often presents substantial heterogeneity that may be difficult
 to model.
 A possibility is to introduce latent random variables.
 This can cause the problem that there may be no known closed form for the
 distribution of observable variables after marginalizing out the unobservable
 latent variables.
 For example, count data (that takes values 
\begin_inset Formula $0,1,2,3,...)$
\end_inset

 is often modeled using the Poisson distribution 
\begin_inset Formula 
\[
\Pr(Y=y_{i})=\frac{\exp(-\lambda)\lambda^{i}}{y_{i}!}
\]

\end_inset

 The mean and variance of the Poisson distribution are both equal to 
\begin_inset Formula $\lambda:$
\end_inset


\begin_inset Formula 
\[
\mathcal{E}(y)=V(y)=\lambda.
\]

\end_inset

 Often, one parameterizes the conditional mean as 
\begin_inset Formula 
\[
\lambda_{i}=\exp(X_{i}\beta).
\]

\end_inset

 This ensures that the mean is positive (as it must be).
 Estimation by ML is straightforward.
\end_layout

\begin_layout Standard
Often, count data exhibits 
\begin_inset Quotes eld
\end_inset

overdispersion
\begin_inset Quotes erd
\end_inset

 which simply means that 
\begin_inset Formula 
\[
V(y)>\mathcal{E}(y).
\]

\end_inset

 If this is the case, a solution is to use the negative binomial distribution
 rather than the Poisson.
 An alternative is to introduce a latent variable that reflects heterogeneity
 into the specification: 
\begin_inset Formula 
\[
\lambda_{i}=\exp(X_{i}\beta+\eta_{i})
\]

\end_inset

 where 
\begin_inset Formula $\eta_{i}$
\end_inset

 has some specified density with support 
\begin_inset Formula $S$
\end_inset

 (this density may depend on additional parameters).
 Let 
\begin_inset Formula $d\mu(\eta_{i})$
\end_inset

 be the density of 
\begin_inset Formula $\eta_{i}.$
\end_inset

 The marginal density of 
\begin_inset Formula $y$
\end_inset

 is
\begin_inset Formula 
\[
\Pr(Y=y_{i}|X_{i})=\int_{S}\frac{\exp\left[-\exp(X_{i}\beta+\eta_{i})\right]\left[\exp(X_{i}\beta+\eta_{i})\right]^{y_{i}}}{y_{i}!}d\mu(\eta_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize
In some cases, this will have a closed-form solution (one can derive the
 negative binomial distribution in this way if 
\begin_inset Formula $\eta$
\end_inset

 has an exponential distribution - see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:negbindensity"

\end_inset

)
\end_layout

\begin_layout Itemize
Often this will not be possible.
 In this case, simulation is a means of calculating 
\begin_inset Formula $\Pr(Y=y_{i}|X_{i}),$
\end_inset

 which is then used to do ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimation.
 This would be an example of the Simulated Maximum Likelihood (SML) estimation.
 
\end_layout

\begin_layout Itemize
In this case, since there is only one latent variable, quadrature is probably
 a better choice.
 But with more random parameters, quadrature becomes too costly and/or inaccurat
e.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Estimation of models specified in terms of stochastic differential equations
\begin_inset CommandInset label
LatexCommand label
name "subsec:sim-baed estimation differential eqns"

\end_inset


\end_layout

\begin_layout Standard
It is often convenient to formulate models in terms of continuous time using
 differential equations.
 An example was the jump-diffusion model discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Diffusion-models"

\end_inset

.
 A realistic model should account for exogenous shocks to the system, which
 can be done by assuming a random component.
 This leads to a model that is expressed as a system of stochastic differential
 equations.
 Consider the process 
\begin_inset Formula 
\[
dy_{t}=g(\theta,y_{t})dt+h(\theta,y_{t})dW_{t}
\]

\end_inset

 which is assumed to be stationary.
 
\begin_inset Formula $\{W_{t}\}$
\end_inset

 is a standard Brownian motion (Weiner process), such that 
\begin_inset Formula 
\[
W(T)=\int_{0}^{T}dW_{t}\sim N(0,T)
\]

\end_inset

 Brownian motion is a continuous-time stochastic process such that
\end_layout

\begin_layout Itemize
\begin_inset Formula $W(0)=0$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\left[W(s)-W(t)\right]\sim N(0,s-t)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\left[W(s)-W(t)\right]$
\end_inset

 and 
\begin_inset Formula $\left[W(j)-W(k)\right]$
\end_inset

 are independent for 
\begin_inset Formula $s>t>j>k.$
\end_inset

 That is, non-overlapping segments are independent.
 
\end_layout

\begin_layout Standard
One can think of Brownian motion the accumulation over time of independent
 normally distributed shocks, each with an infinitesimal variance.
\end_layout

\begin_layout Itemize
The function 
\begin_inset Formula $g(\theta,y_{t})$
\end_inset

 is the deterministic part.
\end_layout

\begin_layout Itemize
\begin_inset Formula $h(\theta,y_{t})$
\end_inset

 determines the instantaneous variance of the shocks.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
To estimate a model of this sort, we typically have data that are assumed
 to be observations of 
\begin_inset Formula $y_{t}$
\end_inset

 in discrete points 
\begin_inset Formula $y_{1},$
\end_inset

 
\begin_inset Formula $y_{2},...y_{T}.$
\end_inset

 That is, although 
\begin_inset Formula $y_{t}$
\end_inset

 is a continuous process, it is observed in discrete time.
 
\emph on
(make a drawing)
\end_layout

\begin_layout Standard
To perform inference on 
\begin_inset Formula $\theta,$
\end_inset

 direct ML or GMM estimation is not usually feasible, because one cannot,
 in general, deduce the discrete time transition density 
\begin_inset Formula $f(y_{t}|y_{t-1},\theta).$
\end_inset

 This density is necessary to evaluate the likelihood function or to evaluate
 moment conditions (which are based upon expectations with respect to this
 density).
\end_layout

\begin_layout Itemize
A typical solution is to 
\begin_inset Quotes eld
\end_inset

discretize
\begin_inset Quotes erd
\end_inset

 the model, by which we mean to find a discrete time approximation to the
 model.
 The discretized version of the model is 
\begin_inset Formula 
\begin{eqnarray*}
y_{t}-y_{t-\Delta} & = & \widetilde{g}(\phi,y_{t-1})\Delta+\sqrt{\Delta}\widetilde{h}(\phi,y_{t-1})\varepsilon_{t}\\
\varepsilon_{t} & \sim & N(0,1)
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\Delta$
\end_inset

 is a discrete time interval
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
I have changed the parameter from 
\begin_inset Formula $\theta$
\end_inset

 to 
\begin_inset Formula $\phi$
\end_inset

 to emphasize that this is an approximation, which will be more or less
 good.
 As such 
\begin_inset Quotes eld
\end_inset

ML
\begin_inset Quotes erd
\end_inset

 estimation of 
\begin_inset Formula $\phi$
\end_inset

 is actually quasi-maximum likelihood estimation.
 When actual data is available on a daily, say, basis, then you could set
 
\begin_inset Formula $\Delta=1,$
\end_inset

 and use the discretized model to do QML estimation.
 However, the time interval 
\begin_inset Formula $\Delta$
\end_inset

 may be too large to give an accurate approximation to the model, and if
 this is the case, the QML estimator could suffer from a large bias for
 estimation of the original parameter, 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Nevertheless, the approximation shouldn't be too bad, especially if 
\begin_inset Formula $\Delta$
\end_inset

 is small.
 For example, one could simulate the model at a frequency of 1 minute, saving
 every 1440th point on the path (60
\begin_inset Formula $\times$
\end_inset

24 = 1440), which would give a good approximation of the evolution of the
 daily observations.
 The 
\begin_inset Quotes sld
\end_inset

Euler approximation
\begin_inset Quotes srd
\end_inset

 method for simulating such models is based upon this fact.
 Simulation-based inference allows for direct inference on 
\begin_inset Formula $\theta$
\end_inset

, which is what we would like to do.
\end_layout

\end_deeper
\begin_layout Itemize
The important point about these three examples is that computational difficultie
s prevent direct application of ML, GMM, etc.
 Nevertheless the model is fully specified in probabilistic terms up to
 a parameter vector.
 This means that the model is simulable, conditional on the parameter vector.
 
\end_layout

\begin_layout Section
Simulated maximum likelihood (SML)
\end_layout

\begin_layout Standard
For simplicity, consider cross-sectional data.
 An ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\end_layout

\end_inset

estimator solves 
\begin_inset Formula 
\[
\hat{\theta}_{ML}=\arg\max s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}\ln f(y_{t}|X_{t},\theta)
\]

\end_inset

 where 
\begin_inset Formula $f(y_{t}|X_{t},\theta)$
\end_inset

 is the density function of the 
\begin_inset Formula $t^{th}$
\end_inset

 observation.
 When 
\begin_inset Formula $f(y_{t}|X_{t},\theta)$
\end_inset

 does not have a known closed form, 
\begin_inset Formula $\hat{\theta}_{ML}$
\end_inset

 is an infeasible estimator.
 However, it may be possible to define a random function such that 
\begin_inset Formula 
\[
\mathcal{E}_{\nu}p(\nu,y_{t}|X_{t},\theta)=f(y_{t}|X_{t},\theta)
\]

\end_inset

 where the density of 
\begin_inset Formula $\nu$
\end_inset

 is known.
 If this is the case, the simulator 
\begin_inset Formula 
\[
\tilde{f}\left(y_{t}|X_{t},\theta\right)=\frac{1}{H}\sum_{s=1}^{H}p(\nu_{ts},y_{t}|X_{t},\theta)
\]

\end_inset

 is unbiased for 
\begin_inset Formula $f(y_{t}|X_{t},\theta).$
\end_inset


\end_layout

\begin_layout Itemize
The SML simply substitutes 
\begin_inset Formula $\tilde{f}\left(y_{t}|X_{t},\theta\right)$
\end_inset

 in place of 
\begin_inset Formula $f(y_{t}|X_{t},\theta)$
\end_inset

 in the log-likelihood function, that is 
\begin_inset Formula 
\[
\hat{\theta}_{SML}=\arg\max s_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\ln\tilde{f}\left(y_{t}|X_{t},\theta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Properties
\end_layout

\begin_layout Standard
The properties of the SML estimator depend on how 
\begin_inset Formula $H$
\end_inset

 is set.
 The following is taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "lee1995asymptotic"
literal "true"

\end_inset

.
\end_layout

\begin_layout Theorem
[Lee] 1) if 
\begin_inset Formula $\lim_{n\rightarrow\infty}n^{1/2}/H=0,$
\end_inset

 then 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{SML}-\theta^{0}\right)\stackrel{d}{\rightarrow}N(0,\mathcal{I}^{-1}(\theta^{0}))
\]

\end_inset


\end_layout

\begin_layout Theorem
2) if 
\begin_inset Formula $\lim_{n\rightarrow\infty}n^{1/2}/H=\lambda,$
\end_inset

 
\begin_inset Formula $\lambda$
\end_inset

 a finite constant, then 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}_{SML}-\theta^{0}\right)\stackrel{d}{\rightarrow}N(B,\mathcal{I}^{-1}(\theta^{0}))
\]

\end_inset

 where 
\begin_inset Formula $B$
\end_inset

 is a finite vector of constants.
 
\end_layout

\begin_layout Itemize
This means that the SML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is asymptotically biased if 
\begin_inset Formula $H$
\end_inset

 doesn't grow faster than 
\begin_inset Formula $n^{1/2}.$
\end_inset


\end_layout

\begin_layout Itemize
The varcov is the typical inverse of the information matrix, so that as
 long as 
\begin_inset Formula $H$
\end_inset

 grows fast enough the estimator is consistent and fully asymptotically
 efficient.
 
\end_layout

\begin_layout Itemize
SML is actually not used nearly as often as is the method of simulated moments,
 in one of its variations.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Method of simulated moments (MSM)
\end_layout

\begin_layout Standard
Suppose we have a DGP
\begin_inset Formula $(y|x,\theta)$
\end_inset

 which is simulable given 
\begin_inset Formula $\theta$
\end_inset

, but is such that the density of 
\begin_inset Formula $y$
\end_inset

 is not calculable.
\end_layout

\begin_layout Standard
Once could, in principle, base a GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator upon the moment conditions 
\begin_inset Formula 
\[
m_{t}(\theta)=\left[K(y_{t},x_{t})-k(x_{t},\theta)\right]z_{t}
\]

\end_inset

 where 
\begin_inset Formula 
\[
k(x_{t},\theta)=\int K(y_{t},x_{t})p(y|x_{t},\theta)dy,
\]

\end_inset

 
\begin_inset Formula $z_{t}$
\end_inset

 is a vector of instruments in the information set and 
\begin_inset Formula $p(y|x_{t},\theta)$
\end_inset

 is the density of 
\begin_inset Formula $y$
\end_inset

 conditional on 
\begin_inset Formula $x_{t}.$
\end_inset

 The problem is that this density is not available.
\end_layout

\begin_layout Itemize
However 
\begin_inset Formula $k(x_{t},\theta)$
\end_inset

 is readily simulated using 
\begin_inset Formula 
\[
\widetilde{k}\left(x_{t},\theta\right)=\frac{1}{H}\sum_{h=1}^{H}K(\widetilde{y}_{t}^{h},x_{t})
\]

\end_inset


\end_layout

\begin_layout Itemize
By the law of large numbers, 
\begin_inset Formula $\widetilde{k}\left(x_{t},\theta\right)\stackrel{a.s.}{\rightarrow}k\left(x_{t},\theta\right),$
\end_inset

 as 
\begin_inset Formula $H\rightarrow\infty,$
\end_inset

 which provides a clear intuitive basis for the estimator, though in fact
 we obtain consistency even for 
\begin_inset Formula $H$
\end_inset

 finite, since a law of large numbers is also operating across the 
\begin_inset Formula $n$
\end_inset

 observations of real data, so errors introduced by simulation cancel themselves
 out.
\end_layout

\begin_layout Itemize
This allows us to form the moment conditions 
\begin_inset Formula 
\begin{equation}
\widetilde{m_{t}}(\theta)=\left[K(y_{t},x_{t})-\widetilde{k}\left(x_{t},\theta\right)\right]z_{t}
\end{equation}

\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 is drawn from the information set.
 As before, form 
\begin_inset Formula 
\begin{eqnarray}
\widetilde{m}(\theta) & = & \frac{1}{n}\sum_{i=1}^{n}\widetilde{m_{t}}(\theta)\nonumber \\
 & = & \frac{1}{n}\sum_{i=1}^{n}\left[K(y_{t},x_{t})-\frac{1}{H}\sum_{h=1}^{H}k(\widetilde{y}_{t}^{h},x_{t})\right]z_{t}\label{Linearity of MSM}
\end{eqnarray}

\end_inset

 with which we form the GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

criterion and estimate as usual.
 Note that the unbiased simulator 
\begin_inset Formula $k(\widetilde{y}_{t}^{h},x_{t})$
\end_inset

 appears linearly within the sums.
 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Properties
\end_layout

\begin_layout Standard
Suppose that the optimal weighting matrix is used.
 
\begin_inset CommandInset citation
LatexCommand citet
key "McFadden1989MSM"
literal "true"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "PakesPollard"
literal "true"

\end_inset

 show that the asymptotic distribution of the MSM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is very similar to that of the infeasible GMM estimator.
 In particular, assuming that the optimal weighting matrix is used, and
 for 
\begin_inset Formula $H$
\end_inset

 finite, 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\theta}_{MSM}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(1+\frac{1}{H}\right)\left(D_{\infty}\Omega^{-1}D_{\infty}^{\prime}\right)^{-1}\right]
\end{equation}

\end_inset

where 
\begin_inset Formula $\left(D_{\infty}\Omega^{-1}D_{\infty}^{\prime}\right)^{-1}$
\end_inset

 is the asymptotic variance of the infeasible GMM estimator.
\end_layout

\begin_layout Itemize
That is, the asymptotic variance is inflated by a factor 
\begin_inset Formula $1+1/H.$
\end_inset

 For this reason the MSM estimator is not fully asymptotically efficient
 relative to the infeasible GMM estimator, for 
\begin_inset Formula $H$
\end_inset

 finite, but the efficiency loss is small and controllable, by setting 
\begin_inset Formula $H$
\end_inset

 reasonably large.
\end_layout

\begin_layout Itemize
The estimator is asymptotically unbiased even for 
\begin_inset Formula $H=1.$
\end_inset

 This is an advantage relative to SML.
\end_layout

\begin_layout Itemize
If one doesn't use the optimal weighting matrix, the asymptotic varcov is
 just the ordinary GMM varcov, inflated by 
\begin_inset Formula $1+1/H.$
\end_inset


\end_layout

\begin_layout Itemize
The above presentation is in terms of a specific moment condition based
 upon the conditional mean.
 The MSM can be applied to moment conditions of other forms, too.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Itemize
A leading example is Indirect Inference, where we set 
\begin_inset Formula $\bar{m}_{n}(\theta)=\text{\hat{\phi}-\ensuremath{\frac{1}{S}\sum\tilde{\phi^{s}}(\theta)}}$
\end_inset

, and then we just do ordinary GMM.
 Note that this is an average over 
\begin_inset Formula $S$
\end_inset

, not over 
\begin_inset Formula $n$
\end_inset

, so this is a departure from the standard presentation of GMM.
\end_layout

\begin_layout Itemize
Here, 
\begin_inset Formula $\hat{\phi}$
\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
is an extremum estimator corresponding to some auxiliary model
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
.
 The 
\begin_inset Formula $\tilde{\phi^{s}}(\theta)$
\end_inset

 are the same extremum estimator, applied to simulated data generated from
 the model.
 The logic is that 
\begin_inset Formula $\hat{\phi}$
\end_inset

 will converge to a pseudo-true value, and 
\begin_inset Formula $\tilde{\phi^{s}}(\theta)$
\end_inset

 will converge to another pseudo-true value, depending on the value of 
\begin_inset Formula $\theta$
\end_inset

 that generated the data.
 When 
\begin_inset Formula $\theta=\theta^{0}$
\end_inset

, the two pseudo-true values will be the same.
 Trying to make the average of the simulated estimators as close as possible
 to the estimator generated by the real data will cause the MSM estimator
 to be consistent, given identification.
 
\end_layout

\begin_layout Itemize
For such an estimator to have good efficiency, we need the auxiliary model
 to fit well: it should pick up the relevant features of the data.
\end_layout

\begin_layout Itemize
a drawback of the II estimator is that the auxiliary model must be estimated
 many times.
 This is not a problem if it's a simple linear model, but it could be a
 problem if it's more complicated.
 For efficiency, we need a good fit, and a simple linear model may not provide
 this.
 The EMM (
\begin_inset CommandInset citation
LatexCommand citet
key "emm"
literal "true"

\end_inset

) estimator discussed below is asymptotically equivalent to II, and it requires
 the auxiliary model to be estimated only once.
\end_layout

\begin_layout Itemize
So, as Gallant and Tauchen ask, 
\begin_inset Quotes sld
\end_inset

Which Moments to Match?
\begin_inset Quotes srd
\end_inset

 is the fundamental question when doing MSM or EMM (or ordinary GMM, for
 that matter).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Comments
\end_layout

\begin_layout Standard
Why is SML inconsistent when the number of simulations is finite, while
 MSM is? The reason is that SML is based upon an average of 
\series bold
logarithms
\series default
 of an unbiased simulator (the densities of the observations).
 To use the multinomial probit model as an example, the log-likelihood function
 is 
\begin_inset Formula 
\[
\ln\mathcal{L}(\beta,\Omega)=\frac{1}{n}\sum_{i=1}^{n}y_{i}^{\prime}\ln p_{i}(\beta,\Omega)
\]

\end_inset

 The SML version is 
\begin_inset Formula 
\[
\ln\mathcal{L}(\beta,\Omega)=\frac{1}{n}\sum_{i=1}^{n}y_{i}^{\prime}\ln\tilde{p}_{i}(\beta,\Omega)
\]

\end_inset

 The problem is that 
\begin_inset Formula 
\[
E\ln(\tilde{p}_{i}(\beta,\Omega))\neq\ln(\mathcal{E}\tilde{p}_{i}(\beta,\Omega))
\]

\end_inset

 in spite of the fact that 
\begin_inset Formula 
\[
\mathcal{E}\tilde{p}_{i}(\beta,\Omega)=p_{i}(\beta,\Omega)
\]

\end_inset

 due to the fact that 
\begin_inset Formula $\ln(\cdot)$
\end_inset

 is a nonlinear transformation.
 The only way for the two to be equal (in the limit)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

is if 
\begin_inset Formula $H$
\end_inset

 tends to infinite so that 
\begin_inset Formula $\tilde{p}\left(\cdot\right)$
\end_inset

 tends to 
\begin_inset Formula $p\left(\cdot\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
The reason that MSM does not suffer from this problem is that in this case
 the unbiased simulator appears 
\emph on
linearly
\emph default
 within every sum of terms, and it appears within a sum over 
\begin_inset Formula $n$
\end_inset

 (see equation [
\begin_inset CommandInset ref
LatexCommand ref
reference "Linearity of MSM"

\end_inset

]).
 Therefore the SLLN applies to cancel out simulation errors, from which
 we get consistency.
 That is, using simple notation for the random sampling case, the moment
 conditions 
\begin_inset Formula 
\begin{eqnarray}
\tilde{m}(\theta) & = & \frac{1}{n}\sum_{i=1}^{n}\left[K(y_{t},x_{t})-\frac{1}{H}\sum_{h=1}^{H}k(\widetilde{y}_{t}^{h},x_{t})\right]z_{t}\\
 & = & \frac{1}{n}\sum_{i=1}^{n}\left[k(x_{t},\theta^{0})+\varepsilon_{t}-\frac{1}{H}\sum_{h=1}^{H}\left[k(x_{t},\theta)+\tilde{\varepsilon}_{ht}\right]\right]z_{t}\label{simulated moments}
\end{eqnarray}

\end_inset

 converge almost surely to 
\begin_inset Formula 
\[
\tilde{m}_{\infty}(\theta)=\int\left[k(x,\theta^{0})-k(x,\theta)\right]z(x)d\mu(x).
\]

\end_inset

 (note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset


\begin_inset Formula $z_{t}$
\end_inset

 is assume to be made up of functions of 
\begin_inset Formula $x_{t}).$
\end_inset

 The objective function converges to 
\begin_inset Formula 
\[
s_{\infty}(\theta)=\tilde{m}_{\infty}(\theta)^{\prime}\Omega_{\infty}^{-1}\tilde{m}_{\infty}(\theta)
\]

\end_inset

 which obviously has a minimum at 
\begin_inset Formula $\theta^{0},$
\end_inset

 henceforth consistency.
\end_layout

\begin_layout Itemize
If you look at equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "simulated moments"

\end_inset

 a bit, you will see why the variance inflation factor is 
\begin_inset Formula $(1+\frac{1}{H})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Subsection
MSM and Indirect inference, and Bayesian limited information simulated likelihoo
d
\end_layout

\begin_layout Standard
The Julia script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimateStochasticVolatilityModel.jl}{https://github.com/mcreel/
Econometrics/blob/master/Examples/SBEM/EstimateStochasticVolatilityModel.jl}
\end_layout

\end_inset

 estimates the stochastic volatility model considered in Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Stochastic-volatility"

\end_inset

 by indirect inference, using simulated annealing to do the minimization
 (To run this, you need the package 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/mcreel/SV
\end_layout

\end_inset

).
 Examine the script to see what auxiliary statistics are used (this is the
 key to success or failure), and think about the problem to try to come
 up with some better ones.
 The results for 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{the sample}{https://github.com/mcreel/Econometrics/blob/master/
Examples/TimeSeries/svdata.txt}
\end_layout

\end_inset

 that is pictured in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SV-model,-typical"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (
\begin_inset Formula $n=1000$
\end_inset

) are
\end_layout

\begin_layout Itemize

\family typewriter
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout

\family typewriter
true value estimate std.
 err.
 CI lower CI upper
\end_layout

\begin_layout Plain Layout

\family typewriter
0.69212 0.73297 0.06199 0.61146 0.85447
\end_layout

\begin_layout Plain Layout

\family typewriter
0.90000 0.95500 0.02430 0.90737 1.00264
\end_layout

\begin_layout Plain Layout

\family typewriter
0.36300 0.25042 0.06463 0.12374 0.37710
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
check the script to see how to compute standard errors, etc.
\end_layout

\begin_layout Itemize
minimization use SA works fine.
 I haven't tried with gradient-based methods.
\end_layout

\begin_layout Standard
The script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimateStochasticVolatilityModel
\backslash
_MCMC.jl}{https://github.com/mcreel/Econometrics/blob/master/Examples/SBEM/Estimat
eStochasticVolatilityModel
\backslash
_MCMC.jl}
\end_layout

\end_inset

 estimates the same model using the same data set and the same auxiliary
 statistics, but using MCMC, so we get a full posterior for each parameter,
 instead of just a point estimate.
 The likelihood is the asymptotic Gaussian likelihood of the auxiliary statistic
s.
 This lets us use Bayesian methods even if the small sample distribution
 of the statistics is unknown.
 This is a little time consuming, with simulation used to get the likelihood,
 and then the MCMC steps.
 However, the results are quite good, as we see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MCMC-estimation-using"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that the true parameter values are 0.692, 0.9 and 0.363.
 The posterior means are very close to the indirect inference point estimates
 that were obtained, above.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:MCMC-estimation-using"

\end_inset

MCMC estimation using simulated moments and limited information quasi-likelihood
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/MCMC.svg
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{Analyze.jl}{https://github.com/mcreel/Econometrics/blob/master/E
xamples/SBEM/NUTS/Analyze.jl}
\end_layout

\end_inset

 also uses MCMC, but using a more sophisticated sampler, the 
\begin_inset Quotes sld
\end_inset

No U-Turn
\begin_inset Quotes srd
\end_inset

 sampler (NUTS), as implemented in the 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/tpapp/DynamicHMC.jl
\end_layout

\end_inset

 package.
 The two versions of MCMC give results that are pretty much the same.
 The NUTS sampler is mostly self-tuning, which can be useful, but it is
 slower to compute, at least for the two implementations given here.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
NUTS MCMC estimation using simulated moments and limited information quasi-likel
ihood
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/SBEM/NUTS/nuts.svg
	width 15cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection
MA model, AR auxiliary model 
\end_layout

\begin_layout Standard
An example of estimation using the MSM is given in the script file 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{MSM
\backslash
_Example.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/SBEM/MSM
\backslash
_Example.m}
\end_layout

\end_inset

.
 The first order moving average (MA(1)) model has been widely used to investigat
e the performance of the indirect inference estimator, and a 
\begin_inset Formula $pth$
\end_inset

-order autoregressive model is often used to generate the auxiliary statistic
 (see, for example, Gouriroux
\emph on
, 
\emph default
Monfort and Renault, 1993; Chumacero, 2001).
 In this section we estimate the MA(1) model
\begin_inset Formula 
\begin{eqnarray*}
y_{t} & = & \epsilon_{t}+\psi\epsilon_{t-1}\\
\epsilon_{t} & \sim & i.i.d.\,N(0,\sigma^{2})
\end{eqnarray*}

\end_inset

The parameter vector is 
\begin_inset Formula $\theta=(\psi,\sigma).$
\end_inset

 We set the parameter space for the initial simulated annealing stage (to
 get good start values for the gradient-based algorithm) to 
\begin_inset Formula $\Theta=\left(-1,1\right)\times\left(0,2\right)$
\end_inset

, which imposes invertibility, which is needed for the parameter to be identifie
d.
 The statistic 
\begin_inset Formula $Z_{n}$
\end_inset

 is the vector of estimated parameters 
\begin_inset Formula $\left(\rho_{0},\rho_{1},...,\rho_{P},\sigma_{\upsilon}^{2}\right)$
\end_inset

 of an AR(
\begin_inset Formula $P$
\end_inset

) model 
\begin_inset Formula $y_{t}=\rho_{0}+\sum_{p=1}^{P}\rho_{p}y_{t-p}+\upsilon_{t}$
\end_inset

, fit to the data using ordinary least squares.
 
\end_layout

\begin_layout Standard
We estimate 
\begin_inset Formula $\theta$
\end_inset

 using MSM implemented as II, using continuously updated GMM (Hanson, Heaton
 and Yaron,
\emph on
 
\emph default
1996).
 The moment conditions that define the continuously updated indirect inference
 (CU-II) estimator are 
\begin_inset Formula $\bar{m}_{n}(\theta)=Z_{n}-\bar{Z}_{S,n}(\theta)$
\end_inset

 where 
\begin_inset Formula $\bar{Z}_{S,n}(\theta)=\frac{1}{S}\sum_{s=1}^{S}Z_{n}^{s}(\theta)$
\end_inset

, and the weight matrix at each iteration is the inverse of 
\begin_inset Formula $\Omega_{n}^{S}(\theta)=\frac{1}{S}\sum_{s=1}^{S}\left[Z_{n}^{s}\left(\theta\right)-\bar{Z}_{S,n}(\theta)\right]\left[Z_{n}^{s}\left(\theta\right)-\bar{Z}_{S,n}(\theta)\right]^{\prime}$
\end_inset

, where 
\begin_inset Formula $S=100.$
\end_inset

 
\end_layout

\begin_layout Subsection
SML of a Poisson model with latent heterogeneity
\begin_inset CommandInset label
LatexCommand label
name "subsec:SML-Poisson-Latent"

\end_inset


\end_layout

\begin_layout Standard
We have seen (see equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:negbindensity"

\end_inset

) that a Poisson model with latent heterogeneity that follows an exponential
 distribution leads to the negative binomial model.
 To illustrate SML, we can integrate out the latent heterogeneity using
 Monte Carlo, rather than the analytical approach which leads to the negative
 binomial model.
 In actual practice, one would not want to use SML in this case, but it
 is a nice example since it allows us to compare SML to the actual ML estimator.
 The Octave function defined by 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{PoissonLatentHet.m}{https://github.com/mcreel/Econometrics/blob/
master/Examples/SBEM/PoissonLatentHet.m}
\end_layout

\end_inset

 calculates the simulated log likelihood for a Poisson model where 
\begin_inset Formula $\lambda=\exp x_{t}^{\prime}\beta+\sigma\eta)$
\end_inset

, where 
\begin_inset Formula $\eta\sim N(0,1)$
\end_inset

.
 This model is similar to the negative binomial model, except that the latent
 variable is normally distributed rather than gamma distributed.
 The Octave script 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{EstimatePoissonLatentHet.m}{https://github.com/mcreel/Econometri
cs/blob/master/Examples/SBEM/EstimatePoissonLatentHet.m}
\end_layout

\end_inset

 estimates this model using the MEPS OBDV data that has already been discussed.
 Note that simulated annealing is used to maximize the log likelihood function.
 Attempting to use BFGS leads to trouble.
 If you run this script, you will see that it takes a long time to get the
 estimation results, which are:
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/SBEM/plh.out"

\end_inset

If you compare these results to the results for the negative binomial model,
 given in subsection (
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Infinite-mixture-models:"

\end_inset

), you can see that the present model fits better according to the CAIC
 criterion.
 It also fits better than the seminonparametric negative binomial model,
 presented in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Semi-nonparametric-maximum-likel"

\end_inset

, which has more parameters.
 The present model is considerably less convenient to work with, however,
 due to the computational requirements.
 The chapter on parallel computing is relevant if you wish to use models
 of this sort.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\series bold
Exercises
\end_layout

\begin_layout Enumerate
(basic) Examine the Octave script and function discussed in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:SML-Poisson-Latent"

\end_inset

 and describe what they do.
\end_layout

\begin_layout Enumerate
(basic) Examine the Octave scripts and functions discussed in subsection
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:EMM-estimation-of"

\end_inset

 and describe what they do.
\end_layout

\begin_layout Enumerate
(advanced, but even if you don't do this you should be able to describe
 what needs to be done) Write Octave code to do SML estimation of the probit
 model.
 Do an estimation using data generated by a probit model (
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{probitdgp.m}{https://github.com/mcreel/Econometrics/blob/master/
MyOctaveFiles/Count/ProbitDGP.m}
\end_layout

\end_inset

 might be helpful).
 Compare the SML estimates to ML estimates.
\end_layout

\begin_layout Enumerate
(more advanced) Do a little Monte Carlo study to compare ML, SML and EMM
 estimation of the probit model.
 Investigate how the number of simulations affect the two simulation-based
 estimators.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Notation and Review
\end_layout

\begin_layout Itemize
All vectors will be column vectors, unless they have a transpose symbol
 (or I forget to apply this rule - your help catching typos and er0rors
 is much appreciated).
 For example, if 
\begin_inset Formula $x_{t}$
\end_inset

 is a 
\begin_inset Formula $p\times1$
\end_inset

 vector, 
\begin_inset Formula $x_{t}^{\prime}$
\end_inset

 is a 
\begin_inset Formula $1\times p$
\end_inset

 vector.
 When I refer to a 
\begin_inset Formula $p$
\end_inset

-vector, I mean a column vector.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Notation-for-differentiation"

\end_inset

Notation for differentiation of vectors and matrices
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "GallantNonlinearStatisticalModels"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $s(\cdot):\Re^{p}\rightarrow\Re$
\end_inset

 be a real valued function of the 
\begin_inset Formula $p$
\end_inset

-vector 
\begin_inset Formula $\theta.$
\end_inset

 Then 
\begin_inset Formula $\frac{\partial s(\theta)}{\partial\theta}$
\end_inset

 is organized as a 
\begin_inset Formula $p$
\end_inset

-vector, 
\begin_inset Formula 
\[
\frac{\partial s(\theta)}{\partial\theta}=\left[\begin{array}{c}
\frac{\partial s(\theta)}{\partial\theta_{1}}\\
\frac{\partial s(\theta)}{\partial\theta_{2}}\\
\vdots\\
\frac{\partial s(\theta)}{\partial\theta_{p}}
\end{array}\right]
\]

\end_inset

 Following this convention,
\begin_inset Formula $\frac{\partial s(\theta)}{\partial\theta^{\prime}}$
\end_inset

is a 
\begin_inset Formula $1\times p$
\end_inset

 vector
\begin_inset Formula $,$
\end_inset

 and 
\begin_inset Formula $\frac{\partial^{2}s(\theta)}{\partial\theta\partial\theta^{\prime}}$
\end_inset

 is a 
\begin_inset Formula $p\times p$
\end_inset

 matrix.
 Also,
\begin_inset Formula 
\[
\frac{\partial^{2}s(\theta)}{\partial\theta\partial\theta^{\prime}}=\frac{\partial}{\partial\theta}\left(\frac{\partial s(\theta)}{\partial\theta^{\prime}}\right)=\frac{\partial}{\partial\theta^{\prime}}\left(\frac{\partial s(\theta)}{\partial\theta}\right).
\]

\end_inset


\end_layout

\begin_layout Exercise
For 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 both 
\begin_inset Formula $p$
\end_inset

-vectors, show that 
\begin_inset Formula $\frac{\partial a^{\prime}x}{\partial x}=a$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f(\theta)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 be a 
\begin_inset Formula $n$
\end_inset

-vector valued function of the 
\begin_inset Formula $p$
\end_inset

-vector 
\begin_inset Formula $\theta$
\end_inset

.
 Let 
\begin_inset Formula $f(\theta)^{\prime}$
\end_inset

 be the 
\begin_inset Formula $1\times n$
\end_inset

 valued transpose of 
\begin_inset Formula $f$
\end_inset

 .
 Then 
\begin_inset Formula $\left(\frac{\partial}{\partial\theta}f(\theta)^{\prime}\right)^{\prime}=\frac{\partial}{\partial\theta^{\prime}}f(\theta).$
\end_inset

 
\end_layout

\begin_layout Definition*
Product rule.
 
\begin_inset CommandInset label
LatexCommand label
name "def: Product-rule.-Let"

\end_inset

Let 
\begin_inset Formula $f(\theta)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 and 
\begin_inset Formula $h(\theta)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 be 
\begin_inset Formula $n$
\end_inset

-vector valued functions of the 
\begin_inset Formula $p$
\end_inset

-vector 
\begin_inset Formula $\theta$
\end_inset

.
 Then 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta^{\prime}}h(\theta)^{\prime}f(\theta)=h^{\prime}\left(\frac{\partial}{\partial\theta^{\prime}}f\right)+f^{\prime}\left(\frac{\partial}{\partial\theta^{\prime}}h\right)
\]

\end_inset

 has dimension 
\begin_inset Formula $1\times p.$
\end_inset

 Applying the transposition rule we get 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}h(\theta)^{\prime}f(\theta)=\left(\frac{\partial}{\partial\theta}f^{\prime}\right)h+\left(\frac{\partial}{\partial\theta}h^{\prime}\right)f
\]

\end_inset

 which has dimension 
\begin_inset Formula $p\times1.$
\end_inset


\end_layout

\begin_layout Exercise
For 
\begin_inset Formula $A$
\end_inset

 a 
\begin_inset Formula $p\times p$
\end_inset

 matrix and 
\begin_inset Formula $x$
\end_inset

 a 
\begin_inset Formula $p\times1$
\end_inset

 vector, show that 
\begin_inset Formula $\frac{\partial x^{\prime}Ax}{\partial x}=\left(A+A^{\prime}\right)x$
\end_inset

.
 Also, what is the result if 
\begin_inset Formula $A$
\end_inset

 is symmetric?
\end_layout

\begin_layout Definition
Chain rule.
\begin_inset CommandInset label
LatexCommand label
name "def: Chain-rule.-Let"

\end_inset

 Let 
\begin_inset Formula $f(\cdot)$
\end_inset

:
\begin_inset Formula $\Re^{p}\rightarrow\Re^{n}$
\end_inset

 a 
\begin_inset Formula $n$
\end_inset

-vector valued function of a 
\begin_inset Formula $p$
\end_inset

-vector argument, and let 
\begin_inset Formula $g()$
\end_inset

:
\begin_inset Formula $\Re^{r}\rightarrow\Re^{p}$
\end_inset

 be a 
\begin_inset Formula $p$
\end_inset

-vector valued function of an 
\begin_inset Formula $r$
\end_inset

-vector valued argument 
\begin_inset Formula $\rho$
\end_inset

.
 Then 
\begin_inset Formula 
\[
\frac{\partial}{\partial\rho^{\prime}}f\left[g\left(\rho\right)\right]=\left.\frac{\partial}{\partial\theta^{\prime}}f(\theta)\right|_{\theta=g(\rho)}\frac{\partial}{\partial\rho^{\prime}}g(\rho)
\]

\end_inset


\end_layout

\begin_layout Standard
has dimension 
\begin_inset Formula $n\times r.$
\end_inset


\end_layout

\begin_layout Exercise
For 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors, show that 
\begin_inset Formula $\frac{\partial\exp(x^{\prime}\beta)}{\partial\beta}=\exp(x^{\prime}\beta)x$
\end_inset

.
\end_layout

\begin_layout Section
Convergence modes 
\end_layout

\begin_layout Standard

\series bold
Readings:
\series default
 Davidson, R.
 and J.G.
 MacKinnon, 
\emph on
Econometric Theory and Methods
\emph default
, Ch.
 4; Gallant, A.R., 
\emph on
An Introduction to Econometric Theory, 
\emph default
Ch.
 4.
\end_layout

\begin_layout Standard
We will consider several modes of convergence.
 The first three modes discussed are simply for background.
 The stochastic modes are those which will be used later in the course.
\end_layout

\begin_layout Definition
 A sequence is a mapping from the natural numbers 
\begin_inset Formula $\{1,2,...\}=\{n\}_{n=1}^{\infty}=\{n\}$
\end_inset

 to some other set, so that the set is ordered according to the natural
 numbers associated with its elements.
 
\end_layout

\begin_layout Subsection*
Real-valued sequences:
\end_layout

\begin_layout Definition

\emph on
[Convergence]
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Convergence, ordinary
\end_layout

\end_inset


\emph default
 A real-valued sequence of vectors 
\begin_inset Formula $\{a_{n}\}$
\end_inset

 
\emph on
converges
\emph default
 to the vector 
\begin_inset Formula $a$
\end_inset

 if for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

 there exists an integer 
\begin_inset Formula $N_{\varepsilon}$
\end_inset

 such that for all 
\begin_inset Formula $n>N_{\varepsilon},\parallel a_{n}-a\parallel<\varepsilon$
\end_inset

 .
 
\begin_inset Formula $a$
\end_inset

 is the 
\emph on
limit
\emph default
 of 
\begin_inset Formula $a_{n},$
\end_inset

 written 
\begin_inset Formula $a_{n}\rightarrow a.$
\end_inset


\end_layout

\begin_layout Subsection*
Deterministic real-valued functions
\end_layout

\begin_layout Standard
Consider a sequence of functions 
\begin_inset Formula $\{f_{n}(\omega)\}$
\end_inset

 where 
\begin_inset Formula 
\[
f_{n}:\Omega\rightarrow T\subseteq\Re.
\]

\end_inset

 
\begin_inset Formula $\Omega$
\end_inset

 may be an arbitrary set.
\end_layout

\begin_layout Definition

\emph on
[Pointwise convergence]
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
convergence, pointwise
\end_layout

\end_inset


\emph default
 A sequence of functions 
\begin_inset Formula $\{f_{n}(\omega)\}$
\end_inset

 
\emph on
converges pointwise
\emph default
 on
\emph on

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 {}
\end_layout

\end_inset


\emph default

\begin_inset Formula $\Omega$
\end_inset

 to the function 
\begin_inset Formula $f$
\end_inset

(
\begin_inset Formula $\omega)$
\end_inset

 if for all 
\begin_inset Formula $\varepsilon>0$
\end_inset

 and 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 there exists an integer 
\begin_inset Formula $N_{\varepsilon\omega}$
\end_inset

 such that 
\begin_inset Formula 
\[
|f_{n}(\omega)-f(\omega)|<\varepsilon,\forall n>N_{\varepsilon\omega}.
\]

\end_inset


\end_layout

\begin_layout Standard
It's important to note that 
\begin_inset Formula $N_{\varepsilon\omega}$
\end_inset

 depends upon 
\begin_inset Formula $\omega,$
\end_inset

 so that converge may be much more rapid for certain 
\begin_inset Formula $\omega$
\end_inset

 than for others.
 Uniform convergence requires a similar rate of convergence throughout 
\begin_inset Formula $\Omega.$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Uniform convergence]
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
convergence, uniform
\end_layout

\end_inset


\emph default
 A sequence of functions 
\begin_inset Formula $\{f_{n}(\omega)\}$
\end_inset

 
\emph on
converges uniformly
\emph default
 on
\emph on

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 {}
\end_layout

\end_inset


\emph default

\begin_inset Formula $\Omega$
\end_inset

 to the function 
\begin_inset Formula $f$
\end_inset

(
\begin_inset Formula $\omega)$
\end_inset

 if for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

 there exists an integer 
\begin_inset Formula $N$
\end_inset

 such that 
\begin_inset Formula 
\[
\sup_{\omega\in\Omega}|f_{n}(\omega)-f(\omega)|<\varepsilon,\forall n>N.
\]

\end_inset

 (insert a diagram here showing the envelope around 
\begin_inset Formula $f(\omega)$
\end_inset

 in which 
\begin_inset Formula $f_{n}(\omega)$
\end_inset

 must lie).
\end_layout

\begin_layout Subsection*
Stochastic sequences
\end_layout

\begin_layout Standard
In econometrics, we typically deal with stochastic sequences.
 Given a probability space 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right),$
\end_inset

 recall that a random variable maps the sample space to the real line
\shape italic
,
\shape default
 
\shape italic
\emph on
i.e.
\emph default
,
\shape default
 
\begin_inset Formula $X(\omega):\Omega\rightarrow\Re.$
\end_inset

 A sequence of random variables 
\begin_inset Formula $\{X_{n}(\omega)\}$
\end_inset

 is a collection of such mappings, 
\emph on
i.e.,
\shape italic
\emph default

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 {}
\end_layout

\end_inset


\shape default
each 
\begin_inset Formula $X_{n}(\omega)$
\end_inset

 is a random variable with respect to the probability space 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right).$
\end_inset

 For example, given the model 
\begin_inset Formula $Y=X\beta^{0}+\varepsilon,$
\end_inset

 the OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator 
\begin_inset Formula $\hat{\beta}_{n}=\left(X^{\prime}X\right)^{-1}X^{\prime}Y,$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the sample size, can be used to form a sequence of random vectors 
\begin_inset Formula $\{\hat{\beta}_{n}\}$
\end_inset

.
 A number of modes of convergence are in use when dealing with sequences
 of random variables.
 Several such modes of convergence should already be familiar:
\end_layout

\begin_layout Definition

\emph on
[Convergence in probability]
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
convergence, in probability
\end_layout

\end_inset


\emph default
 Let 
\begin_inset Formula $X_{n}(\omega)$
\end_inset

 be a sequence of random variables, and let 
\begin_inset Formula $X(\omega)$
\end_inset

 be a random variable.
 Let 
\begin_inset Formula $\mathcal{A}_{n}=\{\omega:|X_{n}(\omega)-X(\omega)|>\varepsilon\}$
\end_inset

.
 Then 
\begin_inset Formula $\{X_{n}(\omega)\}$
\end_inset

 converges in probability to 
\begin_inset Formula $X(\omega)$
\end_inset

 if 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}P\left(\mathcal{A}_{n}\right)=0,\forall\varepsilon>0.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
Convergence in probability is written as 
\begin_inset Formula $X_{n}\stackrel{p}{\rightarrow}X,$
\end_inset

 or plim 
\begin_inset Formula $X_{n}=X.$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Almost sure convergence]
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
convergence, almost sure
\end_layout

\end_inset


\emph default
 Let 
\begin_inset Formula $X_{n}(\omega)$
\end_inset

 be a sequence of random variables, and let 
\begin_inset Formula $X(\omega)$
\end_inset

 be a random variable.
 Let 
\begin_inset Formula $\mathcal{A}=\{\omega:\lim_{n\rightarrow\infty}X_{n}(\omega)=X(\omega)\}$
\end_inset

.
 Then 
\begin_inset Formula $\{X_{n}(\omega)\}$
\end_inset

 converges almost surely to 
\begin_inset Formula $X(\omega)$
\end_inset

 if 
\begin_inset Formula 
\[
P\left(\mathcal{A}\right)=1.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
In other words, 
\begin_inset Formula $X_{n}(\omega)\rightarrow X(\omega)$
\end_inset

 (ordinary convergence of the two functions) except on a set 
\begin_inset Formula $C=\Omega-\mathcal{A}$
\end_inset

 such that 
\begin_inset Formula $P(C)=0.$
\end_inset

 Almost sure convergence is written as 
\begin_inset Formula $X_{n}\stackrel{a.s.}{\rightarrow}X,$
\end_inset

 or 
\begin_inset Formula $X_{n}\rightarrow X,\,a.s.$
\end_inset

 One can show that 
\begin_inset Formula 
\[
X_{n}\stackrel{a.s.}{\rightarrow}X\Rightarrow X_{n}\stackrel{p}{\rightarrow}X.
\]

\end_inset


\end_layout

\begin_layout Definition

\emph on
[Convergence in distribution]
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
convergence, in distribution
\end_layout

\end_inset


\emph default
 Let the r.v.
 
\begin_inset Formula $X_{n}$
\end_inset

 have distribution function 
\begin_inset Formula $F_{n}$
\end_inset

 and the r.v.
 
\begin_inset Formula $X_{n}$
\end_inset

 have distribution function 
\begin_inset Formula $F.$
\end_inset

 If 
\begin_inset Formula $F_{n}\rightarrow F$
\end_inset

 at every continuity point of 
\begin_inset Formula $F,$
\end_inset

 then 
\begin_inset Formula $X_{n}$
\end_inset

 converges in distribution to 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Standard
\noindent
Convergence in distribution is written as 
\begin_inset Formula $X_{n}\stackrel{d}{\rightarrow}X.$
\end_inset

 It can be shown that convergence in probability implies convergence in
 distribution.
\end_layout

\begin_layout Subsection*
Stochastic functions
\end_layout

\begin_layout Standard
Simple laws of large numbers (LLN's) allow us to directly conclude that
 
\begin_inset Formula $\hat{\beta}_{n}\stackrel{a.s.}{\rightarrow}\beta^{0}$
\end_inset

 in the OLS example, since 
\begin_inset Formula 
\[
\hat{\beta}_{n}=\beta^{0}+\left(\frac{X^{\prime}X}{n}\right)^{-1}\left(\frac{X^{\prime}\varepsilon}{n}\right),
\]

\end_inset

 and 
\begin_inset Formula $\frac{X^{\prime}\varepsilon}{n}\stackrel{a.s.}{\rightarrow0}$
\end_inset

 by a SLLN.
 Note that this term is not a function of the parameter 
\begin_inset Formula $\beta.$
\end_inset

 This easy proof is a result of the linearity of the model, which allows
 us to express the estimator in a way that separates parameters from random
 functions.
 In general, this is not possible.
 We often deal with the more complicated situation where the stochastic
 sequence depends on parameters in a manner that is not reducible to a simple
 sequence of random variables.
 In this case, we have a sequence of random functions that depend on 
\begin_inset Formula $\theta$
\end_inset

: 
\begin_inset Formula $\{X_{n}(\omega,\theta)\},$
\end_inset

 where each 
\begin_inset Formula $X_{n}(\omega,\theta)$
\end_inset

 is a random variable with respect to a probability space 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right)$
\end_inset

 and the parameter 
\begin_inset Formula $\theta$
\end_inset

 belongs to a parameter space 
\begin_inset Formula $\theta\in\Theta.$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Uniform almost sure convergence]
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
convergence, uniform almost sure
\end_layout

\end_inset


\emph default
 
\begin_inset Formula $\{X_{n}(\omega,\theta)\}$
\end_inset

 converges uniformly almost surely in 
\begin_inset Formula $\Theta$
\end_inset

 to 
\begin_inset Formula $X(\omega,\theta)$
\end_inset

 if 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{\theta\in\Theta}|X_{n}(\omega,\theta)-X(\omega,\theta)|=0,\text{(a.s.)}
\]

\end_inset


\end_layout

\begin_layout Standard
Implicit is the assumption that all 
\begin_inset Formula $X_{n}(\omega,\theta)$
\end_inset

 and 
\begin_inset Formula $X(\omega,\theta)$
\end_inset

 are random variables w.r.t.
 
\begin_inset Formula $\left(\Omega,\mathcal{F},P\right)$
\end_inset

 for all 
\begin_inset Formula $\theta\in\Theta.$
\end_inset

 We'll indicate uniform almost sure convergence by 
\begin_inset Formula $\stackrel{u.a.s.}{\rightarrow}$
\end_inset

 and uniform convergence in probability by 
\begin_inset Formula $\stackrel{u.p.}{\rightarrow}.$
\end_inset


\end_layout

\begin_layout Itemize
An equivalent definition, based on the fact that 
\begin_inset Quotes eld
\end_inset

almost sure
\begin_inset Quotes erd
\end_inset

 means 
\begin_inset Quotes eld
\end_inset

with probability one
\begin_inset Quotes erd
\end_inset

 is 
\begin_inset Formula 
\[
\Pr\left(\lim_{n\rightarrow\infty}\sup_{\theta\in\Theta}|X_{n}(\omega,\theta)-X(\omega,\theta)|=0\right)=1
\]

\end_inset

 This has a form similar to that of the definition of a.s.
 convergence - the essential difference is the addition of the 
\begin_inset Formula $\sup$
\end_inset

.
 
\end_layout

\begin_layout Section
Rates of convergence and asymptotic equality
\end_layout

\begin_layout Standard
It's often useful to have notation for the relative magnitudes of quantities.
 Quantities that are small relative to others can often be ignored, which
 simplifies analysis.
\end_layout

\begin_layout Definition

\emph on
[Little-o]
\emph default
 Let 
\begin_inset Formula $f(n)$
\end_inset

 and 
\begin_inset Formula $g(n)$
\end_inset

 be two real-valued functions.
 The notation 
\begin_inset Formula $f(n)=o(g(n))$
\end_inset

 means 
\begin_inset Formula $\lim_{n\rightarrow\infty}\frac{f(n)}{g(n)}=0.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Definition

\emph on
[Big-O]
\emph default
 Let 
\begin_inset Formula $f(n)$
\end_inset

 and 
\begin_inset Formula $g(n)$
\end_inset

 be two real-valued functions.
 The notation 
\begin_inset Formula $f(n)=O(g(n))$
\end_inset

 means there exists some 
\begin_inset Formula $N$
\end_inset

 such that for 
\begin_inset Formula $n>N,\left|\frac{f(n)}{g(n)}\right|<K,$
\end_inset

 where 
\begin_inset Formula $K$
\end_inset

 is a finite constant.
 
\end_layout

\begin_layout Standard
This definition doesn't require that 
\begin_inset Formula $\frac{f(n)}{g(n)}$
\end_inset

 have a limit (it may fluctuate boundedly).
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\left\{ f_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ g_{n}\right\} $
\end_inset

 are sequences of random variables analogous definitions are
\end_layout

\begin_layout Definition
The notation 
\begin_inset Formula $f(n)=o_{p}(g(n))$
\end_inset

 means 
\begin_inset Formula $\frac{f(n)}{g(n)}\stackrel{p}{\rightarrow}0.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
 The least squares estimator 
\begin_inset Formula $\hat{\theta}=(X^{\prime}X)^{-1}X^{\prime}Y=(X^{\prime}X)^{-1}X^{\prime}\left(X\theta^{0}+\varepsilon\right)=\theta^{0}+(X^{\prime}X)^{-1}X^{\prime}\varepsilon.$
\end_inset

 Since plim
\begin_inset Formula $\frac{(X^{\prime}X)^{-1}X^{\prime}\varepsilon}{1}=0,$
\end_inset

 we can write 
\begin_inset Formula $(X^{\prime}X)^{-1}X^{\prime}\varepsilon=o_{p}(1)$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}=\theta^{0}+o_{p}(1).$
\end_inset

 Asymptotically, the term 
\begin_inset Formula $o_{p}(1)$
\end_inset

 is negligible.
 This is just a way of indicating that the LS estimator is consistent.
 
\end_layout

\begin_layout Definition
 The notation 
\begin_inset Formula $f(n)=O_{p}(g(n))$
\end_inset

 means there exists some 
\begin_inset Formula $N_{\varepsilon}$
\end_inset

 such that for 
\begin_inset Formula $\varepsilon>0$
\end_inset

 and all 
\begin_inset Formula $n>N_{\varepsilon},$
\end_inset


\begin_inset Formula 
\[
P\left(\left|\frac{f(n)}{g(n)}\right|<K_{\varepsilon}\right)>1-\varepsilon,
\]

\end_inset

 where 
\begin_inset Formula $K_{\varepsilon}$
\end_inset

 is a finite constant.
 
\end_layout

\begin_layout Example
 
\begin_inset CommandInset label
LatexCommand label
name "normop1"

\end_inset

If 
\begin_inset Formula $X_{n}\sim N(0,1)$
\end_inset

 then 
\begin_inset Formula $X_{n}=O_{p}(1),$
\end_inset

 since, given 
\begin_inset Formula $\varepsilon,$
\end_inset

 there is always some 
\begin_inset Formula $K_{\varepsilon}$
\end_inset

 such that 
\begin_inset Formula $P\left(\left|X_{n}\right|<K_{\varepsilon}\right)>1-\varepsilon.$
\end_inset


\end_layout

\begin_layout Standard
Useful rules:
\end_layout

\begin_layout Itemize
\begin_inset Formula $O_{p}(n^{p})O_{p}(n^{q})=O_{p}(n^{p+q})$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $o_{p}(n^{p})o_{p}(n^{q})=o_{p}(n^{p+q})$
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "centered"

\end_inset

Consider a random sample of iid r.v.'s with mean 0 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The estimator of the mean 
\begin_inset Formula $\hat{\theta}=1/n\sum_{i=1}^{n}x_{i}$
\end_inset

 is asymptotically normally distributed, e.g., 
\begin_inset Formula $n^{1/2}\hat{\theta}\stackrel{A}{\sim}N(0,\sigma^{2}).$
\end_inset

 So 
\begin_inset Formula $n^{1/2}\hat{\theta}=O_{p}(1),$
\end_inset

 so 
\begin_inset Formula $\hat{\theta}=O_{p}(n^{-1/2}).$
\end_inset

 Before we had 
\begin_inset Formula $\hat{\theta}=o_{p}(1),$
\end_inset

 now we have have the stronger result that relates the rate of convergence
 to the sample size..
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\,$
\end_inset


\end_layout

\begin_layout Example
 
\begin_inset CommandInset label
LatexCommand label
name "uncentered"

\end_inset

Now consider a random sample of iid r.v.'s with mean 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The estimator of the mean 
\begin_inset Formula $\hat{\theta}=1/n\sum_{i=1}^{n}x_{i}$
\end_inset

 is asymptotically normally distributed, e.g., 
\begin_inset Formula $n^{1/2}\left(\hat{\theta}-\mu\right)\stackrel{A}{\sim}N(0,\sigma^{2}).$
\end_inset

 So 
\begin_inset Formula $n^{1/2}\left(\hat{\theta}-\mu\right)=O_{p}(1),$
\end_inset

 so 
\begin_inset Formula $\hat{\theta}-\mu=O_{p}(n^{-1/2}),$
\end_inset

 so 
\begin_inset Formula $\hat{\theta}=O_{p}(1).$
\end_inset


\end_layout

\begin_layout Standard
These two examples show that averages of centered (mean zero) quantities
 typically have plim 0, while averages of uncentered quantities have finite
 nonzero plims.
 Note that the definition of 
\begin_inset Formula $O_{p}$
\end_inset

 does not mean that 
\begin_inset Formula $f(n)$
\end_inset

 and 
\begin_inset Formula $g(n)$
\end_inset

 are of the same order.
 Asymptotic equality ensures that this is the case.
\end_layout

\begin_layout Definition
Two sequences of random variables 
\begin_inset Formula $\left\{ f_{n}\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ g_{n}\right\} $
\end_inset

 are asymptotically equal
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
asymptotic equality
\end_layout

\end_inset

 (written 
\begin_inset Formula $f_{n}\stackrel{a}{=}g_{n})$
\end_inset

 if 
\begin_inset Formula 
\[
plim\left(\frac{f(n)}{g(n)}\right)=1
\]

\end_inset


\end_layout

\begin_layout Standard
Finally, analogous almost sure versions of 
\begin_inset Formula $o_{p}$
\end_inset

 and 
\begin_inset Formula $O_{p}$
\end_inset

 are defined in the obvious way.
 
\end_layout

\begin_layout Section
Slutsky Theorem and Continuous Mapping Theorem
\end_layout

\begin_layout Standard
The following two theorems are important for getting the asymptotic distribution
 of estimators, and for test statistics that are derived from transformations
 of estimators.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "gallant1997introduction"
literal "true"

\end_inset

, Theorems 4.6 and 4.7.
 Statement of the theorems are here:
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "Slutsky Theorem"
target "https://en.wikipedia.org/wiki/Slutsky%27s_theorem"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "Continuous Mapping Theorem"
target "https://en.wikipedia.org/wiki/Continuous_mapping_theorem"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors, show that 
\begin_inset Formula $D_{x}a^{\prime}x=a$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $A$
\end_inset

 a 
\begin_inset Formula $p\times p$
\end_inset

 matrix and 
\begin_inset Formula $x$
\end_inset

 a 
\begin_inset Formula $p\times1$
\end_inset

 vector, show that 
\begin_inset Formula $D_{x}^{2}x^{\prime}Ax=A+A^{\prime}$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors, show that 
\begin_inset Formula $D_{\beta}\exp x^{\prime}\beta=\exp(x^{\prime}\beta)x$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 both 
\begin_inset Formula $p\times1$
\end_inset

 vectors, find the analytic expression for 
\begin_inset Formula $D_{\beta}^{2}\exp x^{\prime}\beta$
\end_inset

.
\end_layout

\begin_layout Enumerate
Write an Octave program that verifies each of the previous results by taking
 numeric derivatives.
 For a hint, type 
\family typewriter
help numgradient
\family default
 and 
\family typewriter
help numhessian
\family default
 inside octave.
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
The attic
\end_layout

\begin_layout Standard
This holds material that is not really ready to be incorporated into the
 main body, or that I believe distracts from the flow, but that I don't
 want to lose.
 Basically, ignore it.
\end_layout

\begin_layout Section
Efficient method of moments (EMM)
\end_layout

\begin_layout Standard
The choice of which moments upon which to base a GMM estimator can have
 very pronounced effects upon the efficiency of the estimator.
\end_layout

\begin_layout Itemize
A poor choice of moment conditions may lead to very inefficient estimators,
 and can even cause identification problems (as we've seen with the GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

problem set).
\end_layout

\begin_layout Itemize
The drawback of the above approach MSM is that the moment conditions used
 in estimation are selected arbitrarily.
 The asymptotic efficiency of the estimator may be low.
\end_layout

\begin_layout Itemize
The asymptotically optimal choice of moments would be the score vector of
 the likelihood function, 
\begin_inset Formula 
\[
m_{t}(\theta)=D_{\theta}\ln p_{t}(\theta\mid I_{t})
\]

\end_inset

 As before, this choice is unavailable.
\end_layout

\begin_layout Standard
The efficient method of moments (EMM) (see 
\begin_inset CommandInset citation
LatexCommand cite
key "emm"
literal "true"

\end_inset

) seeks to provide moment conditions that closely mimic the score vector.
 If the approximation is very good, the resulting estimator will be very
 nearly fully efficient.
 
\end_layout

\begin_layout Standard
The DGP is characterized by random sampling from the density 
\begin_inset Formula 
\[
p(y_{t}|x_{t},\theta^{0})\equiv p_{t}(\theta^{0})
\]

\end_inset


\end_layout

\begin_layout Standard
We can define an auxiliary model, called the 
\begin_inset Quotes eld
\end_inset

score generator
\begin_inset Quotes erd
\end_inset

, which simply provides a (misspecified) parametric density 
\begin_inset Formula 
\[
f(y|x_{t},\lambda)\equiv f_{t}(\lambda)
\]

\end_inset


\end_layout

\begin_layout Itemize
This density is known up to a parameter 
\begin_inset Formula $\lambda.$
\end_inset

 We assume that this density function 
\emph on
is
\emph default
 calculable.
 Therefore quasi-ML estimation is possible.
 Specifically, 
\begin_inset Formula 
\[
\hat{\lambda}=\arg\max_{\Lambda}s_{n}(\lambda)=\frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\lambda).
\]

\end_inset


\end_layout

\begin_layout Itemize
After determining 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 we can calculate the score functions 
\begin_inset Formula $D_{\lambda}\ln f(y_{t}|x_{t},\hat{\lambda})$
\end_inset

.
\end_layout

\begin_layout Itemize
The important point is that even if the density is misspecified, there is
 a pseudo-true 
\begin_inset Formula $\lambda^{0}$
\end_inset

 for which the true expectation, taken with respect to the true but unknown
 density of 
\begin_inset Formula $y,$
\end_inset

 
\begin_inset Formula $p(y|x_{t},\theta^{0}),$
\end_inset

 and then marginalized over 
\begin_inset Formula $x$
\end_inset

 is zero: 
\begin_inset Formula 
\[
\exists\lambda^{0}:\mathcal{E}_{X}\mathcal{E}_{Y|X}\left[D_{\lambda}\ln f(y|x,\lambda^{0})\right]=\int_{X}\int_{Y|X}D_{\lambda}\ln f(y|x,\lambda^{0})p(y|x,\theta^{0})dyd\mu(x)=0
\]

\end_inset


\end_layout

\begin_layout Itemize
We have seen in the section on QML that 
\begin_inset Formula $\hat{\lambda}\stackrel{p}{\rightarrow}\lambda^{0}$
\end_inset

; this suggests using the moment conditions 
\begin_inset Formula 
\begin{equation}
\bar{m}_{n}(\theta,\hat{\lambda})=\frac{1}{n}\sum_{t=1}^{n}\int D_{\lambda}\ln f_{t}(\hat{\lambda})p_{t}(\theta)dy\label{iimomcond}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
These moment conditions are not calculable, since 
\begin_inset Formula $p_{t}(\theta)$
\end_inset

 is not available, but they are simulable using 
\begin_inset Formula 
\[
\widetilde{\bar{m}_{n}}(\theta,\hat{\lambda})=\frac{1}{n}\sum_{t=1}^{n}\frac{1}{H}\sum_{h=1}^{H}D_{\lambda}\ln f(\widetilde{y}_{t}^{h}|x_{t},\hat{\lambda})
\]

\end_inset

 where 
\begin_inset Formula $\tilde{y}_{t}^{h}$
\end_inset

 is a draw from 
\begin_inset Formula $DGP(\theta),$
\end_inset

 holding 
\begin_inset Formula $x_{t}$
\end_inset

 fixed.
 By the LLN and the fact that 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 converges to 
\begin_inset Formula $\lambda^{0}$
\end_inset

, 
\begin_inset Formula 
\[
\widetilde{m}_{\infty}(\theta^{0},\lambda^{0})=0.
\]

\end_inset

 This is not the case for other values of 
\begin_inset Formula $\theta$
\end_inset

, assuming that 
\begin_inset Formula $\lambda^{0}$
\end_inset

 is identified.
\end_layout

\begin_layout Itemize
The advantage of this procedure is that if 
\begin_inset Formula $f(y_{t}|x_{t},\lambda)$
\end_inset

 closely approximates 
\begin_inset Formula $p(y|x_{t},\theta),$
\end_inset

 then 
\begin_inset Formula $\widetilde{m}_{n}(\theta,\hat{\lambda})$
\end_inset

 will closely approximate the optimal moment conditions which characterize
 maximum likelihood estimation, which is fully efficient.
\end_layout

\begin_layout Itemize
If one has prior information that a certain density approximates the data
 well, it would be a good choice for 
\begin_inset Formula $f(\cdot).$
\end_inset


\end_layout

\begin_layout Itemize
If one has no density in mind, there exist good ways of approximating unknown
 distributions parametrically: Philips' ERA's (
\shape italic
Econometrica
\shape default
, 1983) and Gallant and Nychka's (
\shape italic
Econometrica, 1987)
\shape default
 SNP density estimator which we saw before.
 Since the SNP density is consistent, the efficiency of the indirect estimator
 is the same as the infeasible ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\end_layout

\end_inset

estimator.
 
\end_layout

\begin_layout Subsection
Optimal weighting matrix
\end_layout

\begin_layout Standard
I will present the theory for 
\begin_inset Formula $H$
\end_inset

 finite, and possibly small.
 This is done because it is sometimes impractical to estimate with 
\begin_inset Formula $H$
\end_inset

 very large.
 Gallant and Tauchen give the theory for the case of 
\begin_inset Formula $H$
\end_inset

 so large that it may be treated as infinite (the difference being irrelevant
 given the numerical precision of a computer).
 The theory for the case of 
\begin_inset Formula $H$
\end_inset

 infinite follows directly from the results presented here.
\end_layout

\begin_layout Standard
The moment condition 
\begin_inset Formula $\widetilde{m}(\theta,\hat{\lambda})$
\end_inset

 depends on the pseudo-ML estimate 
\begin_inset Formula $\hat{\lambda}.$
\end_inset

 We can apply Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

 to conclude that 
\begin_inset Formula 
\begin{equation}
\sqrt{n}\left(\hat{\lambda}-\lambda^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}(\lambda^{0})^{-1}\mathcal{I}(\lambda^{0})\mathcal{J}(\lambda^{0})^{-1}\right]\label{lamdist}
\end{equation}

\end_inset

 If the density 
\begin_inset Formula $f(y_{t}|x_{t},\hat{\lambda})$
\end_inset

 were in fact the true density 
\begin_inset Formula $p(y|x_{t},\theta),$
\end_inset

 then 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 would be the maximum likelihood estimator, and 
\begin_inset Formula $\mathcal{J}(\lambda^{0})^{-1}\mathcal{I}(\lambda^{0})$
\end_inset

 would be an identity matrix, due to the information matrix equality.
 However, in the present case we assume that 
\begin_inset Formula $f(y_{t}|x_{t},\hat{\lambda})$
\end_inset

 is only an approximation to 
\begin_inset Formula $p(y|x_{t},\theta),$
\end_inset

 so there is no cancellation.
\end_layout

\begin_layout Standard
Recall that 
\begin_inset Formula $\mathcal{J}(\lambda^{0})\equiv p\lim\left(\frac{\partial^{2}}{\partial\lambda\partial\lambda^{\prime}}s_{n}(\lambda^{0})\right).$
\end_inset

 Comparing the definition of 
\begin_inset Formula $s_{n}(\lambda)$
\end_inset

 with the definition of the moment condition in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "iimomcond"

\end_inset

, we see that 
\begin_inset Formula 
\[
\mathcal{J}(\lambda^{0})=D_{\lambda^{\prime}}m(\theta^{0},\lambda^{0}).
\]

\end_inset

 As in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

, 
\begin_inset Formula 
\[
\mathcal{I}(\lambda^{0})=\lim_{n\rightarrow\infty}\mathcal{E}\left[n\left.\frac{\partial s_{n}(\lambda)}{\partial\lambda}\right|_{\lambda^{0}}\left.\frac{\partial s_{n}(\lambda)}{\partial\lambda^{\prime}}\right|_{\lambda^{0}}\right].
\]

\end_inset

 In this case, this is simply the asymptotic variance covariance matrix
 of the moment conditions, 
\begin_inset Formula $\Omega.$
\end_inset

 Now take a first order Taylor's series approximation to 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\theta^{0},\hat{\lambda})$
\end_inset

 about 
\begin_inset Formula $\lambda^{0}$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sqrt{n}\tilde{m}_{n}(\theta^{0},\hat{\lambda})=\sqrt{n}\tilde{m}_{n}(\theta^{0},\lambda^{0})+\sqrt{n}D_{\lambda^{\prime}}\tilde{m}(\theta^{0},\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)+o_{p}(1)
\]

\end_inset


\end_layout

\begin_layout Standard
First consider 
\begin_inset Formula $\sqrt{n}\tilde{m}_{n}(\theta^{0},\lambda^{0})$
\end_inset

.
 It is straightforward but somewhat tedious to show that the asymptotic
 variance of this term is 
\begin_inset Formula $\frac{1}{H}I_{\infty}(\lambda^{0})$
\end_inset

.
 
\end_layout

\begin_layout Standard
Next consider the second term 
\begin_inset Formula $\sqrt{n}D_{\lambda^{\prime}}\tilde{m}(\theta^{0},\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)$
\end_inset

.
 Note that 
\begin_inset Formula $D_{\lambda^{\prime}}\tilde{m}_{n}(\theta^{0},\lambda^{0})\stackrel{a.s.}{\rightarrow}\mathcal{J}(\lambda^{0}),$
\end_inset

 so we have
\begin_inset Formula 
\[
\sqrt{n}D_{\lambda^{\prime}}\tilde{m}(\theta^{0},\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)=\sqrt{n}\mathcal{J}(\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right),a.s.
\]

\end_inset

 But noting equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "lamdist"

\end_inset


\begin_inset Formula 
\[
\sqrt{n}\mathcal{J}(\lambda^{0})\left(\hat{\lambda}-\lambda^{0}\right)\stackrel{a}{\sim}N\left[0,\mathcal{I}(\lambda^{0})\right]
\]

\end_inset

Now, combining the results for the first and second terms, 
\begin_inset Formula 
\[
\sqrt{n}\tilde{m}_{n}(\theta^{0},\hat{\lambda})\stackrel{a}{\sim}N\left[0,\left(1+\frac{1}{H}\right)\mathcal{I}(\lambda^{0})\right]
\]

\end_inset

 Suppose that 
\begin_inset Formula $\widehat{\mathcal{I}(\lambda^{0})}$
\end_inset

 is a consistent estimator of the asymptotic variance-covariance matrix
 of the moment conditions.
 This may be complicated if the score generator is a poor approximator,
 since the individual score contributions may not have mean zero in this
 case (see the section on QML) .
 Even if this is the case, the individuals means can be calculated by simulation
, so it is always possible to consistently estimate 
\begin_inset Formula $\mathcal{I}(\lambda^{0})$
\end_inset

 when the model is simulable.
 On the other hand, if the score generator is taken to be correctly specified,
 the ordinary estimator of the information matrix is consistent.
 Combining this with the result on the efficient GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

weighting matrix in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "efficient weighting matrix"

\end_inset

, we see that defining 
\begin_inset Formula $\hat{\theta}$
\end_inset

 as 
\begin_inset Formula 
\[
\hat{\theta}=\arg\min_{\Theta}\bar{m}_{n}(\theta,\hat{\lambda})^{\prime}\left[\left(1+\frac{1}{H}\right)\widehat{\mathcal{I}(\lambda^{0})}\right]^{-1}\bar{m}_{n}(\theta,\hat{\lambda})
\]

\end_inset

 is the GMM estimator with the efficient choice of weighting matrix.
\end_layout

\begin_layout Itemize
If one has used the Gallant-Nychka ML estimator as the auxiliary model,
 the appropriate weighting matrix is simply the information matrix of the
 auxiliary model, since the scores are uncorrelated.
 (e.g., it really is ML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimation asymptotically, since the score generator can approximate the
 unknown density arbitrarily well).
 
\end_layout

\begin_layout Subsection
Asymptotic distribution
\end_layout

\begin_layout Standard
Since we use the optimal weighting matrix, the asymptotic distribution is
 as in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "gmm distribution with optimal weighting matrix"

\end_inset

, so we have (using the result in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "lamdist"

\end_inset

): 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\left(D_{\infty}\left[\left(1+\frac{1}{H}\right)\mathcal{I}(\lambda^{0})\right]^{-1}D_{\infty}^{\prime}\right)^{-1}\right],
\]

\end_inset

 where 
\begin_inset Formula 
\[
D_{\infty}=\lim_{n\rightarrow\infty}\mathcal{E}\left[D_{\theta}\bar{m}_{n}(\theta^{0},\lambda^{0})\right].
\]

\end_inset

 This can be consistently estimated using 
\begin_inset Formula 
\[
\hat{D}=D_{\theta}\bar{m}_{n}(\hat{\theta},\hat{\lambda})
\]

\end_inset


\end_layout

\begin_layout Subsection
Diagnostic testing
\end_layout

\begin_layout Standard
The fact that 
\begin_inset Formula 
\[
\sqrt{n}\bar{m}_{n}(\theta^{0},\hat{\lambda})\stackrel{a}{\sim}N\left[0,\left(1+\frac{1}{H}\right)\mathcal{I}(\lambda^{0})\right]
\]

\end_inset

 implies that 
\begin_inset Formula 
\[
n\bar{m}_{n}(\hat{\theta},\hat{\lambda})^{\prime}\left[\left(1+\frac{1}{H}\right)\mathcal{I}(\hat{\lambda})\right]^{-1}\bar{m}_{n}(\hat{\theta},\hat{\lambda})\stackrel{a}{\sim}\chi^{2}(q)
\]

\end_inset

 where 
\begin_inset Formula $q$
\end_inset

 is 
\begin_inset Formula $\dim(\lambda)-\dim(\theta),$
\end_inset

 since without 
\begin_inset Formula $\dim(\theta)$
\end_inset

 moment conditions the model is not identified, so testing is impossible.
 One test of the model is simply based on this statistic: if it exceeds
 the 
\begin_inset Formula $\chi^{2}(q)$
\end_inset

 critical point, something may be wrong (the small sample performance of
 this sort of test would be a topic worth investigating).
\end_layout

\begin_layout Itemize
Information about what is wrong can be gotten from the pseudo-t-statistics:
 
\begin_inset Formula 
\[
\left(\text{diag}\left[\left(1+\frac{1}{H}\right)\mathcal{I}(\hat{\lambda})\right]^{1/2}\right)^{-1}\sqrt{n}\bar{m}_{n}(\hat{\theta},\hat{\lambda})
\]

\end_inset

 can be used to test which moments are not well modeled.
 Since these moments are related to parameters of the score generator, which
 are usually related to certain features of the model, this information
 can be used to revise the model.
 These aren't actually distributed as 
\begin_inset Formula $N(0,1),$
\end_inset

 since 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\theta^{0},\hat{\lambda})$
\end_inset

 and 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\hat{\theta},\hat{\lambda})$
\end_inset

 have different distributions (that of 
\begin_inset Formula $\sqrt{n}\bar{m}_{n}(\hat{\theta},\hat{\lambda})$
\end_inset

 is somewhat more complicated).
 It can be shown that the pseudo-t statistics are biased toward nonrejection.
 See Gourieroux 
\emph on
et.
 al.

\emph default
 or Gallant and Long, 1995, for more details.
 
\end_layout

\begin_layout Section
Indirect likelihood inference and Approximate Bayesian Computing (ABC)
\end_layout

\begin_layout Standard
to be added
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "cha:Parallel-programming-for"

\end_inset

Parallel programming for econometrics
\end_layout

\begin_layout Standard
The following borrows heavily from Creel (2005).
 
\end_layout

\begin_layout Standard
Parallel computing can offer an important reduction in the time to complete
 computations.
 This is well-known, but it bears emphasis since it is the main reason that
 parallel computing may be attractive to users.
 To illustrate, the Intel Pentium IV (Willamette) processor, running at
 1.5GHz, was introduced in November of 2000.
 The Pentium IV (Northwood-HT) processor, running at 3.06GHz, was introduced
 in November of 2002.
 An approximate doubling of the performance of a commodity CPU took place
 in two years.
 Extrapolating this admittedly rough snapshot of the evolution of the performanc
e of commodity processors, one would need to wait more than 6.6 years and
 then purchase a new computer to obtain a 10-fold improvement in computational
 performance.
 The examples in this chapter show that a 10-fold improvement in performance
 can be achieved immediately, using distributed parallel computing on available
 computers.
\end_layout

\begin_layout Standard
Recent (this is written in 2005) developments that may make parallel computing
 attractive to a broader spectrum of researchers who do computations.
 The first is the fact that setting up a cluster of computers for distributed
 parallel computing is not difficult.
 If you are using the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{ParallelKnoppix}{http://pareto.uab.es/mcreel/ParallelKnoppix}
\end_layout

\end_inset

 bootable CD that accompanies these notes, you are less than 10 minutes
 away from creating a cluster, supposing you have a second computer at hand
 and a crossover ethernet cable.
 See the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{ParallelKnoppix tutorial}{http://pareto.uab.es/mcreel/ParallelKn
oppix/ParallelKnoppixTutorial.html}
\end_layout

\end_inset

.
 A second development is the existence of extensions to some of the high-level
 matrix programming (HLMP) languages
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
By 
\begin_inset Quotes sld
\end_inset

high-level matrix programming language
\begin_inset Quotes srd
\end_inset

 I mean languages such as MATLAB (TM the Mathworks, Inc.), Ox (TM OxMetrics
 Technologies, Ltd.), and GNU Octave (
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

www.octave.org
\end_layout

\end_inset

), for example.
\end_layout

\end_inset

 that allow the incorporation of parallelism into programs written in these
 languages.
 A third is the spread of dual and quad-core CPUs, so that an ordinary desktop
 or laptop computer can be made into a mini-cluster.
 Those cores won't work together on a single problem unless they are told
 how to.
\end_layout

\begin_layout Standard
Following are examples of parallel implementations of several mainstream
 problems in econometrics.
 A focus of the examples is on the possibility of hiding parallelization
 from end users of programs.
 If programs that run in parallel have an interface that is nearly identical
 to the interface of equivalent serial versions, end users will find it
 easy to take advantage of parallel computing's performance.
 We continue to use Octave, taking advantage of the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{MPI Toolbox (MPITB) for Octave}{http://atc.ugr.es/javier-bin/mpi
tb}
\end_layout

\end_inset

, by by Fernndez Baldomero 
\emph on
et al.

\emph default
 (2004).
 There are also parallel packages for Ox, R, and Python which may be of
 interest to econometricians, but as of this writing, the following examples
 are the most accessible introduction to parallel programming for econometrician
s.
\end_layout

\begin_layout Subsection
Example problems
\end_layout

\begin_layout Standard
This section introduces example problems from econometrics, and shows how
 they can be parallelized in a natural way.
\end_layout

\begin_layout Subsubsection
Monte Carlo
\end_layout

\begin_layout Standard
A Monte Carlo study involves repeating a random experiment many times under
 identical conditions.
 Several authors have noted that Monte Carlo studies are obvious candidates
 for parallelization (Doornik 
\emph on
et al.

\emph default
 2002; Bruche, 2003) since blocks of replications can be done independently
 on different computers.
 To illustrate the parallelization of a Monte Carlo study, we use same trace
 test example as do Doornik, 
\emph on
et.
 al.

\emph default
 (2002).
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{tracetest.m}{https://github.com/mcreel/Econometrics/blob/master/
Examples/Parallel/montecarlo/tracetest.m}
\end_layout

\end_inset

 is a function that calculates the trace test statistic for the lack of
 cointegration of integrated time series.
 This function is illustrative of the format that we adopt for Monte Carlo
 simulation of a function: it receives a single argument of cell type, and
 it returns a row vector that holds the results of one random simulation.
 The single argument in this case is a cell array that holds the length
 of the series in its first position, and the number of series in the second
 position.
 It generates a random result though a process that is internal to the function,
 and it reports some output in a row vector (in this case the result is
 a scalar).
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{mc
\backslash
_example1.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/Parallel/
montecarlo/mc
\backslash
_example1.m}
\end_layout

\end_inset

 is an Octave script that executes a Monte Carlo study of the trace test
 by repeatedly evaluating the 
\family typewriter
tracetest.m
\family default
 function.
 The main thing to notice about this script is that lines 7 and 10 call
 the function 
\family typewriter
montecarlo.m.

\family default
 When called with 3 arguments, as in line 7, 
\family typewriter
montecarlo.m
\family default
 executes serially on the computer it is called from.
 In line 10, there is a fourth argument.
 When called with four arguments, the last argument is the number of slave
 hosts to use.
 We see that running the Monte Carlo study on one or more processors is
 transparent to the user - he or she must only indicate the number of slave
 computers to be used.
\end_layout

\begin_layout Subsubsection
ML
\end_layout

\begin_layout Standard
For a sample 
\begin_inset Formula $\left\{ (y_{t},x_{t})\right\} _{n}$
\end_inset

 of 
\begin_inset Formula $n$
\end_inset

 observations of a set of dependent and explanatory variables, the maximum
 likelihood estimator of the parameter 
\begin_inset Formula $\theta$
\end_inset

 can be defined as 
\begin_inset Formula 
\[
\hat{\theta}=\arg\max s_{n}(\theta)
\]

\end_inset

where
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}\ln f(y_{t}|x_{t},\theta)
\]

\end_inset

Here, 
\begin_inset Formula $y_{t}$
\end_inset

 may be a vector of random variables, and the model may be dynamic since
 
\begin_inset Formula $x_{t}$
\end_inset

 may contain lags of 
\begin_inset Formula $y_{t}$
\end_inset

.
 As Swann (2002) points out, this can be broken into sums over blocks of
 observations, for example two blocks:
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\left\{ \left(\sum_{t=1}^{n_{1}}\ln f(y_{t}|x_{t},\theta)\right)+\left(\sum_{t=n_{1}+1}^{n}\ln f(y_{t}|x_{t},\theta)\right)\right\} 
\]

\end_inset

Analogously, we can define up to 
\begin_inset Formula $n$
\end_inset

 blocks.
 Again following Swann, parallelization can be done by calculating each
 block on separate computers.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{mle
\backslash
_example1.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/Parallel/
mle/mle
\backslash
_example1.m}
\end_layout

\end_inset

 is an Octave script that calculates the maximum likelihood estimator of
 the parameter vector of a model that assumes that the dependent variable
 is distributed as a Poisson random variable, conditional on some explanatory
 variables.
 In lines 1-3 the data is read, the name of the density function is provided
 in the variable 
\family typewriter
model
\family default
, and the initial value of the parameter vector is set.
 In line 5, the function 
\family typewriter
mle_estimate
\family default
 performs ordinary serial calculation of the ML estimator, while in line
 7 the same function is called with 6 arguments.
 The fourth and fifth arguments are empty placeholders where options to
 
\family typewriter
mle_estimate
\family default
 may be set, while the sixth argument is the number of slave computers to
 use for parallel execution, 1 in this case.
 A person who runs the program sees no parallel programming code - the paralleli
zation is transparent to the end user, beyond having to select the number
 of slave computers.
 When executed, this script prints out the estimates 
\family typewriter
theta_s
\family default
 and 
\family typewriter
theta_p
\family default
, which are identical.
\end_layout

\begin_layout Standard
It is worth noting that a different likelihood function may be used by making
 the 
\family typewriter
model
\family default
 variable point to a different function.
 The likelihood function itself is an ordinary Octave function that is not
 parallelized.
 The 
\family typewriter
mle_estimate
\family default
 function is a generic function that can call any likelihood function that
 has the appropriate input/output syntax for evaluation either serially
 or in parallel.
 Users need only learn how to write the likelihood function using the Octave
 language.
 
\end_layout

\begin_layout Subsubsection
GMM
\end_layout

\begin_layout Standard
For a sample as above, the GMM estimator of the parameter 
\begin_inset Formula $\theta$
\end_inset

 can be defined as
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}s_{n}(\theta)
\]

\end_inset

where 
\begin_inset Formula 
\[
s_{n}(\theta)=\bar{m}_{n}(\theta)^{\prime}W_{n}\bar{m}_{n}(\theta)
\]

\end_inset

 and 
\begin_inset Formula 
\[
\bar{m}_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}m_{t}(y_{t}|x_{t},\theta)
\]

\end_inset

 Since 
\begin_inset Formula $\bar{m}_{n}(\theta)$
\end_inset

 is an average, it can obviously be computed blockwise, using for example
 2 blocks:
\begin_inset Formula 
\begin{equation}
\bar{m}_{n}(\theta)=\frac{1}{n}\left\{ \left(\sum_{t=1}^{n_{1}}m_{t}(y_{t}|x_{t},\theta)\right)+\left(\sum_{t=n_{1}+1}^{n}m_{t}(y_{t}|x_{t},\theta)\right)\right\} \label{eq:gmm moment contributions}
\end{equation}

\end_inset

Likewise, we may define up to 
\begin_inset Formula $n$
\end_inset

 blocks, each of which could potentially be computed on a different machine.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{gmm
\backslash
_example1.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/Parallel/
gmm/gmm
\backslash
_example1.m}
\end_layout

\end_inset

 is a script that illustrates how GMM estimation may be done serially or
 in parallel.
 When this is run, 
\family typewriter
theta_s
\family default
 and 
\family typewriter
theta_p
\family default
 are identical up to the tolerance for convergence of the minimization routine.
 The point to notice here is that an end user can perform the estimation
 in parallel in virtually the same way as it is done serially.
 Again, 
\family typewriter
gmm_estimate
\family default
, used in lines 8 and 10, is a generic function that will estimate any model
 specified by the 
\family typewriter
moments
\family default
 variable - a different model can be estimated by changing the value of
 the 
\family typewriter
moments
\family default
 variable.
 The function that 
\family typewriter
moments
\family default
 points to is an ordinary Octave function that uses no parallel programming,
 so users can write their models using the simple and intuitive HLMP syntax
 of Octave.
 Whether estimation is done in parallel or serially depends only the seventh
 argument to 
\family typewriter
gmm_estimate
\family default
 - when it is missing or zero, estimation is by default done serially with
 one processor.
 When it is positive, it specifies the number of slave nodes to use.
\end_layout

\begin_layout Subsubsection
Kernel regression
\end_layout

\begin_layout Standard
The Nadaraya-Watson kernel regression estimator of a function 
\begin_inset Formula $g(x)$
\end_inset

 at a point 
\begin_inset Formula $x$
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
\hat{g}(x) & = & \frac{\sum_{t=1}^{n}y_{t}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}{\sum_{t=1}^{n}K\left[\left(x-x_{t}\right)/\gamma_{n}\right]}\\
 & \equiv & \sum_{t=1}^{n}w_{t}y_{y}
\end{eqnarray*}

\end_inset

We see that the weight depends upon every data point in the sample.
 To calculate the fit at every point in a sample of size 
\begin_inset Formula $n,$
\end_inset

 on the order of 
\begin_inset Formula $n^{2}k$
\end_inset

 calculations must be done, where 
\begin_inset Formula $k$
\end_inset

 is the dimension of the vector of explanatory variables, 
\begin_inset Formula $x$
\end_inset

.
 Racine (2002) demonstrates that MPI parallelization can be used to speed
 up calculation of the kernel regression estimator by calculating the fits
 for portions of the sample on different computers.
 We follow this implementation here.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{kernel
\backslash
_example1.m}{https://github.com/mcreel/Econometrics/blob/master/Examples/Parallel/
kernel/kernel
\backslash
_example1.m}
\end_layout

\end_inset

 is a script for serial and parallel kernel regression.
 Serial execution is obtained by setting the number of slaves equal to zero,
 in line 15.
 In line 17, a single slave is specified, so execution is in parallel on
 the master and slave nodes.
\end_layout

\begin_layout Standard
The example programs show that parallelization may be mostly hidden from
 end users.
 Users can benefit from parallelization without having to write or understand
 parallel code.
 The speedups one can obtain are highly dependent upon the specific problem
 at hand, as well as the size of the cluster, the efficiency of the network,
 
\emph on
etc.

\emph default
 Some examples of speedups are presented in Creel (2005).
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Speedups-from-parallelization"

\end_inset

 reproduces speedups for some econometric problems on a cluster of 12 desktop
 computers.
 The speedup for 
\begin_inset Formula $k$
\end_inset

 nodes is the time to finish the problem on a single node divided by the
 time to finish the problem on 
\begin_inset Formula $k$
\end_inset

 nodes.
 Note that you can get 10X speedups, as claimed in the introduction.
 It's pretty obvious that much greater speedups could be obtained using
 a larger cluster, for the 
\begin_inset Quotes sld
\end_inset

embarrassingly parallel
\begin_inset Quotes srd
\end_inset

 problems.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Speedups-from-parallelization"

\end_inset

Speedups from parallelization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/speedup.pdf

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection
Duration data and the Weibull model
\end_layout

\begin_layout Standard
In some cases the dependent variable may be the time that passes between
 the occurrence of two events.
 For example, it may be the duration of a strike, or the time needed to
 find a job once one is unemployed.
 Such variables take on values on the positive real line, and are referred
 to as duration data.
\end_layout

\begin_layout Standard
A 
\emph on
spell
\emph default
 is the period of time between the occurrence of initial event and the concludin
g event.
 For example, the initial event could be the loss of a job, and the final
 event is the finding of a new job.
 The spell is the period of unemployment.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $t_{0}$
\end_inset

 be the time the initial event occurs, and 
\begin_inset Formula $t_{1}$
\end_inset

 be the time the concluding event occurs.
 For simplicity, assume that time is measured in years.
 The random variable 
\begin_inset Formula $D$
\end_inset

 is the duration of the spell, 
\begin_inset Formula $D=t_{1}-t_{0}$
\end_inset

.
 Define the density function of 
\begin_inset Formula $D,$
\end_inset

 
\begin_inset Formula $f_{D}(t),$
\end_inset

 with distribution function 
\begin_inset Formula $F_{D}(t)=\Pr(D<t).$
\end_inset


\end_layout

\begin_layout Standard
Several questions may be of interest.
 For example, one might wish to know the expected time one has to wait to
 find a job given that one has already waited 
\begin_inset Formula $s$
\end_inset

 years.
 The probability that a spell lasts more than 
\begin_inset Formula $s$
\end_inset

 years is 
\begin_inset Formula 
\[
\Pr(D>s)=1-\Pr(D\leq s)=1-F_{D}(s).
\]

\end_inset

 The density of 
\begin_inset Formula $D$
\end_inset

 conditional on the spell being longer than 
\begin_inset Formula $s$
\end_inset

 years is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{D}(t|D>s)=\frac{f_{D}(t)}{1-F_{D}(s)}.
\]

\end_inset

 The expected additional time required for the spell to end given that is
 has already lasted 
\begin_inset Formula $s$
\end_inset

 years is the expectation of 
\begin_inset Formula $D$
\end_inset

 with respect to this density, minus 
\begin_inset Formula $s.$
\end_inset


\begin_inset Formula 
\[
E=\mathcal{E}(D|D>s)-s=\left(\int_{t}^{\infty}z\frac{f_{D}(z)}{1-F_{D}(s)}dz\right)-s
\]

\end_inset


\end_layout

\begin_layout Standard
To estimate this function, one needs to specify the density 
\begin_inset Formula $f_{D}(t)$
\end_inset

 as a parametric density, then estimate by maximum likelihood.
 There are a number of possibilities including the exponential density,
 the lognormal, 
\emph on
etc.

\emph default
 A reasonably flexible model that is a generalization of the exponential
 density is the Weibull density
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{D}(t|\theta)=e^{-\left(\lambda t\right)^{\gamma}}\lambda\gamma(\lambda t)^{\gamma-1}.
\]

\end_inset

 According to this model, 
\begin_inset Formula $\mathcal{E}(D)=\lambda^{-\gamma}.$
\end_inset

 The log-likelihood is just the product of the log densities.
\end_layout

\begin_layout Standard
To illustrate application of this model, 402 observations on the lifespan
 of dwarf mongooses in Serengeti National Park (Tanzania) were used to fit
 a Weibull model.
 The 
\begin_inset Quotes sld
\end_inset

spell
\begin_inset Quotes srd
\end_inset

 in this case is the lifetime of an individual mongoose.
 The parameter estimates and standard errors are 
\begin_inset Formula $\hat{\lambda}=0.559\,(0.034)$
\end_inset

 and 
\begin_inset Formula $\hat{\gamma}=0.867\,(0.033)$
\end_inset

 and the log-likelihood value is -659.3.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:Life-expectancy-of"

\end_inset

 presents fitted life expectancy (expected additional years of life) as
 a function of age, with 95%
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

confidence bands.
 The plot is accompanied by a nonparametric Kaplan-Meier estimate of life-expect
ancy.
 This nonparametric estimator simply averages all spell lengths greater
 than age, and then subtracts age.
 This is consistent by the LLN.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:Life-expectancy-of"

\end_inset

Life expectancy of mongooses, Weibull model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/weibull.pdf
	width 6in

\end_inset


\end_layout

\end_inset

In the figure one can see that the model doesn't fit the data well, in that
 it predicts life expectancy quite differently than does the nonparametric
 model.
 For ages 4-6, the nonparametric estimate is outside the confidence interval
 that results from the parametric model, which casts doubt upon the parametric
 model.
 Mongooses that are between 2-6 years old seem to have a lower life expectancy
 than is predicted by the Weibull model, whereas young mongooses that survive
 beyond infancy have a higher life expectancy, up to a bit beyond 2 years.
 Due to the dramatic change in the death rate as a function of 
\begin_inset Formula $t$
\end_inset

, one might specify 
\begin_inset Formula $f_{D}(t)$
\end_inset

 as a mixture of two Weibull densities, 
\begin_inset Formula 
\[
f_{D}(t|\theta)=\delta\left(e^{-\left(\lambda_{1}t\right)^{\gamma_{1}}}\lambda_{1}\gamma_{1}(\lambda_{1}t)^{\gamma_{1}-1}\right)+\left(1-\delta\right)\left(e^{-\left(\lambda_{2}t\right)^{\gamma_{2}}}\lambda_{2}\gamma_{2}(\lambda_{2}t)^{\gamma_{2}-1}\right).
\]

\end_inset

 The parameters 
\begin_inset Formula $\gamma_{i}$
\end_inset

 and 
\begin_inset Formula $\lambda_{i},i=1,2$
\end_inset

 are the parameters of the two Weibull densities, and 
\begin_inset Formula $\delta$
\end_inset

 is the parameter that mixes the two.
\end_layout

\begin_layout Standard
With the same data, 
\begin_inset Formula $\theta$
\end_inset

 can be estimated using the mixed model.
 The results are a log-likelihood = -623.17.
 Note that a standard likelihood ratio test cannot be used to chose between
 the two models, since under the null that 
\begin_inset Formula $\delta=1$
\end_inset

 (single density), the two parameters 
\begin_inset Formula $\lambda_{2}$
\end_inset

 and 
\begin_inset Formula $\gamma_{2}$
\end_inset

 are not identified.
 It is possible to take this into account, but this topic is out of the
 scope of this course.
 Nevertheless, the improvement in the likelihood function is considerable.
 The parameter estimates are
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features firstHeadEmpty="true" tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt">
<column alignment="left" valignment="top" width="0pt">
<column alignment="left" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 Parameter 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 Estimate 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 St.
 Error 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\lambda_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.233 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.016 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\gamma_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 1.722 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.166 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\lambda_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 1.731 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.101 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\gamma_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 1.522 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.096 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $\delta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.428 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 0.035 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that the mixture parameter is highly significant.
 This model leads to the fit in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "mixed weibull"

\end_inset

.
 Note that the parametric and nonparametric fits are quite close to one
 another, up to around 
\begin_inset Formula $6$
\end_inset

 years.
 The disagreement after this point is not too important, since less than
 5% of mongooses live more than 6 years, which implies that the Kaplan-Meier
 nonparametric estimate has a high variance (since it's an average of a
 small number of observations).
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mixed weibull"

\end_inset

Life expectancy of mongooses, mixed Weibull model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Examples/Figures/mixed.pdf
	width 6in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Mixture models are often an effective way to model complex responses, though
 they can suffer from overparameterization.
 Alternatives will be discussed later.
\end_layout

\begin_layout Standard
For examples of MLE using the Poisson model applied to count data, see Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"

\end_inset

 in the chapter on Numerical Optimization.
 You should examine the scripts and run them to see how MLE is actually
 done, and how parameter standard errors are estimated.
 
\end_layout

\begin_layout Section
Quasi-ML
\end_layout

\begin_layout Standard
Quasi-ML is the estimator one obtains when a misspecified probability model
 is used to calculate an 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimator.
\end_layout

\begin_layout Standard
Given a sample of size 
\begin_inset Formula $n$
\end_inset

 of a random vector 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and a vector of conditioning variables 
\begin_inset Formula $\mathbf{x},$
\end_inset

 suppose the joint density of 
\begin_inset Formula $\mathbf{Y}=\left(\begin{array}{ccc}
\mathbf{y}_{1} & \ldots & \mathbf{y}_{n}\end{array}\right)$
\end_inset

 conditional on 
\begin_inset Formula $\mathbf{X}=\left(\begin{array}{ccc}
\mathbf{x}_{1} & \ldots & \mathbf{x}_{n}\end{array}\right)$
\end_inset

 is a member of the parametric family 
\begin_inset Formula $p_{\mathcal{Y}}(\mathbf{Y}|\mathbf{X},\rho),$
\end_inset

 
\begin_inset Formula $\rho\in\Xi.$
\end_inset

 The true joint density is associated with the vector 
\begin_inset Formula $\rho^{0}:$
\end_inset


\begin_inset Formula 
\[
p_{\mathcal{Y}}(\mathbf{Y}|\mathbf{X},\rho^{0}).
\]

\end_inset


\begin_inset Newline newline
\end_inset

As long as the marginal density of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 doesn't depend on 
\begin_inset Formula $\rho^{0},$
\end_inset

 this conditional density fully characterizes the random characteristics
 of samples: i.e., it fully describes the probabilistically important features
 of the d.g.p.
 The 
\emph on
likelihood function
\emph default
 is just this density evaluated at other values 
\begin_inset Formula $\rho$
\end_inset


\begin_inset Formula 
\[
L(\mathbf{Y}|\mathbf{X},\rho)=p_{\mathcal{Y}}(\mathbf{Y}|\mathbf{X},\rho),\rho\in\Xi.
\]

\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\mathbf{Y}_{t-1}=\left(\begin{array}{ccc}
\mathbf{y}_{1} & \ldots & \mathbf{y}_{t-1}\end{array}\right)$
\end_inset

, 
\begin_inset Formula $\mathbf{Y}_{0}=0,$
\end_inset

 and let 
\begin_inset Formula $\mathbf{X}_{t}=\left(\begin{array}{ccc}
\mathbf{x}_{1} & \ldots & \mathbf{x}_{t}\end{array}\right)$
\end_inset

 The likelihood function, taking into account possible dependence of observation
s, can be written as 
\begin_inset Formula 
\begin{eqnarray*}
L(\mathbf{Y}|\mathbf{X},\rho) & = & \prod_{t=1}^{n}p_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\rho)\\
 & \equiv & \prod_{t=1}^{n}p_{t}(\rho)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The average log-likelihood function is: 
\begin_inset Formula 
\[
s_{n}(\rho)=\frac{1}{n}\ln L(\mathbf{Y}|\mathbf{X},\rho)=\frac{1}{n}\sum_{t=1}^{n}\ln p_{t}(\rho)
\]

\end_inset


\end_layout

\begin_layout Itemize
Suppose that we do not have knowledge of the family of densities 
\begin_inset Formula $p_{t}(\rho).$
\end_inset

 Mistakenly, we may assume that the conditional density of 
\begin_inset Formula $\mathbf{y}_{t}$
\end_inset

 is a member of the family 
\begin_inset Formula $f_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\theta),$
\end_inset

 
\begin_inset Formula $\theta\in\Theta,$
\end_inset

 where there is no 
\begin_inset Formula $\theta^{0}$
\end_inset

 such that 
\begin_inset Formula $f_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\theta^{0})=p_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\rho^{0}),\forall t$
\end_inset

 (this is what we mean by 
\begin_inset Quotes eld
\end_inset

misspecified
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Itemize
This setup allows for heterogeneous time series data, with dynamic misspecificat
ion.
 
\end_layout

\begin_layout Standard
The QML estimator is the argument that maximizes the 
\series bold
misspecified
\series default
 average log likelihood, which we refer to as the quasi-log likelihood function.
 This objective function is 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\theta) & = & \frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\mathbf{y}_{t}|\mathbf{Y}_{t-1},\mathbf{X}_{t},\theta^{0})\\
 & \equiv & \frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\theta)
\end{eqnarray*}

\end_inset

 and the QML
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

is 
\begin_inset Formula 
\[
\hat{\theta}_{n}=\arg\max_{\Theta}s_{n}(\theta)
\]

\end_inset

 A SLLN for dependent sequences applies (we assume), so that 
\begin_inset Formula 
\[
s_{n}(\theta)\stackrel{a.s.}{\rightarrow}\lim_{n\rightarrow\infty}\mathcal{E}\frac{1}{n}\sum_{t=1}^{n}\ln f_{t}(\theta)\equiv s_{\infty}(\theta)
\]

\end_inset

 We assume that this can be strengthened to uniform convergence, a.s., following
 the previous arguments.
 The 
\begin_inset Quotes eld
\end_inset

pseudo-true
\begin_inset Quotes erd
\end_inset

 value of 
\begin_inset Formula $\theta$
\end_inset

 is the value that maximizes 
\begin_inset Formula $\bar{s}(\theta)$
\end_inset

: 
\begin_inset Formula 
\[
\theta^{0}=\arg\max_{\Theta}s_{\infty}(\theta)
\]

\end_inset

 Given assumptions so that theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

 is applicable, we obtain 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\hat{\theta}_{n}=\theta^{0},\text{a.s.}
\]

\end_inset

 
\end_layout

\begin_layout Itemize
Applying the asymptotic normality theorem, 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta^{0})^{-1}\mathcal{I}_{\infty}(\theta^{0})\mathcal{J}_{\infty}(\theta^{0})^{-1}\right]
\]

\end_inset

 where 
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\theta^{0})=\lim_{n\rightarrow\infty}\mathcal{E}D_{\theta}^{2}s_{n}(\theta^{0})
\]

\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta^{0})=\lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta^{0}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Note that asymptotic normality only requires that the additional assumptions
 regarding 
\begin_inset Formula $\mathcal{J}$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}$
\end_inset

 hold in a neighborhood of 
\begin_inset Formula $\theta^{0}$
\end_inset

 for 
\begin_inset Formula $\mathcal{J}$
\end_inset

 and at 
\begin_inset Formula $\theta^{0},$
\end_inset

 for 
\begin_inset Formula $\mathcal{I},$
\end_inset

 not throughout 
\begin_inset Formula $\Theta.$
\end_inset

 In this sense, asymptotic normality is a local property.
 
\end_layout

\begin_layout Subsection
Consistent Estimation of Variance Components
\end_layout

\begin_layout Standard
Consistent estimation of 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta^{0})$
\end_inset

 is straightforward.
 Assumption (b) of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

 implies that 
\begin_inset Formula 
\[
\mathcal{J}_{n}(\hat{\theta}_{n})=\frac{1}{n}\sum_{t=1}^{n}D_{\theta}^{2}\ln f_{t}(\hat{\theta}_{n})\stackrel{a.s.}{\rightarrow}\lim_{n\rightarrow\infty}\mathcal{E}\frac{1}{n}\sum_{t=1}^{n}D_{\theta}^{2}\ln f_{t}(\theta^{0})=\mathcal{J}_{\infty}(\theta^{0}).
\]

\end_inset

 That is, just calculate the Hessian using the estimate 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 in place of 
\begin_inset Formula $\theta^{0}.$
\end_inset


\end_layout

\begin_layout Standard
Consistent estimation of 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta^{0})$
\end_inset

 is more difficult, and may be impossible.
\end_layout

\begin_layout Itemize

\series bold
Notation
\series default
: Let 
\begin_inset Formula $g_{t}\equiv D_{\theta}f_{t}(\theta^{0})$
\end_inset


\end_layout

\begin_layout Standard
We need to estimate 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta^{0}) & = & \lim_{n\rightarrow\infty}Var\sqrt{n}D_{\theta}s_{n}(\theta^{0})\\
 & = & \lim_{n\rightarrow\infty}Var\sqrt{n}\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f_{t}(\theta^{0})\\
 & = & \lim_{n\rightarrow\infty}\frac{1}{n}Var\sum_{t=1}^{n}g_{t}\\
 & = & \lim_{n\rightarrow\infty}\frac{1}{n}\mathcal{E}\left\{ \left(\sum_{t=1}^{n}\left(g_{t}-\mathcal{E}g_{t}\right)\right)\left(\sum_{t=1}^{n}\left(g_{t}-\mathcal{E}g_{t}\right)\right)^{\prime}\right\} 
\end{eqnarray*}

\end_inset

 This is going to contain a term 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{t=1}^{n}\left(\mathcal{E}g_{t}\right)\left(\mathcal{E}g_{t}\right)^{\prime}
\]

\end_inset

 which will not tend to zero, in general.
 This term is not consistently estimable in general, since it requires calculati
ng an expectation using the true density under the d.g.p., which is unknown.
\end_layout

\begin_layout Itemize
There are important cases where 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta^{0})$
\end_inset

 
\emph on
is
\emph default
 consistently estimable.
 For example, suppose that the data come from a random sample (
\emph on
i.e.,
\emph default
 they are iid).
 This would be the case with cross sectional data, for example.
 (Note: under i.i.d.
 sampling, the joint distribution of 
\begin_inset Formula $(y_{t},x_{t})$
\end_inset

 is identical.
 This does not imply that the conditional density 
\begin_inset Formula $f(y_{t}|x_{t})$
\end_inset

 is identical).
\end_layout

\begin_layout Itemize
With random sampling, the limiting objective function is simply 
\begin_inset Formula 
\[
s_{\infty}(\theta^{0})=\mathcal{E}_{X}\mathcal{E}_{0}\ln f(y|x,\theta^{0})
\]

\end_inset

 where 
\begin_inset Formula $\mathcal{E}_{0}$
\end_inset

 means expectation of 
\begin_inset Formula $y|x$
\end_inset

 and 
\begin_inset Formula $\mathcal{E}_{X}$
\end_inset

 means expectation respect to the marginal density of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Itemize
By the requirement that the limiting objective function be maximized at
 
\begin_inset Formula $\theta^{0}$
\end_inset

 we have 
\begin_inset Formula 
\[
D_{\theta}\mathcal{E}_{X}\mathcal{E}_{0}\ln f(y|x,\theta^{0})=D_{\theta}s_{\infty}(\theta^{0})=0
\]

\end_inset


\end_layout

\begin_layout Itemize
The dominated convergence theorem allows switching the order of expectation
 and differentiation, so 
\begin_inset Formula 
\[
D_{\theta}\mathcal{E}_{X}\mathcal{E}_{0}\ln f(y|x,\theta^{0})=\mathcal{E}_{X}\mathcal{E}_{0}D_{\theta}\ln f(y|x,\theta^{0})=0
\]

\end_inset

 The CLT
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

implies that 
\begin_inset Formula 
\[
\frac{1}{\sqrt{n}}\sum_{t=1}^{n}D_{\theta}\ln f(y|x,\theta^{0})\stackrel{d}{\rightarrow}N(0,\mathcal{I}_{\infty}(\theta^{0})).
\]

\end_inset

 That is, it's not necessary to subtract the individual means, since they
 are zero.
 Given this, and due to independent observations, a consistent estimator
 is 
\begin_inset Formula 
\[
\widehat{\mathcal{I}}=\frac{1}{n}\sum_{t=1}^{n}D_{\theta}\ln f_{t}(\hat{\theta})D_{\theta^{\prime}}\ln f_{t}(\hat{\theta})
\]

\end_inset


\end_layout

\begin_layout Standard
This is an important case where consistent estimation of the covariance
 matrix is possible.
 Other cases exist, even for dynamically misspecified time series models.
\end_layout

\begin_layout Section
Nonlinear simultaneous equations
\end_layout

\begin_layout Standard
Taken out of GMM chapter.
 GMM provides a convenient way to estimate nonlinear systems of simultaneous
 equations.
 We have a system of equations of the form 
\begin_inset Formula 
\begin{eqnarray*}
y_{1t} & = & f_{1}(\mathbf{z}_{t},\theta_{1}^{0})+\varepsilon_{1t}\\
y_{2t} & = & f_{2}(\mathbf{z}_{t},\theta_{2}^{0})+\varepsilon_{2t}\\
 &  & \vdots\\
y_{Gt} & = & f_{G}(\mathbf{z}_{t},\theta_{G}^{0})+\varepsilon_{Gt},
\end{eqnarray*}

\end_inset

 or in compact notation 
\begin_inset Formula 
\[
y_{t}=f(\mathbf{z}_{t},\theta^{0})+\varepsilon_{t},
\]

\end_inset

 where 
\begin_inset Formula $f(\cdot)$
\end_inset

 is a 
\begin_inset Formula $G$
\end_inset

 -vector valued function, and 
\begin_inset Formula $\theta^{0}=(\theta_{1}^{0\prime},\theta_{2}^{0\prime},\cdots,\theta_{G}^{0\prime})^{\prime}.$
\end_inset

 We assume that 
\begin_inset Formula $\mathbf{z}_{t}$
\end_inset

 contains the current period endogenous variables, so we have a simultaneity
 problem.
\end_layout

\begin_layout Standard
We need to find an 
\begin_inset Formula $A_{i}\times1$
\end_inset

 vector of instruments 
\begin_inset Formula $\mathbf{x}_{it},$
\end_inset

 for each equation, that are uncorrelated with 
\begin_inset Formula $\varepsilon_{it}.$
\end_inset

 Typical instruments would be low order monomials in the exogenous variables
 in 
\begin_inset Formula $\mathbf{z}_{t},$
\end_inset

 with their lagged values.
 Then we can define the 
\begin_inset Formula $\left(\sum_{i=1}^{G}A_{i}\right)\times1$
\end_inset

 orthogonality conditions 
\begin_inset Formula 
\[
m_{t}(\theta)=\left[\begin{array}{c}
\left(y_{1t}-f_{1}(\mathbf{z}_{t},\theta_{1})\right)\mathbf{x}_{1t}\\
\left(y_{2t}-f_{2}(\mathbf{z}_{t},\theta_{2})\right)\mathbf{x}_{2t}\\
\vdots\\
\left(y_{Gt}-f_{G}(\mathbf{z}_{t},\theta_{G})\right)\mathbf{x}_{Gt}
\end{array}\right].
\]

\end_inset


\end_layout

\begin_layout Itemize
once we have gotten this far, we can just proceed with GMM estimation, one-step,
 two-step, CUE, or whatever.
\end_layout

\begin_layout Itemize
A note on identification: selection of instruments that ensure identification
 is a non-trivial problem.
 Identification in nonlinear models is not as easy to check as it is with
 linear models, where counting zero restrictions works.
\end_layout

\begin_layout Itemize
A note on efficiency: the selected set of instruments has important effects
 on the efficiency of estimation.
 There are some papers that study this problem, but the results are fairly
 complicated and difficult to implement.
 I think it's safe to say that the great majority of applied work does not
 attempt to use optimal instruments.
\end_layout

\begin_layout Section
Example: The MEPS data
\end_layout

\begin_layout Standard
Taken out of the GMM chapter, distracting, and not a great example.
 The MEPS data on health care usage discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:MEPS data"

\end_inset

 estimated a Poisson model by 
\begin_inset Quotes sld
\end_inset

maximum likelihood
\begin_inset Quotes srd
\end_inset

 (probably misspecified).
 Perhaps the same latent factors (e.g., chronic illness) that induce one to
 make doctor visits also influence the decision of whether or not to purchase
 insurance.
 If this is the case, the PRIV variable could well be endogenous, in which
 case, the Poisson 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 estimator would be inconsistent, even if the conditional mean were correctly
 specified.
 Suppose that 
\begin_inset Formula 
\[
y=\exp(X\beta+Z\gamma)v
\]

\end_inset

where 
\begin_inset Formula $E(v|X)=1$
\end_inset

 but 
\begin_inset Formula $v$
\end_inset

 may be related to 
\begin_inset Formula $Z,$
\end_inset

 so 
\begin_inset Formula $Z$
\end_inset

 is endogenous.
 Then 
\begin_inset Formula $E(y/\exp(X\beta+Z\gamma)-1|X)=0.$
\end_inset

 This expression can be used to define moment conditions.
 The Octave script 
\begin_inset ERT
status open

\begin_layout Plain Layout

 
\backslash
htmladdnormallink{meps.m}{https://github.com/mcreel/Econometrics/blob/master/Examp
les/GMM/MEPS/meps.m} 
\end_layout

\end_inset

 estimates the parameters of the model presented in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Poisson model OBDV"

\end_inset

, using Poisson 
\begin_inset Quotes sld
\end_inset

ML
\begin_inset Quotes srd
\end_inset

 (better thought of as quasi-ML), and IV estimation
\begin_inset Foot
status open

\begin_layout Plain Layout
The validity of the instruments used may be debatable, but real data sets
 often don't contain ideal instruments.
\end_layout

\end_inset

.
 Both estimation methods are implemented using a GMM form.
 Running that script gives the output 
\begin_inset CommandInset include
LatexCommand verbatiminput
filename "Examples/GMM/MEPS/meps.out"

\end_inset


\end_layout

\begin_layout Standard
Note how the Poisson QML results, estimated here using a GMM routine, are
 the same as were obtained using the ML estimation routine (see subsection
 
\begin_inset CommandInset ref
LatexCommand ref
reference "PoissonOBDV_results"

\end_inset

).
 This is an example of how (Q)ML may be represented as a GMM estimator.
 Also note that the IV and QML results are considerably different.
 Treating PRIV as potentially endogenous causes the sign of its coefficient
 to change.
 Perhaps it is logical that people who own private insurance make fewer
 visits, if they have to make a co-payment.
 Note that income becomes positive and significant when PRIV is treated
 as endogenous.
\end_layout

\begin_layout Standard
Perhaps the difference in the results depending upon whether or not PRIV
 is treated as endogenous can suggest a method for testing exogeneity....
\end_layout

\begin_layout Subsubsection
Invertibility of AR process
\end_layout

\begin_layout Standard
To begin with, define the lag operator 
\begin_inset Formula $L$
\end_inset


\begin_inset Formula 
\[
Ly_{t}=y_{t-1}
\]

\end_inset

 The lag operator is defined to behave just as an algebraic quantity, e.g.,
 
\begin_inset Formula 
\begin{eqnarray*}
L^{2}y_{t} & =L(Ly_{t})\\
 & =Ly_{t-1}\\
 & =y_{t-2}
\end{eqnarray*}

\end_inset

 or 
\begin_inset Formula 
\begin{eqnarray*}
(1-L)(1+L)y_{t} & = & 1-Ly_{t}+Ly_{t}-L^{2}y_{t}\\
 & = & 1-y_{t-2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
A mean-zero AR(p) process can be written as 
\begin_inset Formula 
\[
y_{t}-\phi_{1}y_{t-1}-\phi_{2}y_{t-2}-\cdots-\phi_{p}y_{t-p}=\varepsilon_{t}
\]

\end_inset

 or 
\begin_inset Formula 
\[
y_{t}(1-\phi_{1}L-\phi_{2}L^{2}-\cdots-\phi_{p}L^{p})=\varepsilon_{t}
\]

\end_inset

 Factor this polynomial as 
\begin_inset Formula 
\[
1-\phi_{1}L-\phi_{2}L^{2}-\cdots-\phi_{p}L^{p}=(1-\lambda_{1}L)(1-\lambda_{2}L)\cdots(1-\lambda_{p}L)
\]

\end_inset

 For the moment, just assume that the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 are coefficients to be determined.
 Since 
\begin_inset Formula $L\;$
\end_inset

is defined to operate as an algebraic quantity, determination of the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 is the same as determination of the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 such that the following two expressions are the same for all 
\begin_inset Formula $z:$
\end_inset


\begin_inset Formula 
\[
1-\phi_{1}z-\phi_{2}z^{2}-\cdots-\phi_{p}z^{p}=(1-\lambda_{1}z)(1-\lambda_{2}z)\cdots(1-\lambda_{p}z)
\]

\end_inset

 Multiply both sides by 
\begin_inset Formula $z^{-p}$
\end_inset


\begin_inset Formula 
\[
z^{-p}-\phi_{1}z^{1-p}-\phi_{2}z^{2-p}-\cdots\phi_{p-1}z^{-1}-\phi_{p}=(z^{-1}-\lambda_{1})(z^{-1}-\lambda_{2})\cdots(z^{-1}-\lambda_{p})
\]

\end_inset

 and now define 
\begin_inset Formula $\lambda=z^{-1}$
\end_inset

 so we get 
\begin_inset Formula 
\[
\lambda^{p}-\phi_{1}\lambda^{p-1}-\phi_{2}\lambda^{p-2}-\cdots-\phi_{p-1}\lambda-\phi_{p}=(\lambda-\lambda_{1})(\lambda-\lambda_{2})\cdots(\lambda-\lambda_{p})
\]

\end_inset

 The LHS is precisely the determinantal polynomial that gives the eigenvalues
 of 
\begin_inset Formula $F.$
\end_inset

 Therefore, the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 that are the coefficients of the factorization are simply the eigenvalues
 of the matrix 
\begin_inset Formula $F.$
\end_inset


\end_layout

\begin_layout Standard
Now consider a different stationary process 
\begin_inset Formula 
\[
(1-\phi L)y_{t}=\varepsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Itemize
Stationarity, as above, implies that 
\begin_inset Formula $|\phi|<1.$
\end_inset


\end_layout

\begin_layout Standard
Multiply both sides by 
\begin_inset Formula $1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}$
\end_inset

 to get 
\begin_inset Formula 
\[
\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)(1-\phi L)y_{t}=\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 or, multiplying the polynomials on the LHS, we get
\begin_inset Formula 
\[
\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}-\phi L-\phi^{2}L^{2}-...-\phi^{j}L^{j}-\phi^{j+1}L^{j+1}\right)y_{t}=\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

and with cancellations we have 
\begin_inset Formula 
\[
\left(1-\phi^{j+1}L^{j+1}\right)y_{t}=\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 so 
\begin_inset Formula 
\[
y_{t}=\phi^{j+1}L^{j+1}y_{t}+\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 Now as 
\begin_inset Formula $j\rightarrow\infty,$
\end_inset

 
\begin_inset Formula $\phi^{j+1}L^{j+1}y_{t}\rightarrow0,$
\end_inset

 since 
\begin_inset Formula $|\phi|<1,$
\end_inset

 so 
\begin_inset Formula 
\[
y_{t}\cong\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 and the approximation becomes better and better as 
\begin_inset Formula $j$
\end_inset

 increases.
 However, we started with 
\begin_inset Formula 
\[
(1-\phi L)y_{t}=\varepsilon_{t}
\]

\end_inset

 Substituting this into the above equation we have 
\begin_inset Formula 
\[
y_{t}\cong\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)(1-\phi L)y_{t}
\]

\end_inset

 so 
\begin_inset Formula 
\[
\left(1+\phi L+\phi^{2}L^{2}+...+\phi^{j}L^{j}\right)(1-\phi L)\cong1
\]

\end_inset

 and the approximation becomes arbitrarily good as 
\begin_inset Formula $j$
\end_inset

 increases arbitrarily.
 Therefore, for 
\begin_inset Formula $|\phi|<1,$
\end_inset

 define 
\begin_inset Formula 
\[
(1-\phi L)^{-1}=\sum_{j=0}^{\infty}\phi^{j}L^{j}
\]

\end_inset

 Recall that our mean zero AR(p) process 
\begin_inset Formula 
\[
y_{t}(1-\phi_{1}L-\phi_{2}L^{2}-\cdots-\phi_{p}L^{p})=\varepsilon_{t}
\]

\end_inset

 can be written using the factorization 
\begin_inset Formula 
\[
y_{t}(1-\lambda_{1}L)(1-\lambda_{2}L)\cdots(1-\lambda_{p}L)=\varepsilon_{t}
\]

\end_inset

 where the 
\begin_inset Formula $\lambda$
\end_inset

 are the eigenvalues of 
\begin_inset Formula $F,$
\end_inset

 and given stationarity, all the 
\begin_inset Formula $|\lambda_{i}|<1.$
\end_inset

 Therefore, we can invert each first order polynomial on the LHS to get
 
\begin_inset Formula 
\[
y_{t}=\left(\sum_{j=0}^{\infty}\lambda_{1}^{j}L^{j}\right)\left(\sum_{j=0}^{\infty}\lambda_{2}^{j}L^{j}\right)\cdots\left(\sum_{j=0}^{\infty}\lambda_{p}^{j}L^{j}\right)\varepsilon_{t}
\]

\end_inset

 The RHS is a product of infinite-order polynomials in 
\begin_inset Formula $L,$
\end_inset

 which can be represented as 
\begin_inset Formula 
\[
y_{t}=(1+\psi_{1}L+\psi_{2}L^{2}+\cdots)\varepsilon_{t}
\]

\end_inset

 where the 
\begin_inset Formula $\psi_{i}$
\end_inset

 are real-valued and absolutely summable.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\psi_{i}$
\end_inset

 are formed of products of powers of the 
\begin_inset Formula $\lambda_{i}$
\end_inset

, which are in turn functions of the 
\begin_inset Formula $\phi_{i}.$
\end_inset


\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\psi_{i}$
\end_inset

 are real-valued because any complex-valued 
\begin_inset Formula $\lambda_{i}$
\end_inset

 always occur in conjugate pairs.
 This means that if 
\begin_inset Formula $a+bi$
\end_inset

 is an eigenvalue of 
\begin_inset Formula $F,$
\end_inset

 then so is 
\begin_inset Formula $a-bi.$
\end_inset

 In multiplication 
\begin_inset Formula 
\begin{eqnarray*}
\left(a+bi\right)(a-bi) & = & a^{2}-abi+abi-b^{2}i^{2}\\
 & = & a^{2}+b^{2}
\end{eqnarray*}

\end_inset

 which is real-valued.
\end_layout

\begin_layout Itemize
This shows that an AR(p) process is representable as an infinite-order MA(q)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

process.
\end_layout

\begin_layout Itemize
Recall before that by recursive substitution, an AR(p) process can be written
 as 
\begin_inset Formula 
\[
Y_{t+j}=C+FC+\cdots+F^{j}C+F^{j+1}Y_{t-1}+F^{j}E_{t}+F^{j-1}E_{t+1}+\cdots+FE_{t+j-1}+E_{t+j}
\]

\end_inset

 If the process is mean zero, then everything with a 
\begin_inset Formula $C$
\end_inset

 drops out.
 Take this and lag it by 
\begin_inset Formula $j$
\end_inset

 periods to get 
\begin_inset Formula 
\[
Y_{t}=F^{j+1}Y_{t-j-1}+F^{j}E_{t-j}+F^{j-1}E_{t-j+1}+\cdots+FE_{t-1}+E_{t}
\]

\end_inset

 As 
\begin_inset Formula $j\rightarrow\infty,$
\end_inset

 the lagged 
\begin_inset Formula $Y$
\end_inset

 on the RHS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

drops out.
 The 
\begin_inset Formula $E_{t-s}$
\end_inset

 are vectors of zeros except for their first element, so we see that the
 first equation here, in the limit, is just 
\begin_inset Formula 
\[
y_{t}=\sum_{j=0}^{\infty}\left(F^{j}\right)_{1,1}\varepsilon_{t-j}
\]

\end_inset

 which makes explicit the relationship between the 
\begin_inset Formula $\psi_{i}$
\end_inset

 and the 
\begin_inset Formula $\phi_{i}$
\end_inset

 (and the 
\begin_inset Formula $\lambda_{i}$
\end_inset

 as well, recalling the previous factorization of 
\begin_inset Formula $F^{j}).$
\end_inset


\end_layout

\begin_layout Subsection
Invertibility of MA(q) process
\end_layout

\begin_layout Standard
An MA(q) can be written as 
\begin_inset Formula 
\[
y_{t}-\mu=(1+\theta_{1}L+...+\theta_{q}L^{q})\varepsilon_{t}
\]

\end_inset

 As before, the polynomial on the RHS can be factored as 
\begin_inset Formula 
\[
(1+\theta_{1}L+...+\theta_{q}L^{q})=(1-\eta_{1}L)(1-\eta_{2}L)...(1-\eta_{q}L)
\]

\end_inset

 and each of the 
\begin_inset Formula $(1-\eta_{i}L)$
\end_inset

 can be inverted as long as each of the 
\begin_inset Formula $|\eta_{i}|<1.$
\end_inset

 If this is the case, then we can write 
\begin_inset Formula 
\[
(1+\theta_{1}L+...+\theta_{q}L^{q})^{-1}(y_{t}-\mu)=\varepsilon_{t}
\]

\end_inset

 where 
\begin_inset Formula 
\[
(1+\theta_{1}L+...+\theta_{q}L^{q})^{-1}
\]

\end_inset

 will be an infinite-order polynomial in 
\begin_inset Formula $L,$
\end_inset

 so we get 
\begin_inset Formula 
\[
\sum_{j=0}^{\infty}-\delta_{j}L^{j}(y_{t-j}-\mu)=\varepsilon_{t}
\]

\end_inset

 with 
\begin_inset Formula $\delta_{0}=-1,$
\end_inset

 or 
\begin_inset Formula 
\[
(y_{t}-\mu)-\delta_{1}(y_{t-1}-\mu)-\delta_{2}(y_{t-2}-\mu)+...=\varepsilon_{t}
\]

\end_inset

 or 
\begin_inset Formula 
\[
y_{t}=c+\delta_{1}y_{t-1}+\delta_{2}y_{t-2}+...+\varepsilon_{t}
\]

\end_inset

 where 
\begin_inset Formula 
\[
c=\mu+\delta_{1}\mu+\delta_{2}\mu+...
\]

\end_inset

 So we see that an MA(q)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

has an infinite AR representation, as long as the 
\begin_inset Formula $|\eta_{i}|<1,$
\end_inset

 
\begin_inset Formula $i=1,2,...,q.$
\end_inset


\end_layout

\begin_layout Itemize
It turns out that one can always manipulate the parameters of an MA(q) process
 to find an invertible representation.
 For example, the two MA(1) processes 
\begin_inset Formula 
\[
y_{t}-\mu=(1-\theta L)\varepsilon_{t}
\]

\end_inset

 and 
\begin_inset Formula 
\[
y_{t}^{\ast}-\mu=(1-\theta^{-1}L)\varepsilon_{t}^{\ast}
\]

\end_inset

 have exactly the same moments if 
\begin_inset Formula 
\[
\sigma_{\varepsilon^{\ast}}^{2}=\sigma_{\varepsilon}^{2}\theta^{2}
\]

\end_inset

 For example, we've seen that 
\begin_inset Formula 
\[
\gamma_{0}=\sigma^{2}(1+\theta^{2}).
\]

\end_inset

 Given the above relationships amongst the parameters, 
\begin_inset Formula 
\[
\gamma_{0}^{\ast}=\sigma_{\varepsilon}^{2}\theta^{2}(1+\theta^{-2})=\sigma^{2}(1+\theta^{2})
\]

\end_inset

 so the variances are the same.
 It turns out that 
\emph on
all
\emph default
 the autocovariances will be the same, as is easily checked.
 This means that the two MA processes are 
\emph on
observationally equivalent
\emph default
.
 As before, it's impossible to distinguish between observationally equivalent
 processes on the basis of data.
\end_layout

\begin_layout Itemize
For a given MA(q)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

process, it's always possible to manipulate the parameters to find an invertible
 representation (which is unique).
\end_layout

\begin_layout Itemize
It's important to find an invertible representation, since it's the only
 representation that allows one to represent 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 as a function of past 
\begin_inset Formula $y's.$
\end_inset

 The other representations express 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 as a function of future 
\begin_inset Formula $y's$
\end_inset


\end_layout

\begin_layout Itemize
Why is invertibility important? The most important reason is that it provides
 a justification for the use of parsimonious models.
 Since an AR(1) process has an MA(
\begin_inset Formula $\infty)$
\end_inset

 representation, one can reverse the argument and note that at least some
 MA(
\begin_inset Formula $\infty)$
\end_inset

 processes have an AR(1)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

representation.
 Likewise, some AR(
\begin_inset Formula $\infty)$
\end_inset

 processes have an MA(1) representation.
 At the time of estimation, it's a lot easier to estimate the single AR(1)
 or MA(1) coefficient rather than the infinite number of coefficients associated
 with the MA(
\begin_inset Formula $\infty)$
\end_inset

 or AR(
\begin_inset Formula $\infty)$
\end_inset

 representation.
\end_layout

\begin_layout Itemize
This is the reason that ARMA models are popular.
 Combining low-order AR and MA models can usually offer a satisfactory represent
ation of univariate time series data using a reasonable number of parameters.
\end_layout

\begin_layout Itemize
Stationarity and invertibility of ARMA models is similar to what we've seen
 - we won't go into the details.
 Likewise, calculating moments is similar.
 
\end_layout

\begin_layout Exercise
Calculate the autocovariances of an ARMA(1,1) model:
\begin_inset Formula $(1+\phi L)y_{t}=c+(1+\theta L)\epsilon_{t}$
\end_inset


\end_layout

\begin_layout Subsection
Optimal instruments for GMM
\end_layout

\begin_layout Standard
PLEASE IGNORE THE REST OF THIS SECTION: there is a flaw in the argument
 that needs correction.
 In particular, it may be the case that 
\begin_inset Formula $E(Z_{t}\epsilon_{t})\ne0$
\end_inset

 if instruments are chosen in the way suggested here.
 
\end_layout

\begin_layout Standard
An interesting question that arises is how one should choose the instrumental
 variables 
\begin_inset Formula $Z(w_{t})$
\end_inset

 to achieve maximum efficiency.
\end_layout

\begin_layout Standard
Note that with this choice of moment conditions, we have that 
\begin_inset Formula $D_{n}\equiv\frac{\partial}{\partial\theta}m^{\prime}(\theta)$
\end_inset

 (a 
\begin_inset Formula $K\times g$
\end_inset

 matrix) is 
\begin_inset Formula 
\begin{eqnarray*}
D_{n}(\theta) & = & \frac{\partial}{\partial\theta}\frac{1}{n}\left(Z_{n}^{\prime}h_{n}(\theta)\right)^{\prime}\\
 & = & \frac{1}{n}\left(\frac{\partial}{\partial\theta}h_{n}^{\prime}\left(\theta\right)\right)Z_{n}
\end{eqnarray*}

\end_inset

 which we can define to be 
\begin_inset Formula 
\[
D_{n}(\theta)=\frac{1}{n}H_{n}Z_{n}.
\]

\end_inset

where 
\begin_inset Formula $H_{n}$
\end_inset

 is a 
\begin_inset Formula $K\times n$
\end_inset

 matrix that has the derivatives of the individual moment conditions as
 its columns.
 Likewise, define the var-cov.
 of the moment conditions 
\begin_inset Formula 
\begin{eqnarray*}
\Omega_{n} & = & \mathcal{E}\left[n\bar{m}_{n}(\theta^{0})\bar{m}_{n}(\theta^{0})^{\prime}\right]\\
 & = & \mathcal{E}\left[\frac{1}{n}Z_{n}^{\prime}h_{n}(\theta^{0})h_{n}(\theta^{0})^{\prime}Z_{n}\right]\\
 & = & Z_{n}^{\prime}\mathcal{E}\left(\frac{1}{n}h_{n}(\theta^{0})h_{n}(\theta^{0})^{\prime}\right)Z_{n}\\
 & \equiv & Z_{n}^{\prime}\frac{\Phi_{n}}{n}Z_{n}
\end{eqnarray*}

\end_inset

where we have defined 
\begin_inset Formula $\Phi_{n}=V\left(h_{n}(\theta^{0})\right).$
\end_inset

 Note that the dimension of this matrix is growing with the sample size,
 so it is not consistently estimable without additional assumptions.
\end_layout

\begin_layout Standard
The asymptotic normality theorem above says that the GMM
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator using the optimal weighting matrix is distributed as 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N(0,V_{\infty})
\]

\end_inset

 where 
\begin_inset Formula 
\begin{equation}
V_{\infty}=\lim_{n\rightarrow\infty}\left(\left(\frac{H_{n}Z_{n}}{n}\right)\left(\frac{Z_{n}^{\prime}\Phi_{n}Z_{n}}{n}\right)^{-1}\left(\frac{Z_{n}^{\prime}H_{n}^{\prime}}{n}\right)\right)^{-1}.\label{var-covcondmoments,nonoptimal}
\end{equation}

\end_inset

Using an argument similar to that used to prove that 
\begin_inset Formula $\Omega_{\infty}^{-1}$
\end_inset

 is the efficient weighting matrix, we can show that putting 
\begin_inset Formula 
\[
Z_{n}=\Phi_{n}^{-1}H_{n}^{\prime}
\]

\end_inset

 causes the above var-cov matrix to simplify to 
\begin_inset Formula 
\begin{equation}
V_{\infty}=\lim_{n\rightarrow\infty}\left(\frac{H_{n}\Phi_{n}^{-1}H_{n}^{\prime}}{n}\right)^{-1}.\label{simplevarcov,condmoments}
\end{equation}

\end_inset

 and furthermore, this matrix is smaller that the limiting var-cov for any
 other choice of instrumental variables.
 (To prove this, examine the difference of the inverses of the var-cov matrices
 with the optimal intruments and with non-optimal instruments.
 As above, you can show that the difference is positive semi-definite).
\end_layout

\begin_layout Itemize
Note that both 
\begin_inset Formula $H_{n},$
\end_inset

 which we should write more properly as 
\begin_inset Formula $H_{n}(\theta^{0}),$
\end_inset

 since it depends on 
\begin_inset Formula $\theta^{0},$
\end_inset

 and 
\begin_inset Formula $\Phi$
\end_inset

 must be consistently estimated to apply this.
\end_layout

\begin_layout Itemize
Usually, estimation of 
\begin_inset Formula $H_{n}$
\end_inset

 is straightforward - one just uses 
\begin_inset Formula 
\[
\widehat{H}=\frac{\partial}{\partial\theta}h_{n}^{\prime}\left(\tilde{\theta}\right),
\]

\end_inset

 where 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is some initial consistent estimator based on non-optimal instruments.
\end_layout

\begin_layout Itemize
Estimation of 
\begin_inset Formula $\Phi_{n}$
\end_inset

 may not be possible.
 It is an 
\begin_inset Formula $n\times n$
\end_inset

 matrix, so it has more unique elements than 
\begin_inset Formula $n,$
\end_inset

 the sample size, so without restrictions on the parameters it can't be
 estimated consistently.
 Basically, you need to provide a parametric specification of the covariances
 of the 
\begin_inset Formula $h_{t}(\theta)\;$
\end_inset

in order to be able to use optimal instruments.
 A solution is to approximate this matrix parametrically to define the instrumen
ts.
 Note that the simplified var-cov matrix in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "simplevarcov,condmoments"

\end_inset

 will not apply if approximately optimal instruments are used - it will
 be necessary to use an estimator based upon equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "var-covcondmoments,nonoptimal"

\end_inset

, where the term 
\begin_inset Formula $n^{-1}Z_{n}^{\prime}\Phi_{n}Z_{n}$
\end_inset

 must be estimated consistently apart, for example by the Newey-West procedure.
 
\end_layout

\begin_layout Section
Hurdle models
\end_layout

\begin_layout Standard
Returning to the Poisson model, lets look at actual and fitted count probabiliti
es.
 Actual relative frequencies are 
\begin_inset Formula $f(y=j)=\sum_{i}1(y_{i}=j)/n$
\end_inset

 and fitted frequencies are 
\begin_inset Formula $\hat{f}(y=j)=\sum_{i=1}^{n}f_{Y}(j|x_{i},\hat{\theta})/n$
\end_inset


\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Actual and Poisson fitted frequencies
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.83
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.14
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.19
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.004
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.052
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0002
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.032
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.4e-5
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

We see that for the OBDV measure, there are many more actual zeros than
 predicted.
 For ERV, there are somewhat more actual zeros than fitted, but the difference
 is not too important.
 
\end_layout

\begin_layout Standard
Why might OBDV not fit the zeros well? What if people made the decision
 to contact the doctor for a first visit, they are sick, then the 
\emph on
doctor
\emph default
 decides on whether or not follow-up visits are needed.
 This is a principal/agent type situation, where the total number of visits
 depends upon the decision of both the patient and the doctor.
 Since different parameters may govern the two decision-makers choices,
 we might expect that different parameters govern the probability of zeros
 versus the other counts.
 Let 
\begin_inset Formula $\lambda_{p}$
\end_inset

 be the parameters of the patient's demand for visits, and let 
\begin_inset Formula $\lambda_{d}$
\end_inset

 be the paramter of the doctor's 
\begin_inset Quotes eld
\end_inset

demand
\begin_inset Quotes erd
\end_inset

 for visits.
 The patient will initiate visits according to a discrete choice model,
 for example, a logit model:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Pr(Y=0) & =f_{Y}(0,\lambda_{p})= & 1-1/\left[1+\exp(-\lambda_{p})\right]\\
\Pr(Y>0) & = & 1/\left[1+\exp(-\lambda_{p})\right],
\end{eqnarray*}

\end_inset

 The above probabilities are used to estimate the binary 0/1 hurdle process.
 Then, for the observations where visits are positive, a truncated Poisson
 density is estimated.
 This density is
\begin_inset Formula 
\begin{eqnarray*}
f_{Y}(y,\lambda_{d}|y>0) & = & \frac{f_{Y}(y,\lambda_{d})}{\Pr(y>0)}\\
 & = & \frac{f_{Y}(y,\lambda_{d})}{1-\exp(-\lambda_{d})}
\end{eqnarray*}

\end_inset

since according to the Poisson model with the doctor's paramaters,
\begin_inset Formula 
\[
\Pr(y=0)=\frac{\exp(-\lambda_{d})\lambda_{d}^{0}}{0!}.
\]

\end_inset

Since the hurdle and truncated components of the overall density for 
\begin_inset Formula $Y$
\end_inset

 share no parameters, they may be estimated separately, which is computationally
 more efficient than estimating the overall model.
 (Recall that the BFGS algorithm, for example, will have to invert the approxima
ted Hessian.
 The computational overhead is of order 
\begin_inset Formula $K^{2}$
\end_inset

 where 
\begin_inset Formula $K$
\end_inset

 is the number of parameters to be estimated) .
 The expectation of 
\begin_inset Formula $Y$
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
E(Y|x) & = & \Pr(Y>0|x)E(Y|Y>0,x)\\
 & = & \left(\frac{1}{1+\exp(-\lambda_{p})}\right)\left(\frac{\lambda_{d}}{1-\exp(-\lambda_{d})}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Here are hurdle Poisson estimation results for OBDV, obtained from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{this estimation program}{https://github.com/mcreel/Econometrics
/blob/master/Examples/MEPS-II/estimate
\backslash
_hpoisson.ox} 
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data, OBDV
\end_layout

\begin_layout Standard

\family typewriter
logit results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.58939
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.5502
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.5709
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.5269
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.5560
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.0519
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0520
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0027
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0384
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.45867
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7289
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.6924
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7166
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.63570
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.0873
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1677
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1366
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.018614
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1547
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1969
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1807
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.039606
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.0467
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.98710
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.0222
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.077446
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7655
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1672
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.9601
\end_layout

\begin_layout Standard

\family typewriter
Information Criteria 
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 639.89
\end_layout

\begin_layout Standard

\family typewriter
Schwartz 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 632.89
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 614.96
\end_layout

\begin_layout Standard

\family typewriter
Akaike 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 603.39
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

The results for the truncated part:
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data, OBDV
\end_layout

\begin_layout Standard

\family typewriter
tpoisson results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.7042
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.54254
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.4291
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1747
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.2323
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.31001
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.5708
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7573
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7183
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.014382
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.29433
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.10438
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.18112
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.19075
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 10.293
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1890
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.6942
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.016683
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 16.148
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.5262
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.9814
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.016286
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 4.2144
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.56547
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.6353
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.0079016
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.3186
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.35309
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.96078
\end_layout

\begin_layout Standard

\family typewriter
Information Criteria 
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2754.7
\end_layout

\begin_layout Standard

\family typewriter
Schwartz
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2747.7
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2729.8
\end_layout

\begin_layout Standard

\family typewriter
Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2718.2
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset

Fitted and actual probabilites (NB-II fits are provided as well) are:
\end_layout

\begin_layout Standard
\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Actual and Hurdle Poisson fitted frequencies
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="7">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OBDV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERV
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted HP
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted NB-II
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Actual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted HP
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fitted NB-II
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.34
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.86
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.035
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.16
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.071
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.08
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.004
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.006
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.006
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.052
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.002
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.032
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0005
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.001
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

For the Hurdle Poisson models, the ERV fit is very accurate.
 The OBDV fit is not so good.
 Zeros are exact, but 1's and 2's are underestimated, and higher counts
 are overestimated.
 For the NB-II fits, performance is at least as good as the hurdle Poisson
 model, and one should recall that many fewer parameters are used.
 Hurdle version of the negative binomial model are also widely used.
\end_layout

\begin_layout Section
Finite mixture models
\end_layout

\begin_layout Standard
The following are results for a mixture of 2 negative binomial (NB-I) models,
 for the OBDV data, which you can replicate using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
htmladdnormallink{this estimation program}{https://github.com/mcreel/Econometrics
/blob/master/Examples/MEPS-II/estimate
\backslash
_mixnegbin.ox} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data, OBDV
\end_layout

\begin_layout Standard

\family typewriter
mixnegbin results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.2312
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.64852
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3851
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3226
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.4358
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.062139
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.23188
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.13802
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.18729
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.093396
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.46948
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.33046
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.40854
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.39785
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.6121
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.2148
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.4882
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.015969
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5173
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5475
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.7151
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.049175
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.8013
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.7061
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.8036
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.015880
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.58386
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.76782
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.73281
\end_layout

\begin_layout Standard

\family typewriter
ln_alpha
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.69961
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.3456
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.0396
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.4029
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -3.6130
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.6126
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.7365
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -1.8411
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.3456
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7527
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7677
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.6519
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.77431
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.73854
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1366
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.97338
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.34886
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.80035
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.74016
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.81892
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.021425
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1354
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3032
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3387
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.22461
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.0922
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7826
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.1470
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.019227
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.20453
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.40854
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.36313
\end_layout

\begin_layout Standard

\family typewriter
ln_alpha
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.8419
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.2497
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.8702
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.6182
\end_layout

\begin_layout Standard

\family typewriter
logit_inv_mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.85186
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7096
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.4827
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.7883
\end_layout

\begin_layout Standard

\family typewriter
Information Criteria
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2353.8
\end_layout

\begin_layout Standard

\family typewriter
Schwartz
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2336.8
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2293.3
\end_layout

\begin_layout Standard

\family typewriter
Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2265.2
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
Delta method for mix parameter st.
 err.
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 se_mix
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.70096
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.12043
\end_layout

\begin_layout Itemize
The 95% confidence interval for the mix parameter is perilously close to
 1, which suggests that there may really be only one component density,
 rather than a mixture.
 Again, this is 
\emph on
not
\emph default
 the way to test this - it is merely suggestive.
\end_layout

\begin_layout Itemize
Education is interesting.
 For the subpopulation that is 
\begin_inset Quotes eld
\end_inset

healthy
\begin_inset Quotes erd
\end_inset

, i.e., that makes relatively few visits, education seems to have a positive
 effect on visits.
 For the 
\begin_inset Quotes eld
\end_inset

unhealthy
\begin_inset Quotes erd
\end_inset

 group, education has a negative effect on visits.
 The other results are more mixed.
 A larger sample could help clarify things.
\end_layout

\begin_layout Standard
The following are results for a 2 component constrained mixture negative
 binomial model where all the slope parameters in 
\begin_inset Formula $\lambda_{j}=e^{\mathbf{x}\beta_{j}}$
\end_inset

 are the same across the two components.
 The constants and the overdispersion parameters 
\begin_inset Formula $\alpha_{j}$
\end_inset

 are allowed to differ for the two components.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
MEPS data, OBDV
\end_layout

\begin_layout Standard

\family typewriter
cmixnegbin results
\end_layout

\begin_layout Standard

\family typewriter
Strong convergence
\end_layout

\begin_layout Standard

\family typewriter
Observations = 500
\end_layout

\begin_layout Standard

\family typewriter
Function value
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -2.2441
\end_layout

\begin_layout Standard

\family typewriter
t-Stats
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 params
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(OPG)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Sand.)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 t(Hess)
\end_layout

\begin_layout Standard

\family typewriter
constant
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.34153
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.94203
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.91456
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 -0.97943
\end_layout

\begin_layout Standard

\family typewriter
pub_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.45320
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.6206
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5088
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.7067
\end_layout

\begin_layout Standard

\family typewriter
priv_ins
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.20663
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.4258
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3105
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.3895
\end_layout

\begin_layout Standard

\family typewriter
sex
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.37714
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1948
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.4929
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.5319
\end_layout

\begin_layout Standard

\family typewriter
age
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.015822
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.1212
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7806
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7042
\end_layout

\begin_layout Standard

\family typewriter
educ
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.011784
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.65887
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.50362
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.58331
\end_layout

\begin_layout Standard

\family typewriter
inc
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.014088
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.69088
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.96831
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.83408
\end_layout

\begin_layout Standard

\family typewriter
ln_alpha
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.1798
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 4.6140
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 7.2462
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.4293
\end_layout

\begin_layout Standard

\family typewriter
const_2
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.2621
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.47525
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.5219
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.5060
\end_layout

\begin_layout Standard

\family typewriter
lnalpha_2
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.7769
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.5539
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 6.4918
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 4.2243
\end_layout

\begin_layout Standard

\family typewriter
logit_inv_mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2.4888
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.60073
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 3.7224
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 1.9693
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard

\family typewriter
Information Criteria
\end_layout

\begin_layout Standard

\family typewriter
Consistent Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2323.5
\end_layout

\begin_layout Standard

\family typewriter
Schwartz
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2312.5
\end_layout

\begin_layout Standard

\family typewriter
Hannan-Quinn
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2284.3
\end_layout

\begin_layout Standard

\family typewriter
Akaike
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 2266.1
\end_layout

\begin_layout Standard

\family typewriter
**************************************************************************
\end_layout

\begin_layout Standard

\family typewriter
Delta method for mix parameter st.
 err.
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 mix
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 se_mix
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.92335
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 0.047318
\end_layout

\begin_layout Itemize
Now the mixture parameter is even closer to 1.
\end_layout

\begin_layout Itemize
The slope parameter estimates are pretty close to what we got with the NB-I
 model.
\end_layout

\begin_layout Section
Nonlinear least squares (NLS)
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
: Davidson and MacKinnon, Ch.
 2
\begin_inset Formula $^{*}$
\end_inset

 and 5
\begin_inset Formula $^{*}$
\end_inset

; Gallant, Ch.
 1
\end_layout

\begin_layout Subsection
Introduction and definition
\end_layout

\begin_layout Standard
Nonlinear least squares (NLS) is a means of estimating the parameter of
 the model 
\begin_inset Formula 
\[
y_{t}=f(\mathbf{x}_{t},\theta^{0})+\varepsilon_{t}.
\]

\end_inset


\end_layout

\begin_layout Itemize
In general, 
\begin_inset Formula $\varepsilon_{t}$
\end_inset

 will be heteroscedastic and autocorrelated, and possibly nonnormally distribute
d.
 However, dealing with this is exactly as in the case of linear models,
 so we'll just treat the iid case here, 
\begin_inset Formula 
\[
\varepsilon_{t}\sim iid(0,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
If we stack the observations vertically, defining 
\begin_inset Formula 
\[
\mathbf{y}=(y_{1},y_{2},...,y_{n})^{\prime}
\]

\end_inset

 
\begin_inset Formula 
\[
\mathbf{f}=(f(x_{1},\theta),f(x_{1},\theta),...,f(x_{1},\theta))^{\prime}
\]

\end_inset

 and 
\begin_inset Formula 
\[
\varepsilon=(\varepsilon_{1},\varepsilon_{2},...,\varepsilon_{n})^{\prime}
\]

\end_inset

 we can write the 
\begin_inset Formula $n$
\end_inset

 observations as 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta)+\varepsilon
\]

\end_inset

 Using this notation, the NLS estimator can be defined as 
\begin_inset Formula 
\[
\hat{\theta}\equiv\arg\min_{\Theta}s_{n}(\theta)=\frac{1}{n}\left[\mathbf{y}-\mathbf{f}(\theta)\right]^{\prime}\left[\mathbf{y}-\mathbf{f}(\theta)\right]=\frac{1}{n}\parallel\mathbf{y}-\mathbf{f}(\theta)\parallel^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
The estimator minimizes the weighted sum of squared errors, which is the
 same as minimizing the Euclidean distance between 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{f}(\theta).$
\end_inset


\end_layout

\begin_layout Standard
The objective function can be written as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\left[\mathbf{y}^{\prime}\mathbf{y}-2\mathbf{y}^{\prime}\mathbf{f}(\theta)+\mathbf{f}(\theta)^{\prime}\mathbf{f}(\theta)\right],
\]

\end_inset

 which gives the first order conditions
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
-\left[\frac{\partial}{\partial\theta}\mathbf{f}(\hat{\theta})^{\prime}\right]\mathbf{y}+\left[\frac{\partial}{\partial\theta}\mathbf{f}(\hat{\theta})^{\prime}\right]\mathbf{f}(\hat{\theta})\equiv0.
\]

\end_inset

 Define the 
\begin_inset Formula $n\times K$
\end_inset

 matrix 
\begin_inset Formula 
\begin{equation}
\mathbf{F}(\hat{\theta})\equiv D_{\theta^{\prime}}\mathbf{f}(\hat{\theta}).\label{nlsderiv}
\end{equation}

\end_inset

 In shorthand, use 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 in place of 
\begin_inset Formula $\mathbf{F}(\hat{\theta}).$
\end_inset

 Using this, the first order conditions can be written as 
\begin_inset Formula 
\[
-\hat{\mathbf{F}}^{\prime}\mathbf{y}+\hat{\mathbf{F}}^{\prime}\mathbf{f}(\hat{\theta})\equiv0,
\]

\end_inset

 or 
\begin_inset Formula 
\begin{equation}
\hat{\mathbf{F}}^{\prime}\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]\equiv0.\label{nlsfoc}
\end{equation}

\end_inset

 This bears a good deal of similarity to the f.o.c.
 for the linear model - the derivative of the prediction is orthogonal to
 the prediction error.
 If 
\begin_inset Formula $\mathbf{f}(\theta)=\mathbf{X}\theta,$
\end_inset

 then 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 is simply 
\begin_inset Formula $\mathbf{X},$
\end_inset

 so the f.o.c.
 (with spherical errors)
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

simplify to 
\begin_inset Formula 
\[
\mathbf{X}^{\prime}\mathbf{y}-\mathbf{X}^{\prime}\mathbf{X}\beta=0,
\]

\end_inset

 the usual 0LS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

f.o.c.
\end_layout

\begin_layout Standard
We can interpret this geometrically: 
\shape italic
INSERT
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

drawings of geometrical depiction of OLS and NLS (see Davidson and MacKinnon,
 pgs.
 8,13 and 46).
\end_layout

\begin_layout Itemize
Note that the nonlinearity of the manifold leads to potential multiple local
 maxima, minima and saddlepoints:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the objective function 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 is not necessarily well-behaved and may be difficult to minimize.
 
\end_layout

\begin_layout Subsection
Identification
\end_layout

\begin_layout Standard
As before, identification can be considered conditional on the sample, and
 asymptotically.
 The condition for asymptotic identification is that 
\begin_inset Formula $s_{n}(\theta)$
\end_inset

 tend to a limiting function 
\begin_inset Formula $s_{\infty}(\theta)$
\end_inset

 such that 
\begin_inset Formula $s_{\infty}(\theta^{0})<s_{\infty}(\theta),$
\end_inset

 
\begin_inset Formula $\forall\theta\neq\theta^{0}.$
\end_inset

 This will be the case if 
\begin_inset Formula $s_{\infty}(\theta^{0})$
\end_inset

 is strictly convex at 
\begin_inset Formula $\theta^{0},$
\end_inset

 which requires that 
\begin_inset Formula $D_{\theta}^{2}s_{\infty}(\theta^{0})$
\end_inset

 be positive definite.
 Consider the objective function: 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\theta) & = & \frac{1}{n}\sum_{t=1}^{n}\left[y_{t}-f(\mathbf{x}_{t},\theta)\right]^{2}\\
 & = & \frac{1}{n}\sum_{t=1}^{n}\left[f(\mathbf{x}_{t},\theta^{0})+\varepsilon_{t}-f_{t}(\mathbf{x}_{t},\theta)\right]^{2}\\
 & = & \frac{1}{n}\sum_{t=1}^{n}\left[f_{t}(\theta^{0})-f_{t}(\theta)\right]^{2}+\frac{1}{n}\sum_{t=1}^{n}\left(\varepsilon_{t}\right)^{2}\\
 & - & \frac{2}{n}\sum_{t=1}^{n}\left[f_{t}(\theta^{0})-f_{t}(\theta)\right]\varepsilon_{t}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
As in example 
\begin_inset CommandInset ref
LatexCommand ref
reference "eeolsexample"

\end_inset

, which illustrated the consistency of extremum estimators using OLS, we
 conclude that the second term will converge to a constant which does not
 depend upon 
\begin_inset Formula $\theta.$
\end_inset


\end_layout

\begin_layout Itemize
A LLN can be applied to the third term to conclude that it converges pointwise
 to 0, as long as 
\begin_inset Formula $\mathbf{f}(\theta)\;$
\end_inset

and 
\begin_inset Formula $\varepsilon$
\end_inset

 are uncorrelated.
\end_layout

\begin_layout Itemize
Next, pointwise convergence needs to be strengthened to uniform almost sure
 convergence.
 There are a number of possible assumptions one could use.
 Here, we'll just assume it holds.
\end_layout

\begin_layout Itemize
Turning to the first term, we'll assume a pointwise law of large numbers
 applies, so 
\begin_inset Formula 
\begin{equation}
\frac{1}{n}\sum_{t=1}^{n}\left[f_{t}(\theta^{0})-f_{t}(\theta)\right]^{2}\stackrel{a.s.}{\rightarrow}\int\left[f(z,\theta^{0})-f(z,\theta)\right]^{2}d\mu(z),\label{nlslim}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\mu(x)$
\end_inset

 is the distribution function of 
\begin_inset Formula $x.$
\end_inset

 In many cases, 
\begin_inset Formula $f(x,\theta)$
\end_inset

 
\emph on
will
\emph default
 be bounded and continuous, for all 
\begin_inset Formula $\theta\in\Theta,$
\end_inset

 so strengthening to uniform almost sure convergence is immediate.
 For example if 
\begin_inset Formula $f(x,\theta)=\left[1+\exp(-x\theta)\right]^{-1},$
\end_inset

 
\begin_inset Formula $f:\Re^{K}\rightarrow\left(0,1\right),$
\end_inset

 a bounded range, and the function is continuous in 
\begin_inset Formula $\theta.$
\end_inset


\end_layout

\begin_layout Standard
Given these results, it is clear that a minimizer is 
\begin_inset Formula $\theta^{0}.$
\end_inset

 When considering identification (asymptotic), the question is whether or
 not there may be some other minimizer.
 A local condition for identification is that 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{\infty}(\theta)=\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}\int\left[f(x,\theta^{0})-f(x,\theta)\right]^{2}d\mu(x)
\]

\end_inset

 be positive definite at 
\begin_inset Formula $\theta^{0}.$
\end_inset

 Evaluating this derivative, we obtain (after a little work)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left.\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}\int\left[f(x,\theta^{0})-f(x,\theta)\right]^{2}d\mu(x)\right|_{\theta^{0}}=2\int\left[D_{\theta}f(z,\theta^{0})^{\prime}\right]\left[D_{\theta^{\prime}}f(z,\theta^{0})\right]^{\prime}d\mu(z)
\]

\end_inset

 the expectation of the outer product of the gradient of the regression
 function evaluated at 
\begin_inset Formula $\theta^{0}.$
\end_inset

 (Note:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

the uniform boundedness we have already assumed allows passing the derivative
 through the integral, by the dominated convergence theorem.) This matrix
 will be positive definite (wp1) as long as the gradient vector is of full
 rank (wp1).
 The tangent space to the regression manifold must span a 
\begin_inset Formula $K$
\end_inset

 -dimensional space if we are to consistently estimate a 
\begin_inset Formula $K$
\end_inset

 -dimensional parameter vector.
 This is analogous to the requirement that there be no perfect colinearity
 in a linear model.
 This is a necessary condition for identification.
 Note that the LLN implies that the above expectation is equal to 
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\theta^{0})=2\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n}
\]

\end_inset


\end_layout

\begin_layout Subsection
Consistency
\end_layout

\begin_layout Standard
We simply assume that the conditions of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

 hold, so the estimator is consistent.
 Given that the strong stochastic equicontinuity conditions hold, as discussed
 above, and given the above identification conditions an a compact estimation
 space (the closure of the parameter space 
\begin_inset Formula $\Theta),$
\end_inset

 the consistency proof's assumptions are satisfied.
\end_layout

\begin_layout Subsection
Asymptotic normality
\end_layout

\begin_layout Standard
As in the case of GMM, we also simply assume that the conditions for asymptotic
 normality as in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Normality of ee"

\end_inset

 hold.
 The only remaining problem is to determine the form of the asymptotic variance-
covariance matrix.
 Recall that the result of the asymptotic normality theorem is 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left[0,\mathcal{J}_{\infty}(\theta^{0})^{-1}\mathcal{I}_{\infty}(\theta^{0})\mathcal{J}_{\infty}(\theta^{0})^{-1}\right],
\]

\end_inset

 where 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta^{0})$
\end_inset

 is the almost sure limit of 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta\partial\theta^{\prime}}s_{n}(\theta)$
\end_inset

 evaluated at 
\begin_inset Formula $\theta^{0},$
\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{I}_{\infty}(\theta^{0})=\lim Var\sqrt{n}D_{\theta}s_{n}(\theta^{0})
\]

\end_inset

 The objective function is 
\begin_inset Formula 
\[
s_{n}(\theta)=\frac{1}{n}\sum_{t=1}^{n}\left[y_{t}-f(\mathbf{x}_{t},\theta)\right]^{2}
\]

\end_inset

 So 
\begin_inset Formula 
\[
D_{\theta}s_{n}(\theta)=-\frac{2}{n}\sum_{t=1}^{n}\left[y_{t}-f(\mathbf{x}_{t},\theta)\right]D_{\theta}f(\mathbf{x}_{t},\theta).
\]

\end_inset

 Evaluating at 
\begin_inset Formula $\theta^{0},$
\end_inset


\begin_inset Formula 
\[
D_{\theta}s_{n}(\theta^{0})=-\frac{2}{n}\sum_{t=1}^{n}\varepsilon_{t}D_{\theta}f(\mathbf{x}_{t},\theta^{0}).
\]

\end_inset

 Note that the expectation of this is zero, since 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

 are assumed to be uncorrelated.
 So to calculate the variance, we can simply calculate the second moment
 about zero.
 Also note that 
\begin_inset Formula 
\begin{eqnarray*}
\sum_{t=1}^{n}\varepsilon_{t}D_{\theta}f(\mathbf{x}_{t},\theta^{0}) & = & \frac{\partial}{\partial\theta}\left[\mathbf{f}(\theta^{0})\right]^{\prime}\varepsilon\\
 & = & \mathbf{F}^{\prime}\varepsilon
\end{eqnarray*}

\end_inset

With this we obtain 
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{I}_{\infty}(\theta^{0}) & = & \lim Var\sqrt{n}D_{\theta}s_{n}(\theta^{0})\\
 & = & \lim n\mathcal{E}\frac{4}{n^{2}}\mathbf{F}^{\prime}\varepsilon\varepsilon^{\textrm{'}}\mathbf{F}\\
 & = & 4\sigma^{2}\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n}
\end{eqnarray*}

\end_inset

We've already seen that 
\begin_inset Formula 
\[
\mathcal{J}_{\infty}(\theta^{0})=2\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n},
\]

\end_inset

 where the expectation is with respect to the joint density of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\varepsilon.$
\end_inset

 Combining these expressions for 
\begin_inset Formula $\mathcal{J}_{\infty}(\theta^{0})$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}_{\infty}(\theta^{0}),$
\end_inset

 and the result of the asymptotic normality theorem, we get 
\begin_inset Formula 
\[
\sqrt{n}\left(\hat{\theta}-\theta^{0}\right)\stackrel{d}{\rightarrow}N\left(0,\left(\lim\mathcal{E}\frac{\mathbf{F}^{\prime}\mathbf{F}}{n}\right)^{-1}\sigma^{2}\right).
\]

\end_inset

 We can consistently estimate the variance covariance matrix using 
\begin_inset Formula 
\begin{equation}
\left(\frac{\hat{\mathbf{F}}^{\prime}\hat{\mathbf{F}}}{n}\right)^{-1}\hat{\sigma}^{2},\label{nlsvcov}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 is defined as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlsderiv"

\end_inset

 and 
\begin_inset Formula 
\[
\hat{\sigma}^{2}=\frac{\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]^{\prime}\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]}{n},
\]

\end_inset

 the obvious estimator.
 Note the close correspondence to the results for the linear model.
\end_layout

\begin_layout Subsection
Example:
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

The Poisson model for count data
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $y_{t}$
\end_inset

 conditional on 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

 is independently distributed Poisson.
 A Poisson random variable is a 
\emph on
count data
\emph default
 variable, which means it can take the values {0,1,2,...}.
 This sort of model has been used to study visits to doctors per year, number
 of patents registered by businesses per year, 
\emph on
etc.
\end_layout

\begin_layout Standard
The Poisson density is 
\begin_inset Formula 
\[
f(y_{t})=\frac{\exp(-\lambda_{t})\lambda_{t}^{y_{t}}}{y_{t}!},y_{t}\in\{0,1,2,...\}.
\]

\end_inset

 The mean of 
\begin_inset Formula $y_{t}$
\end_inset

 is 
\begin_inset Formula $\lambda_{t},$
\end_inset

 as is the variance.
 Note that 
\begin_inset Formula $\lambda_{t}$
\end_inset

 must be positive.
 Suppose that the true mean is 
\begin_inset Formula 
\[
\lambda_{t}^{0}=\exp(\mathbf{x}_{t}^{\prime}\beta^{0}),
\]

\end_inset

 which enforces the positivity of 
\begin_inset Formula $\lambda_{t}.$
\end_inset

 Suppose we estimate 
\begin_inset Formula $\beta^{0}$
\end_inset

 by nonlinear least squares: 
\begin_inset Formula 
\[
\hat{\beta}=\arg\min s_{n}(\beta)=\frac{1}{T}\sum_{t=1}^{n}\left(y_{t}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
We can write 
\begin_inset Formula 
\begin{eqnarray*}
s_{n}(\beta) & = & \frac{1}{T}\sum_{t=1}^{n}\left(\exp(\mathbf{x}_{t}^{\prime}\beta^{0}+\varepsilon_{t}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)^{2}\\
 & = & \frac{1}{T}\sum_{t=1}^{n}\left(\exp(\mathbf{x}_{t}^{\prime}\beta^{0}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)^{2}+\frac{1}{T}\sum_{t=1}^{n}\varepsilon_{t}^{2}+2\frac{1}{T}\sum_{t=1}^{n}\varepsilon_{t}\left(\exp(\mathbf{x}_{t}^{\prime}\beta^{0}-\exp(\mathbf{x}_{t}^{\prime}\beta)\right)
\end{eqnarray*}

\end_inset

 The last term has expectation zero since the assumption that 
\begin_inset Formula $\mathcal{E}(y_{t}|\mathbf{x}_{t})=\exp(\mathbf{x}_{t}^{\prime}\beta^{0})$
\end_inset

 implies that 
\begin_inset Formula $\mathcal{E}\left(\varepsilon_{t}|\mathbf{x}_{t}\right)=0,$
\end_inset

 which in turn implies that functions of 
\begin_inset Formula $\mathbf{x}_{t}$
\end_inset

 are uncorrelated with 
\begin_inset Formula $\varepsilon_{t}.$
\end_inset

 Applying a strong LLN, and noting that the objective function is continuous
 on a compact parameter space, we get 
\begin_inset Formula 
\[
s_{\infty}(\beta)=\mathcal{E}_{\mathbf{x}}\left(\exp(\mathbf{x}^{\prime}\beta^{0}-\exp(\mathbf{x}^{\prime}\beta)\right)^{2}+\mathcal{E}_{\mathbf{x}}\exp(\mathbf{x}^{\prime}\beta^{0})
\]

\end_inset

 where the last term comes from the fact that the conditional variance of
 
\begin_inset Formula $\varepsilon$
\end_inset

 is the same as the variance of 
\begin_inset Formula $y.$
\end_inset

 This function is clearly minimized at 
\begin_inset Formula $\beta=\beta^{0},$
\end_inset

 so the NLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator is consistent as long as identification holds.
\end_layout

\begin_layout Exercise
Determine the limiting distribution of 
\begin_inset Formula $\sqrt{n}\left(\hat{\beta}-\beta^{0}\right).$
\end_inset

 This means finding the the specific forms of
\begin_inset Formula $\frac{\partial^{2}}{\partial\beta\partial\beta^{\prime}}s_{n}(\beta)$
\end_inset

, 
\begin_inset Formula $\mathcal{J}(\beta^{0}),\left.\frac{\partial s_{n}(\beta)}{\partial\beta}\right|,$
\end_inset

 and 
\begin_inset Formula $\mathcal{I}(\beta^{0}).$
\end_inset

 Again, use a CLT as needed, no need to verify that it can be applied.
\end_layout

\begin_layout Subsection
The Gauss-Newton algorithm
\end_layout

\begin_layout Standard

\series bold
Readings:
\series default

\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

Davidson and MacKinnon, Chapter 6, pgs.
 201-207
\begin_inset Formula $^{*}$
\end_inset

.
\end_layout

\begin_layout Standard
The Gauss-Newton optimization technique is specifically designed for nonlinear
 least squares.
 The idea is to linearize the nonlinear model, rather than the objective
 function.
 The model is 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta^{0})+\varepsilon.
\]

\end_inset

 At some 
\begin_inset Formula $\theta$
\end_inset

 in the parameter space, not equal to 
\begin_inset Formula $\theta^{0},$
\end_inset

 we have 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta)+\nu
\]

\end_inset

 where 
\begin_inset Formula $\nu$
\end_inset

 is a combination of the fundamental error term 
\begin_inset Formula $\varepsilon$
\end_inset

 and the error due to evaluating the regression function at 
\begin_inset Formula $\theta$
\end_inset

 rather than the true value 
\begin_inset Formula $\theta^{0}.$
\end_inset

 Take a first order Taylor's series approximation around a point 
\begin_inset Formula $\theta^{1}:$
\end_inset


\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\theta^{1})+\left[D_{\theta^{\prime}}\mathbf{f}\left(\theta^{1}\right)\right]\left(\theta-\theta^{1}\right)+\nu+\text{approximation error.}
\]

\end_inset

 Define 
\begin_inset Formula $\mathbf{z}\equiv\mathbf{y}-\mathbf{f}(\theta^{1})$
\end_inset

 and 
\begin_inset Formula $b\equiv(\theta-\theta^{1}).$
\end_inset

 Then the last equation can be written as 
\begin_inset Formula 
\[
\mathbf{z}=\mathbf{F}(\theta^{1})b+\omega\text{,}
\]

\end_inset

 where, as above, 
\begin_inset Formula $\mathbf{F}(\theta^{1})\equiv D_{\theta^{\prime}}\mathbf{f}(\theta^{1})$
\end_inset

 is the 
\begin_inset Formula $n\times K$
\end_inset

 matrix of derivatives of the regression function, evaluated at 
\begin_inset Formula $\theta^{1},$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

 is 
\begin_inset Formula $\nu$
\end_inset

 plus approximation error from the truncated Taylor's series.
\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $\mathbf{F}$
\end_inset

 is known, given 
\begin_inset Formula $\theta^{1}.$
\end_inset


\end_layout

\begin_layout Itemize
Note that one could estimate 
\begin_inset Formula $b$
\end_inset

 simply by performing OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

on the above equation.
\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $\hat{b},$
\end_inset

 we calculate a new round estimate of 
\begin_inset Formula $\theta^{0}$
\end_inset

 as 
\begin_inset Formula $\theta^{2}=\hat{b}+\theta^{1}.$
\end_inset

 With this, take a new Taylor's series expansion around 
\begin_inset Formula $\theta^{2}$
\end_inset

 and repeat the process.
 Stop when 
\begin_inset Formula $\hat{b}=0$
\end_inset

 (to within a specified tolerance).
 
\end_layout

\begin_layout Standard
To see why this might work, consider the above approximation, but evaluated
 at the NLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimator: 
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{f}(\hat{\theta})+\mathbf{F}(\hat{\theta})\left(\theta-\hat{\theta}\right)+\omega
\]

\end_inset

 The OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

estimate of 
\begin_inset Formula $b\equiv\theta-\hat{\theta}$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{b}=\left(\hat{\mathbf{F}}^{\prime}\hat{\mathbf{F}}\right)^{-1}\hat{\mathbf{F}}^{\prime}\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right].
\]

\end_inset

 This must be zero, since 
\begin_inset Formula 
\[
\hat{\mathbf{F}}^{\prime}\left(\hat{\theta}\right)\left[\mathbf{y}-\mathbf{f}(\hat{\theta})\right]\equiv0
\]

\end_inset

 by definition of the NLS estimator (these are the normal equations as in
 equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlsfoc"

\end_inset

, Since 
\begin_inset Formula $\hat{b}$
\end_inset

 
\begin_inset Formula $\equiv0$
\end_inset

 when we evaluate at 
\begin_inset Formula $\hat{\theta},$
\end_inset

 updating would stop.
\end_layout

\begin_layout Itemize
The Gauss-Newton method doesn't require second derivatives, as does the
 Newton-Raphson method, so it's faster.
\end_layout

\begin_layout Itemize
The varcov estimator, as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlsvcov"

\end_inset

 is simple to calculate, since we have 
\begin_inset Formula $\hat{\mathbf{F}}$
\end_inset

 as a by-product of the estimation process (
\emph on
i.e.,
\emph default
 it's just the last round 
\begin_inset Quotes eld
\end_inset

regressor matrix
\begin_inset Quotes erd
\end_inset

).
 In fact, a normal OLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

program will give the NLS
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

varcov estimator directly, since it's just the OLS varcov estimator from
 the last iteration.
\end_layout

\begin_layout Itemize
The method can suffer from convergence problems since 
\begin_inset Formula $\mathbf{F}(\theta)^{\prime}\mathbf{F}(\theta),$
\end_inset

 may be very nearly singular, even with an asymptotically identified model,
 especially if 
\begin_inset Formula $\theta$
\end_inset

 is very far from 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 Consider the example 
\begin_inset Formula 
\[
y=\beta_{1}+\beta_{2}x_{t}\beta^{3}+\varepsilon_{t}
\]

\end_inset

 When evaluated at 
\begin_inset Formula $\beta_{2}\approx0,$
\end_inset

 
\begin_inset Formula $\beta_{3}$
\end_inset

 has virtually no effect on the NLS objective function, so 
\begin_inset Formula $\mathbf{F}$
\end_inset

 will have rank that is 
\begin_inset Quotes eld
\end_inset

essentially
\begin_inset Quotes erd
\end_inset

 2, rather than 3.
 In this case, 
\begin_inset Formula $\mathbf{F}^{\prime}\mathbf{F}$
\end_inset

 will be nearly singular, so 
\begin_inset Formula $(\mathbf{F}^{\prime}\mathbf{F})^{-1}$
\end_inset

 will be subject to large roundoff errors.
 
\end_layout

\begin_layout Subsection
Application: Limited dependent variables and sample selection
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
: Davidson and MacKinnon, Ch.
 15
\begin_inset Formula $^{*}$
\end_inset

 (a quick reading is sufficient), J.
 Heckman, 
\begin_inset Quotes eld
\end_inset

Sample Selection Bias as a Specification Error
\begin_inset Quotes erd
\end_inset

, 
\shape italic
Econometrica
\shape default
, 1979 (This is a classic article, not required for reading, and which is
 a bit out-dated.
 Nevertheless it's a good place to start if you encounter sample selection
 problems in your research).
\end_layout

\begin_layout Standard
Sample selection is a common problem in applied research.
 The problem occurs when observations used in estimation are sampled non-randoml
y, according to some selection scheme.
\end_layout

\begin_layout Subsection
Example: Labor Supply
\end_layout

\begin_layout Standard
Labor supply of a person is a positive number of hours per unit time supposing
 the offer wage is higher than the reservation wage, which is the wage at
 which the person prefers not to work.
 The model (very simple, with 
\begin_inset Formula $t$
\end_inset

 subscripts suppressed):
\end_layout

\begin_layout Itemize
Characteristics of individual: 
\begin_inset Formula $\mathbf{x}$
\end_inset


\end_layout

\begin_layout Itemize
Latent labor supply: 
\begin_inset Formula $s^{*}=\mathbf{x}^{\prime}\beta+\omega$
\end_inset


\end_layout

\begin_layout Itemize
Offer wage: 
\begin_inset Formula $w^{o}=\mathbf{z}^{\prime}\gamma+\nu$
\end_inset


\end_layout

\begin_layout Itemize
Reservation wage: 
\begin_inset Formula $w^{r}=\mathbf{q}^{\prime}\delta+\eta$
\end_inset


\end_layout

\begin_layout Standard
Write the wage differential as 
\begin_inset Formula 
\begin{eqnarray*}
w^{*} & = & \left(\mathbf{z}^{\prime}\gamma+\nu\right)-\left(\mathbf{q}^{\prime}\delta+\eta\right)\\
 & \equiv & \mathbf{r}^{\prime}\theta+\varepsilon
\end{eqnarray*}

\end_inset

 We have the set of equations 
\begin_inset Formula 
\begin{eqnarray*}
s^{*} & = & \mathbf{x}^{\prime}\beta+\omega\\
w^{*} & = & \mathbf{r}^{\prime}\theta+\varepsilon.
\end{eqnarray*}

\end_inset

 Assume that 
\begin_inset Formula 
\[
\left[\begin{array}{c}
\omega\\
\varepsilon
\end{array}\right]\sim N\left(\left[\begin{array}{c}
0\\
0
\end{array}\right],\left[\begin{array}{cc}
\sigma^{2} & \rho\sigma\\
\rho\sigma & 1
\end{array}\right]\right).
\]

\end_inset

 We assume that the offer wage and the reservation wage, as well as the
 latent variable 
\begin_inset Formula $s^{*}$
\end_inset

 are unobservable.
 What is observed is 
\begin_inset Formula 
\begin{eqnarray*}
w & = & 1\left[w^{*}>0\right]\\
s & = & ws^{*}.
\end{eqnarray*}

\end_inset

 In other words, we observe whether or not a person is working.
 If the person is working, we observe labor supply, which is equal to latent
 labor supply, 
\begin_inset Formula $s^{*}.$
\end_inset

 Otherwise, 
\begin_inset Formula $s=0\neq s^{*}.$
\end_inset

 Note that we are using a simplifying assumption that individuals can freely
 choose their weekly hours of work.
\end_layout

\begin_layout Standard
Suppose we estimated the model 
\begin_inset Formula 
\[
s^{*}=\mathbf{x}^{\prime}\beta+\text{residual}
\]

\end_inset

 using only observations for which 
\begin_inset Formula $s>0.$
\end_inset

 The problem is that these observations are those for which 
\begin_inset Formula $w^{*}>0,$
\end_inset

 or equivalently, 
\begin_inset Formula $-\varepsilon<\mathbf{r}^{\prime}\theta$
\end_inset

 and 
\begin_inset Formula 
\[
\mathcal{E}\left[\omega|-\varepsilon<\mathbf{r}^{\prime}\theta\right]\neq0,
\]

\end_inset

 since 
\begin_inset Formula $\varepsilon$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

 are dependent.
 Furthermore, this expectation will in general depend on 
\begin_inset Formula $\mathbf{x}$
\end_inset

 since elements of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can enter in 
\begin_inset Formula $\mathbf{r}.$
\end_inset

 Because of these two facts, least squares estimation is biased and inconsistent.
\end_layout

\begin_layout Standard
Consider more carefully 
\begin_inset Formula $\mathcal{E}\left[\omega|-\varepsilon<\mathbf{r}^{\prime}\theta\right].$
\end_inset

 Given the joint normality of 
\begin_inset Formula $\omega$
\end_inset

 and 
\begin_inset Formula $\varepsilon,$
\end_inset

 we can write (see for example Spanos 
\shape italic
Statistical Foundations of Econometric Modelling,
\shape default
 pg.
 122) 
\begin_inset Formula 
\[
\omega=\rho\sigma\varepsilon+\eta,
\]

\end_inset

 where 
\begin_inset Formula $\eta$
\end_inset

 has mean zero and is independent of 
\begin_inset Formula $\varepsilon$
\end_inset

.
 With this we can write 
\begin_inset Formula 
\[
s^{*}=\mathbf{x}^{\prime}\beta+\rho\sigma\varepsilon+\eta.
\]

\end_inset

 If we condition this equation on 
\begin_inset Formula $-\varepsilon<\mathbf{r}^{\prime}\theta$
\end_inset

 we get 
\begin_inset Formula 
\[
s=\mathbf{x}^{\prime}\beta+\rho\sigma\mathcal{E}(\varepsilon|-\varepsilon<\mathbf{r}^{\prime}\theta)+\eta
\]

\end_inset

which may be written as 
\begin_inset Formula 
\[
s=\mathbf{x}^{\prime}\beta+\rho\sigma\mathcal{E}(\varepsilon|\varepsilon>-\mathbf{r}^{\prime}\theta)+\eta
\]

\end_inset


\end_layout

\begin_layout Itemize
A useful result is that for 
\begin_inset Formula 
\[
z\sim N(0,1)
\]

\end_inset

 
\begin_inset Formula 
\[
E(z|z>z^{*})=\frac{\phi(z^{*})}{\Phi(-z^{*})},
\]

\end_inset

 where 
\begin_inset Formula $\phi\left(\cdot\right)$
\end_inset

 and 
\begin_inset Formula $\Phi\left(\cdot\right)$
\end_inset

 are the standard normal density and distribution function, respectively.
 The quantity on the RHS above is known as the 
\emph on
inverse Mill's ratio:
\emph default
 
\begin_inset Formula 
\[
IMR(\mathbf{z}^{*})=\frac{\phi(z^{*})}{\Phi(-z^{*})}
\]

\end_inset

 With this we can write (making use of the fact that the standard normal
 density is symmetric about zero, so that 
\begin_inset Formula $\phi(-a)=\phi(a)$
\end_inset

): 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
s & = & \mathbf{x}^{\prime}\beta+\rho\sigma\frac{\phi\left(\mathbf{r}^{\prime}\theta\right)}{\Phi\left(\mathbf{r}^{\prime}\theta\right)}+\eta\label{aa}\\
 & \equiv & \left[\begin{array}{cc}
\mathbf{x}^{\prime} & \frac{\phi\left(\mathbf{r}^{\prime}\theta\right)}{\Phi\left(\mathbf{r}^{\prime}\theta\right)}\end{array}\right]\left[\begin{array}{c}
\beta\\
\zeta
\end{array}\right]+\eta.\label{bb}
\end{eqnarray}

\end_inset

 where 
\begin_inset Formula $\zeta=\rho\sigma$
\end_inset

.
 The error term 
\begin_inset Formula $\eta$
\end_inset

 has conditional mean zero, and is uncorrelated with the regressors 
\begin_inset Formula $\begin{array}{cc}
\mathbf{x}^{\prime} & \frac{\phi\left(\mathbf{r}^{\prime}\theta\right)}{\Phi\left(\mathbf{r}^{\prime}\theta\right)}\end{array}.$
\end_inset

 At this point, we can estimate the equation by NLS.
 
\end_layout

\begin_layout Itemize
Heckman showed how one can estimate this in a two step procedure where first
 
\begin_inset Formula $\theta$
\end_inset

 is estimated, then equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "bb"

\end_inset

 is estimated by least squares using the estimated value of 
\begin_inset Formula $\theta$
\end_inset

 to form the regressors.
 This is inefficient and estimation of the covariance is a tricky issue.
 It is probably easier (and more efficient) just to do MLE.
\end_layout

\begin_layout Itemize
The model presented above depends strongly on joint normality.
 There exist many alternative models which weaken the maintained assumptions.
 It is possible to estimate consistently without distributional assumptions.
 See Ahn and Powell, 
\shape italic
Journal of Econometrics
\shape default
, 1994.
 
\end_layout

\begin_layout Section
The Fourier functional form
\end_layout

\begin_layout Standard
This material was removed from the chapter on nonparametric regression,
 to make that chapter easier to read, and to focus on the main ideas.
 
\end_layout

\begin_layout Standard

\series bold
Readings
\series default
: Gallant, 1987, 
\begin_inset Quotes eld
\end_inset

Identification and consistency in semi-nonparametric regression,
\begin_inset Quotes erd
\end_inset

 in 
\emph on
Advances in Econometrics, Fifth World Congress
\shape italic
\emph default
,
\shape default
 V.
 1, Truman Bewley, ed., Cambridge.
\end_layout

\begin_layout Standard
Suppose we have a multivariate model 
\begin_inset Formula 
\[
y=f(\mathbf{x})+\varepsilon,
\]

\end_inset

 where 
\begin_inset Formula $f(x)$
\end_inset

 is of unknown form and 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $P-$
\end_inset

dimensional vector.
 For simplicity, assume that 
\begin_inset Formula $\varepsilon$
\end_inset

 is a classical error.
 Let us take the estimation of the vector of elasticities with typical element
 
\begin_inset Formula 
\[
\xi_{x_{i}}=\frac{\mathbf{x}_{i}}{f(\mathbf{x})}\frac{\partial f(\mathbf{x})}{\partial x_{i}f(x)},
\]

\end_inset

 at an arbitrary point 
\begin_inset Formula $\mathbf{x}_{i}.$
\end_inset


\end_layout

\begin_layout Standard
The Fourier form, following Gallant (1982), but with a somewhat different
 parameterization, may be written as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g_{K}(\mathbf{x}\mid\theta_{K})=\alpha+\mathbf{x}^{\prime}\beta+1/2\mathbf{x}^{\prime}\mathbf{Cx}+\sum_{\alpha=1}^{A}\sum_{j=1}^{J}\left(u_{j\alpha}\cos(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})-v_{j\alpha}\sin(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})\right).\label{FourierForm}
\end{equation}

\end_inset

 where the 
\begin_inset Formula $K$
\end_inset

-dimensional parameter vector 
\begin_inset Formula 
\begin{equation}
\theta_{K}=\{\alpha,\beta^{\prime},vec^{*}(C)^{\prime},u_{11},v_{11},\ldots,u_{JA},v_{JA}\}^{\prime}.\label{thetak}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
We assume that the conditioning variables 
\begin_inset Formula $\mathbf{x}$
\end_inset

 have each been transformed to lie in an interval that is shorter than 
\begin_inset Formula $2\pi.$
\end_inset

 This is required to avoid periodic behavior of the approximation, which
 is desirable since economic functions aren't periodic.
 For example, subtract sample means, divide by the maxima of the conditioning
 variables, and multiply by 
\begin_inset Formula $2\pi-eps,$
\end_inset

 where 
\begin_inset Formula $eps$
\end_inset

 is some positive number less than 
\begin_inset Formula $2\pi$
\end_inset

 in value.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $k_{\alpha}$
\end_inset

 are 
\begin_inset Quotes erd
\end_inset

elementary multi-indices
\begin_inset Quotes erd
\end_inset

 which are simply 
\begin_inset Formula $P-$
\end_inset

 vectors formed of integers (negative, positive and zero).
 The 
\begin_inset Formula $k_{\alpha}$
\end_inset

, 
\begin_inset Formula $\alpha=1,2,...,A$
\end_inset

 are required to be linearly independent, and we follow the convention that
 the first non-zero element be positive.
 For example 
\begin_inset Formula 
\[
\left[\begin{array}{ccccc}
0 & 1 & -1 & 0 & 1\end{array}\right]^{\prime}
\]

\end_inset

 is a potential multi-index to be used, but 
\begin_inset Formula 
\[
\left[\begin{array}{ccccc}
0 & -1 & -1 & 0 & 1\end{array}\right]^{\prime}
\]

\end_inset

 is not since its first nonzero element is negative.
 Nor is 
\begin_inset Formula 
\[
\left[\begin{array}{ccccc}
0 & 2 & -2 & 0 & 2\end{array}\right]^{\prime}
\]

\end_inset

 a multi-index we would use, since it is a scalar multiple of the original
 multi-index.
\end_layout

\begin_layout Itemize
We parameterize the matrix 
\begin_inset Formula $C$
\end_inset

 differently than does Gallant because it simplifies things in practice.
 The cost of this is that we are no longer able to test a quadratic specificatio
n using nested testing.
 
\end_layout

\begin_layout Standard
The vector of first partial derivatives is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{x}g_{K}(\mathbf{x}\mid\theta_{K})=\beta+\mathbf{Cx}+\sum_{\alpha=1}^{A}\sum_{j=1}^{J}\left[\left(-u_{j\alpha}\sin(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})-v_{j\alpha}\cos(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})\right)j\mathbf{k}_{\alpha}\right]\label{firstderivative}
\end{equation}

\end_inset

and the matrix of second partial derivatives is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{x}^{2}g_{K}(\mathbf{x}|\theta_{K})=\mathbf{C}+\sum_{\alpha=1}^{A}\sum_{j=1}^{J}\left[\left(-u_{j\alpha}\cos(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})+v_{j\alpha}\sin(j\mathbf{k}_{\alpha}^{\prime}\mathbf{x})\right)j^{2}\mathbf{k}_{\alpha}\mathbf{k}_{\alpha}^{\prime}\right]\label{secondderivative}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
To define a compact notation for partial derivatives, let 
\begin_inset Formula $\lambda$
\end_inset

 be an 
\begin_inset Formula $N$
\end_inset

-dimensional multi-index with no negative elements.
 Define 
\begin_inset Formula $\mid\lambda\mid^{*}$
\end_inset

 as the sum of the elements of 
\begin_inset Formula $\lambda$
\end_inset

.
 If we have 
\begin_inset Formula $N$
\end_inset

 arguments 
\begin_inset Formula $\mathbf{x}$
\end_inset

 of the (arbitrary) function 
\begin_inset Formula $h(\mathbf{x})$
\end_inset

, use 
\begin_inset Formula $D^{\lambda}h(\mathbf{x})$
\end_inset

 to indicate a certain partial derivative: 
\begin_inset Formula 
\[
D^{\lambda}h(\mathbf{x})\equiv\frac{\partial^{\mid\lambda\mid^{*}}}{\partial x_{1}^{\lambda_{1}}\partial x_{2}^{\lambda_{2}}\cdots\partial x_{N}^{\lambda_{N}}}h(\mathbf{x})
\]

\end_inset

 When 
\begin_inset Formula $\lambda$
\end_inset

 is the zero vector, 
\begin_inset Formula $D^{\lambda}h(\mathbf{x})\equiv h(\mathbf{x})$
\end_inset

.
 Taking this definition and the last few equations into account, we see
 that it is possible to define 
\begin_inset Formula $\left(1\times K\right)$
\end_inset

 vector 
\begin_inset Formula $Z^{\lambda}(\mathbf{x})$
\end_inset

 so that 
\begin_inset Formula 
\begin{equation}
D^{\lambda}g_{K}(\mathbf{x}|\theta_{K})=\mathbf{z}^{\lambda}(\mathbf{x})^{\prime}\theta_{K}.\label{Znotation}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
Both the approximating model and the derivatives of the approximating model
 are linear in the parameters.
 
\end_layout

\begin_layout Itemize
For the approximating model to the function (not derivatives), write 
\begin_inset Formula $g_{K}(\mathbf{x}|\theta_{K})=\mathbf{z}^{\prime}\theta_{K}$
\end_inset

 for simplicity.
\end_layout

\begin_layout Standard
The following theorem can be used to prove the consistency of the Fourier
 form.
\end_layout

\begin_layout Theorem
[Gallant and Nychka, 1987] Suppose that 
\begin_inset Formula $\hat{h}_{n}$
\end_inset

 is obtained by maximizing a sample objective function 
\begin_inset Formula $s_{n}(h)$
\end_inset

 over 
\begin_inset Formula $\mathcal{H}_{K_{n}}$
\end_inset

 where 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is a subset of some function space 
\begin_inset Formula $\mathcal{H}$
\end_inset

 on which is defined a norm 
\begin_inset Formula $\parallel h\parallel$
\end_inset

.
 Consider the following conditions:
\end_layout

\begin_layout Theorem
(a) Compactness: The closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 is compact in the relative topology defined by 
\begin_inset Formula $\parallel h\parallel$
\end_inset

.
\end_layout

\begin_layout Theorem
(b) Denseness: 
\begin_inset Formula $\cup_{K}\mathcal{H}_{K}$
\end_inset

, 
\begin_inset Formula $K=1,2,3,...$
\end_inset

 is a dense subset of the closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 and 
\begin_inset Formula $\mathcal{H}_{K}\subset\mathcal{H}_{K+1}$
\end_inset

.
\end_layout

\begin_layout Theorem
(c) Uniform convergence: There is a point 
\begin_inset Formula $h^{*}$
\end_inset

 in 
\begin_inset Formula $\mathcal{H}$
\end_inset

 and there is a function 
\begin_inset Formula $s_{\infty}(h,h^{*})$
\end_inset

 that is continuous in 
\begin_inset Formula $h$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\sup_{\overline{\mathcal{H}}}\mid s_{n}(h)-s_{\infty}(h,h^{*})\mid=0
\]

\end_inset

 almost surely.
\end_layout

\begin_layout Theorem
(d) Identification: Any point 
\begin_inset Formula $h$
\end_inset

 in the closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 with 
\begin_inset Formula $s_{\infty}(h,h^{*})\geq s_{\infty}(h^{*},h^{*})$
\end_inset

 must have 
\begin_inset Formula $\parallel h-h^{*}\parallel=0$
\end_inset

.
\end_layout

\begin_layout Theorem
Under these conditions 
\begin_inset Formula $\lim_{n\rightarrow\infty}\parallel h^{*}-\hat{h}_{n}\parallel=0$
\end_inset

 almost surely, provided that 
\begin_inset Formula $\lim_{n\rightarrow\infty}K_{n}=\infty$
\end_inset

 almost surely.
 
\end_layout

\begin_layout Standard
The modification of the original statement of the theorem that has been
 made is to set the parameter space 
\begin_inset Formula $\Theta$
\end_inset

 in Gallant and Nychka's (1987) Theorem 0 to a single point and to state
 the theorem in terms of maximization rather than minimization.
\end_layout

\begin_layout Standard
This theorem is very similar in form to Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

.
 The main differences are:
\end_layout

\begin_layout Enumerate
A generic norm 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 is used in place of the Euclidean norm.
 This norm may be stronger than the Euclidean norm, so that convergence
 with respect to 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 implies convergence w.r.t the Euclidean norm.
 Typically we will want to make sure that the norm is strong enough to imply
 convergence of all functions of interest.
\end_layout

\begin_layout Enumerate
The 
\begin_inset Quotes eld
\end_inset

estimation space
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mathcal{H}$
\end_inset

 is a function space.
 It plays the role of the parameter space 
\begin_inset Formula $\Theta$
\end_inset

 in our discussion of parametric estimators.
 There is no restriction to a parametric family, only a restriction to a
 space of functions that satisfy certain conditions.
 This formulation is much less restrictive than the restriction to a parametric
 family.
\end_layout

\begin_layout Enumerate
There is a denseness assumption that was not present in the other theorem.
 
\end_layout

\begin_layout Standard
We will not prove this theorem (the proof is quite similar to the proof
 of theorem [
\begin_inset CommandInset ref
LatexCommand ref
reference "Consistency of ee"

\end_inset

], see Gallant, 1987) but we will discuss its assumptions, in relation to
 the Fourier form as the approximating model.
\end_layout

\begin_layout Paragraph
Sobolev norm
\end_layout

\begin_layout Standard
Since all of the assumptions involve the norm 
\begin_inset Formula $\parallel h\parallel$
\end_inset

 , we need to make explicit what norm we wish to use.
 We need a norm that guarantees that the errors in approximation of the
 functions we are interested in are accounted for.
 Since we are interested in first-order elasticities in the present case,
 we need close approximation of both the function 
\begin_inset Formula $f(x)$
\end_inset

 and its first derivative 
\begin_inset Formula $f^{\prime}(x),$
\end_inset

 throughout the range of 
\begin_inset Formula $x.$
\end_inset

 Let 
\begin_inset Formula $\mathcal{X}$
\end_inset

 be an open set that contains all values of 
\begin_inset Formula $x$
\end_inset

 that we're interested in.
 The Sobolev norm is appropriate in this case.
 It is defined, making use of our notation for partial derivatives, as:
 
\begin_inset Formula 
\[
\parallel h\parallel_{m,\mathcal{X}}=\max_{\left|\lambda^{*}\right|\leq m}\sup_{\mathcal{X}}\left|D^{\lambda}h(x)\right|
\]

\end_inset

 To see whether or not the function 
\begin_inset Formula $f(x)$
\end_inset

 is well approximated by an approximating model 
\begin_inset Formula $g_{K}(x\mid\theta_{K})$
\end_inset

, we would evaluate
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\parallel f(\mathbf{x})-g_{K}(\mathbf{x}\mid\theta_{K})\parallel_{m,\mathcal{X}}.
\]

\end_inset

 We see that this norm takes into account errors in approximating the function
 and partial derivatives up to order 
\begin_inset Formula $m.$
\end_inset

 If we want to estimate first order elasticities, as is the case in this
 example, the relevant 
\begin_inset Formula $m$
\end_inset

 would be 
\begin_inset Formula $m=1.$
\end_inset

 Furthermore, since we examine the 
\begin_inset Formula $\sup$
\end_inset

 over 
\begin_inset Formula $\mathcal{X},$
\end_inset

 convergence w.r.t.
 the Sobolev means 
\shape italic
uniform
\shape default
 convergence, so that we obtain consistent estimates for all values of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Paragraph
Compactness
\end_layout

\begin_layout Standard
Verifying compactness with respect to this norm is quite technical and unenlight
ening.
 It is proven by Elbadawi, Gallant and Souza, 
\shape italic
Econometrica
\shape default
, 1983.
 The basic requirement is that if we need consistency w.r.t.
 
\begin_inset Formula $\parallel h\parallel_{m,\mathcal{X}},$
\end_inset

 then the functions of interest must belong to a Sobolev space which takes
 into account derivatives of order 
\begin_inset Formula $m+1$
\end_inset

.
 A Sobolev space is the set of functions 
\begin_inset Formula 
\[
\mathcal{W}_{m,\mathcal{X}}(D)=\{h(\mathbf{x}):\parallel h(\mathbf{x})\parallel_{m,\mathcal{X}}<D\},
\]

\end_inset

 where 
\begin_inset Formula $D$
\end_inset

 is a finite constant.
 In plain words, the functions must have bounded partial derivatives of
 one order higher than the derivatives we seek to estimate.
\end_layout

\begin_layout Paragraph
The estimation space and the estimation subspace
\end_layout

\begin_layout Standard
Since in our case we're interested in consistent estimation of first-order
 elasticities, we'll define the estimation space as follows:
\end_layout

\begin_layout Definition
[Estimation space] The estimation space 
\begin_inset Formula $\mathcal{H}=\mathcal{W}_{2,\mathcal{X}}(D).$
\end_inset

 The estimation space is an open set, and we presume that 
\begin_inset Formula $h^{*}\in\mathcal{H}.$
\end_inset


\end_layout

\begin_layout Standard
So we are assuming that the function to be estimated has bounded second
 derivatives throughout 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
\end_layout

\begin_layout Standard
With seminonparametric estimators, we don't actually optimize over the estimatio
n space.
 Rather, we optimize over a subspace, 
\begin_inset Formula $\mathcal{H}_{K_{n}},$
\end_inset

 defined as:
\end_layout

\begin_layout Definition
[Estimation subspace] The estimation subspace 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is defined as 
\begin_inset Formula 
\[
\mathcal{H}_{K}=\{g_{K}(\mathbf{x}|\theta_{K}):g_{K}(\mathbf{x}|\theta_{K})\in\mathcal{W}_{2,\mathcal{Z}}(D),\theta_{K}\in\Re^{K}\},
\]

\end_inset

 where 
\begin_inset Formula $g_{K}(\mathbf{x},\theta_{K})$
\end_inset

 is the Fourier form approximation as defined in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "FourierForm"

\end_inset

.
 
\end_layout

\begin_layout Paragraph
Denseness
\end_layout

\begin_layout Standard
The important point here is that 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is a space of functions that is indexed by a finite dimensional parameter
 (
\begin_inset Formula $\theta_{K}$
\end_inset

 has 
\begin_inset Formula $K$
\end_inset

 elements, as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "thetak"

\end_inset

).
 With 
\begin_inset Formula $n$
\end_inset

 observations, 
\begin_inset Formula $n>K,$
\end_inset

 this parameter is estimable.
 Note that the true function 
\begin_inset Formula $h^{*}$
\end_inset

 is not necessarily an element of 
\begin_inset Formula $\mathcal{H}_{K},$
\end_inset

 so optimization over 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 may not lead to a consistent estimator.
 In order for optimization over 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 to be equivalent to optimization over 
\begin_inset Formula $\mathcal{H},$
\end_inset

 at least asymptotically, we need that:
\end_layout

\begin_layout Enumerate
The dimension of the parameter vector, 
\begin_inset Formula $\dim\theta_{K_{n}}\rightarrow\infty$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 This is achieved by making 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $J$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "FourierForm"

\end_inset

 increasing functions of 
\begin_inset Formula $n,$
\end_inset

 the sample size.
 It is clear that 
\begin_inset Formula $K$
\end_inset

 will have to grow more slowly than 
\begin_inset Formula $n$
\end_inset

.
 The second requirement is:
\end_layout

\begin_layout Enumerate
We need that the 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 be dense subsets of 
\begin_inset Formula $\mathcal{H}.$
\end_inset


\end_layout

\begin_layout Standard
The estimation subspace 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

, defined above, is a subset of the closure of the estimation space, 
\begin_inset Formula $\overline{\mathcal{H}}$
\end_inset

 .
 A
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
 
\end_layout

\end_inset

set of subsets 
\begin_inset Formula $\mathcal{A}_{a}$
\end_inset

 of a set 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

dense
\begin_inset Quotes erd
\end_inset

 if the closure of the countable union of the subsets is equal to the closure
 of 
\begin_inset Formula $\mathcal{A}$
\end_inset

: 
\begin_inset Formula 
\[
\overline{\cup_{a=1}^{\infty}\mathcal{A}_{a}}=\overline{\mathcal{A}}
\]

\end_inset

 
\emph on
Use a picture here.
 The rest of the discussion of denseness is provided just for completeness:
 there's no need to study it in detail
\emph default
.
 To show that 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

 is a dense subset of 
\begin_inset Formula $\overline{\mathcal{H}}$
\end_inset

 with respect to 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}},$
\end_inset

 it is useful to apply Theorem 1 of Gallant (1982), who in turn cites Edmunds
 and Moscatelli (1977).
 We reproduce the theorem as presented by Gallant, with minor notational
 changes, for convenience of reference:
\end_layout

\begin_layout Theorem
[Edmunds and Moscatelli, 1977] 
\begin_inset CommandInset label
LatexCommand label
name "EdMosctheorem"

\end_inset

Let the real-valued function 
\begin_inset Formula $h^{*}(\mathbf{x})$
\end_inset

 be continuously differentiable up to order 
\begin_inset Formula $m$
\end_inset

 on an open set containing the closure of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 Then it is possible to choose a triangular array of coefficients 
\begin_inset Formula $\theta_{1},\theta_{2},\ldots\theta_{K},\ldots,$
\end_inset

 such that for every 
\begin_inset Formula $q$
\end_inset

 with 
\begin_inset Formula $0\leq q<m$
\end_inset

, and every 
\begin_inset Formula $\varepsilon>0,$
\end_inset

 
\begin_inset Formula $\parallel h^{*}(\mathbf{x})-h_{K}(\mathbf{x}|\theta_{K})\parallel_{q,\mathcal{X}}=o(K^{-m+q+\varepsilon})$
\end_inset

 as 
\begin_inset Formula $K\rightarrow\infty.$
\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
In the present application, 
\begin_inset Formula $q=1$
\end_inset

, and 
\begin_inset Formula $m=2$
\end_inset

.
 By definition of the estimation space, the elements of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 are once continuously differentiable on 
\begin_inset Formula $\mathcal{X}$
\end_inset

, which is open and contains the closure of 
\begin_inset Formula $\mathcal{X}$
\end_inset

, so the theorem is applicable.
 Closely following Gallant and Nychka (1987), 
\begin_inset Formula $\cup_{\infty}\mathcal{H}_{K}$
\end_inset

 is the countable union of the 
\begin_inset Formula $\mathcal{H}_{K}$
\end_inset

.
 The implication of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "EdMosctheorem"

\end_inset

 is that there is a sequence of {
\begin_inset Formula $h_{K}$
\end_inset

} from 
\begin_inset Formula $\cup_{\infty}\mathcal{H}_{K}$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{K\rightarrow\infty}\parallel h^{*}-h_{K}\parallel_{1,\mathcal{X}}=0,
\]

\end_inset

 for all 
\begin_inset Formula $h^{*}\in\mathcal{H}$
\end_inset

.
 Therefore, 
\begin_inset Formula 
\[
\mathcal{H}\subset\overline{\cup_{\infty}\mathcal{H}_{K}}.
\]

\end_inset

However, 
\begin_inset Formula 
\[
\cup_{\infty}\mathcal{H}_{K}\subset\mathcal{H},
\]

\end_inset

 so 
\begin_inset Formula 
\[
\overline{\cup_{\infty}\mathcal{H}_{K}}\subset\overline{\mathcal{H}}.
\]

\end_inset

Therefore 
\begin_inset Formula 
\[
\overline{\mathcal{H}}=\overline{\cup_{\infty}\mathcal{H}_{K}},
\]

\end_inset

 so 
\begin_inset Formula $\cup_{\infty}\mathcal{H}_{K}$
\end_inset

 is a dense subset of 
\begin_inset Formula $\mathcal{H}$
\end_inset

, with respect to the norm 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Uniform convergence
\end_layout

\begin_layout Standard
We now turn to the limiting objective function.
 We estimate by OLS.
 The sample objective function stated in terms of maximization is 
\begin_inset Formula 
\[
s_{n}(\theta_{K})=-\frac{1}{n}\sum_{t=1}^{n}\left(y_{t}-g_{K}(\mathbf{x}_{t}\mid\theta_{K})\right)^{2}
\]

\end_inset

With random sampling, as in the case of Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "olslim"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "nlslim"

\end_inset

, the limiting objective function is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
s_{\infty}\left(g,f\right)=-\int_{\mathcal{X}}\left(f(\mathbf{x})-g(\mathbf{x})\right)^{2}d\mu x-\sigma_{\varepsilon}^{2}.\label{limobjfn}
\end{equation}

\end_inset

 where the true function 
\begin_inset Formula $f(x)$
\end_inset

 takes the place of the generic function 
\begin_inset Formula $h^{*}$
\end_inset

 in the presentation of the theorem.
 Both 
\begin_inset Formula $g(x)$
\end_inset

 and 
\begin_inset Formula $f(x)$
\end_inset

 are elements of 
\begin_inset Formula $\overline{\cup_{\infty}\mathcal{H}_{K}}$
\end_inset

.
\end_layout

\begin_layout Standard
The pointwise convergence of the objective function needs to be strengthened
 to uniform convergence.
 We will simply assume that this holds, since the way to verify this depends
 upon the specific application.
 We also have continuity of the objective function in 
\begin_inset Formula $g,$
\end_inset

 with respect to the norm 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}}$
\end_inset

 since 
\begin_inset Formula 
\begin{eqnarray*}
 &  & \lim_{\parallel g^{1}-g^{0}\parallel_{1,\mathcal{X}}\rightarrow0}\left\{ s_{\infty}\left(g^{1},f)\right)-s_{\infty}\left(g^{0},f)\right)\right\} \\
 & = & \lim_{\parallel g^{1}-g^{0}\parallel_{1,\mathcal{X}}\rightarrow0}\int_{\mathcal{X}}\left[\left(g^{1}(\mathbf{x})-f(\mathbf{x})\right)^{2}-\left(g^{0}(\mathbf{x})-f(\mathbf{x})\right)^{2}\right]d\mu x.
\end{eqnarray*}

\end_inset

By the dominated convergence theorem (which applies since the finite bound
 
\begin_inset Formula $D$
\end_inset

 used to define 
\begin_inset Formula $\mathcal{W}_{2,\mathcal{Z}}(D)$
\end_inset

 is dominated by an integrable function), the limit and the integral can
 be interchanged, so by inspection, the limit is zero.
\end_layout

\begin_layout Paragraph
Identification
\end_layout

\begin_layout Standard
The identification condition requires that for any point 
\begin_inset Formula $(g,f)$
\end_inset

 in 
\begin_inset Formula $\overline{\mathcal{H}}\times\overline{\mathcal{H}},$
\end_inset

 
\begin_inset Formula $s_{\infty}(g,f)\geq s_{\infty}(f,f)$
\end_inset

 
\begin_inset Formula $\Rightarrow$
\end_inset

 
\begin_inset Formula $\parallel g-f\parallel_{1,\mathcal{X}}=0$
\end_inset

.
 This condition is clearly satisfied given that 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $f$
\end_inset

 are once continuously differentiable (by the assumption that defines the
 estimation space).
\end_layout

\begin_layout Paragraph
Review of concepts
\end_layout

\begin_layout Standard
For the example of estimation of first-order elasticities, the relevant
 concepts are:
\end_layout

\begin_layout Itemize
Estimation space 
\begin_inset Formula $\mathcal{H}=\mathcal{W}_{2,\mathcal{X}}(D)$
\end_inset

: the function space in the closure of which the true function must lie.
\end_layout

\begin_layout Itemize
Consistency norm 
\begin_inset Formula $\parallel h\parallel_{1,\mathcal{X}}.$
\end_inset

 The closure of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 is compact with respect to this norm.
\end_layout

\begin_layout Itemize
Estimation subspace 
\begin_inset Formula $\mathcal{H}_{K}.$
\end_inset

 The estimation subspace is the subset of 
\begin_inset Formula $\mathcal{H}$
\end_inset

 that is representable by a Fourier form with parameter 
\begin_inset Formula $\theta_{K}.$
\end_inset

 These are dense subsets of 
\begin_inset Formula $\mathcal{H}.$
\end_inset


\end_layout

\begin_layout Itemize
Sample objective function 
\begin_inset Formula $s_{n}(\theta_{K}),$
\end_inset

 the negative of the sum of squares.
 By standard arguments this converges uniformly to the
\end_layout

\begin_layout Itemize
Limiting objective function 
\begin_inset Formula $s_{\infty}($
\end_inset

 
\begin_inset Formula $g,f),$
\end_inset

 which is continuous in 
\begin_inset Formula $g$
\end_inset

 and has a global maximum in its first argument, over the closure of the
 infinite union of the estimation subpaces, at 
\begin_inset Formula $g=f.$
\end_inset


\end_layout

\begin_layout Itemize
As a result of this, first order elasticities 
\begin_inset Formula 
\[
\frac{\mathbf{x}_{i}}{f(\mathbf{x})}\frac{\partial f(\mathbf{x})}{\partial x_{i}f(x)}
\]

\end_inset

 are consistently estimated for all 
\begin_inset Formula $\mathbf{x}\in\mathcal{X}.$
\end_inset


\end_layout

\begin_layout Paragraph
Discussion
\end_layout

\begin_layout Standard
Consistency requires that the number of parameters used in the expansion
 increase with the sample size, tending to infinity.
 If parameters are added at a high rate, the bias tends relatively rapidly
 to zero.
 A basic problem is that a high rate of inclusion of additional parameters
 causes the variance to tend more slowly to zero.
 The issue of how to chose the rate at which parameters are added and which
 to add first is fairly complex.
 A problem is that the allowable rates for asymptotic normality to obtain
 (Andrews 1991; Gallant and Souza, 1991) are very strict.
 Supposing we stick to these rates, our approximating model is: 
\begin_inset Formula 
\[
g_{K}(\mathbf{x}|\theta_{K})=\mathbf{z}^{\prime}\theta_{K}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Define 
\begin_inset Formula $\mathbf{Z}_{K}$
\end_inset

 as the 
\begin_inset Formula $n\times K$
\end_inset

 matrix of regressors obtained by stacking observations.
 The LS estimator is 
\begin_inset Formula 
\[
\hat{\theta}_{K}=\left(\mathbf{Z}_{K}^{\prime}\mathbf{Z}_{K}\right)^{+}\mathbf{Z}_{K}^{\prime}y,
\]

\end_inset

 where 
\begin_inset Formula $\left(\cdot\right)^{+}$
\end_inset

 is the Moore-Penrose generalized inverse.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is used since 
\begin_inset Formula $\mathbf{Z}_{K}^{\prime}\mathbf{Z}_{K}$
\end_inset

 may be singular, as would be the case for 
\begin_inset Formula $K(n)$
\end_inset

 large enough when some dummy variables are included.
 
\end_layout

\end_deeper
\begin_layout Itemize
.
 The prediction, 
\begin_inset Formula $\mathbf{z}^{\prime}\hat{\theta}_{K},$
\end_inset

 of the unknown function 
\begin_inset Formula $f(\mathbf{x})$
\end_inset

 is asymptotically normally distributed: 
\begin_inset Formula 
\[
\sqrt{n}\left(\mathbf{z}^{\prime}\hat{\theta}_{K}-f(x)\right)\stackrel{d}{\rightarrow}N(0,AV),
\]

\end_inset

 where 
\begin_inset Formula 
\[
AV=\lim_{n\rightarrow\infty}E\left[\mathbf{z}^{\prime}\left(\frac{\mathbf{Z}_{K}^{\prime}\mathbf{Z}_{K}}{n}\right)^{+}\mathbf{z}\hat{\sigma}^{2}\right].
\]

\end_inset

 Formally, this is exactly the same as if we were dealing with a parametric
 linear model.
 I emphasize, though, that this is only valid if 
\begin_inset Formula $K$
\end_inset

 grows very slowly as 
\begin_inset Formula $n$
\end_inset

 grows.
 If we can't stick to acceptable rates, we should probably use some other
 method of approximating the small sample distribution.
 Bootstrapping is a possibility.
 We'll discuss this in the section on simulation.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "econometrics"
options "bibtotoc,plainnm"

\end_inset


\begin_inset CommandInset index_print
LatexCommand printindex
type "idx"
name "Index"
literal "true"

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
